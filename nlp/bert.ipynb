{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert4keras\n",
    "by 苏剑林\n",
    "\n",
    "https://github.com/bojone/bert4keras\n",
    "\n",
    "https://bert4keras.spaces.ac.cn/\n",
    "\n",
    "## 功能\n",
    "\n",
    "    加载bert/roberta/albert的预训练权重进行finetune；\n",
    "    实现语言模型、seq2seq所需要的attention mask；\n",
    "    丰富的examples；https://github.com/bojone/bert4keras/tree/master/examples\n",
    "    从零预训练代码（支持TPU、多GPU，请看pretraining）；\n",
    "    兼容keras、tf.keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装\n",
    "安装稳定版：\n",
    "\n",
    "pip install bert4keras\n",
    "\n",
    "安装最新版：\n",
    "\n",
    "pip install git+https://www.github.com/bojone/bert4keras.git\n",
    "## 权重\n",
    "\n",
    "目前支持加载的权重：\n",
    "\n",
    "    Google原版bert: https://github.com/google-research/bert\n",
    "    brightmart版roberta: https://github.com/brightmart/roberta_zh\n",
    "    哈工大版roberta: https://github.com/ymcui/Chinese-BERT-wwm\n",
    "    Google原版albert[例子]: https://github.com/google-research/ALBERT\n",
    "    brightmart版albert: https://github.com/brightmart/albert_zh\n",
    "    转换后的albert: https://github.com/bojone/albert_zh\n",
    "    华为的NEZHA: https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA\n",
    "    自研语言模型: https://github.com/ZhuiyiTechnology/pretrained-models\n",
    "    T5模型: https://github.com/google-research/text-to-text-transfer-transformer\n",
    "    GPT2_ML: https://github.com/imcaspar/gpt2-ml\n",
    "    Google原版ELECTRA: https://github.com/google-research/electra\n",
    "    哈工大版ELECTRA: https://github.com/ymcui/Chinese-ELECTRA\n",
    "    CLUE版ELECTRA: https://github.com/CLUEbenchmark/ELECTRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert4keras==0.7.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://www.github.com/bojone/bert4keras.git\n",
    "!pip freeze | grep keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "# 测试代码可用性\n",
    "\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras==2.3.1\r\n",
      "Keras-Applications==1.0.8\r\n",
      "Keras-Preprocessing==1.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.1.0\r\n",
      "tensorflow-datasets==1.3.0\r\n",
      "tensorflow-estimator==2.1.0\r\n",
      "tensorflow-metadata==0.15.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_dir = '/Users/luoyonggui/Documents/nlpdata/chinese_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = f'{bert_dir}/bert_config.json'\n",
    "checkpoint_path = f'{bert_dir}/bert_model.ckpt'\n",
    "dict_path = f'{bert_dir}/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.63250965  0.20302312  0.07936583 ...  0.49122566 -0.20493367\n",
      "    0.25752527]\n",
      "  [-0.7588357   0.09651838  1.0718755  ... -0.61096966  0.0431218\n",
      "    0.03881414]\n",
      "  [ 0.547703   -0.79211694  0.44435284 ...  0.42449164  0.41105747\n",
      "    0.08222783]\n",
      "  [-0.29242492  0.6052705   0.49968675 ...  0.86041355 -0.65331644\n",
      "    0.5369077 ]\n",
      "  [-0.7473448   0.49431536  0.7185178  ...  0.38486043 -0.7409052\n",
      "    0.39056796]\n",
      "  [-0.87413776 -0.21650389  1.3388399  ...  0.5816858  -0.4373227\n",
      "    0.56181794]]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(dict_path) # 建立分词器\n",
    "model = build_transformer_model(config_path, checkpoint_path) # 建立模型，加载权重\n",
    "\n",
    "# 编码测试\n",
    "token_ids, segment_ids = tokenizer.encode('语言模型')\n",
    "print(model.predict([np.array([token_ids]), np.array([segment_ids])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
