{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark==2.4.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/latest/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境配置问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python中安装spark自带pyspark\n",
    "使用pip install pyspark按照的pyspark版本可能和spark环境不兼容，为了保险起见，需要按照spark安装包中自带的pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $SPARK_HOME/python/\n",
    "python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark-submit & spark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the spark-shell\n",
    "在启动shell后，shell中已经自动创建以下对象\n",
    "* SparkContext对象sc, rdd的核心对象\n",
    "* SparkSession对象spark， DataFrame的核心对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.104.17:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Python Spark SQL basic example>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.104.17:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x117b02940>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ ./bin/pyspark --master local[*] --driver-memory 4G --executor-memory 16G\n",
    "# Or, to also add code.py to the search path (in order to later be able to import code), use:\n",
    "$ ./bin/pyspark --master local[4] --py-files code.py\n",
    "\n",
    "nohup pyspark2 --master local[*] --driver-memory 32G --executor-memory 32G &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者配置环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vim /etc/bashrc\n",
    "\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter \n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --ip=* --port=1414 --allow-root --no-browser --NotebookApp.token=aaa --notebook-dir=/home/ian/code/pyspark\"\n",
    "\n",
    "source /etc/bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数解释：\n",
    "* --ip=* ：指定允许访问的ip，这里允许所有ip访问\n",
    "* --allow-root：允许root启动jupyter\n",
    "* --no-browser：不启动浏览器\n",
    "* --NotebookApp.token=aaa：指定token\n",
    "* --notebook-dir=/home/ian/code/pyspark：指定jupyter根目录文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exception: Python in worker has different version 2.7 than that in driver 3.7  \n",
    "这就需要在spark所有的节点上配置环境变量 PYSPARK_PYTHON=pythonpath\n",
    "* Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 7, nn1.leapstack.cn, executor 1): java.io.IOException: Cannot run program \"/root/.pyenv/shims/python\": error=13, Permission denied  \n",
    "这个问题的定位其实很简单，就是文件权限问题，但要注意的是不止是文件权限要改，包含他的文件夹的权限也要改。chmod a+x /root  在这里主要是/root文件夹一般没有+x权限\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用spark-submit提交.py文件\n",
    "\n",
    "在命令行输入spark-submit wordcount.py，会在当前文件夹生成一个result文件夹。part-00000为运行结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@file:wordcount.py\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "master = 'local'\n",
    "spark = SparkSession.builder.appName('test').master(master).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "filename = 'f:/README.md'\n",
    "logData = sc.textFile(filename)\n",
    "wordsRDD = logData.flatMap(lambda x:x.split(\" \")).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "out_filename = 'result'\n",
    "wordsRDD.saveAsTextFile(out_filename)\n",
    "words = wordsRDD.collect()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "Resilient Distributed Datasets, 弹性分布式数据集\n",
    "\n",
    "* 从名字上其实就可以很容易的说明。首先它是一个dataset（数据的集合）;\n",
    "* resilient弹性的，即数据集的大小可以改变；\n",
    "* distributed分布式的，即数据集存储在分布式集群上。\n",
    "* RDD是只读的\n",
    "* RDD是一个逻辑概念，主要由Dependency、Partition(分区)、Partitioner(分区器)组成。\n",
    "    * Partition记录了数据split的逻辑\n",
    "    一个RDD又分成多个Partition（**注意这里的Partition也不是实际物理存储，如rdd1的3个Partition通过转换生成rdd2的4个Partition，那么rdd2的Partitions只会记录一下他的数据是通过rdd1的哪个partition通过什么转换得到的，而不会实际计算**）。**每个Task处理一个Partition**。\n",
    "    * Dependency记录的是transformation操作过程中Partition的演化\n",
    "    * Partitioner是shuffle过程中key重分区时的策略，即计算key决定k-v属于哪个分区。\n",
    "    https://blog.csdn.net/u011564172/article/details/54667057\n",
    "* 在spark中，多个job间是串行执行的，同一个job 的多个stage是串行执行，只有一个stage里的多个task是并行执行的。如果希望task并行执行，那么数据必须是能够并行访问的。每个task的执行的操作一样、计算一样，只是数据不一样。（一般情况下一个task又对应一个cpu core）\n",
    "\n",
    "![](imgs/2850424-bd83ee9d357a2a79.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Dependency依赖\n",
    "### Spark Job逻辑执行图\n",
    "逻辑执行图描述的是job 的数据流\n",
    "* job 会经过哪些transformation\n",
    "* 中间生成哪些RDD\n",
    "* RDD 之间的依赖关系\n",
    "\n",
    "![](imgs/2850424-055e2f8200f9b269.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何建立RDD间依赖\n",
    "RDD 之间的数据依赖问题包含三部分\n",
    "* RDD 本身的依赖关系。要生成的RDD（以后用RDD x 表示）是依赖一个parent RDD，还是多个parent RDDs？\n",
    "    • rddC = rddA.map(func) // rddC依赖于rddA\n",
    "    • rddC = rddA.join(rddB) // rddC同时依赖于rddA与rddB\n",
    "* RDD x 中会有多少个partition ？\n",
    "    • 默认partition数是所有父RDD的partition的最大值\n",
    "    • 部分RDD的partition数可由用户指定，如ShuffledRDD\n",
    "* RDD x 与其parent RDDs 中partition 之间是什么依赖关系？是依赖parent RDD 中一个还是多个partition？\n",
    "    • RDD x 中每个partition 可以依赖于parent RDD 中一个或者多个partition\n",
    "    • 这个依赖可以是完全依赖或者部分依赖\n",
    "\n",
    "![](imgs/2850424-54a0159a0e0b0262.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Partition(分区)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分区的概念\n",
    "分区是RDD内部并行计算的一个计算单元，RDD的数据集在逻辑上被划分为多个分片，每一个分片称为分区，分区的格式决定了并行计算的粒度，\n",
    "\n",
    "__每个分区的数值计算都是在一个TASK任务中进行的，因此任务的个数，也是由RDD(准确来说是作业最后一个RDD)的分区数决定__。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分区的作用\n",
    "* 增加并行计算\n",
    "* 减少网络通信开销\n",
    "Spark把key－value rdd通过key的hashcode进行分区，而且保证相同的key存储在同一个节点上，这样对改rdd进行key聚合时，就不需要shuffle过程，  \n",
    "我们进行mapreduce计算的时候为什么要进行shuffle？，就是说mapreduce里面网络传输主要在shuffle阶段，shuffle的根本原因是相同的key存在不同的节点上，按key进行聚合的时候不得不进行shuffle。shuffle是非常影响网络的，它要把所有的数据混在一起走网络，然后它才能把相同的key走到一起。要进行shuffle是存储决定的。  \n",
    "Spark从这个教训中得到启发，spark会把key进行分区，也就是key的hashcode进行分区，相同的key，hashcode肯定是一样的，所以它进行分区的时候100t的数据分成10分，每部分10个t，它能确保相同的key肯定在一个分区里面，而且它能保证存储的时候相同的key能够存在同一个节点上。比如一个rdd分成了100份，集群有10个节点，所以每个节点存10份，每一分称为每个分区，spark能保证相同的key存在同一个节点上，实际上相同的key存在同一个分区。  \n",
    "key的分布不均决定了有的分区大有的分区小。没法分区保证完全相等，但它会保证在一个接近的范围。所以mapreduce里面做的某些工作里边，spark就不需要shuffle了，spark解决网络传输这块的根本原理就是这个。  \n",
    "进行join的时候是两个表，不可能把两个表都分区好，通常情况下是把用的频繁的大表事先进行分区，小表进行关联它的时候小表进行shuffle过程。  \n",
    "大表不需要shuffle。  \n",
    "需要在工作节点间进行数据混洗的转换极大地受益于分区。这样的转换是  cogroup，groupWith，join，leftOuterJoin，rightOuterJoin，groupByKey，reduceByKey，combineByKey 和lookup。  \n",
    "分区是可配置的，只要RDD是基于键值对的即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分区原则：\n",
    "尽可能是得分区的个数等于集群核心数目\n",
    "\n",
    "无论是本地模式、Standalone模式、YARN模式或Mesos模式，我们都可以通过spark.default.parallelism来配置其默认分区个数，若没有设置该值，则根据不同的集群环境确定该值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以人为指定partition数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(name, minPartitions=None, use_unicode=True)\n",
    "sc.parallelize(c, numSlices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioner(分区器)\n",
    "![](imgs/partitioner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义Partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_partitioner(s):\n",
    "    return s%5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(range(20), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10, 11],\n",
       " [12, 13, 14, 15],\n",
       " [16, 17, 18, 19]]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 5, 10, 15],\n",
       " [1, 6, 11, 16],\n",
       " [2, 7, 12, 17],\n",
       " [3, 8, 13, 18],\n",
       " [4, 9, 14, 19]]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x.map(lambda s: (s,1)).partitionBy(5, my_partitioner).map(lambda s: s[0])\n",
    "x1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 5, 10, 15],\n",
       " [1, 6, 11, 16],\n",
       " [2, 7, 12, 17],\n",
       " [3, 8, 13, 18],\n",
       " [4, 9, 14, 19],\n",
       " []]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = x.map(lambda s: (s,1)).partitionBy(6, my_partitioner).map(lambda s: s[0])\n",
    "x1.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从外部文件创建\n",
    "* 支持本地磁盘文件\n",
    "* 支持整个目录、多文件、通配符\n",
    "* 支持压缩文件\n",
    "* 支持HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext#通过sparkSession获取上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['238\\x01val_238', '86\\x01val_86', '311\\x01val_311']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过读取本地文件的方式生成rdd\n",
    "rdd = sc.textFile(\"data/kv1.txt\")\n",
    "rdd.collect()#查看读取的文件的内容, 文件的每一行会生成一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['238\\x01val_238',\n",
       " '86\\x01val_86',\n",
       " '311\\x01val_311',\n",
       " 'Michael, 29',\n",
       " 'Andy, 30',\n",
       " 'Justin, 19']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过读取本地多个文件的方式生成rdd\n",
    "rdd = sc.textFile(\"data/kv1.txt,data/people.txt\")# 注意逗号后不能有空格\n",
    "rdd.collect()#查看读取的文件的内容, 文件的每一行会生成一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29',\n",
       " 'Andy, 30',\n",
       " 'Justin, 19',\n",
       " '238\\x01val_238',\n",
       " '86\\x01val_86',\n",
       " '311\\x01val_311']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#整个目录、多文件、通配符\n",
    "rdd = sc.textFile(\"data/*.txt\")\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#支持压缩文件\n",
    "rdd = sc.textFile(\"f:/test.gz\")\n",
    "rdd.count()#查看rdd中包含的元素个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集合并行化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原生数据空间（本地非分布式空间）和分布式空间\n",
    "![](imgs/WX20200302-115453.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从父RDD生成子RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD算子\n",
    "https://www.iteblog.com/archives/1395.html#union\n",
    "\n",
    "http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
    "\n",
    "RDD中有两类算子：Transformation和Action。它们的区别：\n",
    "* Transformation类型算子：输入为RDD，输出为RDD；而Action输入为RDD，输出为其它类型；\n",
    "* Transformation类型时延迟执行的，而Action是立即执行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs\\WX20200302-134715@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 延迟执行和立即执行：\n",
    "指的是解释器执行到Transformation时并不会执行其语句，而只是简单地记录一下该操作，直到遇到Action时才会执行前面的Transformation中的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "![](imgs/map.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# sc = spark context, parallelize creates an RDD from the passed object\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.map(lambda x: (x,x**2))\n",
    " \n",
    "# collect copies RDD elements to a list on the driver\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "![](imgs/flatMap.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex\n",
    "![](imgs/mapPartitionsWithIndex.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[(0, 1)], [(1, 5)]]\n"
     ]
    }
   ],
   "source": [
    "# mapPartitionsWithIndex\n",
    "x = sc.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    " \n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())  \n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey\n",
    "![](imgs/groupByKey.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', <pyspark.resultiterable.ResultIterable at 0x11c2d94a8>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x11c2d92e8>)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [5, 4]), ('A', [3, 2, 1])]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(j[0],[i for i in j[1]]) for j in y.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "![](imgs/reduceByKey.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 3), ('A', 12)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "![](imgs/filter.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glom\n",
    "Return an RDD created by coalescing all elements within each partition\n",
    "into a list.\n",
    "![](imgs/glom.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A']\n",
      "[['C'], ['B', 'A']]\n"
     ]
    }
   ],
   "source": [
    "# glom\n",
    "x = sc.parallelize(['C','B','A'], 2)\n",
    "y = x.glom()\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition\n",
    "Return an RDD created by coalescing all elements within each partition\n",
    "into a list.\n",
    "![](imgs/repartition.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n",
      "[[], [1, 2], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.repartition(numPartitions=3)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coalesce\n",
    "Return an RDD created by coalescing all elements within each partition\n",
    "into a list.\n",
    "![](imgs/coalesce.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n",
      "[[1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.coalesce(numPartitions=1)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce\n",
    "![](imgs/reduce.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.reduce(lambda obj, accumulated: obj + accumulated)  # computes a cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect\n",
    "![](imgs/collect.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[395] at parallelize at PythonRDD.scala:195\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.collect()\n",
    "print(x)  # distributed\n",
    "print(y)  # not distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take\n",
    "![](imgs/take.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "[1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.take(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first\n",
    "![](imgs/first.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,3,1,2,3])\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach\n",
    "Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.  \n",
    "Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.\n",
    "![](imgs/foreach.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "None\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "def f(el):\n",
    "    '''side effect: append the current RDD elements to a file'''\n",
    "    with open(\"data/foreachExample.txt\", 'a+') as f1:\n",
    "        print(el,file=f1)\n",
    "\n",
    "y = x.foreach(f) # writes into foreachExample.txt\n",
    " \n",
    "print(x.collect())\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "with open(\"data/foreachExample.txt\", \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum\n",
    "Run a function func on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems.  \n",
    "Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.\n",
    "![](imgs/sum.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce\n",
    "![](imgs/reduce.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([1,2,3])\n",
    "x.reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Persistence持久化\n",
    "http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence\n",
    "\n",
    "One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. \n",
    "\n",
    "When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). \n",
    "\n",
    "This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n",
    "\n",
    "You can mark an RDD to be persisted using the persist() or cache() methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark’s cache is fault-tolerant – if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n",
    "\n",
    "In addition, each persisted RDD can be stored using a different storage level, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. These levels are set by passing a StorageLevel object (Scala, Java, Python) to persist(). \n",
    "\n",
    "\n",
    "* rdd.persist(storageLevel=StorageLevel(False, True, False, False, 1))  \n",
    "返回rdd本身\n",
    "* cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage Level\n",
    "* MEMORY_ONLY\n",
    "* MEMORY_AND_DISK\n",
    "* ...\n",
    "\n",
    "Note: In Python, stored objects will always be serialized with the Pickle library, so it does not matter whether you choose a serialized level. \n",
    "\n",
    "The available storage levels in Python include \n",
    "* MEMORY_ONLY, \n",
    "* MEMORY_ONLY_2, \n",
    "* MEMORY_AND_DISK, \n",
    "* MEMORY_AND_DISK_2, \n",
    "* DISK_ONLY, \n",
    "* and DISK_ONLY_2.\n",
    "\n",
    "Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist. This is done to avoid recomputing the entire input if a node fails during the shuffle. We still recommend users call persist on the resulting RDD if they plan to reuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Storage Level to Choose?\n",
    "\n",
    "Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:\n",
    "\n",
    "    If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.\n",
    "\n",
    "    If not, try using MEMORY_ONLY_SER and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)\n",
    "\n",
    "    Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.\n",
    "\n",
    "    Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Data\n",
    "\n",
    "Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. \n",
    "\n",
    "If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the RDD.unpersist() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/wordcount MapPartitionsRDD[421] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/wordcount MapPartitionsRDD[421] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/wordcount MapPartitionsRDD[421] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD优化方法\n",
    "* 提高数据本地化存储，减少网络传输\n",
    "    * 计算和存储同节点。executor和hdfs的DataNode、HBASE的region server同节点\n",
    "    * executor数目要合适\n",
    "    * 适当增加数据副本数量\n",
    "* 列式存储\n",
    "    * 数据分析中，很多时候只需要读取几列，所以采用列式存储减少磁盘IO，增加效率；\n",
    "    * 列式存储时，每列的数据类型是相同的，这样可以采用一些压缩策略，数据存储可以有很高的压缩比\n",
    "    * 常见的列式存储格式：\n",
    "        * ORC\n",
    "         [Apache *ORC* • High-Performance Columnar Storage for Hadoop](https://orc.apache.org/)\n",
    "        * parquet\n",
    "        * hbase的Column Family也是列式存储\n",
    "* filter后重分区\n",
    "* 数据倾斜\n",
    "任务的完成时间取决于最长的task的完成时间，任务是否能够完成也取决于task是否能够完成\n",
    "产生数据倾斜的原因：某些key的value数量太多\n",
    "数据倾斜解决方案：\n",
    "* 预结算\n",
    "* 调整并行度\n",
    "* 广播小数据集\n",
    "适用于一个大表，一个小表\n",
    "* 对发生倾斜的RDD key增加随机前缀\n",
    "* 如果少量的key发生倾斜，可以先过滤出一个单独的RDD，之后再合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例1\n",
    "自己准备数据，完成如下任务：\n",
    "* 从身份证号中提取年龄\n",
    "* 从身份证号中提取性别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile(\"f:/idNO.txt\")\n",
    "#从身份证号中提取年龄\n",
    "rdd.map(lambda x:2018-int(x[6:10])).collect()\n",
    "[31, 35, 34, 33, 32, 71, 61, 51, 41, 21]\n",
    "#从身份证号中提取性别\n",
    "def gender(x):\n",
    "    if x%2==0: return 'male'\n",
    "    else: return 'female'\n",
    "rdd.map(lambda x:gender(int(x[-2]))).collect()\n",
    "['male', 'female', 'male', 'female', 'male', 'male', 'male', 'female', 'female', 'male']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "rdd = sc.textFile(\"data/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1|The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures\\ufeff.',\n",
       " '2|Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.',\n",
       " '3|Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use!',\n",
       " '4|Below is a high-level overview of Apache Flink and stream processing. For a more technical introduction, we recommend the “Concepts” page in the Flink documentation. ']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('the', 1),\n",
       " ('apache', 1),\n",
       " ('hadoop', 1),\n",
       " ('software', 1),\n",
       " ('library', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('framework', 1),\n",
       " ('that', 1),\n",
       " ('allows', 1),\n",
       " ('for', 1),\n",
       " ('the', 1),\n",
       " ('distributed', 1),\n",
       " ('processing', 1),\n",
       " ('of', 1),\n",
       " ('large', 1),\n",
       " ('data', 1),\n",
       " ('sets', 1),\n",
       " ('across', 1),\n",
       " ('clusters', 1),\n",
       " ('of', 1),\n",
       " ('computers', 1),\n",
       " ('using', 1),\n",
       " ('simple', 1),\n",
       " ('programming', 1),\n",
       " ('models', 1),\n",
       " ('it', 1),\n",
       " ('is', 1),\n",
       " ('designed', 1),\n",
       " ('to', 1),\n",
       " ('scale', 1),\n",
       " ('up', 1),\n",
       " ('from', 1),\n",
       " ('single', 1),\n",
       " ('servers', 1),\n",
       " ('to', 1),\n",
       " ('thousands', 1),\n",
       " ('of', 1),\n",
       " ('machines', 1),\n",
       " ('each', 1),\n",
       " ('offering', 1),\n",
       " ('local', 1),\n",
       " ('computation', 1),\n",
       " ('and', 1),\n",
       " ('storage', 1),\n",
       " ('rather', 1),\n",
       " ('than', 1),\n",
       " ('rely', 1),\n",
       " ('on', 1),\n",
       " ('hardware', 1),\n",
       " ('to', 1),\n",
       " ('deliver', 1),\n",
       " ('high-availability', 1),\n",
       " ('the', 1),\n",
       " ('library', 1),\n",
       " ('itself', 1),\n",
       " ('is', 1),\n",
       " ('designed', 1),\n",
       " ('to', 1),\n",
       " ('detect', 1),\n",
       " ('and', 1),\n",
       " ('handle', 1),\n",
       " ('failures', 1),\n",
       " ('at', 1),\n",
       " ('the', 1),\n",
       " ('application', 1),\n",
       " ('layer', 1),\n",
       " ('so', 1),\n",
       " ('delivering', 1),\n",
       " ('a', 1),\n",
       " ('highly-available', 1),\n",
       " ('service', 1),\n",
       " ('on', 1),\n",
       " ('top', 1),\n",
       " ('of', 1),\n",
       " ('a', 1),\n",
       " ('cluster', 1),\n",
       " ('of', 1),\n",
       " ('computers', 1),\n",
       " ('each', 1),\n",
       " ('of', 1),\n",
       " ('which', 1),\n",
       " ('may', 1),\n",
       " ('be', 1),\n",
       " ('prone', 1),\n",
       " ('to', 1),\n",
       " ('failures\\ufeff', 1),\n",
       " ('2', 1),\n",
       " ('apache', 1),\n",
       " ('spark', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('fast', 1),\n",
       " ('and', 1),\n",
       " ('general-purpose', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('system', 1),\n",
       " ('it', 1),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('apis', 1),\n",
       " ('in', 1),\n",
       " ('java', 1),\n",
       " ('scala', 1),\n",
       " ('python', 1),\n",
       " ('and', 1),\n",
       " ('r', 1),\n",
       " ('and', 1),\n",
       " ('an', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('that', 1),\n",
       " ('supports', 1),\n",
       " ('general', 1),\n",
       " ('execution', 1),\n",
       " ('graphs', 1),\n",
       " ('it', 1),\n",
       " ('also', 1),\n",
       " ('supports', 1),\n",
       " ('a', 1),\n",
       " ('rich', 1),\n",
       " ('set', 1),\n",
       " ('of', 1),\n",
       " ('higher-level', 1),\n",
       " ('tools', 1),\n",
       " ('including', 1),\n",
       " ('spark', 1),\n",
       " ('sql', 1),\n",
       " ('for', 1),\n",
       " ('sql', 1),\n",
       " ('and', 1),\n",
       " ('structured', 1),\n",
       " ('data', 1),\n",
       " ('processing', 1),\n",
       " ('mllib', 1),\n",
       " ('for', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1),\n",
       " ('graphx', 1),\n",
       " ('for', 1),\n",
       " ('graph', 1),\n",
       " ('processing', 1),\n",
       " ('and', 1),\n",
       " ('spark', 1),\n",
       " ('streaming', 1),\n",
       " ('3', 1),\n",
       " ('apache', 1),\n",
       " ('storm', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('free', 1),\n",
       " ('and', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('distributed', 1),\n",
       " ('realtime', 1),\n",
       " ('computation', 1),\n",
       " ('system', 1),\n",
       " ('storm', 1),\n",
       " ('makes', 1),\n",
       " ('it', 1),\n",
       " ('easy', 1),\n",
       " ('to', 1),\n",
       " ('reliably', 1),\n",
       " ('process', 1),\n",
       " ('unbounded', 1),\n",
       " ('streams', 1),\n",
       " ('of', 1),\n",
       " ('data', 1),\n",
       " ('doing', 1),\n",
       " ('for', 1),\n",
       " ('realtime', 1),\n",
       " ('processing', 1),\n",
       " ('what', 1),\n",
       " ('hadoop', 1),\n",
       " ('did', 1),\n",
       " ('for', 1),\n",
       " ('batch', 1),\n",
       " ('processing', 1),\n",
       " ('storm', 1),\n",
       " ('is', 1),\n",
       " ('simple', 1),\n",
       " ('can', 1),\n",
       " ('be', 1),\n",
       " ('used', 1),\n",
       " ('with', 1),\n",
       " ('any', 1),\n",
       " ('programming', 1),\n",
       " ('language', 1),\n",
       " ('and', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('lot', 1),\n",
       " ('of', 1),\n",
       " ('fun', 1),\n",
       " ('to', 1),\n",
       " ('use', 1),\n",
       " ('4', 1),\n",
       " ('below', 1),\n",
       " ('is', 1),\n",
       " ('a', 1),\n",
       " ('high-level', 1),\n",
       " ('overview', 1),\n",
       " ('of', 1),\n",
       " ('apache', 1),\n",
       " ('flink', 1),\n",
       " ('and', 1),\n",
       " ('stream', 1),\n",
       " ('processing', 1),\n",
       " ('for', 1),\n",
       " ('a', 1),\n",
       " ('more', 1),\n",
       " ('technical', 1),\n",
       " ('introduction', 1),\n",
       " ('we', 1),\n",
       " ('recommend', 1),\n",
       " ('the', 1),\n",
       " ('“concepts”', 1),\n",
       " ('page', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('flink', 1),\n",
       " ('documentation', 1)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.flatMap(lambda s: [(i.lower(), 1) for i in re.split(r'[\\||\\s+!\\.,]', s) if i!=''])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 1),\n",
       " ('hadoop', 2),\n",
       " ('library', 2),\n",
       " ('is', 8),\n",
       " ('framework', 1),\n",
       " ('of', 10),\n",
       " ('large', 1),\n",
       " ('sets', 1),\n",
       " ('clusters', 1),\n",
       " ('computers', 1),\n",
       " ('using', 1),\n",
       " ('simple', 1),\n",
       " ('programming', 2),\n",
       " ('models.', 1),\n",
       " ('designed', 2),\n",
       " ('single', 1),\n",
       " ('servers', 1),\n",
       " ('offering', 1),\n",
       " ('computation', 2),\n",
       " ('rather', 1),\n",
       " ('than', 1),\n",
       " ('rely', 1),\n",
       " ('hardware', 1),\n",
       " ('high-availability,', 1),\n",
       " ('handle', 1),\n",
       " ('at', 1),\n",
       " ('delivering', 1),\n",
       " ('service', 1),\n",
       " ('computers,', 1),\n",
       " ('may', 1),\n",
       " ('prone', 1),\n",
       " ('general-purpose', 1),\n",
       " ('provides', 1),\n",
       " ('high-level', 2),\n",
       " ('in', 2),\n",
       " ('java,', 1),\n",
       " ('python', 1),\n",
       " ('r,', 1),\n",
       " ('an', 1),\n",
       " ('optimized', 1),\n",
       " ('engine', 1),\n",
       " ('supports', 2),\n",
       " ('execution', 1),\n",
       " ('set', 1),\n",
       " ('tools', 1),\n",
       " ('processing,', 2),\n",
       " ('machine', 1),\n",
       " ('learning,', 1),\n",
       " ('graph', 1),\n",
       " ('storm', 3),\n",
       " ('free', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('process', 1),\n",
       " ('unbounded', 1),\n",
       " ('streams', 1),\n",
       " ('used', 1),\n",
       " ('use!', 1),\n",
       " ('4', 1),\n",
       " ('below', 1),\n",
       " ('overview', 1),\n",
       " ('more', 1),\n",
       " ('technical', 1),\n",
       " ('we', 1),\n",
       " ('recommend', 1),\n",
       " ('page', 1),\n",
       " ('documentation.', 1),\n",
       " ('the', 6),\n",
       " ('apache', 4),\n",
       " ('software', 1),\n",
       " ('a', 9),\n",
       " ('that', 2),\n",
       " ('allows', 1),\n",
       " ('for', 7),\n",
       " ('distributed', 2),\n",
       " ('processing', 2),\n",
       " ('data', 2),\n",
       " ('across', 1),\n",
       " ('it', 4),\n",
       " ('to', 7),\n",
       " ('scale', 1),\n",
       " ('up', 1),\n",
       " ('from', 1),\n",
       " ('thousands', 1),\n",
       " ('machines,', 1),\n",
       " ('each', 2),\n",
       " ('local', 1),\n",
       " ('and', 10),\n",
       " ('storage.', 1),\n",
       " ('on', 2),\n",
       " ('deliver', 1),\n",
       " ('itself', 1),\n",
       " ('detect', 1),\n",
       " ('failures', 1),\n",
       " ('application', 1),\n",
       " ('layer,', 1),\n",
       " ('so', 1),\n",
       " ('highly-available', 1),\n",
       " ('top', 1),\n",
       " ('cluster', 2),\n",
       " ('which', 1),\n",
       " ('be', 2),\n",
       " ('failures\\ufeff.', 1),\n",
       " ('2', 1),\n",
       " ('spark', 3),\n",
       " ('fast', 1),\n",
       " ('computing', 1),\n",
       " ('system.', 2),\n",
       " ('apis', 1),\n",
       " ('scala,', 1),\n",
       " ('general', 1),\n",
       " ('graphs.', 1),\n",
       " ('also', 1),\n",
       " ('rich', 1),\n",
       " ('higher-level', 1),\n",
       " ('including', 1),\n",
       " ('sql', 2),\n",
       " ('structured', 1),\n",
       " ('mllib', 1),\n",
       " ('graphx', 1),\n",
       " ('streaming.', 1),\n",
       " ('3', 1),\n",
       " ('realtime', 2),\n",
       " ('makes', 1),\n",
       " ('easy', 1),\n",
       " ('reliably', 1),\n",
       " ('data,', 1),\n",
       " ('doing', 1),\n",
       " ('what', 1),\n",
       " ('did', 1),\n",
       " ('batch', 1),\n",
       " ('processing.', 2),\n",
       " ('simple,', 1),\n",
       " ('can', 1),\n",
       " ('with', 1),\n",
       " ('any', 1),\n",
       " ('language,', 1),\n",
       " ('lot', 1),\n",
       " ('fun', 1),\n",
       " ('flink', 2),\n",
       " ('stream', 1),\n",
       " ('introduction,', 1),\n",
       " ('“concepts”', 1)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduceByKey(lambda a, b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK SQL前世今生\n",
    "### HIVE  \n",
    "hive的本质是把sql语句转换成map-reduce执行程序  \n",
    "![](imgs/hive1.png)\n",
    "在Spark SQL出现以前，Hive以HDFS为存储，以Map Reduce为执行引擎，同时提供Metastore, 再配合Hive的语法解析器和查询优化器，是事实上的SQL on Hadoop的标准解决方案。\n",
    "\n",
    "注意，**Hive自身的东西是语法解析器和查询优化器，即下图中的Client部分**。\n",
    "![](imgs/hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHARK\n",
    "早期的Spark主要是提供类似于Map Reduce的计算功能。故一个非常自然的想法是，用Spark替换Map Reduce作为Hive的执行引擎。这就是Spark SQL的前身——Shark\n",
    "\n",
    "Shark的一个直接的问题就是需要对Hive依赖太强，这样对于自身的升级和更新有影响。\n",
    "![](imgs/shark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARK SQL\n",
    "目前Spark SQL已完全与Hive分离，但与其兼容（SQLContext用来执行标准sql，HiveContext用来执行Hive Sql）。同时Metastore不再是必须项，且支持HDFS以外的数据源，如RDBMS，JSON文件或Parquet文件\n",
    "![](imgs/spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么SPARK要推出DF/DS的API?\n",
    "虽然说spark和map-reduce相比，很多情况下都能做得好，但是并不是说，任何人随便写个程序都比别人写的map-reduce效率高，因为很多地方需要做优化。\n",
    "这样就有了一个很自然的想法，提供DF/DS的api，前面的业务逻辑随便你怎么写，都能够在DF/DS内部做优化。不用用户去解决优化问题，而是框架本身来考虑优化问题。\n",
    "下图展示SPARK做优化的过程，可以看出，最终优化的结果还是RDD！！！\n",
    "![](imgs/df优化过程.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark SQL做优化的流程：首先生成逻辑执行计划，然后对逻辑执行计划做优化，生成若干个物理执行计划，通过Cost Optimizer评估出一个最优的物理执行计划，然后生成真正执行的代码。\n",
    "![](imgs/df1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用DataFrame API的一个好处是其是一个语言独立的API，由下图可以看出，无论是使用sql, r, python, java/scala，程序的性能都是差不多的，都远优于直接使用RDD API编写的程序。\n",
    "使用DataFrame API的好处：就是之前如果使用RDD API，那么如果用java语言编写，会生成javaRDD，如果用python语言编写，会生成pythonRDD；而如果使用DataFrame API，则生成的底层RDD是一样的！\n",
    "\n",
    "![](imgs/df2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame vs RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/df3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RDD是一个java对象列表，并不知道对象内部的结构  \n",
    "    行式存储，\n",
    "    RDD<Person>虽以Person为类型，但Spark不了解Person的内部结构，如Person里面有哪些属性，属性的名称和类型是什么\n",
    "    RDD强调不可变性，无状态，方便函数式编程。\n",
    "    但在数据复用性上弱，如我们只需要通过map算子的lambda表达式修改person的一个属性，但是还是会生成新的Person对象，那么在一系列的rdd转换过程中就会生成大量的中间对象这样就造成了GC压力较大\n",
    "    * 优点:\n",
    "    1）编译时类型安全，编译时就能检查出类型错误。\n",
    "    2）面向对象的编程风格，直接通过类名点的方式来操作数据。\n",
    "    * 缺点:\n",
    "    1）序列化和反序列化的性能开销，无论是集群间的通信, 还是IO操作都需要对对象的结构和数据进行序列化和反序列化。\n",
    "    2）GC的性能开销，频繁的创建和销毁对象, 势必会增加GC。\n",
    "    rdd是只读的。好处是这样就不需要考虑并发修改的问题，吞吐量就可以做大！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrame\n",
    "    结构化存储，每一列都有类型，\n",
    "    DataFrame等价于Dataset<Row>\n",
    "    DataFrame包含数据结构信息，即schema\n",
    "    DataFrame提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集包含哪些列，每列的名称和类型是什么\n",
    "    每一行是有结构的，如Name是String型，Age是Int型，Height是Double型\n",
    "    \n",
    "DataFrame核心特征：  \n",
    "* Schema : 包含了以ROW为单位的每行数据的列的信息； Spark通过Schema就能够读懂数据, 因此在通信和IO时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。\n",
    "* off-heap（堆外内存） : Spark能够以二进制的形式序列化数据(不包括结构)到off-heap中, 当要操作数据时, 就直接操作off-heap内存。\n",
    "* Tungsten：新的执行引擎；\n",
    "* Catalyst：新的语法解析框架；\n",
    "* 优点：  \n",
    "    off-heap就像地盘, schema就像地图, Spark有地图又有自己地盘了, 就可以自己说了算了, 不再受JVM的限制, 也就不再收GC的困扰了，通过schema和off-heap, DataFrame解决了RDD的缺点。对比RDD提升计算效率、减少数据读取、底层计算优化；\n",
    "    DataFrame 提供了比RDD更丰富的算子\n",
    "* 缺点:  \n",
    "    DataFrame解决了RDD的缺点, 但是却丢了RDD的优点。DataFrame不是类型安全的, API也不是面向对象风格的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API不是面向对象的\n",
    "idAgeDF.filter(idAgeDF.col(\"age\") > 25)\n",
    "# 不会报错, DataFrame不是编译时类型安全的\n",
    "idAgeDF.filter(idAgeDF.col(\"age\") > \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Point: SparkSession\n",
    "http://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "\n",
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating SparkSession 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untyped Dataset Operations (aka DataFrame Operations)\n",
    "\n",
    "DataFrames provide a domain-specific language for structured data manipulation in Scala, Java, Python and R.\n",
    "\n",
    "As mentioned above, in Spark 2.0, __DataFrames are just Dataset of Rows in Scala and Java API__. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets.\n",
    "\n",
    "Here we include some basic examples of structured data processing using Datasets:\n",
    "\n",
    "In Python, it’s possible to access a DataFrame’s columns either by attribute (df.age) or by indexing (df['age']). While the former is convenient for interactive data exploration, users are highly encouraged to use the latter form, which is future proof and won’t break with column names that are also attributes on the DataFrame class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. \n",
    "\n",
    "Registering a DataFrame as a temporary view allows you to run SQL queries over its data. \n",
    "\n",
    "\n",
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column相当于pandas中的Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select a column out of a DataFrame\n",
    "\n",
    "df.colName\n",
    "df[\"colName\"]\n",
    "\n",
    "# 2. Create from an expression\n",
    "df.colName + 1\n",
    "1 / df.colName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### desc() | asc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alias(str) | name(str)\n",
    "name() is an alias for alias()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getField(name)[source]\n",
    "\n",
    "    An expression that gets a field by name in a StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     r|\n",
      "+------+\n",
      "|[1, b]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| bb|\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.r.getField(\"b\").alias('bb')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getItem(key)[source]\n",
    "\n",
    "    An expression that gets an item at position ordinal out of a list, or gets an item by key out of a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|l[0]|d[key]|\n",
      "+----+------+\n",
      "|   1| value|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
    "\n",
    "df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| dd|d[key]|\n",
      "+---+------+\n",
      "|  1| value|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.l[0].alias('dd'), df.d[\"key\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isNotNull() |  isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "|    80| Tom|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
    "\n",
    "df.filter(df.height.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "|    80| Tom|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.height.isNotNull()].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|height| name|\n",
      "+------+-----+\n",
      "|  null|Alice|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.height.isNull()].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isin(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.name.isin(\"Bob\", \"Mike\")].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "|    80| Tom|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.height.isin(80)].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|name|\n",
      "+------+----+\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 注意 这里会把空字符串也过滤掉\n",
    "df[df.height.isin(80)==False].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### startswith(other) | endswith(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.endswith('ice')).collect()\n",
    "[Row(age=2, name='Alice')]\n",
    "\n",
    "df.filter(df.name.endswith('ice$')).collect()\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### like(other) |  rlike(other)\n",
    "\n",
    "    SQL like expression. Returns a boolean Column based on a SQL LIKE match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=None, name='Alice')]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.name.like('Al%')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=None, name='Alice')]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.name.rlike('ice$')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=None, name='Alice')]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.name.rlike(r'ice$')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### substr(startPos, length)[source]\n",
    "\n",
    "    Return a Column which is a substring of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col='Tom'), Row(col='Ali')]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.name.substr(1, 3).alias(\"col\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序生成DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表头\n",
    "fields = [\n",
    "    StructField('name', StringType(), nullable=True),\n",
    "    StructField('age', IntegerType(), nullable=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表中记录\n",
    "lines = sc.textFile('data/people.txt').map(lambda s: [i.strip() for i in s.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(lines, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(\n",
    "    data,\n",
    "    schema=None,\n",
    "    samplingRatio=None,\n",
    "    verifySchema=True,\n",
    ")\n",
    "\n",
    "data: RDD, list, or pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|words|num|\n",
      "+-----+---+\n",
      "|    a|  1|\n",
      "|    b|  2|\n",
      "|    c|  3|\n",
      "|    d|  4|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a',1),\n",
    "                            ('b',2),\n",
    "                            ('c',3),\n",
    "                            ('d',4)], [\"words\", \"num\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = pd.DataFrame({'a':[1, 2, 4], 'b': list('abc')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  4|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(dfp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "|  4|  c|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 或者稍微复杂的方式：先把dfp转换为list，在转换为spark df\n",
    "spark.createDataFrame(dfp.values.tolist(), dfp.columns.tolist()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. \n",
    "\n",
    "This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.\n",
    "\n",
    "从数据源读取数据，spark提供了集中方式：\n",
    "* 通用方式 spark.read.load(path=None, format=None, schema=None, **options)\n",
    "* 简便方式 \n",
    "spark.read.{format}(*paths)\n",
    "\n",
    "向数据源写入数据，spark也是提供了两种方式：\n",
    "* 通用方式  \n",
    "    df.write.save(\n",
    "        path=None,\n",
    "        format=None,\n",
    "        mode=None,\n",
    "        partitionBy=None,\n",
    "        **options,\n",
    "    )\n",
    "* 简写形式\n",
    "df.write.{format}(path, mode=None, partitionBy=None, compression=None)\n",
    "    * path: 注意这里的path是文件夹的名字，在这个文件夹里会生成相应格式的文件。  \n",
    "    * param mode: specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
    "        * ``overwrite``: Overwrite existing data.\n",
    "        * ``ignore``: Silently ignore this operation if data already exists.\n",
    "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parquet\n",
    "In the simplest form, the default data source (parquet unless otherwise configured by spark.sql.sources.default) will be used for all operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string, favorite_numbers: array<int>]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"data/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分批量读取\n",
    "有时候会碰到一种情况，数据量太大，一次读取后会造成OOM，需要分批读取文件夹中的parquet文件\n",
    "\n",
    "方法：采用通配符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('file:///opt/dataProcessResult/hz/register_merge/part-000*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save\n",
    ":param mode: specifies the behavior of the save operation when data already exists.\n",
    "\n",
    "    * ``append``: Append contents of this :class:`DataFrame` to existing data.  注: 使用append方式, spark会保证文件夹中命名不重复\n",
    "    * ``overwrite``: Overwrite existing data.\n",
    "    * ``ignore``: Silently ignore this operation if data already exists.\n",
    "    * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"favorite_color\").write.save(\"data/namesAndFavColors.parquet\", mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", \"favorite_color\").write.csv(\"data/namesAndFavColors\", mode='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"data/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/people.json\", format=\"json\")\n",
    "# df.select(\"name\", \"age\").write.save(\"data/namesAndAges.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"data/people.csv\",\n",
    "                     format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/people.csv\", sep=\";\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark sql连接mysql数据库\n",
    "可以直接把mysql中的表读成DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备工作\n",
    "下载mysql JDBC驱动程序jar包；\n",
    "放到$SPARK_HOME/jars/下即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jdbcDF = spark.read\\\n",
    "#         .format('jdbc')\\\n",
    "#         .option('driver', 'com.mysql.jdbc.Driver')\\\n",
    "#         .option('url', 'jdbc:mysql://localhost:3306/dics')\\\n",
    "#         .option('dbtable', 'disease_dic')\\\n",
    "#         .option('user', 'root')\\\n",
    "#         .option('password', '123456').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`data/users.parquet`\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rdd和DataFrame互相转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame转换rdd\n",
    "DataFrame的每一行是一个Row对象，作为rdd的一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alyssa', favorite_color=None, favorite_numbers=[3, 9, 15, 20]),\n",
       " Row(name='Ben', favorite_color='red', favorite_numbers=[])]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alyssa', 'Ben']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.map(lambda s: s.name).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rdd转DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alyssa', favorite_color=None, favorite_numbers=[3, 9, 15, 20]),\n",
       " Row(name='Ben', favorite_color='red', favorite_numbers=[])]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Michael', ' 29'],\n",
       " ['Andy', ' 30'],\n",
       " ['Justin', ' 19'],\n",
       " ['Michael', ' 29'],\n",
       " ['Andy1', ' 30'],\n",
       " ['Justin', ' 19']]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"data/people.txt\").map(lambda s: s.split(','))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：由于没有指定schema，所以都是字符串类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "|Michael| 29|\n",
      "|  Andy1| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 29|Michael|\n",
      "| 30|  Andy1|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda s: Row(name=s[0], age=int(s[1].strip())))\n",
    "rdd1.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. \n",
    "\n",
    "Registering a DataFrame as a temporary view allows you to run SQL queries over its data. \n",
    "\n",
    "\n",
    "The sql function on a SparkSession enables applications to run SQL queries programmatically and returns the result as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给DataFrame新增一列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新增一个常数列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pyspark.sql.functions.lit(col)\n",
    "\n",
    "    Creates a Column of literal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|height|spark_user|\n",
      "+------+----------+\n",
      "|     5|      true|\n",
      "|     5|      true|\n",
      "|     5|      true|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lit(5).alias('height')).withColumn('spark_user', lit(True)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+------+\n",
      "| age|   name|                   a|height|\n",
      "+----+-------+--------------------+------+\n",
      "|null|Michael|[M, i, c, h, a, e...|     5|\n",
      "|  30|   Andy|      [A, n, d, y, ]|     5|\n",
      "|  19| Justin|[J, u, s, t, i, n, ]|     5|\n",
      "+----+-------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('height', lit(5))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+------+\n",
      "| age|   name|                   a|height|\n",
      "+----+-------+--------------------+------+\n",
      "|null|Michael|[M, i, c, h, a, e...| hello|\n",
      "|  30|   Andy|      [A, n, d, y, ]| hello|\n",
      "|  19| Justin|[J, u, s, t, i, n, ]| hello|\n",
      "+----+-------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('height', lit('hello'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+------+\n",
      "| age|   name|                   a|height|\n",
      "+----+-------+--------------------+------+\n",
      "|null|Michael|[M, i, c, h, a, e...|  true|\n",
      "|  30|   Andy|      [A, n, d, y, ]|  true|\n",
      "|  19| Justin|[J, u, s, t, i, n, ]|  true|\n",
      "+----+-------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('height', lit(True))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于已有列新增一列\n",
    "如果列名不存在，则新增一列；\n",
    "\n",
    "如果列名存在，则覆盖；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('a', split(df.name, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+\n",
      "| age|   name|                   a|\n",
      "+----+-------+--------------------+\n",
      "|null|Michael|[M, i, c, h, a, e...|\n",
      "|  30|   Andy|      [A, n, d, y, ]|\n",
      "|  19| Justin|[J, u, s, t, i, n, ]|\n",
      "+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ian| 33| Engineer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|      job|\n",
      "+-----+---------+\n",
      "|Jorge|Developer|\n",
      "|  Bob|Developer|\n",
      "|  Ian| Engineer|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Jorge|\n",
      "|  Bob|\n",
      "|  Ian|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('age', 'job').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Error: col should be a string or a Column\n",
    "df.drop(['age', 'job']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除重复行\n",
    "dropDuplicates(subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed(existing, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|age1|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age', 'age1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 强制类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string')]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "df = df.withColumn('ages', col('age').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'bigint'), ('name', 'string'), ('ages', 'string')]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.dropna(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'age'>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    _c0|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "|  Andy1|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_c0').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+\n",
      "|    _c0|    _c0|_c1|\n",
      "+-------+-------+---+\n",
      "|Michael|Michael| 29|\n",
      "|   Andy|   Andy| 30|\n",
      "| Justin| Justin| 19|\n",
      "|Michael|Michael| 29|\n",
      "|  Andy1|  Andy1| 30|\n",
      "| Justin| Justin| 19|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_c0', '_c0', '_c1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+\n",
      "|    _c0|    _c0|_c1|\n",
      "+-------+-------+---+\n",
      "|Michael|Michael| 29|\n",
      "|   Andy|   Andy| 30|\n",
      "| Justin| Justin| 19|\n",
      "|Michael|Michael| 29|\n",
      "|  Andy1|  Andy1| 30|\n",
      "| Justin| Justin| 19|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['_c0', '_c0', '_c1']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    _c0|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|Michael|\n",
      "|  Andy1|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['_c0']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(_c0 + s)|\n",
      "+---------+\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['_c0']+'s').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+\n",
      "|    _c0|    _c0|_c1|\n",
      "+-------+-------+---+\n",
      "|Michael|Michael| 29|\n",
      "|   Andy|   Andy| 30|\n",
      "| Justin| Justin| 19|\n",
      "|Michael|Michael| 29|\n",
      "|  Andy1|  Andy1| 30|\n",
      "| Justin| Justin| 19|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['_c0'], df['_c0'], df['_c1']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show(n=20, truncate=True, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Jorge', age=30, job='Developer')"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Jorge', age=30, job='Developer'),\n",
       " Row(name='Bob', age=32, job='Developer')]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Jorge', age=30, job='Developer')"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Jorge', age=30, job='Developer'),\n",
       " Row(name='Bob', age=32, job='Developer')]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.sort(*cols, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ian| 33| Engineer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|  Ian| 33| Engineer|\n",
      "|  Bob| 32|Developer|\n",
      "|Jorge| 30|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df['age'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df['age'].desc(), df['name'].asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/people.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "|Michael| 29|\n",
      "|  Andy1| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c1|\n",
      "+---+\n",
      "| 29|\n",
      "| 19|\n",
      "| 30|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_c1').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|_c1|    _c0|\n",
      "+---+-------+\n",
      "| 30|   Andy|\n",
      "| 29|Michael|\n",
      "| 30|  Andy1|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select('_c1','_c0')\n",
    "df1.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.limit(num)\n",
    "取前n行，返回DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.sample(withReplacement=None, fraction=None, seed=None)\n",
    "This is not guaranteed to provide exactly the fraction specified of the total count of the given :class:`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 2.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(fraction=0.4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.groupBy(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.agg(*exprs)\n",
    "Aggregate on the entire :class:`DataFrame` without groups\n",
    "(shorthand for ``df.groupBy.agg()``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|words|num|\n",
      "+-----+---+\n",
      "|    a|  1|\n",
      "|    b|  2|\n",
      "|    c|  3|\n",
      "|    d|  4|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|max(num)|avg(num)|\n",
      "+--------+--------+\n",
      "|       4|     2.5|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as pyspark_max, avg\n",
    "df.agg(pyspark_max(\"num\"), avg(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|max(num)|avg(num)|\n",
      "+--------+--------+\n",
      "|       4|     2.5|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().agg(pyspark_max(\"num\"), avg(\"num\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(num)|\n",
      "+--------+\n",
      "|     2.5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"num\": \"max\", \"num\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dfg.apply(udf)\n",
    "Docstring:\n",
    "Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\n",
    "as a `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "\n",
    "    (\"id\", \"v\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyArrow\n",
    "# !pip freeze | grep pyarrow  # pyarrow==0.16.0\n",
    "\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \n",
    "def normalize(pdf):\n",
    "\n",
    "    v = pdf.v\n",
    "\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, v: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg = df.groupby(\"id\")\n",
    "dfg.apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交集df.intersect(other)\n",
    "Return a new :class:`DataFrame` containing rows only in\n",
    "both this frame and another frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.intersect(df2).sort(\"C1\", \"C2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df1.intersectAll(other)\n",
    "Return a new :class:`DataFrame` containing rows in both this dataframe and other\n",
    "dataframe while preserving duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.intersectAll(df2).sort(\"C1\", \"C2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 差集df1.exceptAll(other)\n",
    "Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
    "not in another :class:`DataFrame` while preserving duplicates.\n",
    "\n",
    "This is equivalent to `EXCEPT ALL` in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.exceptAll(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df1.crossJoin(other)\n",
    "Returns the cartesian product(笛卡尔乘积) with another :class:`DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ian| 33| Engineer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/people.csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.select('name', 'age')\n",
    "df2 = df.select('name', 'job').limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---------+\n",
      "| name|age| name|      job|\n",
      "+-----+---+-----+---------+\n",
      "|Jorge| 30|Jorge|Developer|\n",
      "|  Bob| 32|Jorge|Developer|\n",
      "|  Ian| 33|Jorge|Developer|\n",
      "|Jorge| 30|  Bob|Developer|\n",
      "|  Bob| 32|  Bob|Developer|\n",
      "|  Ian| 33|  Bob|Developer|\n",
      "|Jorge| 30|  Ian| Engineer|\n",
      "|  Bob| 32|  Ian| Engineer|\n",
      "|  Ian| 33|  Ian| Engineer|\n",
      "+-----+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crossJoin(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df1.join(other, on=None, how=None)\n",
    "* param on: \n",
    "    * a string for the join column name, \n",
    "    * a list of column names,\n",
    "    * a join expression (Column), \n",
    "    * or a list of Columns.\n",
    "    \n",
    "If `on` is a string or a list of strings indicating the name of the join column(s),\n",
    "    the column(s) must exist on both sides, and this performs an equi-join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, 'name', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---------+\n",
      "| name|age| name|      job|\n",
      "+-----+---+-----+---------+\n",
      "|Jorge| 30|Jorge|Developer|\n",
      "|  Bob| 32|  Bob|Developer|\n",
      "+-----+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, df1.name==df2.name, how='inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.union(other)\n",
    "Return a new :class:`DataFrame` containing union of rows in this and another frame.\n",
    "\n",
    "注意：两个DataFrame要有相同的列数，否则报错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "| name|      age|\n",
      "+-----+---------+\n",
      "|Jorge|       30|\n",
      "|  Bob|       32|\n",
      "|  Ian|       33|\n",
      "|Jorge|Developer|\n",
      "|  Bob|Developer|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.union(df1).show()\n",
    "#Py4JJavaError: An error occurred while calling o3619.union.\n",
    "#: org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the same number of columns, but the first table has 3 columns and the second table has 2 columns;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内置函数pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split\n",
    "Splits str around pattern (pattern is a regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|     s|\n",
      "+------+\n",
      "|ab12cd|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|split(s, [0-9]+)|\n",
      "+----------------+\n",
      "|        [ab, cd]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(df.s, '[0-9]+')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|       s|\n",
      "+--------+\n",
      "|[ab, cd]|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(df.s, '[0-9]+').alias('s')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|     s|  x_list|\n",
      "+------+--------+\n",
      "|ab12cd|[ab, cd]|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('x_list', split(df.s, '[0-9]+')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upper | lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|upper(s)|\n",
      "+--------+\n",
      "|  AB12CD|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(upper(df['s'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|     s|s_upper|\n",
      "+------+-------+\n",
      "|ab12cd| AB12CD|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('s_upper', upper(df['s'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[height: bigint, name: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel(True, True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[height: bigint, name: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df被赋值给其它变量，该怎么对之前的做unpersist? 可以利用其返回值\n",
    "df = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df.persist(StorageLevel(True, True, False, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'unpersist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-19b0c7938d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'unpersist'"
     ]
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[height: bigint, name: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StorageLevel.MEMORY_AND_DISK_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[height: bigint, name: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()  # the default storage level (C{MEMORY_AND_DISK})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 综合实例：疾病对码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ill_align(city):\n",
    "    \"\"\"\n",
    "    分别给各个城市疾病对码\n",
    "    \"\"\"\n",
    "    print(city)\n",
    "    df = spark.read.parquet(f'file://{os.path.join(data_dir, city, \"register\")}')\n",
    "    cols = 'out_hosp_diagnosiscode out_hosp_diagnosisname'\n",
    "    # 提取名称中第一个疾病\n",
    "    print('提取名称中第一个疾病')                        \n",
    "    from pyspark.sql.functions import split\n",
    "    split_pattern = r'[，,\\^\\s+，、;\\.]'\n",
    "    df = df.withColumn('out_hosp_diagnosisname_first', split(df.out_hosp_diagnosisname, split_pattern).getItem(0))                        \n",
    "    print(f'df.count: {df.count()}')\n",
    "    # 以疾病名称为主对               \n",
    "    print('以疾病名称为主对')                    \n",
    "    df1 = df.join(df_base, df['out_hosp_diagnosisname_first']==df_base['疾病名称'])                      \n",
    "    print(f'df1.count: {df1.count()}')                   \n",
    "    from pyspark.sql.functions import lit\n",
    "    df1 = df1.withColumn('sim', lit(999))     \n",
    "    df1.write.parquet(f\"file://{os.path.join(data_dir, city, 'disease_align_name')}\")                       \n",
    "    # 剩下的以代码为主对        \n",
    "    print('剩下的以代码为主对')                           \n",
    "    dft = df[df['out_hosp_diagnosisname_first'].isin(list_df_base_ill)==False]                        \n",
    "    # 标准化疾病代码    \n",
    "    print('标准化疾病代码')                               \n",
    "    from pyspark.sql.functions import upper\n",
    "    dft = dft.withColumn('out_hosp_diagnosiscode_upper', upper(df['out_hosp_diagnosiscode']))                        \n",
    "    df2 = dft.join(df_base, dft['out_hosp_diagnosiscode_upper']==df_base['主要编码'])                     \n",
    "    print(f'df2.count: {df2.count()}')                                         \n",
    "    df2 = df2.withColumn('sim', lit(999))\n",
    "    df2 = df2.drop('out_hosp_diagnosiscode_upper')                        \n",
    "    df2.write.parquet(f\"file://{os.path.join(data_dir, city, 'disease_align_code')}\")\n",
    "    ll_df2_code = df2.select('out_hosp_diagnosiscode').rdd.map(lambda s: s.out_hosp_diagnosiscode).collect()\n",
    "\n",
    "    ll_df2_code = list(set(ll_df2_code))                  \n",
    "    # 剩下的对疾病名称进行TFIDF匹配\n",
    "    print('剩下的对疾病名称进行TFIDF匹配')           \n",
    "    dft = dft[dft['out_hosp_diagnosiscode'].isin(ll_df2_code)==False]                  \n",
    "    from myutils import NLPDataPrepareWrapper as npw\n",
    "\n",
    "    # 先取出所有未匹配的name\n",
    "    print('先取出所有未匹配的name')           \n",
    "    ll_dft_name = dft.select(dft['out_hosp_diagnosisname']).distinct().rdd.map(lambda s: s['out_hosp_diagnosisname']).collect()                  \n",
    "    df_target = pd.DataFrame({'name': ll_dft_name})                  \n",
    "    num = df_target.shape[0]//5000+1\n",
    "    dfr = pd.DataFrame()\n",
    "    for i in range(num):\n",
    "        c = i*5000\n",
    "        dftt = npw.match(df_base_pd, '疾病名称', df_target.iloc[c:(c+5000)], 'name', 'aa')\n",
    "        if dfr.empty:\n",
    "            dfr = dftt\n",
    "        else:\n",
    "            dfr = pd.concat([dfr, dftt])  \n",
    "    dfr = dfr[['name', '疾病名称', '主要编码','sim']]   \n",
    "    dfr.loc[:, 'name'] = dfr.name.map(str)\n",
    "\n",
    "    dfr.loc[:, '疾病名称'] = dfr.疾病名称.map(str)\n",
    "\n",
    "    dfr.loc[:, '主要编码'] = dfr.主要编码.map(str)\n",
    "\n",
    "    dfr_spark = spark.createDataFrame(dfr)\n",
    "\n",
    "    del dfr                  \n",
    "    df3 = dft.join(dfr_spark, dft['out_hosp_diagnosisname']==dfr_spark['name'])                  \n",
    "    df3 = df3.drop('name')\n",
    "\n",
    "    df3 = df3.drop('out_hosp_diagnosiscode_upper')                    \n",
    "    print(f'df3.count: {df3.count()}')                   \n",
    "    print('df3写入')                  \n",
    "    df3.write.parquet(f\"file://{os.path.join(data_dir, city, 'disease_align_tfidf')}\")                  \n",
    "    print('complete!')                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparkml API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成SparkSession实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "     .appName(\"Word Count\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.feature module\n",
    "封装了特征工程的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rtokenizer = RegexTokenizer(inputCol='text', outputCol='words')\n",
    "rtokenizer.setPattern('[.,\\s]+')\n",
    "df3 = rtokenizer.transform(df1)\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|doc_id1|doc_id2|soure|                name|            doc_name|country|typename|                text|               words|              rwords|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|1494604| 549745|    7|Curriculum Vitae-...|curriculum.vitae....|     en|Business|My professional r...|[my, professional...|[professional, re...|\n",
      "|1494602| 604435|    7|Five Questions (S...|com.kevinhecker.t...|     en|  Puzzle|Five Questions is...|[five, questions,...|[five, questions,...|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"rwords\", stopWords=StopWordsRemover.loadDefaultStopWords('english'))\n",
    "df4 = remover.transform(df3)\n",
    "df4.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.HashingTF\n",
    "使用哈希技巧将一个术语序列映射到它们的术语频率。目前我们使用Austin Appleby的MurmurHash3算法(MurmurHash3 x86 32)来计算术语对象的哈希码值。由于使用简单的模将哈希函数转换为列索引，所以建议使用2的幂作为numFeatures参数;否则，特性将不会均匀地映射到列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],),\n",
    "                            ([\"d\", \"b\", \"c\"],),\n",
    "                            ([\"d\", \"b\", \"c\",\"d\"],),\n",
    "                            ([\"a\", \"b\", \"c\",\"d\"],)], [\"words\"])\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"tf\")\n",
    "\n",
    "df = hashingTF.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果SparseVector(稀疏向量，是一个三元组)表示，\n",
    "* 第一个元素是numFeatures，\n",
    "* 第二个元素是一个数组，指是出现了哪几个索引\n",
    "* 第三个元素是一个数组，和第二个元素的数据等长，指对于的索引位置出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------+\n",
      "|words       |tf                              |\n",
      "+------------+--------------------------------+\n",
      "|[a, b, c]   |(10,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|[d, b, c]   |(10,[1,2,4],[1.0,1.0,1.0])      |\n",
      "|[d, b, c, d]|(10,[1,2,4],[1.0,1.0,2.0])      |\n",
      "|[a, b, c, d]|(10,[0,1,2,4],[1.0,1.0,1.0,1.0])|\n",
      "+------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.IDF\n",
    "Compute the Inverse Document Frequency (IDF) given a collection of documents.\n",
    "\n",
    "https://blog.csdn.net/q1w2e3r4470/article/details/50534336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(minDocFreq=1, inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "model = idf.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------+---------------------------------------------------------------+\n",
      "|words       |tf                              |tfidf                                                          |\n",
      "+------------+--------------------------------+---------------------------------------------------------------+\n",
      "|[a, b, c]   |(10,[0,1,2],[1.0,1.0,1.0])      |(10,[0,1,2],[0.5108256237659907,0.0,0.0])                      |\n",
      "|[d, b, c]   |(10,[1,2,4],[1.0,1.0,1.0])      |(10,[1,2,4],[0.0,0.0,0.22314355131420976])                     |\n",
      "|[d, b, c, d]|(10,[1,2,4],[1.0,1.0,2.0])      |(10,[1,2,4],[0.0,0.0,0.44628710262841953])                     |\n",
      "|[a, b, c, d]|(10,[0,1,2,4],[1.0,1.0,1.0,1.0])|(10,[0,1,2,4],[0.5108256237659907,0.0,0.0,0.22314355131420976])|\n",
      "+------------+--------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # 构建IDF模型，训练集和测试集都用它\n",
    "df = model.transform(df)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意观察，在3和4样本中的d，可以看到3样本中的d赋值是4样本中d的两倍，显然考虑了词频，所以结果为tfidf！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Normalizer\n",
    "L2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|tfidf                                                          |tfidf_norm                                                     |\n",
      "+---------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|(10,[0,1,2],[0.5108256237659907,0.0,0.0])                      |(10,[0,1,2],[1.0,0.0,0.0])                                     |\n",
      "|(10,[1,2,4],[0.0,0.0,0.22314355131420976])                     |(10,[1,2,4],[0.0,0.0,1.0])                                     |\n",
      "|(10,[1,2,4],[0.0,0.0,0.44628710262841953])                     |(10,[1,2,4],[0.0,0.0,1.0])                                     |\n",
      "|(10,[0,1,2,4],[0.5108256237659907,0.0,0.0,0.22314355131420976])|(10,[0,1,2,4],[0.9163829172606391,0.0,0.0,0.40030282156497543])|\n",
      "+---------------------------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(p=2.0, inputCol=\"tfidf\", outputCol=\"tfidf_norm\")\n",
    "df = normalizer.transform(df)\n",
    "df.select(\"tfidf tfidf_norm\".split()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.linalg module\n",
    "MLlib utilities for linear algebra. For dense vectors, MLlib uses the NumPy array type, so you can simply pass NumPy arrays around. For sparse vectors, users can construct a SparseVector object from MLlib or pass SciPy scipy.sparse column vectors if SciPy is available in their environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- tfidf: vector (nullable = true)\n",
      " |-- tfidf_norm: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.select('tfidf_norm').rdd.map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(10, {0: 1.0, 1: 0.0, 2: 0.0}),\n",
       " SparseVector(10, {1: 0.0, 2: 0.0, 4: 1.0}),\n",
       " SparseVector(10, {1: 0.0, 2: 0.0, 4: 1.0}),\n",
       " SparseVector(10, {0: 0.9164, 1: 0.0, 2: 0.0, 4: 0.4003})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [SparseVector(10, {0: 1.0, 1: 0.0, 2: 0.0}), SparseVector(10, {0: 2.0, 1: 0.0, 2: 0.0})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0], [0.0, 0.0], [0.0, 0.0], [0.9163829172606391, 1.8327658345212783]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x: [x.dot(i) for i in base])\n",
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x: x.index(max(x)))\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Vectors.sparse(4, [(0, 1), (3, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 4.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# examples & api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparkmlhw01  词频统计\n",
    "采用Datasets操作，实现WordCount实例，并且按照count值降序显示前50行数据，其中word转换成小写，去除标点符号，去除停用词，考查点：  \n",
    "1）  spark读取文件  \n",
    "2）  dataset转换操作、聚合操作  \n",
    "重点在数据清洗，转成小写，去除标点、停用词等，这里需要自己自定义停用词集合 和 标点符号集合  \n",
    "3）  dataset排序及显示  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"file:///Users/luoyonggui/PycharmProjects/mayiexamples/pyspark/data/wordcount\", sep='|', header=False)\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|                 _c1|\n",
      "+---+--------------------+\n",
      "|  1|The Apache Hadoop...|\n",
      "|  2|Apache Spark is a...|\n",
      "|  3|Apache Storm is a...|\n",
      "|  4|Below is a high-l...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed('_c0','index').withColumnRenamed('_c1','content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('index', 'string'), ('content', 'string')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.select('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             content|\n",
      "+--------------------+\n",
      "|The Apache Hadoop...|\n",
      "|Apache Spark is a...|\n",
      "|Apache Storm is a...|\n",
      "|Below is a high-l...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(content='The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures\\ufeff.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = df2.rdd.flatMap(lambda x:x.content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Apache', 'Hadoop', 'software', 'library']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apache', 1),\n",
       " ('hadoop', 1),\n",
       " ('software', 1),\n",
       " ('library', 1),\n",
       " ('framework', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x:x.lower())\\\n",
    "    .filter(lambda x: x not in list(\",.:!';\"))\\\n",
    "    .filter(lambda x: x not in [\"the\",\"of\",\"a\",\"to\",\"that\",\"it\",\"for\",\"is\",\"and\"])\\\n",
    "    .map(lambda x:(x,1))\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd3 = rdd2.reduceByKey(lambda a,b:a+b)\n",
    "# rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apache', 4),\n",
       " ('hadoop', 2),\n",
       " ('software', 1),\n",
       " ('library', 2),\n",
       " ('framework', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd3 = rdd2.reduceByKey(add)\n",
    "rdd3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       _1| _2|\n",
      "+---------+---+\n",
      "|   apache|  4|\n",
      "|   hadoop|  2|\n",
      "| software|  1|\n",
      "|  library|  2|\n",
      "|framework|  1|\n",
      "+---------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = rdd3.toDF()\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|         _1| _2|\n",
      "+-----------+---+\n",
      "|     apache|  4|\n",
      "|      storm|  3|\n",
      "|      spark|  3|\n",
      "|     hadoop|  2|\n",
      "|    library|  2|\n",
      "|distributed|  2|\n",
      "|       data|  2|\n",
      "|   designed|  2|\n",
      "|computation|  2|\n",
      "| processing|  2|\n",
      "|    system.|  2|\n",
      "|programming|  2|\n",
      "|        sql|  2|\n",
      "|       each|  2|\n",
      "|         on|  2|\n",
      "|    cluster|  2|\n",
      "|         be|  2|\n",
      "| high-level|  2|\n",
      "|         in|  2|\n",
      "|   supports|  2|\n",
      "+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.sort(df3['_2'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparkmlhw02\n",
    "采用ML Pipelines构建一个文档分类器，需要将模型进行保存，并且加载模型后对测试样本进行预测，考查点：\n",
    "\n",
    "1）  spark读取文件\n",
    "\n",
    "2）  数据清洗，考查Datasets的基本操作\n",
    "\n",
    "3）  构建分类器的管道，考查构建各种转换操作\n",
    "\n",
    "4）  读取模型，读取测试数据，并且进行模型测试\n",
    "\n",
    " \n",
    "\n",
    "数据格式：\n",
    "\n",
    "myapp_id|typenameid|typename|myapp_word|myapp_word_all\n",
    "\n",
    " \n",
    "\n",
    "其中文档ID字段为：myapp_id\n",
    "\n",
    "其中文档类别字段为：typenameid\n",
    "\n",
    "其中文档内容为：myapp_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------------------+--------------------+\n",
      "|myapp_id|typenameid|typename|          myapp_word|      myapp_word_all|\n",
      "+--------+----------+--------+--------------------+--------------------+\n",
      "| 1376533|         2|  action|game, android, world|game, android, wo...|\n",
      "| 1376542|         2|  action|                game|game, app, enjoy,...|\n",
      "| 1376603|         2|  action|run, tap, collect...|run, tap, collect...|\n",
      "| 1376792|         2|  action|                 run|run, ath, game, m...|\n",
      "| 1376941|         2|  action|fight, game, play...|fight, game, play...|\n",
      "+--------+----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"file:///Users/luoyonggui/PycharmProjects/mayiexamples/sparkml/data/doc_class.dat\", sep='|', header=True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('myapp_id', 'string'),\n",
       " ('typenameid', 'string'),\n",
       " ('typename', 'string'),\n",
       " ('myapp_word', 'string'),\n",
       " ('myapp_word_all', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['myapp_id', 'typenameid', 'typename', 'myapp_word', 'myapp_word_all']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir('CheckpointDir/tt123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[myapp_id: string, typenameid: string, typename: string, myapp_word: string, myapp_word_all: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed('typenameid','label')\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "df1 = df1.withColumn('label', col('label').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('myapp_id', 'string'),\n",
       " ('label', 'int'),\n",
       " ('typename', 'string'),\n",
       " ('myapp_word', 'string'),\n",
       " ('myapp_word_all', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分训练集和测试集,会先打乱数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#切分训练集和测试集,会先打乱数据集\n",
    "train_set, test_set = df1.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301005"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练Pipeline  LogisticRegression HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.490482807159424s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "tokenizer = Tokenizer(inputCol='myapp_word_all', outputCol='words')\n",
    "hashingTF = HashingTF(numFeatures=1000, inputCol='words', outputCol='features')\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "model = pipeline.fit(train_set)\n",
    "print(f'{time()-start}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------------+--------------------+----------+\n",
      "|myapp_id|label|         typename|         probability|prediction|\n",
      "+--------+-----+-----------------+--------------------+----------+\n",
      "| 1376501|    4|           arcade|[4.11158459184166...|       4.0|\n",
      "| 1376523|   14|           casual|[2.94084062076931...|      14.0|\n",
      "| 1376606|    4|           arcade|[6.08513932347101...|       4.0|\n",
      "| 1376635|    4|           arcade|[6.08513932347101...|       4.0|\n",
      "| 1376656|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1376782|   14|           casual|[1.46631980338166...|      14.0|\n",
      "| 1377005|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1377059|    4|           arcade|[6.08513932347101...|       4.0|\n",
      "| 1377153|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1377192|    7|books & reference|[1.21296495436360...|       7.0|\n",
      "| 1377195|    4|           arcade|[5.37814702677680...|       4.0|\n",
      "| 1377499|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1377543|    3|        adventure|[1.56060558512871...|       3.0|\n",
      "| 1377626|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1377648|   16|    communication|[1.78901353628456...|      16.0|\n",
      "| 1377738|    7|books & reference|[1.21296495436360...|       7.0|\n",
      "| 1377944|   14|           casual|[2.94084062076931...|      14.0|\n",
      "| 1378012|   14|           casual|[1.46631980338166...|      14.0|\n",
      "| 1378114|    4|           arcade|[6.08513932347101...|       4.0|\n",
      "| 1378332|   16|    communication|[1.78901353628456...|      16.0|\n",
      "+--------+-----+-----------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = model.transform(test_set)\n",
    "p.select(\"myapp_id\",\"label\",\"typename\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型保存和加载\n",
    "model.save('file:///tmp/testModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmetadata\u001b[m\u001b[m \u001b[1m\u001b[36mstages\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/testModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "! open /tmp/testModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModel = PipelineModel.load('file:///tmp/testModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------------+--------------------+----------+\n",
      "|myapp_id|label|     typename|         probability|prediction|\n",
      "+--------+-----+-------------+--------------------+----------+\n",
      "| 1376501|    4|       arcade|[4.11158459184166...|       4.0|\n",
      "| 1376523|   14|       casual|[2.94084062076931...|      14.0|\n",
      "| 1376606|    4|       arcade|[6.08513932347101...|       4.0|\n",
      "| 1376635|    4|       arcade|[6.08513932347101...|       4.0|\n",
      "| 1376656|   16|communication|[1.78901353628456...|      16.0|\n",
      "+--------+-----+-------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = savedModel.transform(test_set)\n",
    "p.select(\"myapp_id\",\"label\",\"typename\",\"probability\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save('file:///tmp/pp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmetadata\u001b[m\u001b[m \u001b[1m\u001b[36mstages\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Pipeline.load('file:///tmp/pp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparkmlhw03\n",
    "第3次课的作业和第2次的课一样，只是需要采用交叉验证的方法来做，并且还要计算各种分类指标，这里要分2次实验，第1次设计1个2元分类器，第2次设计1个多元分类器。\n",
    "\n",
    " \n",
    "\n",
    "采用ML Pipelines构建一个文档分类器，需要将模型进行保存，并且加载模型后对测试样本进行预测，考查点：\n",
    "\n",
    "1）  spark读取文件\n",
    "\n",
    "2）  数据清洗，考查Datasets的基本操作\n",
    "\n",
    "3）  构建分类器的管道，考查构建各种转换操作\n",
    "\n",
    "4）  读取模型，读取测试数据，并且进行模型测试\n",
    "\n",
    "5）  重点：自己设置交叉验证的网格参数，采用交叉验证的模型来做\n",
    "\n",
    "6）  重点：计算分类结果的指标\n",
    "\n",
    " \n",
    "\n",
    "数据格式：\n",
    "\n",
    "myapp_id|typenameid|typename|myapp_word|myapp_word_all\n",
    "\n",
    " \n",
    "\n",
    "其中文档ID字段为：myapp_id\n",
    "\n",
    "其中文档类别字段为：typenameid\n",
    "\n",
    "其中文档内容为：myapp_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+--------------------+--------------------+\n",
      "|myapp_id|typenameid|typename|          myapp_word|      myapp_word_all|\n",
      "+--------+----------+--------+--------------------+--------------------+\n",
      "| 1376533|         2|  action|game, android, world|game, android, wo...|\n",
      "| 1376542|         2|  action|                game|game, app, enjoy,...|\n",
      "| 1376603|         2|  action|run, tap, collect...|run, tap, collect...|\n",
      "| 1376792|         2|  action|                 run|run, ath, game, m...|\n",
      "| 1376941|         2|  action|fight, game, play...|fight, game, play...|\n",
      "+--------+----------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"file:///Users/luoyonggui/PycharmProjects/mayiexamples/sparkml/data/doc_class.dat\", sep='|', header=True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(myapp_id='1376533', typenameid='2', typename='action', myapp_word='game, android, world', myapp_word_all='game, android, world, control, devic, experi, free, gameplay, play, screen, time, touch, war, action, addict, app, ath, attack, battl, challeng, collect, complet, descript, easi, enemi, enjoy, featur, fight, find, friend, fun, gamec, graphic, great, gun, high, kill, level, make, mission, mode, move, player, power, read, real, run, score, shoot, shooter, simpl, skill, sound, special, surviv, tap, uniqu, upgrad, weapon, zombi'),\n",
       " Row(myapp_id='1376542', typenameid='2', typename='action', myapp_word='game', myapp_word_all='game, app, enjoy, free, high, play, run, action, addict, android, ath, attack, battl, challeng, collect, complet, control, descript, devic, easi, enemi, experi, featur, fight, find, friend, fun, gamec, gameplay, graphic, great, gun, kill, level, make, mission, mode, move, player, power, read, real, score, screen, shoot, shooter, simpl, skill, sound, special, surviv, tap, time, touch, uniqu, upgrad, war, weapon, world, zombi'),\n",
       " Row(myapp_id='1376603', typenameid='2', typename='action', myapp_word='run, tap, collect, game', myapp_word_all='run, tap, collect, game, featur, gamec, graphic, great, play, score, screen, sound, special, action, addict, android, app, ath, attack, battl, challeng, complet, control, descript, devic, easi, enemi, enjoy, experi, fight, find, free, friend, fun, gameplay, gun, high, kill, level, make, mission, mode, move, player, power, read, real, shoot, shooter, simpl, skill, surviv, time, touch, uniqu, upgrad, war, weapon, world, zombi'),\n",
       " Row(myapp_id='1376792', typenameid='2', typename='action', myapp_word='run', myapp_word_all='run, ath, game, make, screen, skill, touch, action, addict, android, app, attack, battl, challeng, collect, complet, control, descript, devic, easi, enemi, enjoy, experi, featur, fight, find, free, friend, fun, gamec, gameplay, graphic, great, gun, high, kill, level, mission, mode, move, play, player, power, read, real, score, shoot, shooter, simpl, sound, special, surviv, tap, time, uniqu, upgrad, war, weapon, world, zombi'),\n",
       " Row(myapp_id='1376941', typenameid='2', typename='action', myapp_word='fight, game, player, action', myapp_word_all='fight, game, player, action, complet, experi, friend, play, shoot, time, weapon, world, addict, android, app, ath, attack, battl, challeng, collect, control, descript, devic, easi, enemi, enjoy, featur, find, free, fun, gamec, gameplay, graphic, great, gun, high, kill, level, make, mission, mode, move, power, read, real, run, score, screen, shooter, simpl, skill, sound, special, surviv, tap, touch, uniqu, upgrad, war, zombi')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('typenameid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(typenameid='7', count(myapp_id)=13122),\n",
       " Row(typenameid='15', count(myapp_id)=1785),\n",
       " Row(typenameid='11', count(myapp_id)=3828),\n",
       " Row(typenameid='29', count(myapp_id)=11588),\n",
       " Row(typenameid='42', count(myapp_id)=1439),\n",
       " Row(typenameid='3', count(myapp_id)=2181),\n",
       " Row(typenameid='30', count(myapp_id)=10974),\n",
       " Row(typenameid='34', count(myapp_id)=15295),\n",
       " Row(typenameid='8', count(myapp_id)=245),\n",
       " Row(typenameid='22', count(myapp_id)=10084),\n",
       " Row(typenameid='28', count(myapp_id)=587),\n",
       " Row(typenameid='16', count(myapp_id)=9981),\n",
       " Row(typenameid='35', count(myapp_id)=3148),\n",
       " Row(typenameid='47', count(myapp_id)=2582),\n",
       " Row(typenameid='43', count(myapp_id)=26346),\n",
       " Row(typenameid='5', count(myapp_id)=213),\n",
       " Row(typenameid='31', count(myapp_id)=17319),\n",
       " Row(typenameid='18', count(myapp_id)=2782),\n",
       " Row(typenameid='27', count(myapp_id)=5766),\n",
       " Row(typenameid='17', count(myapp_id)=17553),\n",
       " Row(typenameid='26', count(myapp_id)=6107),\n",
       " Row(typenameid='46', count(myapp_id)=1444),\n",
       " Row(typenameid='6', count(myapp_id)=1317),\n",
       " Row(typenameid='23', count(myapp_id)=9912),\n",
       " Row(typenameid='41', count(myapp_id)=24),\n",
       " Row(typenameid='38', count(myapp_id)=1615),\n",
       " Row(typenameid='40', count(myapp_id)=14655),\n",
       " Row(typenameid='25', count(myapp_id)=16618),\n",
       " Row(typenameid='44', count(myapp_id)=5201),\n",
       " Row(typenameid='33', count(myapp_id)=13462),\n",
       " Row(typenameid='48', count(myapp_id)=1001),\n",
       " Row(typenameid='9', count(myapp_id)=2),\n",
       " Row(typenameid='24', count(myapp_id)=2263),\n",
       " Row(typenameid='32', count(myapp_id)=5753),\n",
       " Row(typenameid='20', count(myapp_id)=21647),\n",
       " Row(typenameid='36', count(myapp_id)=845),\n",
       " Row(typenameid='10', count(myapp_id)=14530),\n",
       " Row(typenameid='37', count(myapp_id)=6021),\n",
       " Row(typenameid='4', count(myapp_id)=11530),\n",
       " Row(typenameid='39', count(myapp_id)=9128),\n",
       " Row(typenameid='12', count(myapp_id)=46),\n",
       " Row(typenameid='13', count(myapp_id)=1681),\n",
       " Row(typenameid='14', count(myapp_id)=12260),\n",
       " Row(typenameid='21', count(myapp_id)=1923),\n",
       " Row(typenameid='2', count(myapp_id)=2689),\n",
       " Row(typenameid='45', count(myapp_id)=16008)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupBy('typenameid').agg({'myapp_id': 'count'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='myapp_word_all', outputCol='words')\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='features')\n",
    "df2 = df1.withColumnRenamed('typenameid','label').withColumn('label', col('label').cast(IntegerType()))\n",
    "#切分训练集和测试集,会先打乱数据集\n",
    "train_set, test_set = df2.randomSplit([0.9,0.1])\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网格调参grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures,[10, 100, 1000]).addGrid(lr.regParam, [0.1, 0.01]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175.16829204559326\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "cvModel = cv.fit(train_set)\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|   16|[-7.4650506355219...|[3.28917268712459...|      43.0|\n",
      "|    2|[-7.4655121275154...|[3.76707131453514...|      31.0|\n",
      "|   16|[-7.4650506355219...|[3.28917268712459...|      43.0|\n",
      "|    4|[-7.4650726649511...|[3.09555661679601...|      43.0|\n",
      "|   14|[-7.4646128253141...|[3.53625473567110...|      43.0|\n",
      "|    4|[-7.4650726649511...|[3.09555661679601...|      43.0|\n",
      "|   11|[-7.4654049429803...|[3.73730792079566...|      29.0|\n",
      "|    4|[-7.4650726649511...|[3.09555661679601...|      43.0|\n",
      "|   11|[-7.4654049429803...|[3.73730792079566...|      29.0|\n",
      "|   16|[-7.4650400569118...|[3.49055120567496...|      43.0|\n",
      "|    2|[-7.4655227447676...|[3.78054958781725...|      31.0|\n",
      "|    4|[-7.4650889276234...|[2.81868035613385...|       4.0|\n",
      "|   14|[-7.4645965626417...|[3.47603574364158...|      14.0|\n",
      "|   17|[-7.4650890332807...|[2.69941023577164...|      17.0|\n",
      "|   16|[-7.4650506355219...|[3.28917268712459...|      43.0|\n",
      "|    2|[-7.4655121275154...|[3.76707131453514...|      31.0|\n",
      "|    7|[-7.4649737684079...|[2.98518416937552...|       7.0|\n",
      "|   16|[-7.4650506355219...|[3.28917268712459...|      43.0|\n",
      "|   14|[-7.4645965626417...|[3.47603574364158...|      14.0|\n",
      "|   10|[-7.4649634849549...|[3.25912816707411...|      10.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = cvModel.transform(test_set)\n",
    "p.select(['label', 'rawPrediction', 'probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator1 = MulticlassClassificationEvaluator()\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator2 = MulticlassClassificationEvaluator(metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CrossValidator(estimator=pipeline, evaluator=evaluator1, estimatorParamMaps=paramGrid, numFolds=2)\n",
    "\n",
    "cv3 = CrossValidator(estimator=pipeline, evaluator=evaluator2, estimatorParamMaps=paramGrid, numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|myapp_id|label|         typename|          myapp_word|      myapp_word_all|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 1376505|   16|    communication|sms, messag, free...|sms, messag, free...|[sms,, messag,, f...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1376533|    2|           action|game, android, world|game, android, wo...|[game,, android,,...|(1000,[10,39,40,4...|[-7.4647958040277...|[1.07198843263364...|       2.0|\n",
      "| 1376604|   16|    communication|app, phone, devic...|app, phone, devic...|[app,, phone,, de...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1376684|    4|           arcade|               level|level, action, ad...|[level,, action,,...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376839|   14|           casual|             android|android, make, ph...|[android,, make,,...|(1000,[39,48,59,8...|[-7.4646630049933...|[7.30395990264080...|      14.0|\n",
      "| 1376906|    4|           arcade|                game|game, addict, and...|[game,, addict,, ...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376908|   11|             card|                hand|hand, android, bo...|[hand,, android,,...|(1000,[20,39,48,5...|[-7.4647826458355...|[1.19795805136172...|      11.0|\n",
      "| 1376912|    4|           arcade|           run, game|run, game, addict...|[run,, game,, add...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376989|   11|             card|game, app, play, ...|game, app, play, ...|[game,, app,, pla...|(1000,[20,39,48,5...|[-7.4647826458355...|[1.19795805136172...|      11.0|\n",
      "| 1376992|   16|    communication|download, mobil, ...|download, mobil, ...|[download,, mobil...|(1000,[39,52,57,5...|[-7.4647744990065...|[1.93631551662281...|      16.0|\n",
      "| 1377045|    2|           action|gun, time, enemi,...|gun, time, enemi,...|[gun,, time,, ene...|(1000,[39,40,48,5...|[-7.4647950810290...|[4.80249281422778...|       2.0|\n",
      "| 1377103|    4|           arcade|game, play, ath, ...|game, play, ath, ...|[game,, play,, at...|(1000,[39,57,59,6...|[-7.4647283691459...|[3.28646288369307...|       4.0|\n",
      "| 1377120|   14|           casual| game, make, android|game, make, andro...|[game,, make,, an...|(1000,[39,48,59,8...|[-7.4646679328056...|[4.10294705962015...|      14.0|\n",
      "| 1377131|   17|        education|includ, app, andr...|includ, app, andr...|[includ,, app,, a...|(1000,[14,39,52,5...|[-7.4647763644927...|[8.72301039985774...|      17.0|\n",
      "| 1377146|   16|    communication|  free, app, connect|free, app, connec...|[free,, app,, con...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1377191|    2|           action|       shoot, upgrad|shoot, upgrad, at...|[shoot,, upgrad,,...|(1000,[10,39,40,4...|[-7.4647958040277...|[1.07198843263364...|       2.0|\n",
      "| 1377389|    7|books & reference|              offlin|offlin, access, a...|[offlin,, access,...|(1000,[2,39,57,59...|[-7.4647429060411...|[1.04868081614551...|       7.0|\n",
      "| 1377499|   16|    communication|app, phone, messa...|app, phone, messa...|[app,, phone,, me...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1377582|   14|           casual|game, app, fun, play|game, app, fun, p...|[game,, app,, fun...|(1000,[39,48,59,8...|[-7.4646679328056...|[4.10294705962015...|      14.0|\n",
      "| 1377654|   10|         business|busi, contact, sh...|busi, contact, sh...|[busi,, contact,,...|(1000,[10,39,52,5...|[-7.4647426566642...|[1.66655308833598...|      10.0|\n",
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "242.40430903434753\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "cvModel2 = cv2.fit(train_set)\n",
    "p2 = cvModel2.transform(test_set)\n",
    "p2.show()\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|myapp_id|label|         typename|          myapp_word|      myapp_word_all|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| 1376505|   16|    communication|sms, messag, free...|sms, messag, free...|[sms,, messag,, f...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1376533|    2|           action|game, android, world|game, android, wo...|[game,, android,,...|(1000,[10,39,40,4...|[-7.4647958040277...|[1.07198843263364...|       2.0|\n",
      "| 1376604|   16|    communication|app, phone, devic...|app, phone, devic...|[app,, phone,, de...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1376684|    4|           arcade|               level|level, action, ad...|[level,, action,,...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376839|   14|           casual|             android|android, make, ph...|[android,, make,,...|(1000,[39,48,59,8...|[-7.4646630049933...|[7.30395990264080...|      14.0|\n",
      "| 1376906|    4|           arcade|                game|game, addict, and...|[game,, addict,, ...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376908|   11|             card|                hand|hand, android, bo...|[hand,, android,,...|(1000,[20,39,48,5...|[-7.4647826458355...|[1.19795805136172...|      11.0|\n",
      "| 1376912|    4|           arcade|           run, game|run, game, addict...|[run,, game,, add...|(1000,[39,57,59,6...|[-7.4647332969582...|[3.60233232848064...|       4.0|\n",
      "| 1376989|   11|             card|game, app, play, ...|game, app, play, ...|[game,, app,, pla...|(1000,[20,39,48,5...|[-7.4647826458355...|[1.19795805136172...|      11.0|\n",
      "| 1376992|   16|    communication|download, mobil, ...|download, mobil, ...|[download,, mobil...|(1000,[39,52,57,5...|[-7.4647744990065...|[1.93631551662281...|      16.0|\n",
      "| 1377045|    2|           action|gun, time, enemi,...|gun, time, enemi,...|[gun,, time,, ene...|(1000,[39,40,48,5...|[-7.4647950810290...|[4.80249281422778...|       2.0|\n",
      "| 1377103|    4|           arcade|game, play, ath, ...|game, play, ath, ...|[game,, play,, at...|(1000,[39,57,59,6...|[-7.4647283691459...|[3.28646288369307...|       4.0|\n",
      "| 1377120|   14|           casual| game, make, android|game, make, andro...|[game,, make,, an...|(1000,[39,48,59,8...|[-7.4646679328056...|[4.10294705962015...|      14.0|\n",
      "| 1377131|   17|        education|includ, app, andr...|includ, app, andr...|[includ,, app,, a...|(1000,[14,39,52,5...|[-7.4647763644927...|[8.72301039985774...|      17.0|\n",
      "| 1377146|   16|    communication|  free, app, connect|free, app, connec...|[free,, app,, con...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1377191|    2|           action|       shoot, upgrad|shoot, upgrad, at...|[shoot,, upgrad,,...|(1000,[10,39,40,4...|[-7.4647958040277...|[1.07198843263364...|       2.0|\n",
      "| 1377389|    7|books & reference|              offlin|offlin, access, a...|[offlin,, access,...|(1000,[2,39,57,59...|[-7.4647429060411...|[1.04868081614551...|       7.0|\n",
      "| 1377499|   16|    communication|app, phone, messa...|app, phone, messa...|[app,, phone,, me...|(1000,[39,52,57,5...|[-7.4647759316632...|[1.65150546698479...|      16.0|\n",
      "| 1377582|   14|           casual|game, app, fun, play|game, app, fun, p...|[game,, app,, fun...|(1000,[39,48,59,8...|[-7.4646679328056...|[4.10294705962015...|      14.0|\n",
      "| 1377654|   10|         business|busi, contact, sh...|busi, contact, sh...|[busi,, contact,,...|(1000,[10,39,52,5...|[-7.4647426566642...|[1.66655308833598...|      10.0|\n",
      "+--------+-----+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "188.49880480766296\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "cvModel3 = cv3.fit(train_set)\n",
    "p3 = cvModel3.transform(test_set)\n",
    "p3.show()\n",
    "print(time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparkmlhw04\n",
    "采用ML Pipelines构建一个文档分类器，需要进行文档清洗、提取文档关键字、得到文档向量、提取分类主题的关键字，最后建立一个文档主题目分类器，进行模型训练、测试、分类指标：\n",
    "\n",
    "数据： data_app\n",
    "\n",
    "数据格式： doc_id1| doc_id2| soure|name|doc_name|country| typename|text\n",
    "\n",
    "其中文档ID字段为：doc_id1   \n",
    "其中文档名称字段为：doc_name   \n",
    "其中文档类别字段为：typename   \n",
    "其中文档内容为：text\n",
    "\n",
    "采用ML Pipelines构建一个文档分类器，需要进行文档清洗、提取文档关键字、得到文档向量、提取分类主题的关键字，最后建立一个文档主题目分类器，进行模型训练、测试、分类指标：   \n",
    "1） 文档清洗   \n",
    "2） 文档关键字提取   \n",
    "3） 主题的关键字提取   \n",
    "4） 文档向量化，采用3种向量方法，进行测试   \n",
    "5） 建立主题分类器   \n",
    "6） 训练、测试、指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/PycharmProjects/mayiexamples/sparkml\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"file:///Users/luoyonggui/PycharmProjects/mayiexamples/sparkml/data/data_app\", sep='|', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed('_c0','doc_id1')\\\n",
    "    .withColumnRenamed('_c1','doc_id2')\\\n",
    "    .withColumnRenamed('_c2','soure')\\\n",
    "    .withColumnRenamed('_c3','name')\\\n",
    "    .withColumnRenamed('_c4','doc_name')\\\n",
    "    .withColumnRenamed('_c5','country')\\\n",
    "    .withColumnRenamed('_c6','typename')\\\n",
    "    .withColumnRenamed('_c7','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+\n",
      "|doc_id1|doc_id2|soure|                name|            doc_name|country|        typename|                text|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+\n",
      "|1494604| 549745|    7|Curriculum Vitae-...|curriculum.vitae....|     en|        Business|My professional r...|\n",
      "|1494602| 604435|    7|Five Questions (S...|com.kevinhecker.t...|     en|          Puzzle|Five Questions is...|\n",
      "|1494599| 495855|    7|      RhymasaurusRex|com.yeti.rhymasau...|     en|   Entertainment|Are you a songwri...|\n",
      "|1494596| 714480|    7|            helphelp|com.triplekingkon...|     en|       Lifestyle|guardian angel %2...|\n",
      "|1494593| 568853|    7|Calorie Intake Ca...|com.piusvelte.cal...|     en|Health & Fitness|Description Estim...|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335317"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(typename='en'),\n",
       " Row(typename='Music & Audio'),\n",
       " Row(typename='Education'),\n",
       " Row(typename='Trivia'),\n",
       " Row(typename='Entertainment'),\n",
       " Row(typename='Cards & Casino'),\n",
       " Row(typename='Adventure'),\n",
       " Row(typename='Arcade'),\n",
       " Row(typename='Sports'),\n",
       " Row(typename='Travel & Local'),\n",
       " Row(typename=None),\n",
       " Row(typename='Brain & Puzzle'),\n",
       " Row(typename='Sports Games'),\n",
       " Row(typename='Role Playing'),\n",
       " Row(typename='Media & Video'),\n",
       " Row(typename='Finance'),\n",
       " Row(typename='Personalization'),\n",
       " Row(typename='Arcade & Action'),\n",
       " Row(typename='Racing'),\n",
       " Row(typename='Tools'),\n",
       " Row(typename='Family'),\n",
       " Row(typename='Educational'),\n",
       " Row(typename='Comics'),\n",
       " Row(typename='Social'),\n",
       " Row(typename='Libraries & Demo'),\n",
       " Row(typename='Shopping'),\n",
       " Row(typename='Health & Fitness'),\n",
       " Row(typename='Productivity'),\n",
       " Row(typename='Card'),\n",
       " Row(typename='Casino'),\n",
       " Row(typename='Music'),\n",
       " Row(typename='Photography'),\n",
       " Row(typename='Brain &amp; Puzzle'),\n",
       " Row(typename='Lifestyle'),\n",
       " Row(typename='Business'),\n",
       " Row(typename='News & Magazines'),\n",
       " Row(typename='Books & Reference'),\n",
       " Row(typename='Weather'),\n",
       " Row(typename='Puzzle'),\n",
       " Row(typename='Casual'),\n",
       " Row(typename='Board'),\n",
       " Row(typename='Medical'),\n",
       " Row(typename='Communication'),\n",
       " Row(typename='Word'),\n",
       " Row(typename='Action'),\n",
       " Row(typename='Strategy'),\n",
       " Row(typename='Transportation'),\n",
       " Row(typename='Simulation')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('typename').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+--------------------+\n",
      "|doc_id1|doc_id2|soure|                name|            doc_name|country|        typename|                text|               words|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+--------------------+\n",
      "|1494604| 549745|    7|Curriculum Vitae-...|curriculum.vitae....|     en|        Business|My professional r...|[my, professional...|\n",
      "|1494602| 604435|    7|Five Questions (S...|com.kevinhecker.t...|     en|          Puzzle|Five Questions is...|[five, questions,...|\n",
      "|1494599| 495855|    7|      RhymasaurusRex|com.yeti.rhymasau...|     en|   Entertainment|Are you a songwri...|[are, you, a, son...|\n",
      "|1494596| 714480|    7|            helphelp|com.triplekingkon...|     en|       Lifestyle|guardian angel %2...|[guardian, angel,...|\n",
      "|1494593| 568853|    7|Calorie Intake Ca...|com.piusvelte.cal...|     en|Health & Fitness|Description Estim...|[description, est...|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rtokenizer = RegexTokenizer(inputCol='text', outputCol='words')\n",
    "rtokenizer.setPattern('[.,\\s]+')\n",
    "df3 = rtokenizer.transform(df1)\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|doc_id1|doc_id2|soure|                name|            doc_name|country|typename|                text|               words|              rwords|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|1494604| 549745|    7|Curriculum Vitae-...|curriculum.vitae....|     en|Business|My professional r...|[my, professional...|[professional, re...|\n",
      "|1494602| 604435|    7|Five Questions (S...|com.kevinhecker.t...|     en|  Puzzle|Five Questions is...|[five, questions,...|[five, questions,...|\n",
      "+-------+-------+-----+--------------------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"rwords\", stopWords=StopWordsRemover.loadDefaultStopWords('english'))\n",
    "df4 = remover.transform(df3)\n",
    "df4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-636388843594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rwords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "df4.select['tt']=df4.select('rwords').rdd.map(lambda l: [i.strip() for i in l['rwords'] if i.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档特征化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                                                                                                                                                                                |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(1000,[69,70,102,124,144,155,159,160,169,239,242,256,265,322,338,356,364,399,411,522,537,556,586,621,648,666,705,765,841,882,911,923,928,953,955,958],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(numFeatures=1000, inputCol='rwords', outputCol='features')\n",
    "df5 = hashingTF.transform(df4)\n",
    "df5.select('features').show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14840.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 898.0 failed 1 times, most recent failure: Lost task 2.0 in stage 898.0 (TID 3850, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:196)\n\tat org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:309)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-d7403b41e6b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wvfeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14840.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 898.0 failed 1 times, most recent failure: Lost task 2.0 in stage 898.0 (TID 3850, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.mllib.feature.Word2Vec.learnVocab(Word2Vec.scala:196)\n\tat org.apache.spark.mllib.feature.Word2Vec.fit(Word2Vec.scala:309)\n\tat org.apache.spark.ml.feature.Word2Vec.fit(Word2Vec.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n"
     ]
    }
   ],
   "source": [
    "#pyspark.ml.feature.Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "wv = Word2Vec(vectorSize=128, inputCol='rwords', outputCol='wvfeatures')\n",
    "wvModel = wv.fit(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "322px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
