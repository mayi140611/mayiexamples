{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/chengstone/movie_recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)  # 设置显示数据的最大列数，防止出现省略号…，导致数据显示不全\n",
    "pd.set_option('expand_frame_repr', False)  # 当列太多时不自动换行\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='Arial Unicode MS')  # 解决Seaborn中文显示问题\n",
    "import sys\n",
    "sys.path.append('/Users/luoyonggui/PycharmProjects/mayiutils_n1/mayiutils/data_prepare')\n",
    "from data_explore import DataExplore as de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml_1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of records: 6040, num of columns: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count Missing</th>\n",
       "      <th>% Missing</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Count Mode</th>\n",
       "      <th>% Mode</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <td>int64</td>\n",
       "      <td>6040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0165563</td>\n",
       "      <td>3020.5</td>\n",
       "      <td>1743.74</td>\n",
       "      <td>1</td>\n",
       "      <td>1510.75</td>\n",
       "      <td>3020.5</td>\n",
       "      <td>4530.25</td>\n",
       "      <td>6040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>4331</td>\n",
       "      <td>71.7053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>int64</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>2096</td>\n",
       "      <td>34.702</td>\n",
       "      <td>30.6392</td>\n",
       "      <td>12.896</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OccupationID</th>\n",
       "      <td>int64</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>759</td>\n",
       "      <td>12.5662</td>\n",
       "      <td>8.14685</td>\n",
       "      <td>6.32951</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zip-code</th>\n",
       "      <td>object</td>\n",
       "      <td>3439</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48104</td>\n",
       "      <td>19</td>\n",
       "      <td>0.31457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data Type  Unique Values  count Missing  % Missing   Mode  Count Mode     % Mode     mean      std  min      25%     50%      75%   max\n",
       "UserID           int64           6040              0        0.0      1           1  0.0165563   3020.5  1743.74    1  1510.75  3020.5  4530.25  6040\n",
       "Gender          object              2              0        0.0      M        4331    71.7053      NaN      NaN  NaN      NaN     NaN      NaN   NaN\n",
       "Age              int64              7              0        0.0     25        2096     34.702  30.6392   12.896    1       25      25       35    56\n",
       "OccupationID     int64             21              0        0.0      4         759    12.5662  8.14685  6.32951    0        3       7       14    20\n",
       "Zip-code        object           3439              0        0.0  48104          19    0.31457      NaN      NaN  NaN      NaN     NaN      NaN   NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.describe(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del users['Zip-code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改变User数据中性别和年龄\n",
    "gender_map = {'F':0, 'M':1}\n",
    "users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "users['Age'] = users['Age'].map(age_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml_1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of records: 3883, num of columns: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count Missing</th>\n",
       "      <th>% Missing</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Count Mode</th>\n",
       "      <th>% Mode</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MovieID</th>\n",
       "      <td>int64</td>\n",
       "      <td>3883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0257533</td>\n",
       "      <td>1986.05</td>\n",
       "      <td>1146.78</td>\n",
       "      <td>1</td>\n",
       "      <td>982.5</td>\n",
       "      <td>2010</td>\n",
       "      <td>2980.5</td>\n",
       "      <td>3952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>object</td>\n",
       "      <td>3883</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$1,000,000 Duck (1971)</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0257533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genres</th>\n",
       "      <td>object</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Drama</td>\n",
       "      <td>843</td>\n",
       "      <td>21.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Data Type  Unique Values  count Missing  % Missing                    Mode  Count Mode     % Mode     mean      std  min    25%   50%     75%   max\n",
       "MovieID     int64           3883              0        0.0                       1           1  0.0257533  1986.05  1146.78    1  982.5  2010  2980.5  3952\n",
       "Title      object           3883              0        0.0  $1,000,000 Duck (1971)           1  0.0257533      NaN      NaN  NaN    NaN   NaN     NaN   NaN\n",
       "Genres     object            301              0        0.0                   Drama         843      21.71      NaN      NaN  NaN    NaN   NaN     NaN   NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.describe(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将Title中的年份去掉\n",
    "movies['Title'] = movies.Title.str.replace(r'\\(\\d*\\)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#电影类型转数字字典\n",
    "genres2int = {v1: i1 for i1, v1 in enumerate(set([ii for i in movies.Genres.str.split('|').tolist() for ii in i if ii != '']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Western': 0,\n",
       " 'Thriller': 1,\n",
       " \"Children's\": 2,\n",
       " 'Romance': 3,\n",
       " 'Horror': 4,\n",
       " 'Documentary': 5,\n",
       " 'War': 6,\n",
       " 'Drama': 7,\n",
       " 'Crime': 8,\n",
       " 'Mystery': 9,\n",
       " 'Animation': 10,\n",
       " 'Musical': 11,\n",
       " 'Adventure': 12,\n",
       " 'Action': 13,\n",
       " 'Sci-Fi': 14,\n",
       " 'Fantasy': 15,\n",
       " 'Film-Noir': 16,\n",
       " 'Comedy': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将电影类型转成等长数字列表，长度是18\n",
    "def t(s):\n",
    "    lt = [0] * 18\n",
    "    for i in s.split('|'):\n",
    "        lt[genres2int[i]] = 1\n",
    "    return lt\n",
    "movies['Genres'] = movies.Genres.map(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#电影Title转数字字典\n",
    "title2int = {v1: i1 for i1, v1 in enumerate(set([ii for i in movies.Title.str.split(r'\\s+|\\(|\\)').tolist() for ii in i if ii != '']))}\n",
    "\n",
    "title2int['<PAD>'] = len(title2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将电影Title转成等长数字列表，长度是18\n",
    "movies.Title.str.split(r'\\s+|\\(|\\)').str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Toy', 'Story', '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\s+|\\(|\\)', 'Toy Story ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count = 18\n",
    "def t(s):\n",
    "    lt = [title2int['<PAD>']] * title_count\n",
    "    count = 0\n",
    "    for i in re.split(r'\\s+|\\(|\\)', s):\n",
    "        if i != '':\n",
    "            lt[count] = title2int[i]\n",
    "            count += 1\n",
    "    return lt\n",
    "movies['Title'] = movies.Title.map(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml_1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of records: 1000209, num of columns: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>count Missing</th>\n",
       "      <th>% Missing</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Count Mode</th>\n",
       "      <th>% Mode</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <td>int64</td>\n",
       "      <td>6040</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4169</td>\n",
       "      <td>2314</td>\n",
       "      <td>0.231352</td>\n",
       "      <td>3.024512e+03</td>\n",
       "      <td>1.728413e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>4476.0</td>\n",
       "      <td>6.040000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovieID</th>\n",
       "      <td>int64</td>\n",
       "      <td>3706</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2858</td>\n",
       "      <td>3428</td>\n",
       "      <td>0.342728</td>\n",
       "      <td>1.865540e+03</td>\n",
       "      <td>1.096041e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>2770.0</td>\n",
       "      <td>3.952000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <td>int64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>348971</td>\n",
       "      <td>34.8898</td>\n",
       "      <td>3.581564e+00</td>\n",
       "      <td>1.117102e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamps</th>\n",
       "      <td>int64</td>\n",
       "      <td>458455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>975528402</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00299937</td>\n",
       "      <td>9.722437e+08</td>\n",
       "      <td>1.215256e+07</td>\n",
       "      <td>956703932.0</td>\n",
       "      <td>965302637.0</td>\n",
       "      <td>973018006.0</td>\n",
       "      <td>975220939.0</td>\n",
       "      <td>1.046455e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Data Type  Unique Values  count Missing  % Missing       Mode  Count Mode      % Mode          mean           std          min          25%          50%          75%           max\n",
       "UserID         int64           6040              0        0.0       4169        2314    0.231352  3.024512e+03  1.728413e+03          1.0       1506.0       3070.0       4476.0  6.040000e+03\n",
       "MovieID        int64           3706              0        0.0       2858        3428    0.342728  1.865540e+03  1.096041e+03          1.0       1030.0       1835.0       2770.0  3.952000e+03\n",
       "Rating         int64              5              0        0.0          4      348971     34.8898  3.581564e+00  1.117102e+00          1.0          3.0          4.0          4.0  5.000000e+00\n",
       "timestamps     int64         458455              0        0.0  975528402          30  0.00299937  9.722437e+08  1.215256e+07  956703932.0  965302637.0  973018006.0  975220939.0  1.046455e+09"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.describe(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ratings['timestamps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并三个表\n",
    "data = pd.merge(pd.merge(ratings, users), movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据分成X和y两张表\n",
    "target_fields = ['Rating']\n",
    "features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "\n",
    "features = features_pd.values\n",
    "targets_values = targets_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((title_count, title2int, genres2int, features, targets_values, ratings, users, movies, data), open('preprocess.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  OccupationID\n",
       "0       1       0    0            10\n",
       "1       2       1    5            16\n",
       "2       3       1    6            15\n",
       "3       4       1    2             7\n",
       "4       5       1    6            20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[5121, 1104, 5127, 5127, 5127, 5127, 5127, 512...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1579, 5127, 5127, 5127, 5127, 5127, 5127, 512...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1119, 238, 1008, 5127, 5127, 5127, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[3607, 4180, 1400, 5127, 5127, 5127, 5127, 512...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[4006, 3230, 3308, 2505, 1068, 3898, 5127, 512...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title                                             Genres\n",
       "0        1  [5121, 1104, 5127, 5127, 5127, 5127, 5127, 512...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, ...\n",
       "1        2  [1579, 5127, 5127, 5127, 5127, 5127, 5127, 512...  [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ...\n",
       "2        3  [1119, 238, 1008, 5127, 5127, 5127, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...\n",
       "3        4  [3607, 4180, 1400, 5127, 5127, 5127, 5127, 512...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, ...\n",
       "4        5  [4006, 3230, 3308, 2505, 1068, 3898, 5127, 512...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>[3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>[3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>[3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Gender  Age  OccupationID                                              Title                                             Genres\n",
       "0       1     1193       5       0    0            10  [3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       2     1193       5       1    5            16  [3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2      12     1193       4       1    6            12  [3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3      15     1193       4       1    6             7  [3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4      17     1193       5       1    3             1  [3592, 3839, 2500, 2141, 592, 4350, 5127, 5127...  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(data.UserID) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(data.Gender) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(data.Age) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(data.OccupationID) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(data.MovieID) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = len(genres2int) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title2int) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 18\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfapi构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 18], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0414 19:00:06.759146 140735557628800 deprecation.py:506] From <ipython-input-65-bb012f2abe28>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0414 19:00:06.963695 140735557628800 deprecation.py:323] From /Users/luoyonggui/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/luoyonggui/PycharmProjects/mayiexamples/RecommendatIon_System/runs/1586862129\n",
      "\n",
      "2020-04-14T19:02:15.827812: Epoch   0 Batch    0/3125   train_loss = 10.035\n",
      "2020-04-14T19:02:16.668557: Epoch   0 Batch   20/3125   train_loss = 4.024\n",
      "2020-04-14T19:02:17.476023: Epoch   0 Batch   40/3125   train_loss = 2.715\n",
      "2020-04-14T19:02:18.342124: Epoch   0 Batch   60/3125   train_loss = 2.043\n",
      "2020-04-14T19:02:19.168840: Epoch   0 Batch   80/3125   train_loss = 1.752\n",
      "2020-04-14T19:02:20.120446: Epoch   0 Batch  100/3125   train_loss = 1.695\n",
      "2020-04-14T19:02:20.909156: Epoch   0 Batch  120/3125   train_loss = 1.864\n",
      "2020-04-14T19:02:21.744214: Epoch   0 Batch  140/3125   train_loss = 1.645\n",
      "2020-04-14T19:02:22.586203: Epoch   0 Batch  160/3125   train_loss = 1.579\n",
      "2020-04-14T19:02:23.425597: Epoch   0 Batch  180/3125   train_loss = 1.511\n",
      "2020-04-14T19:02:24.229579: Epoch   0 Batch  200/3125   train_loss = 1.645\n",
      "2020-04-14T19:02:25.083944: Epoch   0 Batch  220/3125   train_loss = 1.430\n",
      "2020-04-14T19:02:25.907789: Epoch   0 Batch  240/3125   train_loss = 1.468\n",
      "2020-04-14T19:02:26.756173: Epoch   0 Batch  260/3125   train_loss = 1.393\n",
      "2020-04-14T19:02:27.598346: Epoch   0 Batch  280/3125   train_loss = 1.486\n",
      "2020-04-14T19:02:28.418611: Epoch   0 Batch  300/3125   train_loss = 1.481\n",
      "2020-04-14T19:02:29.245357: Epoch   0 Batch  320/3125   train_loss = 1.471\n",
      "2020-04-14T19:02:30.078401: Epoch   0 Batch  340/3125   train_loss = 1.295\n",
      "2020-04-14T19:02:30.977038: Epoch   0 Batch  360/3125   train_loss = 1.385\n",
      "2020-04-14T19:02:31.876349: Epoch   0 Batch  380/3125   train_loss = 1.310\n",
      "2020-04-14T19:02:32.839559: Epoch   0 Batch  400/3125   train_loss = 1.288\n",
      "2020-04-14T19:02:33.800573: Epoch   0 Batch  420/3125   train_loss = 1.216\n",
      "2020-04-14T19:02:34.759146: Epoch   0 Batch  440/3125   train_loss = 1.347\n",
      "2020-04-14T19:02:36.025806: Epoch   0 Batch  460/3125   train_loss = 1.351\n",
      "2020-04-14T19:02:37.218351: Epoch   0 Batch  480/3125   train_loss = 1.438\n",
      "2020-04-14T19:02:38.718094: Epoch   0 Batch  500/3125   train_loss = 0.971\n",
      "2020-04-14T19:02:40.382085: Epoch   0 Batch  520/3125   train_loss = 1.359\n",
      "2020-04-14T19:02:41.388084: Epoch   0 Batch  540/3125   train_loss = 1.232\n",
      "2020-04-14T19:02:42.189067: Epoch   0 Batch  560/3125   train_loss = 1.481\n",
      "2020-04-14T19:02:43.000087: Epoch   0 Batch  580/3125   train_loss = 1.451\n",
      "2020-04-14T19:02:43.806585: Epoch   0 Batch  600/3125   train_loss = 1.316\n",
      "2020-04-14T19:02:44.635668: Epoch   0 Batch  620/3125   train_loss = 1.462\n",
      "2020-04-14T19:02:45.433242: Epoch   0 Batch  640/3125   train_loss = 1.347\n",
      "2020-04-14T19:02:46.252035: Epoch   0 Batch  660/3125   train_loss = 1.315\n",
      "2020-04-14T19:02:47.146753: Epoch   0 Batch  680/3125   train_loss = 1.135\n",
      "2020-04-14T19:02:48.041894: Epoch   0 Batch  700/3125   train_loss = 1.291\n",
      "2020-04-14T19:02:48.901384: Epoch   0 Batch  720/3125   train_loss = 1.233\n",
      "2020-04-14T19:02:49.785494: Epoch   0 Batch  740/3125   train_loss = 1.388\n",
      "2020-04-14T19:02:50.733002: Epoch   0 Batch  760/3125   train_loss = 1.447\n",
      "2020-04-14T19:02:51.596972: Epoch   0 Batch  780/3125   train_loss = 1.471\n",
      "2020-04-14T19:02:52.523097: Epoch   0 Batch  800/3125   train_loss = 1.295\n",
      "2020-04-14T19:02:53.376876: Epoch   0 Batch  820/3125   train_loss = 1.250\n",
      "2020-04-14T19:02:54.253608: Epoch   0 Batch  840/3125   train_loss = 1.273\n",
      "2020-04-14T19:02:55.115681: Epoch   0 Batch  860/3125   train_loss = 1.201\n",
      "2020-04-14T19:02:55.954965: Epoch   0 Batch  880/3125   train_loss = 1.231\n",
      "2020-04-14T19:02:56.780485: Epoch   0 Batch  900/3125   train_loss = 1.238\n",
      "2020-04-14T19:02:57.658466: Epoch   0 Batch  920/3125   train_loss = 1.359\n",
      "2020-04-14T19:02:58.518024: Epoch   0 Batch  940/3125   train_loss = 1.494\n",
      "2020-04-14T19:02:59.385080: Epoch   0 Batch  960/3125   train_loss = 1.300\n",
      "2020-04-14T19:03:00.233890: Epoch   0 Batch  980/3125   train_loss = 1.421\n",
      "2020-04-14T19:03:01.060392: Epoch   0 Batch 1000/3125   train_loss = 1.369\n",
      "2020-04-14T19:03:01.884948: Epoch   0 Batch 1020/3125   train_loss = 1.342\n",
      "2020-04-14T19:03:02.918986: Epoch   0 Batch 1040/3125   train_loss = 1.209\n",
      "2020-04-14T19:03:03.809025: Epoch   0 Batch 1060/3125   train_loss = 1.433\n",
      "2020-04-14T19:03:04.706075: Epoch   0 Batch 1080/3125   train_loss = 1.215\n",
      "2020-04-14T19:03:05.549667: Epoch   0 Batch 1100/3125   train_loss = 1.295\n",
      "2020-04-14T19:03:06.378812: Epoch   0 Batch 1120/3125   train_loss = 1.195\n",
      "2020-04-14T19:03:07.226916: Epoch   0 Batch 1140/3125   train_loss = 1.302\n",
      "2020-04-14T19:03:08.034029: Epoch   0 Batch 1160/3125   train_loss = 1.319\n",
      "2020-04-14T19:03:08.860731: Epoch   0 Batch 1180/3125   train_loss = 1.234\n",
      "2020-04-14T19:03:09.688945: Epoch   0 Batch 1200/3125   train_loss = 1.224\n",
      "2020-04-14T19:03:10.545593: Epoch   0 Batch 1220/3125   train_loss = 1.234\n",
      "2020-04-14T19:03:11.394142: Epoch   0 Batch 1240/3125   train_loss = 1.150\n",
      "2020-04-14T19:03:12.249217: Epoch   0 Batch 1260/3125   train_loss = 1.322\n",
      "2020-04-14T19:03:13.072027: Epoch   0 Batch 1280/3125   train_loss = 1.345\n",
      "2020-04-14T19:03:14.063109: Epoch   0 Batch 1300/3125   train_loss = 1.298\n",
      "2020-04-14T19:03:15.073917: Epoch   0 Batch 1320/3125   train_loss = 1.347\n",
      "2020-04-14T19:03:16.147409: Epoch   0 Batch 1340/3125   train_loss = 1.122\n",
      "2020-04-14T19:03:17.159154: Epoch   0 Batch 1360/3125   train_loss = 1.120\n",
      "2020-04-14T19:03:18.265547: Epoch   0 Batch 1380/3125   train_loss = 1.083\n",
      "2020-04-14T19:03:19.151164: Epoch   0 Batch 1400/3125   train_loss = 1.291\n",
      "2020-04-14T19:03:19.935936: Epoch   0 Batch 1420/3125   train_loss = 1.356\n",
      "2020-04-14T19:03:20.740092: Epoch   0 Batch 1440/3125   train_loss = 1.216\n",
      "2020-04-14T19:03:21.608636: Epoch   0 Batch 1460/3125   train_loss = 1.256\n",
      "2020-04-14T19:03:22.741359: Epoch   0 Batch 1480/3125   train_loss = 1.315\n",
      "2020-04-14T19:03:23.824720: Epoch   0 Batch 1500/3125   train_loss = 1.335\n",
      "2020-04-14T19:03:24.757320: Epoch   0 Batch 1520/3125   train_loss = 1.279\n",
      "2020-04-14T19:03:25.699743: Epoch   0 Batch 1540/3125   train_loss = 1.282\n",
      "2020-04-14T19:03:26.556489: Epoch   0 Batch 1560/3125   train_loss = 1.182\n",
      "2020-04-14T19:03:27.443531: Epoch   0 Batch 1580/3125   train_loss = 1.277\n",
      "2020-04-14T19:03:28.243926: Epoch   0 Batch 1600/3125   train_loss = 1.318\n",
      "2020-04-14T19:03:29.114276: Epoch   0 Batch 1620/3125   train_loss = 1.286\n",
      "2020-04-14T19:03:29.912773: Epoch   0 Batch 1640/3125   train_loss = 1.325\n",
      "2020-04-14T19:03:30.758758: Epoch   0 Batch 1660/3125   train_loss = 1.302\n",
      "2020-04-14T19:03:31.571050: Epoch   0 Batch 1680/3125   train_loss = 1.233\n",
      "2020-04-14T19:03:32.407542: Epoch   0 Batch 1700/3125   train_loss = 1.096\n",
      "2020-04-14T19:03:33.270990: Epoch   0 Batch 1720/3125   train_loss = 1.085\n",
      "2020-04-14T19:03:34.089090: Epoch   0 Batch 1740/3125   train_loss = 1.225\n",
      "2020-04-14T19:03:35.049942: Epoch   0 Batch 1760/3125   train_loss = 1.298\n",
      "2020-04-14T19:03:35.878549: Epoch   0 Batch 1780/3125   train_loss = 1.078\n",
      "2020-04-14T19:03:36.794789: Epoch   0 Batch 1800/3125   train_loss = 1.216\n",
      "2020-04-14T19:03:37.681132: Epoch   0 Batch 1820/3125   train_loss = 1.249\n",
      "2020-04-14T19:03:38.549822: Epoch   0 Batch 1840/3125   train_loss = 1.291\n",
      "2020-04-14T19:03:39.497613: Epoch   0 Batch 1860/3125   train_loss = 1.279\n",
      "2020-04-14T19:03:40.392147: Epoch   0 Batch 1880/3125   train_loss = 1.226\n",
      "2020-04-14T19:03:41.220433: Epoch   0 Batch 1900/3125   train_loss = 1.102\n",
      "2020-04-14T19:03:42.139523: Epoch   0 Batch 1920/3125   train_loss = 1.233\n",
      "2020-04-14T19:03:43.193252: Epoch   0 Batch 1940/3125   train_loss = 1.059\n",
      "2020-04-14T19:03:44.130715: Epoch   0 Batch 1960/3125   train_loss = 1.167\n",
      "2020-04-14T19:03:45.292921: Epoch   0 Batch 1980/3125   train_loss = 1.164\n",
      "2020-04-14T19:03:46.605850: Epoch   0 Batch 2000/3125   train_loss = 1.391\n",
      "2020-04-14T19:03:47.472048: Epoch   0 Batch 2020/3125   train_loss = 1.331\n",
      "2020-04-14T19:03:48.270064: Epoch   0 Batch 2040/3125   train_loss = 1.098\n",
      "2020-04-14T19:03:49.138174: Epoch   0 Batch 2060/3125   train_loss = 1.038\n",
      "2020-04-14T19:03:49.925815: Epoch   0 Batch 2080/3125   train_loss = 1.400\n",
      "2020-04-14T19:03:50.756184: Epoch   0 Batch 2100/3125   train_loss = 1.152\n",
      "2020-04-14T19:03:51.616387: Epoch   0 Batch 2120/3125   train_loss = 1.112\n",
      "2020-04-14T19:03:52.465497: Epoch   0 Batch 2140/3125   train_loss = 1.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:03:53.318237: Epoch   0 Batch 2160/3125   train_loss = 1.163\n",
      "2020-04-14T19:03:54.160136: Epoch   0 Batch 2180/3125   train_loss = 1.179\n",
      "2020-04-14T19:03:55.021606: Epoch   0 Batch 2200/3125   train_loss = 1.136\n",
      "2020-04-14T19:03:55.905619: Epoch   0 Batch 2220/3125   train_loss = 1.211\n",
      "2020-04-14T19:03:56.815538: Epoch   0 Batch 2240/3125   train_loss = 1.007\n",
      "2020-04-14T19:03:57.750232: Epoch   0 Batch 2260/3125   train_loss = 1.156\n",
      "2020-04-14T19:03:58.607664: Epoch   0 Batch 2280/3125   train_loss = 1.201\n",
      "2020-04-14T19:03:59.462853: Epoch   0 Batch 2300/3125   train_loss = 1.197\n",
      "2020-04-14T19:04:00.353768: Epoch   0 Batch 2320/3125   train_loss = 1.302\n",
      "2020-04-14T19:04:01.210700: Epoch   0 Batch 2340/3125   train_loss = 1.218\n",
      "2020-04-14T19:04:02.084197: Epoch   0 Batch 2360/3125   train_loss = 1.226\n",
      "2020-04-14T19:04:02.932981: Epoch   0 Batch 2380/3125   train_loss = 1.180\n",
      "2020-04-14T19:04:03.784872: Epoch   0 Batch 2400/3125   train_loss = 1.324\n",
      "2020-04-14T19:04:04.762297: Epoch   0 Batch 2420/3125   train_loss = 1.093\n",
      "2020-04-14T19:04:05.611713: Epoch   0 Batch 2440/3125   train_loss = 1.300\n",
      "2020-04-14T19:04:06.596861: Epoch   0 Batch 2460/3125   train_loss = 1.087\n",
      "2020-04-14T19:04:07.641760: Epoch   0 Batch 2480/3125   train_loss = 1.126\n",
      "2020-04-14T19:04:08.679837: Epoch   0 Batch 2500/3125   train_loss = 1.106\n",
      "2020-04-14T19:04:09.705861: Epoch   0 Batch 2520/3125   train_loss = 1.132\n",
      "2020-04-14T19:04:10.750028: Epoch   0 Batch 2540/3125   train_loss = 1.057\n",
      "2020-04-14T19:04:11.785414: Epoch   0 Batch 2560/3125   train_loss = 0.923\n",
      "2020-04-14T19:04:12.622491: Epoch   0 Batch 2580/3125   train_loss = 1.089\n",
      "2020-04-14T19:04:13.609468: Epoch   0 Batch 2600/3125   train_loss = 1.149\n",
      "2020-04-14T19:04:14.468842: Epoch   0 Batch 2620/3125   train_loss = 1.097\n",
      "2020-04-14T19:04:15.374808: Epoch   0 Batch 2640/3125   train_loss = 1.101\n",
      "2020-04-14T19:04:16.250229: Epoch   0 Batch 2660/3125   train_loss = 1.168\n",
      "2020-04-14T19:04:17.132499: Epoch   0 Batch 2680/3125   train_loss = 1.037\n",
      "2020-04-14T19:04:17.994648: Epoch   0 Batch 2700/3125   train_loss = 1.255\n",
      "2020-04-14T19:04:18.902786: Epoch   0 Batch 2720/3125   train_loss = 1.011\n",
      "2020-04-14T19:04:19.776676: Epoch   0 Batch 2740/3125   train_loss = 1.193\n",
      "2020-04-14T19:04:20.647138: Epoch   0 Batch 2760/3125   train_loss = 1.154\n",
      "2020-04-14T19:04:21.466204: Epoch   0 Batch 2780/3125   train_loss = 1.109\n",
      "2020-04-14T19:04:22.280731: Epoch   0 Batch 2800/3125   train_loss = 1.377\n",
      "2020-04-14T19:04:23.088194: Epoch   0 Batch 2820/3125   train_loss = 1.401\n",
      "2020-04-14T19:04:23.915991: Epoch   0 Batch 2840/3125   train_loss = 1.210\n",
      "2020-04-14T19:04:24.747806: Epoch   0 Batch 2860/3125   train_loss = 1.104\n",
      "2020-04-14T19:04:25.758614: Epoch   0 Batch 2880/3125   train_loss = 1.142\n",
      "2020-04-14T19:04:26.601064: Epoch   0 Batch 2900/3125   train_loss = 1.233\n",
      "2020-04-14T19:04:27.514665: Epoch   0 Batch 2920/3125   train_loss = 1.158\n",
      "2020-04-14T19:04:28.523476: Epoch   0 Batch 2940/3125   train_loss = 1.189\n",
      "2020-04-14T19:04:29.384397: Epoch   0 Batch 2960/3125   train_loss = 1.211\n",
      "2020-04-14T19:04:30.280952: Epoch   0 Batch 2980/3125   train_loss = 1.139\n",
      "2020-04-14T19:04:31.130555: Epoch   0 Batch 3000/3125   train_loss = 1.167\n",
      "2020-04-14T19:04:31.972450: Epoch   0 Batch 3020/3125   train_loss = 1.192\n",
      "2020-04-14T19:04:32.778071: Epoch   0 Batch 3040/3125   train_loss = 1.130\n",
      "2020-04-14T19:04:33.596559: Epoch   0 Batch 3060/3125   train_loss = 1.146\n",
      "2020-04-14T19:04:34.440424: Epoch   0 Batch 3080/3125   train_loss = 1.255\n",
      "2020-04-14T19:04:35.429587: Epoch   0 Batch 3100/3125   train_loss = 1.151\n",
      "2020-04-14T19:04:36.306290: Epoch   0 Batch 3120/3125   train_loss = 1.002\n",
      "2020-04-14T19:04:36.958022: Epoch   0 Batch    0/781   test_loss = 0.987\n",
      "2020-04-14T19:04:37.284600: Epoch   0 Batch   20/781   test_loss = 1.153\n",
      "2020-04-14T19:04:37.573263: Epoch   0 Batch   40/781   test_loss = 1.103\n",
      "2020-04-14T19:04:37.888664: Epoch   0 Batch   60/781   test_loss = 1.301\n",
      "2020-04-14T19:04:38.242595: Epoch   0 Batch   80/781   test_loss = 1.382\n",
      "2020-04-14T19:04:38.615308: Epoch   0 Batch  100/781   test_loss = 1.357\n",
      "2020-04-14T19:04:38.913753: Epoch   0 Batch  120/781   test_loss = 1.189\n",
      "2020-04-14T19:04:39.204945: Epoch   0 Batch  140/781   test_loss = 1.249\n",
      "2020-04-14T19:04:39.461754: Epoch   0 Batch  160/781   test_loss = 1.401\n",
      "2020-04-14T19:04:39.742326: Epoch   0 Batch  180/781   test_loss = 1.193\n",
      "2020-04-14T19:04:40.029348: Epoch   0 Batch  200/781   test_loss = 1.197\n",
      "2020-04-14T19:04:40.307673: Epoch   0 Batch  220/781   test_loss = 0.961\n",
      "2020-04-14T19:04:40.569218: Epoch   0 Batch  240/781   test_loss = 1.192\n",
      "2020-04-14T19:04:40.841326: Epoch   0 Batch  260/781   test_loss = 1.169\n",
      "2020-04-14T19:04:41.110418: Epoch   0 Batch  280/781   test_loss = 1.419\n",
      "2020-04-14T19:04:41.508980: Epoch   0 Batch  300/781   test_loss = 1.193\n",
      "2020-04-14T19:04:42.199903: Epoch   0 Batch  320/781   test_loss = 1.428\n",
      "2020-04-14T19:04:42.802581: Epoch   0 Batch  340/781   test_loss = 0.878\n",
      "2020-04-14T19:04:43.366735: Epoch   0 Batch  360/781   test_loss = 1.339\n",
      "2020-04-14T19:04:44.032659: Epoch   0 Batch  380/781   test_loss = 1.202\n",
      "2020-04-14T19:04:44.361488: Epoch   0 Batch  400/781   test_loss = 1.115\n",
      "2020-04-14T19:04:44.655225: Epoch   0 Batch  420/781   test_loss = 1.046\n",
      "2020-04-14T19:04:44.866119: Epoch   0 Batch  440/781   test_loss = 1.274\n",
      "2020-04-14T19:04:45.130344: Epoch   0 Batch  460/781   test_loss = 1.167\n",
      "2020-04-14T19:04:45.386270: Epoch   0 Batch  480/781   test_loss = 1.041\n",
      "2020-04-14T19:04:45.618667: Epoch   0 Batch  500/781   test_loss = 0.994\n",
      "2020-04-14T19:04:45.875082: Epoch   0 Batch  520/781   test_loss = 1.168\n",
      "2020-04-14T19:04:46.137415: Epoch   0 Batch  540/781   test_loss = 1.082\n",
      "2020-04-14T19:04:46.388023: Epoch   0 Batch  560/781   test_loss = 1.301\n",
      "2020-04-14T19:04:46.586727: Epoch   0 Batch  580/781   test_loss = 1.178\n",
      "2020-04-14T19:04:46.802428: Epoch   0 Batch  600/781   test_loss = 1.164\n",
      "2020-04-14T19:04:47.060349: Epoch   0 Batch  620/781   test_loss = 1.188\n",
      "2020-04-14T19:04:47.297562: Epoch   0 Batch  640/781   test_loss = 1.255\n",
      "2020-04-14T19:04:47.580394: Epoch   0 Batch  660/781   test_loss = 1.111\n",
      "2020-04-14T19:04:47.867447: Epoch   0 Batch  680/781   test_loss = 1.450\n",
      "2020-04-14T19:04:48.183136: Epoch   0 Batch  700/781   test_loss = 1.099\n",
      "2020-04-14T19:04:48.488387: Epoch   0 Batch  720/781   test_loss = 1.240\n",
      "2020-04-14T19:04:48.806370: Epoch   0 Batch  740/781   test_loss = 1.219\n",
      "2020-04-14T19:04:49.105880: Epoch   0 Batch  760/781   test_loss = 1.185\n",
      "2020-04-14T19:04:49.403858: Epoch   0 Batch  780/781   test_loss = 1.147\n",
      "2020-04-14T19:04:51.459726: Epoch   1 Batch   15/3125   train_loss = 1.185\n",
      "2020-04-14T19:04:52.319360: Epoch   1 Batch   35/3125   train_loss = 1.091\n",
      "2020-04-14T19:04:53.145310: Epoch   1 Batch   55/3125   train_loss = 1.209\n",
      "2020-04-14T19:04:53.963664: Epoch   1 Batch   75/3125   train_loss = 1.145\n",
      "2020-04-14T19:04:54.839313: Epoch   1 Batch   95/3125   train_loss = 0.969\n",
      "2020-04-14T19:04:55.758603: Epoch   1 Batch  115/3125   train_loss = 1.172\n",
      "2020-04-14T19:04:56.614398: Epoch   1 Batch  135/3125   train_loss = 1.033\n",
      "2020-04-14T19:04:57.457105: Epoch   1 Batch  155/3125   train_loss = 1.133\n",
      "2020-04-14T19:04:58.313426: Epoch   1 Batch  175/3125   train_loss = 1.081\n",
      "2020-04-14T19:04:59.139805: Epoch   1 Batch  195/3125   train_loss = 1.164\n",
      "2020-04-14T19:05:00.019626: Epoch   1 Batch  215/3125   train_loss = 1.113\n",
      "2020-04-14T19:05:00.882795: Epoch   1 Batch  235/3125   train_loss = 1.100\n",
      "2020-04-14T19:05:01.744387: Epoch   1 Batch  255/3125   train_loss = 1.265\n",
      "2020-04-14T19:05:02.621660: Epoch   1 Batch  275/3125   train_loss = 1.006\n",
      "2020-04-14T19:05:03.470304: Epoch   1 Batch  295/3125   train_loss = 0.967\n",
      "2020-04-14T19:05:04.348396: Epoch   1 Batch  315/3125   train_loss = 1.099\n",
      "2020-04-14T19:05:05.212259: Epoch   1 Batch  335/3125   train_loss = 1.004\n",
      "2020-04-14T19:05:06.128526: Epoch   1 Batch  355/3125   train_loss = 1.124\n",
      "2020-04-14T19:05:06.982535: Epoch   1 Batch  375/3125   train_loss = 1.171\n",
      "2020-04-14T19:05:07.845085: Epoch   1 Batch  395/3125   train_loss = 1.067\n",
      "2020-04-14T19:05:08.762721: Epoch   1 Batch  415/3125   train_loss = 1.227\n",
      "2020-04-14T19:05:09.677227: Epoch   1 Batch  435/3125   train_loss = 1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:05:10.598509: Epoch   1 Batch  455/3125   train_loss = 1.177\n",
      "2020-04-14T19:05:11.557245: Epoch   1 Batch  475/3125   train_loss = 1.228\n",
      "2020-04-14T19:05:12.584240: Epoch   1 Batch  495/3125   train_loss = 1.068\n",
      "2020-04-14T19:05:13.479683: Epoch   1 Batch  515/3125   train_loss = 1.143\n",
      "2020-04-14T19:05:14.440584: Epoch   1 Batch  535/3125   train_loss = 1.218\n",
      "2020-04-14T19:05:15.373553: Epoch   1 Batch  555/3125   train_loss = 1.215\n",
      "2020-04-14T19:05:16.274203: Epoch   1 Batch  575/3125   train_loss = 1.107\n",
      "2020-04-14T19:05:17.146574: Epoch   1 Batch  595/3125   train_loss = 1.237\n",
      "2020-04-14T19:05:18.099461: Epoch   1 Batch  615/3125   train_loss = 1.029\n",
      "2020-04-14T19:05:19.011230: Epoch   1 Batch  635/3125   train_loss = 1.107\n",
      "2020-04-14T19:05:19.840440: Epoch   1 Batch  655/3125   train_loss = 0.969\n",
      "2020-04-14T19:05:20.670872: Epoch   1 Batch  675/3125   train_loss = 0.925\n",
      "2020-04-14T19:05:21.553484: Epoch   1 Batch  695/3125   train_loss = 1.140\n",
      "2020-04-14T19:05:22.386753: Epoch   1 Batch  715/3125   train_loss = 1.144\n",
      "2020-04-14T19:05:23.337931: Epoch   1 Batch  735/3125   train_loss = 0.995\n",
      "2020-04-14T19:05:24.187039: Epoch   1 Batch  755/3125   train_loss = 1.191\n",
      "2020-04-14T19:05:25.054198: Epoch   1 Batch  775/3125   train_loss = 1.016\n",
      "2020-04-14T19:05:26.057838: Epoch   1 Batch  795/3125   train_loss = 1.236\n",
      "2020-04-14T19:05:26.995640: Epoch   1 Batch  815/3125   train_loss = 1.108\n",
      "2020-04-14T19:05:27.901919: Epoch   1 Batch  835/3125   train_loss = 1.041\n",
      "2020-04-14T19:05:28.986132: Epoch   1 Batch  855/3125   train_loss = 1.237\n",
      "2020-04-14T19:05:29.887004: Epoch   1 Batch  875/3125   train_loss = 1.206\n",
      "2020-04-14T19:05:30.808320: Epoch   1 Batch  895/3125   train_loss = 1.015\n",
      "2020-04-14T19:05:31.781817: Epoch   1 Batch  915/3125   train_loss = 1.130\n",
      "2020-04-14T19:05:32.662802: Epoch   1 Batch  935/3125   train_loss = 1.200\n",
      "2020-04-14T19:05:33.568603: Epoch   1 Batch  955/3125   train_loss = 1.128\n",
      "2020-04-14T19:05:34.518916: Epoch   1 Batch  975/3125   train_loss = 1.084\n",
      "2020-04-14T19:05:35.505948: Epoch   1 Batch  995/3125   train_loss = 0.912\n",
      "2020-04-14T19:05:36.386149: Epoch   1 Batch 1015/3125   train_loss = 1.195\n",
      "2020-04-14T19:05:37.196971: Epoch   1 Batch 1035/3125   train_loss = 1.124\n",
      "2020-04-14T19:05:38.041620: Epoch   1 Batch 1055/3125   train_loss = 1.079\n",
      "2020-04-14T19:05:38.880340: Epoch   1 Batch 1075/3125   train_loss = 1.027\n",
      "2020-04-14T19:05:39.677750: Epoch   1 Batch 1095/3125   train_loss = 0.993\n",
      "2020-04-14T19:05:40.587652: Epoch   1 Batch 1115/3125   train_loss = 1.081\n",
      "2020-04-14T19:05:41.669344: Epoch   1 Batch 1135/3125   train_loss = 0.995\n",
      "2020-04-14T19:05:42.520695: Epoch   1 Batch 1155/3125   train_loss = 1.118\n",
      "2020-04-14T19:05:43.543777: Epoch   1 Batch 1175/3125   train_loss = 1.111\n",
      "2020-04-14T19:05:44.426198: Epoch   1 Batch 1195/3125   train_loss = 1.271\n",
      "2020-04-14T19:05:45.358042: Epoch   1 Batch 1215/3125   train_loss = 0.922\n",
      "2020-04-14T19:05:46.252857: Epoch   1 Batch 1235/3125   train_loss = 1.125\n",
      "2020-04-14T19:05:47.124277: Epoch   1 Batch 1255/3125   train_loss = 0.990\n",
      "2020-04-14T19:05:48.017657: Epoch   1 Batch 1275/3125   train_loss = 0.929\n",
      "2020-04-14T19:05:48.892817: Epoch   1 Batch 1295/3125   train_loss = 1.128\n",
      "2020-04-14T19:05:49.786964: Epoch   1 Batch 1315/3125   train_loss = 1.202\n",
      "2020-04-14T19:05:50.620227: Epoch   1 Batch 1335/3125   train_loss = 1.067\n",
      "2020-04-14T19:05:51.477376: Epoch   1 Batch 1355/3125   train_loss = 1.011\n",
      "2020-04-14T19:05:52.365786: Epoch   1 Batch 1375/3125   train_loss = 1.124\n",
      "2020-04-14T19:05:53.389632: Epoch   1 Batch 1395/3125   train_loss = 1.064\n",
      "2020-04-14T19:05:54.258046: Epoch   1 Batch 1415/3125   train_loss = 1.100\n",
      "2020-04-14T19:05:55.186994: Epoch   1 Batch 1435/3125   train_loss = 1.137\n",
      "2020-04-14T19:05:56.184237: Epoch   1 Batch 1455/3125   train_loss = 1.149\n",
      "2020-04-14T19:05:57.168289: Epoch   1 Batch 1475/3125   train_loss = 1.110\n",
      "2020-04-14T19:05:58.080505: Epoch   1 Batch 1495/3125   train_loss = 1.062\n",
      "2020-04-14T19:05:58.901232: Epoch   1 Batch 1515/3125   train_loss = 1.002\n",
      "2020-04-14T19:05:59.783392: Epoch   1 Batch 1535/3125   train_loss = 0.942\n",
      "2020-04-14T19:06:00.637346: Epoch   1 Batch 1555/3125   train_loss = 1.137\n",
      "2020-04-14T19:06:01.514434: Epoch   1 Batch 1575/3125   train_loss = 1.012\n",
      "2020-04-14T19:06:02.413575: Epoch   1 Batch 1595/3125   train_loss = 1.123\n",
      "2020-04-14T19:06:03.378804: Epoch   1 Batch 1615/3125   train_loss = 1.053\n",
      "2020-04-14T19:06:04.376901: Epoch   1 Batch 1635/3125   train_loss = 1.155\n",
      "2020-04-14T19:06:05.238961: Epoch   1 Batch 1655/3125   train_loss = 1.182\n",
      "2020-04-14T19:06:06.111064: Epoch   1 Batch 1675/3125   train_loss = 1.063\n",
      "2020-04-14T19:06:07.020197: Epoch   1 Batch 1695/3125   train_loss = 1.000\n",
      "2020-04-14T19:06:07.920291: Epoch   1 Batch 1715/3125   train_loss = 0.915\n",
      "2020-04-14T19:06:08.798367: Epoch   1 Batch 1735/3125   train_loss = 1.256\n",
      "2020-04-14T19:06:09.764527: Epoch   1 Batch 1755/3125   train_loss = 1.043\n",
      "2020-04-14T19:06:10.730771: Epoch   1 Batch 1775/3125   train_loss = 1.100\n",
      "2020-04-14T19:06:11.705866: Epoch   1 Batch 1795/3125   train_loss = 1.018\n",
      "2020-04-14T19:06:12.526543: Epoch   1 Batch 1815/3125   train_loss = 1.041\n",
      "2020-04-14T19:06:13.331700: Epoch   1 Batch 1835/3125   train_loss = 1.156\n",
      "2020-04-14T19:06:14.193755: Epoch   1 Batch 1855/3125   train_loss = 1.036\n",
      "2020-04-14T19:06:14.991927: Epoch   1 Batch 1875/3125   train_loss = 1.136\n",
      "2020-04-14T19:06:15.869405: Epoch   1 Batch 1895/3125   train_loss = 0.997\n",
      "2020-04-14T19:06:16.641296: Epoch   1 Batch 1915/3125   train_loss = 0.964\n",
      "2020-04-14T19:06:17.462390: Epoch   1 Batch 1935/3125   train_loss = 0.992\n",
      "2020-04-14T19:06:18.252275: Epoch   1 Batch 1955/3125   train_loss = 0.950\n",
      "2020-04-14T19:06:19.068427: Epoch   1 Batch 1975/3125   train_loss = 1.033\n",
      "2020-04-14T19:06:19.874652: Epoch   1 Batch 1995/3125   train_loss = 1.180\n",
      "2020-04-14T19:06:20.699086: Epoch   1 Batch 2015/3125   train_loss = 1.157\n",
      "2020-04-14T19:06:21.667951: Epoch   1 Batch 2035/3125   train_loss = 1.099\n",
      "2020-04-14T19:06:22.507993: Epoch   1 Batch 2055/3125   train_loss = 0.969\n",
      "2020-04-14T19:06:23.348964: Epoch   1 Batch 2075/3125   train_loss = 1.109\n",
      "2020-04-14T19:06:24.223589: Epoch   1 Batch 2095/3125   train_loss = 0.957\n",
      "2020-04-14T19:06:25.080789: Epoch   1 Batch 2115/3125   train_loss = 1.097\n",
      "2020-04-14T19:06:25.901042: Epoch   1 Batch 2135/3125   train_loss = 1.027\n",
      "2020-04-14T19:06:26.790986: Epoch   1 Batch 2155/3125   train_loss = 1.010\n",
      "2020-04-14T19:06:27.660837: Epoch   1 Batch 2175/3125   train_loss = 1.041\n",
      "2020-04-14T19:06:28.508895: Epoch   1 Batch 2195/3125   train_loss = 1.062\n",
      "2020-04-14T19:06:29.424637: Epoch   1 Batch 2215/3125   train_loss = 1.030\n",
      "2020-04-14T19:06:30.404028: Epoch   1 Batch 2235/3125   train_loss = 1.189\n",
      "2020-04-14T19:06:31.218325: Epoch   1 Batch 2255/3125   train_loss = 1.089\n",
      "2020-04-14T19:06:32.035481: Epoch   1 Batch 2275/3125   train_loss = 0.929\n",
      "2020-04-14T19:06:32.884063: Epoch   1 Batch 2295/3125   train_loss = 1.280\n",
      "2020-04-14T19:06:33.708048: Epoch   1 Batch 2315/3125   train_loss = 1.289\n",
      "2020-04-14T19:06:34.507423: Epoch   1 Batch 2335/3125   train_loss = 1.071\n",
      "2020-04-14T19:06:36.014037: Epoch   1 Batch 2355/3125   train_loss = 1.002\n",
      "2020-04-14T19:06:37.143571: Epoch   1 Batch 2375/3125   train_loss = 1.321\n",
      "2020-04-14T19:06:38.512180: Epoch   1 Batch 2395/3125   train_loss = 1.017\n",
      "2020-04-14T19:06:39.657517: Epoch   1 Batch 2415/3125   train_loss = 1.131\n",
      "2020-04-14T19:06:40.447607: Epoch   1 Batch 2435/3125   train_loss = 1.073\n",
      "2020-04-14T19:06:41.261935: Epoch   1 Batch 2455/3125   train_loss = 1.143\n",
      "2020-04-14T19:06:42.504099: Epoch   1 Batch 2475/3125   train_loss = 0.991\n",
      "2020-04-14T19:06:43.301088: Epoch   1 Batch 2495/3125   train_loss = 0.996\n",
      "2020-04-14T19:06:44.179156: Epoch   1 Batch 2515/3125   train_loss = 1.062\n",
      "2020-04-14T19:06:45.077505: Epoch   1 Batch 2535/3125   train_loss = 1.138\n",
      "2020-04-14T19:06:46.190777: Epoch   1 Batch 2555/3125   train_loss = 0.928\n",
      "2020-04-14T19:06:47.175518: Epoch   1 Batch 2575/3125   train_loss = 0.904\n",
      "2020-04-14T19:06:48.214082: Epoch   1 Batch 2595/3125   train_loss = 1.030\n",
      "2020-04-14T19:06:49.277966: Epoch   1 Batch 2615/3125   train_loss = 1.140\n",
      "2020-04-14T19:06:50.331058: Epoch   1 Batch 2635/3125   train_loss = 0.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:06:51.422022: Epoch   1 Batch 2655/3125   train_loss = 1.046\n",
      "2020-04-14T19:06:52.410885: Epoch   1 Batch 2675/3125   train_loss = 1.042\n",
      "2020-04-14T19:06:53.453820: Epoch   1 Batch 2695/3125   train_loss = 1.059\n",
      "2020-04-14T19:06:54.390573: Epoch   1 Batch 2715/3125   train_loss = 0.992\n",
      "2020-04-14T19:06:55.462299: Epoch   1 Batch 2735/3125   train_loss = 0.844\n",
      "2020-04-14T19:06:56.347774: Epoch   1 Batch 2755/3125   train_loss = 1.005\n",
      "2020-04-14T19:06:57.168544: Epoch   1 Batch 2775/3125   train_loss = 1.053\n",
      "2020-04-14T19:06:58.005171: Epoch   1 Batch 2795/3125   train_loss = 1.053\n",
      "2020-04-14T19:06:58.816944: Epoch   1 Batch 2815/3125   train_loss = 1.035\n",
      "2020-04-14T19:06:59.754493: Epoch   1 Batch 2835/3125   train_loss = 1.136\n",
      "2020-04-14T19:07:00.804820: Epoch   1 Batch 2855/3125   train_loss = 1.018\n",
      "2020-04-14T19:07:01.926605: Epoch   1 Batch 2875/3125   train_loss = 1.035\n",
      "2020-04-14T19:07:02.926703: Epoch   1 Batch 2895/3125   train_loss = 1.018\n",
      "2020-04-14T19:07:03.905838: Epoch   1 Batch 2915/3125   train_loss = 0.948\n",
      "2020-04-14T19:07:05.040343: Epoch   1 Batch 2935/3125   train_loss = 1.096\n",
      "2020-04-14T19:07:05.958143: Epoch   1 Batch 2955/3125   train_loss = 1.069\n",
      "2020-04-14T19:07:06.833249: Epoch   1 Batch 2975/3125   train_loss = 0.967\n",
      "2020-04-14T19:07:07.642313: Epoch   1 Batch 2995/3125   train_loss = 0.955\n",
      "2020-04-14T19:07:08.450687: Epoch   1 Batch 3015/3125   train_loss = 0.973\n",
      "2020-04-14T19:07:09.276300: Epoch   1 Batch 3035/3125   train_loss = 1.040\n",
      "2020-04-14T19:07:10.165475: Epoch   1 Batch 3055/3125   train_loss = 1.120\n",
      "2020-04-14T19:07:10.976232: Epoch   1 Batch 3075/3125   train_loss = 0.934\n",
      "2020-04-14T19:07:11.843189: Epoch   1 Batch 3095/3125   train_loss = 1.016\n",
      "2020-04-14T19:07:12.664252: Epoch   1 Batch 3115/3125   train_loss = 0.873\n",
      "2020-04-14T19:07:13.294653: Epoch   1 Batch   19/781   test_loss = 1.073\n",
      "2020-04-14T19:07:13.598426: Epoch   1 Batch   39/781   test_loss = 0.881\n",
      "2020-04-14T19:07:13.958726: Epoch   1 Batch   59/781   test_loss = 0.872\n",
      "2020-04-14T19:07:14.271864: Epoch   1 Batch   79/781   test_loss = 1.042\n",
      "2020-04-14T19:07:14.575579: Epoch   1 Batch   99/781   test_loss = 1.036\n",
      "2020-04-14T19:07:14.954967: Epoch   1 Batch  119/781   test_loss = 0.954\n",
      "2020-04-14T19:07:15.270635: Epoch   1 Batch  139/781   test_loss = 1.017\n",
      "2020-04-14T19:07:15.600568: Epoch   1 Batch  159/781   test_loss = 1.028\n",
      "2020-04-14T19:07:15.964971: Epoch   1 Batch  179/781   test_loss = 0.934\n",
      "2020-04-14T19:07:16.312788: Epoch   1 Batch  199/781   test_loss = 0.976\n",
      "2020-04-14T19:07:16.597867: Epoch   1 Batch  219/781   test_loss = 1.086\n",
      "2020-04-14T19:07:16.962690: Epoch   1 Batch  239/781   test_loss = 1.206\n",
      "2020-04-14T19:07:17.290278: Epoch   1 Batch  259/781   test_loss = 0.973\n",
      "2020-04-14T19:07:17.621344: Epoch   1 Batch  279/781   test_loss = 1.128\n",
      "2020-04-14T19:07:17.985781: Epoch   1 Batch  299/781   test_loss = 1.246\n",
      "2020-04-14T19:07:18.354737: Epoch   1 Batch  319/781   test_loss = 0.962\n",
      "2020-04-14T19:07:18.668130: Epoch   1 Batch  339/781   test_loss = 0.877\n",
      "2020-04-14T19:07:19.047153: Epoch   1 Batch  359/781   test_loss = 0.940\n",
      "2020-04-14T19:07:19.425923: Epoch   1 Batch  379/781   test_loss = 1.099\n",
      "2020-04-14T19:07:19.733971: Epoch   1 Batch  399/781   test_loss = 0.899\n",
      "2020-04-14T19:07:20.041753: Epoch   1 Batch  419/781   test_loss = 0.965\n",
      "2020-04-14T19:07:20.343431: Epoch   1 Batch  439/781   test_loss = 1.009\n",
      "2020-04-14T19:07:20.665354: Epoch   1 Batch  459/781   test_loss = 1.119\n",
      "2020-04-14T19:07:21.038824: Epoch   1 Batch  479/781   test_loss = 1.047\n",
      "2020-04-14T19:07:21.374057: Epoch   1 Batch  499/781   test_loss = 1.001\n",
      "2020-04-14T19:07:21.732267: Epoch   1 Batch  519/781   test_loss = 1.031\n",
      "2020-04-14T19:07:22.034849: Epoch   1 Batch  539/781   test_loss = 0.879\n",
      "2020-04-14T19:07:22.441574: Epoch   1 Batch  559/781   test_loss = 1.168\n",
      "2020-04-14T19:07:22.734336: Epoch   1 Batch  579/781   test_loss = 1.086\n",
      "2020-04-14T19:07:23.090116: Epoch   1 Batch  599/781   test_loss = 1.020\n",
      "2020-04-14T19:07:23.414980: Epoch   1 Batch  619/781   test_loss = 1.131\n",
      "2020-04-14T19:07:23.704441: Epoch   1 Batch  639/781   test_loss = 0.945\n",
      "2020-04-14T19:07:24.049909: Epoch   1 Batch  659/781   test_loss = 1.146\n",
      "2020-04-14T19:07:24.408590: Epoch   1 Batch  679/781   test_loss = 1.197\n",
      "2020-04-14T19:07:24.740520: Epoch   1 Batch  699/781   test_loss = 0.878\n",
      "2020-04-14T19:07:25.029639: Epoch   1 Batch  719/781   test_loss = 0.936\n",
      "2020-04-14T19:07:25.376782: Epoch   1 Batch  739/781   test_loss = 0.968\n",
      "2020-04-14T19:07:25.643046: Epoch   1 Batch  759/781   test_loss = 0.973\n",
      "2020-04-14T19:07:26.092948: Epoch   1 Batch  779/781   test_loss = 0.818\n",
      "2020-04-14T19:07:27.948513: Epoch   2 Batch   10/3125   train_loss = 0.940\n",
      "2020-04-14T19:07:28.982563: Epoch   2 Batch   30/3125   train_loss = 1.025\n",
      "2020-04-14T19:07:29.852984: Epoch   2 Batch   50/3125   train_loss = 1.044\n",
      "2020-04-14T19:07:30.687123: Epoch   2 Batch   70/3125   train_loss = 1.039\n",
      "2020-04-14T19:07:31.534536: Epoch   2 Batch   90/3125   train_loss = 1.055\n",
      "2020-04-14T19:07:32.534484: Epoch   2 Batch  110/3125   train_loss = 0.915\n",
      "2020-04-14T19:07:33.398755: Epoch   2 Batch  130/3125   train_loss = 1.015\n",
      "2020-04-14T19:07:34.440647: Epoch   2 Batch  150/3125   train_loss = 1.095\n",
      "2020-04-14T19:07:35.282818: Epoch   2 Batch  170/3125   train_loss = 1.008\n",
      "2020-04-14T19:07:36.493650: Epoch   2 Batch  190/3125   train_loss = 1.034\n",
      "2020-04-14T19:07:37.896130: Epoch   2 Batch  210/3125   train_loss = 0.948\n",
      "2020-04-14T19:07:39.031453: Epoch   2 Batch  230/3125   train_loss = 1.006\n",
      "2020-04-14T19:07:40.093896: Epoch   2 Batch  250/3125   train_loss = 0.915\n",
      "2020-04-14T19:07:41.586865: Epoch   2 Batch  270/3125   train_loss = 0.840\n",
      "2020-04-14T19:07:43.923106: Epoch   2 Batch  290/3125   train_loss = 1.037\n",
      "2020-04-14T19:07:45.792960: Epoch   2 Batch  310/3125   train_loss = 0.968\n",
      "2020-04-14T19:07:47.392158: Epoch   2 Batch  330/3125   train_loss = 1.022\n",
      "2020-04-14T19:07:48.677468: Epoch   2 Batch  350/3125   train_loss = 0.968\n",
      "2020-04-14T19:07:49.823840: Epoch   2 Batch  370/3125   train_loss = 1.172\n",
      "2020-04-14T19:07:51.139891: Epoch   2 Batch  390/3125   train_loss = 1.158\n",
      "2020-04-14T19:07:52.191057: Epoch   2 Batch  410/3125   train_loss = 0.972\n",
      "2020-04-14T19:07:53.046976: Epoch   2 Batch  430/3125   train_loss = 1.096\n",
      "2020-04-14T19:07:53.951307: Epoch   2 Batch  450/3125   train_loss = 1.004\n",
      "2020-04-14T19:07:54.805138: Epoch   2 Batch  470/3125   train_loss = 0.920\n",
      "2020-04-14T19:07:55.652387: Epoch   2 Batch  490/3125   train_loss = 1.053\n",
      "2020-04-14T19:07:56.574581: Epoch   2 Batch  510/3125   train_loss = 1.087\n",
      "2020-04-14T19:07:57.488939: Epoch   2 Batch  530/3125   train_loss = 0.937\n",
      "2020-04-14T19:07:58.542474: Epoch   2 Batch  550/3125   train_loss = 0.935\n",
      "2020-04-14T19:07:59.755867: Epoch   2 Batch  570/3125   train_loss = 1.120\n",
      "2020-04-14T19:08:00.967367: Epoch   2 Batch  590/3125   train_loss = 1.026\n",
      "2020-04-14T19:08:01.992659: Epoch   2 Batch  610/3125   train_loss = 0.942\n",
      "2020-04-14T19:08:02.993311: Epoch   2 Batch  630/3125   train_loss = 1.010\n",
      "2020-04-14T19:08:03.840985: Epoch   2 Batch  650/3125   train_loss = 1.098\n",
      "2020-04-14T19:08:04.681835: Epoch   2 Batch  670/3125   train_loss = 0.977\n",
      "2020-04-14T19:08:05.501271: Epoch   2 Batch  690/3125   train_loss = 0.952\n",
      "2020-04-14T19:08:06.443618: Epoch   2 Batch  710/3125   train_loss = 0.937\n",
      "2020-04-14T19:08:07.356636: Epoch   2 Batch  730/3125   train_loss = 0.828\n",
      "2020-04-14T19:08:08.197741: Epoch   2 Batch  750/3125   train_loss = 0.958\n",
      "2020-04-14T19:08:09.060154: Epoch   2 Batch  770/3125   train_loss = 0.855\n",
      "2020-04-14T19:08:09.885249: Epoch   2 Batch  790/3125   train_loss = 0.951\n",
      "2020-04-14T19:08:10.711695: Epoch   2 Batch  810/3125   train_loss = 0.937\n",
      "2020-04-14T19:08:11.553068: Epoch   2 Batch  830/3125   train_loss = 0.869\n",
      "2020-04-14T19:08:12.382256: Epoch   2 Batch  850/3125   train_loss = 1.016\n",
      "2020-04-14T19:08:13.210831: Epoch   2 Batch  870/3125   train_loss = 0.859\n",
      "2020-04-14T19:08:14.045924: Epoch   2 Batch  890/3125   train_loss = 0.918\n",
      "2020-04-14T19:08:14.873345: Epoch   2 Batch  910/3125   train_loss = 1.034\n",
      "2020-04-14T19:08:15.729332: Epoch   2 Batch  930/3125   train_loss = 0.949\n",
      "2020-04-14T19:08:16.581988: Epoch   2 Batch  950/3125   train_loss = 0.947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:08:17.404106: Epoch   2 Batch  970/3125   train_loss = 1.078\n",
      "2020-04-14T19:08:18.228490: Epoch   2 Batch  990/3125   train_loss = 0.873\n",
      "2020-04-14T19:08:19.047559: Epoch   2 Batch 1010/3125   train_loss = 1.100\n",
      "2020-04-14T19:08:19.853374: Epoch   2 Batch 1030/3125   train_loss = 0.955\n",
      "2020-04-14T19:08:20.693817: Epoch   2 Batch 1050/3125   train_loss = 0.959\n",
      "2020-04-14T19:08:21.575296: Epoch   2 Batch 1070/3125   train_loss = 0.985\n",
      "2020-04-14T19:08:22.471094: Epoch   2 Batch 1090/3125   train_loss = 1.033\n",
      "2020-04-14T19:08:23.297248: Epoch   2 Batch 1110/3125   train_loss = 1.043\n",
      "2020-04-14T19:08:24.166447: Epoch   2 Batch 1130/3125   train_loss = 0.958\n",
      "2020-04-14T19:08:25.099582: Epoch   2 Batch 1150/3125   train_loss = 0.991\n",
      "2020-04-14T19:08:25.933640: Epoch   2 Batch 1170/3125   train_loss = 0.977\n",
      "2020-04-14T19:08:26.822969: Epoch   2 Batch 1190/3125   train_loss = 1.063\n",
      "2020-04-14T19:08:27.688026: Epoch   2 Batch 1210/3125   train_loss = 0.871\n",
      "2020-04-14T19:08:28.514378: Epoch   2 Batch 1230/3125   train_loss = 0.852\n",
      "2020-04-14T19:08:29.343578: Epoch   2 Batch 1250/3125   train_loss = 1.002\n",
      "2020-04-14T19:08:30.187338: Epoch   2 Batch 1270/3125   train_loss = 1.019\n",
      "2020-04-14T19:08:30.989721: Epoch   2 Batch 1290/3125   train_loss = 0.911\n",
      "2020-04-14T19:08:31.872365: Epoch   2 Batch 1310/3125   train_loss = 1.051\n",
      "2020-04-14T19:08:32.690748: Epoch   2 Batch 1330/3125   train_loss = 1.078\n",
      "2020-04-14T19:08:33.579604: Epoch   2 Batch 1350/3125   train_loss = 0.888\n",
      "2020-04-14T19:08:34.430792: Epoch   2 Batch 1370/3125   train_loss = 0.799\n",
      "2020-04-14T19:08:35.264480: Epoch   2 Batch 1390/3125   train_loss = 1.053\n",
      "2020-04-14T19:08:36.051595: Epoch   2 Batch 1410/3125   train_loss = 0.944\n",
      "2020-04-14T19:08:36.870011: Epoch   2 Batch 1430/3125   train_loss = 1.047\n",
      "2020-04-14T19:08:37.659237: Epoch   2 Batch 1450/3125   train_loss = 1.041\n",
      "2020-04-14T19:08:38.467542: Epoch   2 Batch 1470/3125   train_loss = 1.020\n",
      "2020-04-14T19:08:39.283462: Epoch   2 Batch 1490/3125   train_loss = 1.034\n",
      "2020-04-14T19:08:40.199236: Epoch   2 Batch 1510/3125   train_loss = 0.962\n",
      "2020-04-14T19:08:41.033654: Epoch   2 Batch 1530/3125   train_loss = 1.074\n",
      "2020-04-14T19:08:41.880263: Epoch   2 Batch 1550/3125   train_loss = 0.897\n",
      "2020-04-14T19:08:42.698772: Epoch   2 Batch 1570/3125   train_loss = 0.950\n",
      "2020-04-14T19:08:43.532762: Epoch   2 Batch 1590/3125   train_loss = 1.006\n",
      "2020-04-14T19:08:44.341628: Epoch   2 Batch 1610/3125   train_loss = 0.955\n",
      "2020-04-14T19:08:45.161739: Epoch   2 Batch 1630/3125   train_loss = 1.038\n",
      "2020-04-14T19:08:46.014312: Epoch   2 Batch 1650/3125   train_loss = 0.849\n",
      "2020-04-14T19:08:46.856815: Epoch   2 Batch 1670/3125   train_loss = 0.858\n",
      "2020-04-14T19:08:47.705598: Epoch   2 Batch 1690/3125   train_loss = 0.982\n",
      "2020-04-14T19:08:48.669470: Epoch   2 Batch 1710/3125   train_loss = 0.994\n",
      "2020-04-14T19:08:49.568264: Epoch   2 Batch 1730/3125   train_loss = 1.006\n",
      "2020-04-14T19:08:50.464553: Epoch   2 Batch 1750/3125   train_loss = 0.885\n",
      "2020-04-14T19:08:51.453175: Epoch   2 Batch 1770/3125   train_loss = 1.112\n",
      "2020-04-14T19:08:52.505037: Epoch   2 Batch 1790/3125   train_loss = 1.013\n",
      "2020-04-14T19:08:53.486836: Epoch   2 Batch 1810/3125   train_loss = 0.971\n",
      "2020-04-14T19:08:54.548379: Epoch   2 Batch 1830/3125   train_loss = 1.025\n",
      "2020-04-14T19:08:55.507840: Epoch   2 Batch 1850/3125   train_loss = 0.903\n",
      "2020-04-14T19:08:56.360383: Epoch   2 Batch 1870/3125   train_loss = 0.968\n",
      "2020-04-14T19:08:57.202030: Epoch   2 Batch 1890/3125   train_loss = 0.824\n",
      "2020-04-14T19:08:58.047316: Epoch   2 Batch 1910/3125   train_loss = 0.893\n",
      "2020-04-14T19:08:58.896733: Epoch   2 Batch 1930/3125   train_loss = 1.020\n",
      "2020-04-14T19:08:59.717168: Epoch   2 Batch 1950/3125   train_loss = 0.940\n",
      "2020-04-14T19:09:00.587017: Epoch   2 Batch 1970/3125   train_loss = 0.987\n",
      "2020-04-14T19:09:01.449865: Epoch   2 Batch 1990/3125   train_loss = 0.823\n",
      "2020-04-14T19:09:02.309261: Epoch   2 Batch 2010/3125   train_loss = 0.775\n",
      "2020-04-14T19:09:03.175682: Epoch   2 Batch 2030/3125   train_loss = 0.917\n",
      "2020-04-14T19:09:04.000580: Epoch   2 Batch 2050/3125   train_loss = 0.964\n",
      "2020-04-14T19:09:04.855261: Epoch   2 Batch 2070/3125   train_loss = 0.995\n",
      "2020-04-14T19:09:05.691214: Epoch   2 Batch 2090/3125   train_loss = 0.910\n",
      "2020-04-14T19:09:06.521498: Epoch   2 Batch 2110/3125   train_loss = 0.998\n",
      "2020-04-14T19:09:07.370093: Epoch   2 Batch 2130/3125   train_loss = 0.944\n",
      "2020-04-14T19:09:08.215812: Epoch   2 Batch 2150/3125   train_loss = 1.055\n",
      "2020-04-14T19:09:09.069539: Epoch   2 Batch 2170/3125   train_loss = 0.868\n",
      "2020-04-14T19:09:09.883077: Epoch   2 Batch 2190/3125   train_loss = 0.991\n",
      "2020-04-14T19:09:10.706680: Epoch   2 Batch 2210/3125   train_loss = 0.895\n",
      "2020-04-14T19:09:11.541854: Epoch   2 Batch 2230/3125   train_loss = 0.904\n",
      "2020-04-14T19:09:12.398158: Epoch   2 Batch 2250/3125   train_loss = 1.024\n",
      "2020-04-14T19:09:13.237959: Epoch   2 Batch 2270/3125   train_loss = 0.884\n",
      "2020-04-14T19:09:14.089119: Epoch   2 Batch 2290/3125   train_loss = 0.849\n",
      "2020-04-14T19:09:14.994582: Epoch   2 Batch 2310/3125   train_loss = 0.878\n",
      "2020-04-14T19:09:15.899375: Epoch   2 Batch 2330/3125   train_loss = 1.043\n",
      "2020-04-14T19:09:16.739161: Epoch   2 Batch 2350/3125   train_loss = 0.992\n",
      "2020-04-14T19:09:17.540752: Epoch   2 Batch 2370/3125   train_loss = 0.904\n",
      "2020-04-14T19:09:18.368834: Epoch   2 Batch 2390/3125   train_loss = 0.954\n",
      "2020-04-14T19:09:19.235031: Epoch   2 Batch 2410/3125   train_loss = 1.080\n",
      "2020-04-14T19:09:20.092349: Epoch   2 Batch 2430/3125   train_loss = 0.908\n",
      "2020-04-14T19:09:21.010000: Epoch   2 Batch 2450/3125   train_loss = 0.899\n",
      "2020-04-14T19:09:21.897858: Epoch   2 Batch 2470/3125   train_loss = 0.988\n",
      "2020-04-14T19:09:22.779485: Epoch   2 Batch 2490/3125   train_loss = 1.091\n",
      "2020-04-14T19:09:23.595719: Epoch   2 Batch 2510/3125   train_loss = 1.074\n",
      "2020-04-14T19:09:24.392373: Epoch   2 Batch 2530/3125   train_loss = 0.793\n",
      "2020-04-14T19:09:25.276334: Epoch   2 Batch 2550/3125   train_loss = 1.025\n",
      "2020-04-14T19:09:26.092749: Epoch   2 Batch 2570/3125   train_loss = 0.992\n",
      "2020-04-14T19:09:26.978726: Epoch   2 Batch 2590/3125   train_loss = 0.949\n",
      "2020-04-14T19:09:27.804305: Epoch   2 Batch 2610/3125   train_loss = 1.020\n",
      "2020-04-14T19:09:28.637459: Epoch   2 Batch 2630/3125   train_loss = 0.755\n",
      "2020-04-14T19:09:29.409829: Epoch   2 Batch 2650/3125   train_loss = 0.959\n",
      "2020-04-14T19:09:30.212105: Epoch   2 Batch 2670/3125   train_loss = 0.992\n",
      "2020-04-14T19:09:30.983306: Epoch   2 Batch 2690/3125   train_loss = 0.993\n",
      "2020-04-14T19:09:31.837341: Epoch   2 Batch 2710/3125   train_loss = 0.934\n",
      "2020-04-14T19:09:32.688089: Epoch   2 Batch 2730/3125   train_loss = 1.095\n",
      "2020-04-14T19:09:33.521386: Epoch   2 Batch 2750/3125   train_loss = 1.058\n",
      "2020-04-14T19:09:34.384566: Epoch   2 Batch 2770/3125   train_loss = 0.972\n",
      "2020-04-14T19:09:35.196430: Epoch   2 Batch 2790/3125   train_loss = 0.880\n",
      "2020-04-14T19:09:36.059372: Epoch   2 Batch 2810/3125   train_loss = 0.937\n",
      "2020-04-14T19:09:36.889973: Epoch   2 Batch 2830/3125   train_loss = 0.844\n",
      "2020-04-14T19:09:37.755782: Epoch   2 Batch 2850/3125   train_loss = 0.997\n",
      "2020-04-14T19:09:38.616875: Epoch   2 Batch 2870/3125   train_loss = 0.836\n",
      "2020-04-14T19:09:39.476223: Epoch   2 Batch 2890/3125   train_loss = 0.788\n",
      "2020-04-14T19:09:40.356248: Epoch   2 Batch 2910/3125   train_loss = 1.040\n",
      "2020-04-14T19:09:41.179524: Epoch   2 Batch 2930/3125   train_loss = 0.782\n",
      "2020-04-14T19:09:42.082094: Epoch   2 Batch 2950/3125   train_loss = 1.109\n",
      "2020-04-14T19:09:42.916252: Epoch   2 Batch 2970/3125   train_loss = 0.952\n",
      "2020-04-14T19:09:43.777844: Epoch   2 Batch 2990/3125   train_loss = 0.857\n",
      "2020-04-14T19:09:44.650968: Epoch   2 Batch 3010/3125   train_loss = 0.998\n",
      "2020-04-14T19:09:45.485016: Epoch   2 Batch 3030/3125   train_loss = 0.982\n",
      "2020-04-14T19:09:46.326037: Epoch   2 Batch 3050/3125   train_loss = 0.943\n",
      "2020-04-14T19:09:47.374994: Epoch   2 Batch 3070/3125   train_loss = 0.898\n",
      "2020-04-14T19:09:49.312131: Epoch   2 Batch 3090/3125   train_loss = 0.790\n",
      "2020-04-14T19:09:50.148961: Epoch   2 Batch 3110/3125   train_loss = 0.849\n",
      "2020-04-14T19:09:50.937359: Epoch   2 Batch   18/781   test_loss = 0.842\n",
      "2020-04-14T19:09:51.154537: Epoch   2 Batch   38/781   test_loss = 0.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:09:51.393174: Epoch   2 Batch   58/781   test_loss = 0.877\n",
      "2020-04-14T19:09:51.639861: Epoch   2 Batch   78/781   test_loss = 0.954\n",
      "2020-04-14T19:09:51.884534: Epoch   2 Batch   98/781   test_loss = 0.991\n",
      "2020-04-14T19:09:52.127489: Epoch   2 Batch  118/781   test_loss = 0.891\n",
      "2020-04-14T19:09:52.368272: Epoch   2 Batch  138/781   test_loss = 0.960\n",
      "2020-04-14T19:09:52.607895: Epoch   2 Batch  158/781   test_loss = 0.879\n",
      "2020-04-14T19:09:52.890680: Epoch   2 Batch  178/781   test_loss = 0.862\n",
      "2020-04-14T19:09:53.130539: Epoch   2 Batch  198/781   test_loss = 0.913\n",
      "2020-04-14T19:09:53.402072: Epoch   2 Batch  218/781   test_loss = 1.046\n",
      "2020-04-14T19:09:53.675180: Epoch   2 Batch  238/781   test_loss = 0.952\n",
      "2020-04-14T19:09:54.001577: Epoch   2 Batch  258/781   test_loss = 0.992\n",
      "2020-04-14T19:09:54.307302: Epoch   2 Batch  278/781   test_loss = 1.147\n",
      "2020-04-14T19:09:54.571790: Epoch   2 Batch  298/781   test_loss = 0.901\n",
      "2020-04-14T19:09:54.904835: Epoch   2 Batch  318/781   test_loss = 0.948\n",
      "2020-04-14T19:09:55.208391: Epoch   2 Batch  338/781   test_loss = 0.944\n",
      "2020-04-14T19:09:55.520495: Epoch   2 Batch  358/781   test_loss = 0.955\n",
      "2020-04-14T19:09:55.862161: Epoch   2 Batch  378/781   test_loss = 0.888\n",
      "2020-04-14T19:09:56.185463: Epoch   2 Batch  398/781   test_loss = 0.823\n",
      "2020-04-14T19:09:56.535834: Epoch   2 Batch  418/781   test_loss = 0.925\n",
      "2020-04-14T19:09:56.871447: Epoch   2 Batch  438/781   test_loss = 1.027\n",
      "2020-04-14T19:09:57.333590: Epoch   2 Batch  458/781   test_loss = 0.942\n",
      "2020-04-14T19:09:57.657360: Epoch   2 Batch  478/781   test_loss = 0.994\n",
      "2020-04-14T19:09:57.964239: Epoch   2 Batch  498/781   test_loss = 0.832\n",
      "2020-04-14T19:09:58.308687: Epoch   2 Batch  518/781   test_loss = 0.924\n",
      "2020-04-14T19:09:58.649849: Epoch   2 Batch  538/781   test_loss = 0.846\n",
      "2020-04-14T19:09:58.974279: Epoch   2 Batch  558/781   test_loss = 0.928\n",
      "2020-04-14T19:09:59.375849: Epoch   2 Batch  578/781   test_loss = 0.917\n",
      "2020-04-14T19:09:59.696971: Epoch   2 Batch  598/781   test_loss = 1.050\n",
      "2020-04-14T19:10:00.001595: Epoch   2 Batch  618/781   test_loss = 0.864\n",
      "2020-04-14T19:10:00.304737: Epoch   2 Batch  638/781   test_loss = 0.856\n",
      "2020-04-14T19:10:00.610358: Epoch   2 Batch  658/781   test_loss = 1.129\n",
      "2020-04-14T19:10:00.896067: Epoch   2 Batch  678/781   test_loss = 0.940\n",
      "2020-04-14T19:10:01.194077: Epoch   2 Batch  698/781   test_loss = 0.883\n",
      "2020-04-14T19:10:01.495059: Epoch   2 Batch  718/781   test_loss = 1.009\n",
      "2020-04-14T19:10:01.790523: Epoch   2 Batch  738/781   test_loss = 0.841\n",
      "2020-04-14T19:10:02.087969: Epoch   2 Batch  758/781   test_loss = 0.995\n",
      "2020-04-14T19:10:02.365713: Epoch   2 Batch  778/781   test_loss = 0.951\n",
      "2020-04-14T19:10:04.252089: Epoch   3 Batch    5/3125   train_loss = 0.955\n",
      "2020-04-14T19:10:05.145693: Epoch   3 Batch   25/3125   train_loss = 0.967\n",
      "2020-04-14T19:10:05.999506: Epoch   3 Batch   45/3125   train_loss = 0.829\n",
      "2020-04-14T19:10:06.841630: Epoch   3 Batch   65/3125   train_loss = 0.925\n",
      "2020-04-14T19:10:07.682873: Epoch   3 Batch   85/3125   train_loss = 0.838\n",
      "2020-04-14T19:10:08.548735: Epoch   3 Batch  105/3125   train_loss = 0.776\n",
      "2020-04-14T19:10:09.410496: Epoch   3 Batch  125/3125   train_loss = 0.890\n",
      "2020-04-14T19:10:10.280645: Epoch   3 Batch  145/3125   train_loss = 0.945\n",
      "2020-04-14T19:10:11.188840: Epoch   3 Batch  165/3125   train_loss = 0.911\n",
      "2020-04-14T19:10:12.087726: Epoch   3 Batch  185/3125   train_loss = 0.816\n",
      "2020-04-14T19:10:12.969722: Epoch   3 Batch  205/3125   train_loss = 0.797\n",
      "2020-04-14T19:10:13.796043: Epoch   3 Batch  225/3125   train_loss = 0.815\n",
      "2020-04-14T19:10:14.664475: Epoch   3 Batch  245/3125   train_loss = 1.109\n",
      "2020-04-14T19:10:15.515597: Epoch   3 Batch  265/3125   train_loss = 0.852\n",
      "2020-04-14T19:10:16.370244: Epoch   3 Batch  285/3125   train_loss = 0.934\n",
      "2020-04-14T19:10:17.143114: Epoch   3 Batch  305/3125   train_loss = 0.815\n",
      "2020-04-14T19:10:17.952099: Epoch   3 Batch  325/3125   train_loss = 0.988\n",
      "2020-04-14T19:10:18.739365: Epoch   3 Batch  345/3125   train_loss = 0.958\n",
      "2020-04-14T19:10:19.600639: Epoch   3 Batch  365/3125   train_loss = 0.888\n",
      "2020-04-14T19:10:20.606143: Epoch   3 Batch  385/3125   train_loss = 0.977\n",
      "2020-04-14T19:10:21.871941: Epoch   3 Batch  405/3125   train_loss = 0.865\n",
      "2020-04-14T19:10:22.878082: Epoch   3 Batch  425/3125   train_loss = 0.894\n",
      "2020-04-14T19:10:23.865639: Epoch   3 Batch  445/3125   train_loss = 0.936\n",
      "2020-04-14T19:10:24.741591: Epoch   3 Batch  465/3125   train_loss = 0.910\n",
      "2020-04-14T19:10:25.565239: Epoch   3 Batch  485/3125   train_loss = 1.017\n",
      "2020-04-14T19:10:26.418584: Epoch   3 Batch  505/3125   train_loss = 0.807\n",
      "2020-04-14T19:10:27.314930: Epoch   3 Batch  525/3125   train_loss = 1.002\n",
      "2020-04-14T19:10:28.155882: Epoch   3 Batch  545/3125   train_loss = 0.861\n",
      "2020-04-14T19:10:28.977099: Epoch   3 Batch  565/3125   train_loss = 1.106\n",
      "2020-04-14T19:10:29.805417: Epoch   3 Batch  585/3125   train_loss = 0.844\n",
      "2020-04-14T19:10:30.628020: Epoch   3 Batch  605/3125   train_loss = 0.943\n",
      "2020-04-14T19:10:31.477345: Epoch   3 Batch  625/3125   train_loss = 0.881\n",
      "2020-04-14T19:10:32.425167: Epoch   3 Batch  645/3125   train_loss = 0.919\n",
      "2020-04-14T19:10:33.345820: Epoch   3 Batch  665/3125   train_loss = 1.037\n",
      "2020-04-14T19:10:34.259965: Epoch   3 Batch  685/3125   train_loss = 0.924\n",
      "2020-04-14T19:10:35.228660: Epoch   3 Batch  705/3125   train_loss = 1.017\n",
      "2020-04-14T19:10:36.048363: Epoch   3 Batch  725/3125   train_loss = 0.886\n",
      "2020-04-14T19:10:36.951569: Epoch   3 Batch  745/3125   train_loss = 0.866\n",
      "2020-04-14T19:10:37.868237: Epoch   3 Batch  765/3125   train_loss = 0.888\n",
      "2020-04-14T19:10:38.794458: Epoch   3 Batch  785/3125   train_loss = 1.079\n",
      "2020-04-14T19:10:39.696225: Epoch   3 Batch  805/3125   train_loss = 0.842\n",
      "2020-04-14T19:10:40.539389: Epoch   3 Batch  825/3125   train_loss = 0.888\n",
      "2020-04-14T19:10:41.488013: Epoch   3 Batch  845/3125   train_loss = 0.864\n",
      "2020-04-14T19:10:43.038874: Epoch   3 Batch  865/3125   train_loss = 1.048\n",
      "2020-04-14T19:10:44.643855: Epoch   3 Batch  885/3125   train_loss = 0.944\n",
      "2020-04-14T19:10:46.510541: Epoch   3 Batch  905/3125   train_loss = 1.045\n",
      "2020-04-14T19:10:47.683808: Epoch   3 Batch  925/3125   train_loss = 0.953\n",
      "2020-04-14T19:10:48.739805: Epoch   3 Batch  945/3125   train_loss = 1.031\n",
      "2020-04-14T19:10:49.741579: Epoch   3 Batch  965/3125   train_loss = 0.769\n",
      "2020-04-14T19:10:50.866924: Epoch   3 Batch  985/3125   train_loss = 0.981\n",
      "2020-04-14T19:10:51.999627: Epoch   3 Batch 1005/3125   train_loss = 0.834\n",
      "2020-04-14T19:10:53.178874: Epoch   3 Batch 1025/3125   train_loss = 0.944\n",
      "2020-04-14T19:10:54.229203: Epoch   3 Batch 1045/3125   train_loss = 1.084\n",
      "2020-04-14T19:10:55.221032: Epoch   3 Batch 1065/3125   train_loss = 0.899\n",
      "2020-04-14T19:10:56.118721: Epoch   3 Batch 1085/3125   train_loss = 0.819\n",
      "2020-04-14T19:10:56.962338: Epoch   3 Batch 1105/3125   train_loss = 0.864\n",
      "2020-04-14T19:10:57.922490: Epoch   3 Batch 1125/3125   train_loss = 0.853\n",
      "2020-04-14T19:10:58.823074: Epoch   3 Batch 1145/3125   train_loss = 0.904\n",
      "2020-04-14T19:10:59.670645: Epoch   3 Batch 1165/3125   train_loss = 1.047\n",
      "2020-04-14T19:11:00.548587: Epoch   3 Batch 1185/3125   train_loss = 0.856\n",
      "2020-04-14T19:11:01.481940: Epoch   3 Batch 1205/3125   train_loss = 0.886\n",
      "2020-04-14T19:11:02.395829: Epoch   3 Batch 1225/3125   train_loss = 0.973\n",
      "2020-04-14T19:11:03.243903: Epoch   3 Batch 1245/3125   train_loss = 1.063\n",
      "2020-04-14T19:11:04.149983: Epoch   3 Batch 1265/3125   train_loss = 0.936\n",
      "2020-04-14T19:11:05.117435: Epoch   3 Batch 1285/3125   train_loss = 0.992\n",
      "2020-04-14T19:11:06.186918: Epoch   3 Batch 1305/3125   train_loss = 0.777\n",
      "2020-04-14T19:11:07.077160: Epoch   3 Batch 1325/3125   train_loss = 0.920\n",
      "2020-04-14T19:11:07.962938: Epoch   3 Batch 1345/3125   train_loss = 0.936\n",
      "2020-04-14T19:11:08.787591: Epoch   3 Batch 1365/3125   train_loss = 0.805\n",
      "2020-04-14T19:11:09.630166: Epoch   3 Batch 1385/3125   train_loss = 0.831\n",
      "2020-04-14T19:11:10.456195: Epoch   3 Batch 1405/3125   train_loss = 0.879\n",
      "2020-04-14T19:11:11.311947: Epoch   3 Batch 1425/3125   train_loss = 1.021\n",
      "2020-04-14T19:11:12.204204: Epoch   3 Batch 1445/3125   train_loss = 0.960\n",
      "2020-04-14T19:11:13.160884: Epoch   3 Batch 1465/3125   train_loss = 0.853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:11:14.125930: Epoch   3 Batch 1485/3125   train_loss = 1.001\n",
      "2020-04-14T19:11:15.023492: Epoch   3 Batch 1505/3125   train_loss = 0.763\n",
      "2020-04-14T19:11:15.864792: Epoch   3 Batch 1525/3125   train_loss = 0.780\n",
      "2020-04-14T19:11:16.802734: Epoch   3 Batch 1545/3125   train_loss = 0.876\n",
      "2020-04-14T19:11:17.717284: Epoch   3 Batch 1565/3125   train_loss = 0.994\n",
      "2020-04-14T19:11:18.764122: Epoch   3 Batch 1585/3125   train_loss = 0.808\n",
      "2020-04-14T19:11:19.721619: Epoch   3 Batch 1605/3125   train_loss = 0.917\n",
      "2020-04-14T19:11:20.667463: Epoch   3 Batch 1625/3125   train_loss = 0.957\n",
      "2020-04-14T19:11:21.523594: Epoch   3 Batch 1645/3125   train_loss = 0.954\n",
      "2020-04-14T19:11:22.476418: Epoch   3 Batch 1665/3125   train_loss = 0.897\n",
      "2020-04-14T19:11:23.420015: Epoch   3 Batch 1685/3125   train_loss = 1.055\n",
      "2020-04-14T19:11:24.395733: Epoch   3 Batch 1705/3125   train_loss = 0.921\n",
      "2020-04-14T19:11:25.294900: Epoch   3 Batch 1725/3125   train_loss = 0.891\n",
      "2020-04-14T19:11:26.265396: Epoch   3 Batch 1745/3125   train_loss = 0.847\n",
      "2020-04-14T19:11:27.254502: Epoch   3 Batch 1765/3125   train_loss = 0.880\n",
      "2020-04-14T19:11:28.255207: Epoch   3 Batch 1785/3125   train_loss = 1.023\n",
      "2020-04-14T19:11:29.114598: Epoch   3 Batch 1805/3125   train_loss = 0.907\n",
      "2020-04-14T19:11:29.968008: Epoch   3 Batch 1825/3125   train_loss = 1.017\n",
      "2020-04-14T19:11:30.891924: Epoch   3 Batch 1845/3125   train_loss = 0.911\n",
      "2020-04-14T19:11:31.715639: Epoch   3 Batch 1865/3125   train_loss = 0.818\n",
      "2020-04-14T19:11:32.646830: Epoch   3 Batch 1885/3125   train_loss = 0.994\n",
      "2020-04-14T19:11:33.573270: Epoch   3 Batch 1905/3125   train_loss = 0.873\n",
      "2020-04-14T19:11:34.444682: Epoch   3 Batch 1925/3125   train_loss = 0.873\n",
      "2020-04-14T19:11:35.328077: Epoch   3 Batch 1945/3125   train_loss = 0.905\n",
      "2020-04-14T19:11:36.202913: Epoch   3 Batch 1965/3125   train_loss = 0.890\n",
      "2020-04-14T19:11:37.158669: Epoch   3 Batch 1985/3125   train_loss = 0.852\n",
      "2020-04-14T19:11:38.104946: Epoch   3 Batch 2005/3125   train_loss = 0.890\n",
      "2020-04-14T19:11:38.991198: Epoch   3 Batch 2025/3125   train_loss = 0.928\n",
      "2020-04-14T19:11:39.817985: Epoch   3 Batch 2045/3125   train_loss = 0.776\n",
      "2020-04-14T19:11:40.740153: Epoch   3 Batch 2065/3125   train_loss = 0.837\n",
      "2020-04-14T19:11:41.586008: Epoch   3 Batch 2085/3125   train_loss = 1.016\n",
      "2020-04-14T19:11:42.374657: Epoch   3 Batch 2105/3125   train_loss = 0.849\n",
      "2020-04-14T19:11:43.190024: Epoch   3 Batch 2125/3125   train_loss = 0.972\n",
      "2020-04-14T19:11:44.004158: Epoch   3 Batch 2145/3125   train_loss = 0.982\n",
      "2020-04-14T19:11:44.843967: Epoch   3 Batch 2165/3125   train_loss = 0.880\n",
      "2020-04-14T19:11:45.669925: Epoch   3 Batch 2185/3125   train_loss = 0.947\n",
      "2020-04-14T19:11:46.500286: Epoch   3 Batch 2205/3125   train_loss = 0.980\n",
      "2020-04-14T19:11:47.334452: Epoch   3 Batch 2225/3125   train_loss = 0.817\n",
      "2020-04-14T19:11:48.176416: Epoch   3 Batch 2245/3125   train_loss = 0.786\n",
      "2020-04-14T19:11:49.009895: Epoch   3 Batch 2265/3125   train_loss = 0.907\n",
      "2020-04-14T19:11:49.848462: Epoch   3 Batch 2285/3125   train_loss = 1.088\n",
      "2020-04-14T19:11:50.728578: Epoch   3 Batch 2305/3125   train_loss = 0.838\n",
      "2020-04-14T19:11:51.785342: Epoch   3 Batch 2325/3125   train_loss = 0.850\n",
      "2020-04-14T19:11:52.680045: Epoch   3 Batch 2345/3125   train_loss = 0.897\n",
      "2020-04-14T19:11:53.601298: Epoch   3 Batch 2365/3125   train_loss = 0.732\n",
      "2020-04-14T19:11:54.810874: Epoch   3 Batch 2385/3125   train_loss = 0.959\n",
      "2020-04-14T19:11:56.665396: Epoch   3 Batch 2405/3125   train_loss = 0.932\n",
      "2020-04-14T19:11:57.941030: Epoch   3 Batch 2425/3125   train_loss = 0.907\n",
      "2020-04-14T19:11:58.803993: Epoch   3 Batch 2445/3125   train_loss = 0.924\n",
      "2020-04-14T19:11:59.652158: Epoch   3 Batch 2465/3125   train_loss = 0.761\n",
      "2020-04-14T19:12:00.488841: Epoch   3 Batch 2485/3125   train_loss = 0.826\n",
      "2020-04-14T19:12:01.304422: Epoch   3 Batch 2505/3125   train_loss = 0.822\n",
      "2020-04-14T19:12:02.147178: Epoch   3 Batch 2525/3125   train_loss = 0.850\n",
      "2020-04-14T19:12:02.969493: Epoch   3 Batch 2545/3125   train_loss = 0.974\n",
      "2020-04-14T19:12:03.839296: Epoch   3 Batch 2565/3125   train_loss = 0.909\n",
      "2020-04-14T19:12:04.748242: Epoch   3 Batch 2585/3125   train_loss = 0.807\n",
      "2020-04-14T19:12:05.588935: Epoch   3 Batch 2605/3125   train_loss = 0.840\n",
      "2020-04-14T19:12:06.408671: Epoch   3 Batch 2625/3125   train_loss = 0.983\n",
      "2020-04-14T19:12:07.284889: Epoch   3 Batch 2645/3125   train_loss = 0.885\n",
      "2020-04-14T19:12:08.207786: Epoch   3 Batch 2665/3125   train_loss = 0.932\n",
      "2020-04-14T19:12:09.115645: Epoch   3 Batch 2685/3125   train_loss = 0.856\n",
      "2020-04-14T19:12:10.101472: Epoch   3 Batch 2705/3125   train_loss = 0.798\n",
      "2020-04-14T19:12:11.001091: Epoch   3 Batch 2725/3125   train_loss = 1.005\n",
      "2020-04-14T19:12:11.919680: Epoch   3 Batch 2745/3125   train_loss = 0.899\n",
      "2020-04-14T19:12:12.783077: Epoch   3 Batch 2765/3125   train_loss = 0.852\n",
      "2020-04-14T19:12:13.631917: Epoch   3 Batch 2785/3125   train_loss = 0.957\n",
      "2020-04-14T19:12:14.497174: Epoch   3 Batch 2805/3125   train_loss = 0.847\n",
      "2020-04-14T19:12:15.330428: Epoch   3 Batch 2825/3125   train_loss = 0.862\n",
      "2020-04-14T19:12:16.233145: Epoch   3 Batch 2845/3125   train_loss = 0.851\n",
      "2020-04-14T19:12:17.158929: Epoch   3 Batch 2865/3125   train_loss = 0.837\n",
      "2020-04-14T19:12:18.056943: Epoch   3 Batch 2885/3125   train_loss = 0.927\n",
      "2020-04-14T19:12:18.977352: Epoch   3 Batch 2905/3125   train_loss = 0.964\n",
      "2020-04-14T19:12:19.902893: Epoch   3 Batch 2925/3125   train_loss = 0.894\n",
      "2020-04-14T19:12:21.032006: Epoch   3 Batch 2945/3125   train_loss = 0.960\n",
      "2020-04-14T19:12:22.058278: Epoch   3 Batch 2965/3125   train_loss = 0.993\n",
      "2020-04-14T19:12:22.923266: Epoch   3 Batch 2985/3125   train_loss = 0.849\n",
      "2020-04-14T19:12:23.776041: Epoch   3 Batch 3005/3125   train_loss = 0.840\n",
      "2020-04-14T19:12:24.597445: Epoch   3 Batch 3025/3125   train_loss = 0.931\n",
      "2020-04-14T19:12:25.431755: Epoch   3 Batch 3045/3125   train_loss = 0.905\n",
      "2020-04-14T19:12:26.254908: Epoch   3 Batch 3065/3125   train_loss = 0.788\n",
      "2020-04-14T19:12:27.117870: Epoch   3 Batch 3085/3125   train_loss = 0.863\n",
      "2020-04-14T19:12:28.055965: Epoch   3 Batch 3105/3125   train_loss = 0.915\n",
      "2020-04-14T19:12:29.080144: Epoch   3 Batch   17/781   test_loss = 0.929\n",
      "2020-04-14T19:12:29.390717: Epoch   3 Batch   37/781   test_loss = 0.885\n",
      "2020-04-14T19:12:29.700583: Epoch   3 Batch   57/781   test_loss = 0.934\n",
      "2020-04-14T19:12:29.987310: Epoch   3 Batch   77/781   test_loss = 0.897\n",
      "2020-04-14T19:12:30.298144: Epoch   3 Batch   97/781   test_loss = 0.781\n",
      "2020-04-14T19:12:30.608000: Epoch   3 Batch  117/781   test_loss = 0.975\n",
      "2020-04-14T19:12:30.904736: Epoch   3 Batch  137/781   test_loss = 0.933\n",
      "2020-04-14T19:12:31.207239: Epoch   3 Batch  157/781   test_loss = 0.939\n",
      "2020-04-14T19:12:31.500028: Epoch   3 Batch  177/781   test_loss = 0.897\n",
      "2020-04-14T19:12:31.805596: Epoch   3 Batch  197/781   test_loss = 0.918\n",
      "2020-04-14T19:12:32.091734: Epoch   3 Batch  217/781   test_loss = 0.766\n",
      "2020-04-14T19:12:32.369918: Epoch   3 Batch  237/781   test_loss = 0.792\n",
      "2020-04-14T19:12:32.633176: Epoch   3 Batch  257/781   test_loss = 1.046\n",
      "2020-04-14T19:12:32.957152: Epoch   3 Batch  277/781   test_loss = 0.965\n",
      "2020-04-14T19:12:33.255869: Epoch   3 Batch  297/781   test_loss = 0.965\n",
      "2020-04-14T19:12:33.520634: Epoch   3 Batch  317/781   test_loss = 1.021\n",
      "2020-04-14T19:12:33.776199: Epoch   3 Batch  337/781   test_loss = 0.935\n",
      "2020-04-14T19:12:34.045986: Epoch   3 Batch  357/781   test_loss = 0.937\n",
      "2020-04-14T19:12:34.360636: Epoch   3 Batch  377/781   test_loss = 0.994\n",
      "2020-04-14T19:12:34.609538: Epoch   3 Batch  397/781   test_loss = 0.955\n",
      "2020-04-14T19:12:34.890786: Epoch   3 Batch  417/781   test_loss = 0.807\n",
      "2020-04-14T19:12:35.159757: Epoch   3 Batch  437/781   test_loss = 0.794\n",
      "2020-04-14T19:12:35.378313: Epoch   3 Batch  457/781   test_loss = 0.741\n",
      "2020-04-14T19:12:35.645252: Epoch   3 Batch  477/781   test_loss = 0.890\n",
      "2020-04-14T19:12:35.938964: Epoch   3 Batch  497/781   test_loss = 0.802\n",
      "2020-04-14T19:12:36.189035: Epoch   3 Batch  517/781   test_loss = 0.880\n",
      "2020-04-14T19:12:36.439831: Epoch   3 Batch  537/781   test_loss = 0.838\n",
      "2020-04-14T19:12:36.738594: Epoch   3 Batch  557/781   test_loss = 1.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:12:37.039154: Epoch   3 Batch  577/781   test_loss = 0.915\n",
      "2020-04-14T19:12:37.339648: Epoch   3 Batch  597/781   test_loss = 0.897\n",
      "2020-04-14T19:12:37.640244: Epoch   3 Batch  617/781   test_loss = 0.837\n",
      "2020-04-14T19:12:37.956031: Epoch   3 Batch  637/781   test_loss = 0.810\n",
      "2020-04-14T19:12:38.248151: Epoch   3 Batch  657/781   test_loss = 1.007\n",
      "2020-04-14T19:12:38.541615: Epoch   3 Batch  677/781   test_loss = 0.903\n",
      "2020-04-14T19:12:38.821516: Epoch   3 Batch  697/781   test_loss = 0.942\n",
      "2020-04-14T19:12:39.099737: Epoch   3 Batch  717/781   test_loss = 0.837\n",
      "2020-04-14T19:12:39.354778: Epoch   3 Batch  737/781   test_loss = 0.742\n",
      "2020-04-14T19:12:39.627907: Epoch   3 Batch  757/781   test_loss = 1.050\n",
      "2020-04-14T19:12:39.920574: Epoch   3 Batch  777/781   test_loss = 0.934\n",
      "2020-04-14T19:12:41.086288: Epoch   4 Batch    0/3125   train_loss = 1.028\n",
      "2020-04-14T19:12:41.907295: Epoch   4 Batch   20/3125   train_loss = 0.876\n",
      "2020-04-14T19:12:42.701264: Epoch   4 Batch   40/3125   train_loss = 0.957\n",
      "2020-04-14T19:12:43.558530: Epoch   4 Batch   60/3125   train_loss = 0.725\n",
      "2020-04-14T19:12:44.411637: Epoch   4 Batch   80/3125   train_loss = 0.865\n",
      "2020-04-14T19:12:45.247273: Epoch   4 Batch  100/3125   train_loss = 0.910\n",
      "2020-04-14T19:12:46.084066: Epoch   4 Batch  120/3125   train_loss = 0.964\n",
      "2020-04-14T19:12:47.143519: Epoch   4 Batch  140/3125   train_loss = 0.941\n",
      "2020-04-14T19:12:48.130112: Epoch   4 Batch  160/3125   train_loss = 0.812\n",
      "2020-04-14T19:12:50.209977: Epoch   4 Batch  180/3125   train_loss = 0.885\n",
      "2020-04-14T19:12:50.974744: Epoch   4 Batch  200/3125   train_loss = 1.152\n",
      "2020-04-14T19:12:51.749395: Epoch   4 Batch  220/3125   train_loss = 0.852\n",
      "2020-04-14T19:12:52.549713: Epoch   4 Batch  240/3125   train_loss = 0.974\n",
      "2020-04-14T19:12:53.332056: Epoch   4 Batch  260/3125   train_loss = 0.900\n",
      "2020-04-14T19:12:54.172004: Epoch   4 Batch  280/3125   train_loss = 0.932\n",
      "2020-04-14T19:12:55.004890: Epoch   4 Batch  300/3125   train_loss = 1.078\n",
      "2020-04-14T19:12:55.833959: Epoch   4 Batch  320/3125   train_loss = 0.922\n",
      "2020-04-14T19:12:56.686766: Epoch   4 Batch  340/3125   train_loss = 0.747\n",
      "2020-04-14T19:12:57.566037: Epoch   4 Batch  360/3125   train_loss = 0.858\n",
      "2020-04-14T19:12:58.417208: Epoch   4 Batch  380/3125   train_loss = 0.825\n",
      "2020-04-14T19:12:59.303473: Epoch   4 Batch  400/3125   train_loss = 0.808\n",
      "2020-04-14T19:13:00.133208: Epoch   4 Batch  420/3125   train_loss = 0.827\n",
      "2020-04-14T19:13:00.984132: Epoch   4 Batch  440/3125   train_loss = 0.887\n",
      "2020-04-14T19:13:01.812027: Epoch   4 Batch  460/3125   train_loss = 0.867\n",
      "2020-04-14T19:13:02.648154: Epoch   4 Batch  480/3125   train_loss = 0.977\n",
      "2020-04-14T19:13:03.538377: Epoch   4 Batch  500/3125   train_loss = 0.680\n",
      "2020-04-14T19:13:04.419649: Epoch   4 Batch  520/3125   train_loss = 0.937\n",
      "2020-04-14T19:13:05.262980: Epoch   4 Batch  540/3125   train_loss = 0.820\n",
      "2020-04-14T19:13:06.104163: Epoch   4 Batch  560/3125   train_loss = 1.017\n",
      "2020-04-14T19:13:06.947979: Epoch   4 Batch  580/3125   train_loss = 0.991\n",
      "2020-04-14T19:13:07.768058: Epoch   4 Batch  600/3125   train_loss = 0.933\n",
      "2020-04-14T19:13:08.622417: Epoch   4 Batch  620/3125   train_loss = 0.957\n",
      "2020-04-14T19:13:09.474653: Epoch   4 Batch  640/3125   train_loss = 0.883\n",
      "2020-04-14T19:13:10.373297: Epoch   4 Batch  660/3125   train_loss = 0.882\n",
      "2020-04-14T19:13:11.220267: Epoch   4 Batch  680/3125   train_loss = 0.946\n",
      "2020-04-14T19:13:12.056816: Epoch   4 Batch  700/3125   train_loss = 0.911\n",
      "2020-04-14T19:13:12.885734: Epoch   4 Batch  720/3125   train_loss = 0.777\n",
      "2020-04-14T19:13:13.718534: Epoch   4 Batch  740/3125   train_loss = 0.926\n",
      "2020-04-14T19:13:14.528474: Epoch   4 Batch  760/3125   train_loss = 0.830\n",
      "2020-04-14T19:13:15.456888: Epoch   4 Batch  780/3125   train_loss = 0.882\n",
      "2020-04-14T19:13:16.285114: Epoch   4 Batch  800/3125   train_loss = 0.790\n",
      "2020-04-14T19:13:17.155031: Epoch   4 Batch  820/3125   train_loss = 0.809\n",
      "2020-04-14T19:13:18.015305: Epoch   4 Batch  840/3125   train_loss = 0.795\n",
      "2020-04-14T19:13:18.891038: Epoch   4 Batch  860/3125   train_loss = 0.863\n",
      "2020-04-14T19:13:19.694411: Epoch   4 Batch  880/3125   train_loss = 0.812\n",
      "2020-04-14T19:13:20.501733: Epoch   4 Batch  900/3125   train_loss = 0.820\n",
      "2020-04-14T19:13:21.379208: Epoch   4 Batch  920/3125   train_loss = 0.998\n",
      "2020-04-14T19:13:22.188104: Epoch   4 Batch  940/3125   train_loss = 0.870\n",
      "2020-04-14T19:13:23.000527: Epoch   4 Batch  960/3125   train_loss = 0.913\n",
      "2020-04-14T19:13:23.848750: Epoch   4 Batch  980/3125   train_loss = 0.984\n",
      "2020-04-14T19:13:24.680678: Epoch   4 Batch 1000/3125   train_loss = 0.952\n",
      "2020-04-14T19:13:25.503184: Epoch   4 Batch 1020/3125   train_loss = 0.911\n",
      "2020-04-14T19:13:26.314534: Epoch   4 Batch 1040/3125   train_loss = 0.756\n",
      "2020-04-14T19:13:27.204519: Epoch   4 Batch 1060/3125   train_loss = 0.934\n",
      "2020-04-14T19:13:27.986294: Epoch   4 Batch 1080/3125   train_loss = 0.920\n",
      "2020-04-14T19:13:28.803276: Epoch   4 Batch 1100/3125   train_loss = 0.854\n",
      "2020-04-14T19:13:29.611834: Epoch   4 Batch 1120/3125   train_loss = 0.863\n",
      "2020-04-14T19:13:30.424336: Epoch   4 Batch 1140/3125   train_loss = 0.904\n",
      "2020-04-14T19:13:31.227774: Epoch   4 Batch 1160/3125   train_loss = 0.860\n",
      "2020-04-14T19:13:32.083725: Epoch   4 Batch 1180/3125   train_loss = 0.797\n",
      "2020-04-14T19:13:32.963116: Epoch   4 Batch 1200/3125   train_loss = 0.979\n",
      "2020-04-14T19:13:33.811344: Epoch   4 Batch 1220/3125   train_loss = 0.963\n",
      "2020-04-14T19:13:34.670550: Epoch   4 Batch 1240/3125   train_loss = 0.777\n",
      "2020-04-14T19:13:35.484739: Epoch   4 Batch 1260/3125   train_loss = 0.899\n",
      "2020-04-14T19:13:36.332578: Epoch   4 Batch 1280/3125   train_loss = 0.901\n",
      "2020-04-14T19:13:37.475768: Epoch   4 Batch 1300/3125   train_loss = 0.821\n",
      "2020-04-14T19:13:38.315978: Epoch   4 Batch 1320/3125   train_loss = 0.913\n",
      "2020-04-14T19:13:39.394966: Epoch   4 Batch 1340/3125   train_loss = 0.763\n",
      "2020-04-14T19:13:40.652088: Epoch   4 Batch 1360/3125   train_loss = 0.823\n",
      "2020-04-14T19:13:41.631711: Epoch   4 Batch 1380/3125   train_loss = 0.807\n",
      "2020-04-14T19:13:42.984736: Epoch   4 Batch 1400/3125   train_loss = 0.933\n",
      "2020-04-14T19:13:45.245152: Epoch   4 Batch 1420/3125   train_loss = 0.926\n",
      "2020-04-14T19:13:46.549217: Epoch   4 Batch 1440/3125   train_loss = 0.783\n",
      "2020-04-14T19:13:47.902607: Epoch   4 Batch 1460/3125   train_loss = 0.840\n",
      "2020-04-14T19:13:48.952720: Epoch   4 Batch 1480/3125   train_loss = 0.873\n",
      "2020-04-14T19:13:49.803796: Epoch   4 Batch 1500/3125   train_loss = 0.883\n",
      "2020-04-14T19:13:50.610917: Epoch   4 Batch 1520/3125   train_loss = 0.850\n",
      "2020-04-14T19:13:51.353663: Epoch   4 Batch 1540/3125   train_loss = 0.971\n",
      "2020-04-14T19:13:52.231370: Epoch   4 Batch 1560/3125   train_loss = 0.812\n",
      "2020-04-14T19:13:53.171955: Epoch   4 Batch 1580/3125   train_loss = 0.981\n",
      "2020-04-14T19:13:53.981872: Epoch   4 Batch 1600/3125   train_loss = 0.873\n",
      "2020-04-14T19:13:54.904741: Epoch   4 Batch 1620/3125   train_loss = 0.852\n",
      "2020-04-14T19:13:55.750446: Epoch   4 Batch 1640/3125   train_loss = 0.924\n",
      "2020-04-14T19:13:56.709210: Epoch   4 Batch 1660/3125   train_loss = 1.009\n",
      "2020-04-14T19:13:57.641813: Epoch   4 Batch 1680/3125   train_loss = 0.912\n",
      "2020-04-14T19:13:58.492787: Epoch   4 Batch 1700/3125   train_loss = 0.777\n",
      "2020-04-14T19:13:59.441841: Epoch   4 Batch 1720/3125   train_loss = 0.856\n",
      "2020-04-14T19:14:00.367817: Epoch   4 Batch 1740/3125   train_loss = 0.918\n",
      "2020-04-14T19:14:01.225645: Epoch   4 Batch 1760/3125   train_loss = 0.920\n",
      "2020-04-14T19:14:02.161360: Epoch   4 Batch 1780/3125   train_loss = 0.918\n",
      "2020-04-14T19:14:03.264627: Epoch   4 Batch 1800/3125   train_loss = 0.819\n",
      "2020-04-14T19:14:04.281311: Epoch   4 Batch 1820/3125   train_loss = 0.832\n",
      "2020-04-14T19:14:05.165059: Epoch   4 Batch 1840/3125   train_loss = 0.960\n",
      "2020-04-14T19:14:06.016011: Epoch   4 Batch 1860/3125   train_loss = 0.972\n",
      "2020-04-14T19:14:06.938001: Epoch   4 Batch 1880/3125   train_loss = 0.869\n",
      "2020-04-14T19:14:07.817668: Epoch   4 Batch 1900/3125   train_loss = 0.728\n",
      "2020-04-14T19:14:08.644830: Epoch   4 Batch 1920/3125   train_loss = 0.856\n",
      "2020-04-14T19:14:09.490913: Epoch   4 Batch 1940/3125   train_loss = 0.808\n",
      "2020-04-14T19:14:10.287340: Epoch   4 Batch 1960/3125   train_loss = 0.783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14T19:14:11.200119: Epoch   4 Batch 1980/3125   train_loss = 0.866\n",
      "2020-04-14T19:14:12.049590: Epoch   4 Batch 2000/3125   train_loss = 0.964\n",
      "2020-04-14T19:14:12.904411: Epoch   4 Batch 2020/3125   train_loss = 1.013\n",
      "2020-04-14T19:14:13.773431: Epoch   4 Batch 2040/3125   train_loss = 0.768\n",
      "2020-04-14T19:14:14.764907: Epoch   4 Batch 2060/3125   train_loss = 0.851\n",
      "2020-04-14T19:14:15.678843: Epoch   4 Batch 2080/3125   train_loss = 0.985\n",
      "2020-04-14T19:14:16.659614: Epoch   4 Batch 2100/3125   train_loss = 0.839\n",
      "2020-04-14T19:14:17.606571: Epoch   4 Batch 2120/3125   train_loss = 0.853\n",
      "2020-04-14T19:14:18.453634: Epoch   4 Batch 2140/3125   train_loss = 0.862\n",
      "2020-04-14T19:14:19.346321: Epoch   4 Batch 2160/3125   train_loss = 0.857\n",
      "2020-04-14T19:14:20.256243: Epoch   4 Batch 2180/3125   train_loss = 0.930\n",
      "2020-04-14T19:14:21.077964: Epoch   4 Batch 2200/3125   train_loss = 0.789\n",
      "2020-04-14T19:14:21.901470: Epoch   4 Batch 2220/3125   train_loss = 0.881\n",
      "2020-04-14T19:14:22.748085: Epoch   4 Batch 2240/3125   train_loss = 0.814\n",
      "2020-04-14T19:14:23.540218: Epoch   4 Batch 2260/3125   train_loss = 0.884\n",
      "2020-04-14T19:14:24.353544: Epoch   4 Batch 2280/3125   train_loss = 0.937\n",
      "2020-04-14T19:14:25.163961: Epoch   4 Batch 2300/3125   train_loss = 0.870\n",
      "2020-04-14T19:14:25.983017: Epoch   4 Batch 2320/3125   train_loss = 0.913\n",
      "2020-04-14T19:14:26.817805: Epoch   4 Batch 2340/3125   train_loss = 0.868\n",
      "2020-04-14T19:14:27.782604: Epoch   4 Batch 2360/3125   train_loss = 0.858\n",
      "2020-04-14T19:14:28.690861: Epoch   4 Batch 2380/3125   train_loss = 0.872\n",
      "2020-04-14T19:14:29.770552: Epoch   4 Batch 2400/3125   train_loss = 0.955\n",
      "2020-04-14T19:14:30.588680: Epoch   4 Batch 2420/3125   train_loss = 0.760\n",
      "2020-04-14T19:14:31.460898: Epoch   4 Batch 2440/3125   train_loss = 0.835\n",
      "2020-04-14T19:14:32.393746: Epoch   4 Batch 2460/3125   train_loss = 0.819\n",
      "2020-04-14T19:14:33.202681: Epoch   4 Batch 2480/3125   train_loss = 0.981\n",
      "2020-04-14T19:14:34.034633: Epoch   4 Batch 2500/3125   train_loss = 0.778\n",
      "2020-04-14T19:14:34.868233: Epoch   4 Batch 2520/3125   train_loss = 0.930\n",
      "2020-04-14T19:14:35.719726: Epoch   4 Batch 2540/3125   train_loss = 0.804\n",
      "2020-04-14T19:14:36.513804: Epoch   4 Batch 2560/3125   train_loss = 0.660\n",
      "2020-04-14T19:14:37.367833: Epoch   4 Batch 2580/3125   train_loss = 0.884\n",
      "2020-04-14T19:14:38.183777: Epoch   4 Batch 2600/3125   train_loss = 0.878\n",
      "2020-04-14T19:14:39.070933: Epoch   4 Batch 2620/3125   train_loss = 0.792\n",
      "2020-04-14T19:14:39.896398: Epoch   4 Batch 2640/3125   train_loss = 0.823\n",
      "2020-04-14T19:14:40.702710: Epoch   4 Batch 2660/3125   train_loss = 0.998\n",
      "2020-04-14T19:14:41.521767: Epoch   4 Batch 2680/3125   train_loss = 0.814\n",
      "2020-04-14T19:14:42.321525: Epoch   4 Batch 2700/3125   train_loss = 0.907\n",
      "2020-04-14T19:14:43.154861: Epoch   4 Batch 2720/3125   train_loss = 0.788\n",
      "2020-04-14T19:14:43.990818: Epoch   4 Batch 2740/3125   train_loss = 0.883\n",
      "2020-04-14T19:14:44.893595: Epoch   4 Batch 2760/3125   train_loss = 0.779\n",
      "2020-04-14T19:14:45.962008: Epoch   4 Batch 2780/3125   train_loss = 0.883\n",
      "2020-04-14T19:14:46.799874: Epoch   4 Batch 2800/3125   train_loss = 1.030\n",
      "2020-04-14T19:14:47.651198: Epoch   4 Batch 2820/3125   train_loss = 1.023\n",
      "2020-04-14T19:14:48.581768: Epoch   4 Batch 2840/3125   train_loss = 0.866\n",
      "2020-04-14T19:14:49.429580: Epoch   4 Batch 2860/3125   train_loss = 0.819\n",
      "2020-04-14T19:14:50.451174: Epoch   4 Batch 2880/3125   train_loss = 0.853\n",
      "2020-04-14T19:14:51.452407: Epoch   4 Batch 2900/3125   train_loss = 0.889\n",
      "2020-04-14T19:14:52.429819: Epoch   4 Batch 2920/3125   train_loss = 0.838\n",
      "2020-04-14T19:14:53.384082: Epoch   4 Batch 2940/3125   train_loss = 0.873\n",
      "2020-04-14T19:14:54.983468: Epoch   4 Batch 2960/3125   train_loss = 0.881\n",
      "2020-04-14T19:14:55.991289: Epoch   4 Batch 2980/3125   train_loss = 0.836\n",
      "2020-04-14T19:14:56.857156: Epoch   4 Batch 3000/3125   train_loss = 0.919\n",
      "2020-04-14T19:14:57.638135: Epoch   4 Batch 3020/3125   train_loss = 1.000\n",
      "2020-04-14T19:14:58.431248: Epoch   4 Batch 3040/3125   train_loss = 0.907\n",
      "2020-04-14T19:14:59.256606: Epoch   4 Batch 3060/3125   train_loss = 0.795\n",
      "2020-04-14T19:15:00.143439: Epoch   4 Batch 3080/3125   train_loss = 0.980\n",
      "2020-04-14T19:15:01.013306: Epoch   4 Batch 3100/3125   train_loss = 1.000\n",
      "2020-04-14T19:15:01.852990: Epoch   4 Batch 3120/3125   train_loss = 0.795\n",
      "2020-04-14T19:15:02.250556: Epoch   4 Batch   16/781   test_loss = 0.806\n",
      "2020-04-14T19:15:02.536013: Epoch   4 Batch   36/781   test_loss = 0.969\n",
      "2020-04-14T19:15:02.973698: Epoch   4 Batch   56/781   test_loss = 0.944\n",
      "2020-04-14T19:15:03.281073: Epoch   4 Batch   76/781   test_loss = 0.971\n",
      "2020-04-14T19:15:03.638465: Epoch   4 Batch   96/781   test_loss = 1.036\n",
      "2020-04-14T19:15:03.927288: Epoch   4 Batch  116/781   test_loss = 0.823\n",
      "2020-04-14T19:15:04.424905: Epoch   4 Batch  136/781   test_loss = 0.804\n",
      "2020-04-14T19:15:04.782191: Epoch   4 Batch  156/781   test_loss = 0.906\n",
      "2020-04-14T19:15:05.089681: Epoch   4 Batch  176/781   test_loss = 0.882\n",
      "2020-04-14T19:15:05.444027: Epoch   4 Batch  196/781   test_loss = 0.815\n",
      "2020-04-14T19:15:05.774087: Epoch   4 Batch  216/781   test_loss = 0.972\n",
      "2020-04-14T19:15:06.079671: Epoch   4 Batch  236/781   test_loss = 0.812\n",
      "2020-04-14T19:15:06.400898: Epoch   4 Batch  256/781   test_loss = 0.799\n",
      "2020-04-14T19:15:06.695341: Epoch   4 Batch  276/781   test_loss = 1.133\n",
      "2020-04-14T19:15:06.984593: Epoch   4 Batch  296/781   test_loss = 0.839\n",
      "2020-04-14T19:15:07.296062: Epoch   4 Batch  316/781   test_loss = 0.837\n",
      "2020-04-14T19:15:07.577179: Epoch   4 Batch  336/781   test_loss = 0.775\n",
      "2020-04-14T19:15:07.892509: Epoch   4 Batch  356/781   test_loss = 0.907\n",
      "2020-04-14T19:15:08.205692: Epoch   4 Batch  376/781   test_loss = 0.881\n",
      "2020-04-14T19:15:08.528173: Epoch   4 Batch  396/781   test_loss = 0.857\n",
      "2020-04-14T19:15:08.845760: Epoch   4 Batch  416/781   test_loss = 0.967\n",
      "2020-04-14T19:15:09.149965: Epoch   4 Batch  436/781   test_loss = 0.885\n",
      "2020-04-14T19:15:09.424700: Epoch   4 Batch  456/781   test_loss = 0.732\n",
      "2020-04-14T19:15:09.729090: Epoch   4 Batch  476/781   test_loss = 0.931\n",
      "2020-04-14T19:15:09.985037: Epoch   4 Batch  496/781   test_loss = 0.997\n",
      "2020-04-14T19:15:10.338021: Epoch   4 Batch  516/781   test_loss = 0.767\n",
      "2020-04-14T19:15:10.629880: Epoch   4 Batch  536/781   test_loss = 0.954\n",
      "2020-04-14T19:15:10.901692: Epoch   4 Batch  556/781   test_loss = 0.854\n",
      "2020-04-14T19:15:11.198141: Epoch   4 Batch  576/781   test_loss = 0.950\n",
      "2020-04-14T19:15:11.470062: Epoch   4 Batch  596/781   test_loss = 0.982\n",
      "2020-04-14T19:15:11.792421: Epoch   4 Batch  616/781   test_loss = 0.933\n",
      "2020-04-14T19:15:12.107685: Epoch   4 Batch  636/781   test_loss = 0.829\n",
      "2020-04-14T19:15:12.419435: Epoch   4 Batch  656/781   test_loss = 0.919\n",
      "2020-04-14T19:15:12.710202: Epoch   4 Batch  676/781   test_loss = 1.067\n",
      "2020-04-14T19:15:12.995644: Epoch   4 Batch  696/781   test_loss = 0.856\n",
      "2020-04-14T19:15:13.314082: Epoch   4 Batch  716/781   test_loss = 0.914\n",
      "2020-04-14T19:15:13.642853: Epoch   4 Batch  736/781   test_loss = 1.066\n",
      "2020-04-14T19:15:13.941396: Epoch   4 Batch  756/781   test_loss = 0.830\n",
      "2020-04-14T19:15:14.254796: Epoch   4 Batch  776/781   test_loss = 0.781\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[3.137316]], dtype=float32)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_orig = pd.read_table('./ml_1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您的推荐：\n",
      "2816\n",
      "[2885 'Guinevere (1999)' 'Drama|Romance']\n",
      "1380\n",
      "[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "1990\n",
      "[2059 'Parent Trap, The (1998)' \"Children's|Drama\"]\n",
      "620\n",
      "[625 'Asfour Stah (1990)' 'Drama']\n",
      "533\n",
      "[537 'Sirens (1994)' 'Comedy|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{533, 620, 1380, 1990, 2816}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是给您的推荐：\n",
      "847\n",
      "[858 'Godfather, The (1972)' 'Action|Crime|Drama']\n",
      "49\n",
      "[50 'Usual Suspects, The (1995)' 'Crime|Thriller']\n",
      "2836\n",
      "[2905 'Sanjuro (1962)' 'Action|Adventure']\n",
      "1240\n",
      "[1260 'M (1931)' 'Crime|Film-Noir|Thriller']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{49, 847, 1240, 1950, 2836}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_orig = pd.read_table('./ml_1m/users.dat', sep='::', header=None, names=users_title, engine = 'python').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[3703 'M' 18 12 '97402']\n",
      " [1763 'M' 35 7 '76248']\n",
      " [5254 'M' 18 4 '32606']\n",
      " [493 'M' 50 7 '55016']\n",
      " [3485 'M' 25 0 '94121']\n",
      " [74 'M' 35 14 '94530']\n",
      " [5513 'M' 35 1 '91768']\n",
      " [3764 'M' 25 1 '06111']\n",
      " [3285 'M' 25 4 '44706']\n",
      " [100 'M' 35 17 '95401']\n",
      " [3031 'M' 18 4 '48135']\n",
      " [2338 'M' 45 17 '13152']\n",
      " [4571 'M' 35 7 '21013']\n",
      " [3603 'F' 35 7 '78704']\n",
      " [212 'M' 25 16 '53714']\n",
      " [4800 'M' 18 4 '80521']\n",
      " [4085 'F' 25 6 '79416']\n",
      " [2002 'F' 56 13 '02136-1522']\n",
      " [5458 'F' 18 2 '98102']\n",
      " [2154 'M' 25 12 '68508']]\n",
      "喜欢看这个电影的人还喜欢看：\n",
      "2131\n",
      "[2200 'Under Capricorn (1949)' 'Drama']\n",
      "2836\n",
      "[2905 'Sanjuro (1962)' 'Action|Adventure']\n",
      "1176\n",
      "[1193 \"One Flew Over the Cuckoo's Nest (1975)\" 'Drama']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n",
      "735\n",
      "[745 'Close Shave, A (1995)' 'Animation|Comedy|Thriller']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{735, 1176, 1950, 2131, 2836}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf_keras构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
    "    user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_gender')  \n",
    "    user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
    "    user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
    "\n",
    "    movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
    "    movie_categories = tf.keras.layers.Input(shape=(1, 18,), name='movie_categories') \n",
    "#     movie_categories = tf.keras.layers.Input(shape=(1, 18,), dtype='int32', name='movie_categories') \n",
    "    movie_titles = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_titles') \n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid)\n",
    "    gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
    "    age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
    "    job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    #第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
    "    gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
    "    age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
    "    job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "    user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
    "\n",
    "    user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max, embed_dim, input_length=1, name='movie_id_embed_layer')(movie_id)\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, input_length=18, name='movie_title_embed_layer')(movie_titles)\n",
    "    sp=movie_title_embed_layer.shape\n",
    "    movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')(movie_title_embed_layer_expand)\n",
    "        maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1), strides=1)(conv_layer)\n",
    "        pool_layer_lst.append(maxpool_layer)\n",
    "    #Dropout层\n",
    "    pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
    "    max_num = len(window_sizes) * filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
    "\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    #第一层全连接\n",
    "    movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
    "    movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_categories_fc_layer\", activation='relu')(movie_categories_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  \n",
    "    movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
    "\n",
    "    movie_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取输入占位符\n",
    "uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
    "# 获取User的4个嵌入向量\n",
    "uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender,\n",
    "                                                                                           user_age, user_job)\n",
    "# 得到用户特征\n",
    "user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer,\n",
    "                                                                     age_embed_layer, job_embed_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取电影ID的嵌入向量\n",
    "movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "# 获取电影类型的嵌入向量\n",
    "# movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "# 获取电影名的特征向量\n",
    "pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "# 得到电影特征\n",
    "movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,\n",
    "#                                                                         movie_categories_embed_layer,\n",
    "                                                                        movie_categories,\n",
    "                                                                        dropout_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 计算出评分\n",
    "# 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "inference = tf.keras.layers.Lambda(lambda layer: \n",
    "    tf.reduce_sum(layer[0] * layer[1], axis=1), name=\"inference\")((user_combine_layer_flat, movie_combine_layer_flat))\n",
    "inference = tf.keras.layers.Lambda(lambda layer: tf.expand_dims(layer, axis=1))(inference)\n",
    "\n",
    "# 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "#                                                       1)  # (?, 400)\n",
    "# 你可以使用下面这个全连接层，试试效果\n",
    "#inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "#    inference_layer)\n",
    "#         inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles],\n",
    "    outputs=[inference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_titles (InputLayer)       [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_title_embed_layer (Embedd (None, 18, 32)       164096      movie_titles[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 18, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 17, 1, 8)     520         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 1, 8)     776         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 15, 1, 8)     1032        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 14, 1, 8)     1288        reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 8)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_age (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_job (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories (InputLayer)   [(None, 1, 18)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_fc_layer (Dens (None, 1, 32)        608         movie_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
      "                                                                 gender_fc_layer[0][0]            \n",
      "                                                                 age_fc_layer[0][0]               \n",
      "                                                                 job_fc_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
      "                                                                 movie_categories_fc_layer[0][0]  \n",
      "                                                                 dropout_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 200)       25800       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 200)       19400       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "movie_combine_layer_flat (Resha (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
      "                                                                 movie_combine_layer_flat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           inference[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 537,552\n",
      "Trainable params: 537,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "import time\n",
    "\n",
    "MODEL_DIR = \"./models\"\n",
    "\n",
    "\n",
    "class mv_network(object):\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.best_loss = 9999\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "\n",
    "        # 获取输入占位符\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
    "        # 获取User的4个嵌入向量\n",
    "        uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender,\n",
    "                                                                                                   user_age, user_job)\n",
    "        # 得到用户特征\n",
    "        user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer,\n",
    "                                                                             age_embed_layer, job_embed_layer)\n",
    "        # 获取电影ID的嵌入向量\n",
    "        movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "        # 获取电影类型的嵌入向量\n",
    "#         movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "        # 获取电影名的特征向量\n",
    "        pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "        # 得到电影特征\n",
    "        movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer,\n",
    "#                                                                                 movie_categories_embed_layer,\n",
    "                                                                                movie_categories,\n",
    "                                                                                dropout_layer)\n",
    "        # 计算出评分\n",
    "        # 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "        inference = tf.keras.layers.Lambda(lambda layer: \n",
    "            tf.reduce_sum(layer[0] * layer[1], axis=1), name=\"inference\")((user_combine_layer_flat, movie_combine_layer_flat))\n",
    "        inference = tf.keras.layers.Lambda(lambda layer: tf.expand_dims(layer, axis=1))(inference)\n",
    "        \n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "#                                                       1)  # (?, 400)\n",
    "        # 你可以使用下面这个全连接层，试试效果\n",
    "        #inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "        #    inference_layer)\n",
    "#         inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=[uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles],\n",
    "            outputs=[inference])\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        self.ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
    "        self.ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "        if tf.io.gfile.exists(MODEL_DIR):\n",
    "            #             print('Removing existing model dir: {}'.format(MODEL_DIR))\n",
    "            #             tf.io.gfile.rmtree(MODEL_DIR)\n",
    "            pass\n",
    "        else:\n",
    "            tf.io.gfile.makedirs(MODEL_DIR)\n",
    "\n",
    "        train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
    "        test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
    "\n",
    "        #         self.train_summary_writer = summary_ops_v2.create_file_writer(train_dir, flush_millis=10000)\n",
    "        #         self.test_summary_writer = summary_ops_v2.create_file_writer(test_dir, flush_millis=10000, name='test')\n",
    "\n",
    "        checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
    "        self.checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "        self.checkpoint = tf.train.Checkpoint(model=self.model, optimizer=self.optimizer)\n",
    "\n",
    "        # Restore variables on creation if a checkpoint exists.\n",
    "        self.checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    def compute_loss(self, labels, logits):\n",
    "        return tf.reduce_mean(tf.keras.losses.mse(labels, logits))\n",
    "\n",
    "    def compute_metrics(self, labels, logits):\n",
    "        return tf.keras.metrics.mae(labels, logits)  #\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        # Record the operations used to compute the loss, so that the gradient\n",
    "        # of the loss with respect to the variables can be computed.\n",
    "        #         metrics = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model([x[0],\n",
    "                                 x[1],\n",
    "                                 x[2],\n",
    "                                 x[3],\n",
    "                                 x[4],\n",
    "                                 x[5],\n",
    "                                 x[6]], training=True)\n",
    "            loss = self.ComputeLoss(y, logits)\n",
    "            # loss = self.compute_loss(labels, logits)\n",
    "            self.ComputeMetrics(y, logits)\n",
    "            # metrics = self.compute_metrics(labels, logits)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, logits\n",
    "\n",
    "    def training(self, features, targets_values, epochs=5, log_freq=50):\n",
    "\n",
    "        for epoch_i in range(epochs):\n",
    "            # 将数据集分成训练集和测试集，随机种子不固定\n",
    "            train_X, test_X, train_y, test_y = train_test_split(features,\n",
    "                                                                targets_values,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "            train_batches = get_batches(train_X, train_y, self.batch_size)\n",
    "            batch_num = (len(train_X) // self.batch_size)\n",
    "\n",
    "            train_start = time.time()\n",
    "            #             with self.train_summary_writer.as_default():\n",
    "            if True:\n",
    "                start = time.time()\n",
    "                # Metrics are stateful. They accumulate values and return a cumulative\n",
    "                # result when you call .result(). Clear accumulated values with .reset_states()\n",
    "                avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "                #                 avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "                # Datasets can be iterated over like any other Python iterable.\n",
    "                for batch_i in range(batch_num):\n",
    "                    x, y = next(train_batches)\n",
    "                    categories = np.zeros([self.batch_size,1, 18])\n",
    "                    for i in range(self.batch_size):\n",
    "                        categories[i,0] = x.take(6, 1)[i]\n",
    "\n",
    "                    titles = np.zeros([self.batch_size, sentences_size])\n",
    "                    for i in range(self.batch_size):\n",
    "                        titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "                    loss, logits = self.train_step([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                                    categories.astype(np.float32),\n",
    "                                                    titles.astype(np.float32)],\n",
    "                                                   np.reshape(y, [self.batch_size, 1]).astype(np.float32))\n",
    "                    avg_loss(loss)\n",
    "                    #                     avg_mae(metrics)\n",
    "                    self.losses['train'].append(loss)\n",
    "\n",
    "#                     if tf.equal(self.optimizer.iterations % log_freq, 0):\n",
    "                        #                         summary_ops_v2.scalar('loss', avg_loss.result(), step=self.optimizer.iterations)\n",
    "                        #                         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=self.optimizer.iterations)\n",
    "                        # summary_ops_v2.scalar('mae', avg_mae.result(), step=self.optimizer.iterations)\n",
    "\n",
    "                    rate = log_freq / (time.time() - start)\n",
    "                    print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                        self.optimizer.iterations.numpy(),\n",
    "                        epoch_i,\n",
    "                        batch_i,\n",
    "                        batch_num,\n",
    "                        loss, (self.ComputeMetrics.result()), rate))\n",
    "                    # print('Step #{}\\tLoss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                    #     self.optimizer.iterations.numpy(), loss, (avg_mae.result()), rate))\n",
    "                    avg_loss.reset_states()\n",
    "                    self.ComputeMetrics.reset_states()\n",
    "                    # avg_mae.reset_states()\n",
    "                    start = time.time()\n",
    "\n",
    "            train_end = time.time()\n",
    "            print(\n",
    "                '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, self.optimizer.iterations.numpy(),\n",
    "                                                                         train_end - train_start))\n",
    "            #             with self.test_summary_writer.as_default():\n",
    "            self.testing((test_X, test_y), self.optimizer.iterations)\n",
    "            # self.checkpoint.save(self.checkpoint_prefix)\n",
    "        self.export_path = os.path.join(MODEL_DIR, 'export')\n",
    "        tf.saved_model.save(self.model, self.export_path)\n",
    "\n",
    "    def testing(self, test_dataset, step_num):\n",
    "        test_X, test_y = test_dataset\n",
    "        test_batches = get_batches(test_X, test_y, self.batch_size)\n",
    "\n",
    "        \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "        #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "        batch_num = (len(test_X) // self.batch_size)\n",
    "        for batch_i in range(batch_num):\n",
    "            x, y = next(test_batches)\n",
    "            categories = np.zeros([self.batch_size,1, 18])\n",
    "            for i in range(self.batch_size):\n",
    "                categories[i,0] = x.take(6, 1)[i]\n",
    "\n",
    "            titles = np.zeros([self.batch_size, sentences_size])\n",
    "            for i in range(self.batch_size):\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "            logits = self.model([np.reshape(x.take(0, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(2, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(3, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(4, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 np.reshape(x.take(1, 1), [self.batch_size, 1]).astype(np.float32),\n",
    "                                 categories.astype(np.float32),\n",
    "                                 titles.astype(np.float32)], training=False)\n",
    "            test_loss = self.ComputeLoss(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "            avg_loss(test_loss)\n",
    "            # 保存测试损失\n",
    "            self.losses['test'].append(test_loss)\n",
    "            self.ComputeMetrics(np.reshape(y, [self.batch_size, 1]).astype(np.float32), logits)\n",
    "            # avg_loss(self.compute_loss(labels, logits))\n",
    "            # avg_mae(self.compute_metrics(labels, logits))\n",
    "\n",
    "        print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), self.ComputeMetrics.result()))\n",
    "        # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
    "        #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
    "        #         summary_ops_v2.scalar('mae', self.ComputeMetrics.result(), step=step_num)\n",
    "        # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
    "\n",
    "        if avg_loss.result() < self.best_loss:\n",
    "            self.best_loss = avg_loss.result()\n",
    "            print(\"best loss = {}\".format(self.best_loss))\n",
    "            self.checkpoint.save(self.checkpoint_prefix)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        predictions = self.model(xs)\n",
    "        # logits = tf.nn.softmax(predictions)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1193, 0, ..., 10,\n",
       "        list([3745, 4551, 1235, 3308, 3810, 4559, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])],\n",
       "       [2, 1193, 1, ..., 16,\n",
       "        list([3745, 4551, 1235, 3308, 3810, 4559, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])],\n",
       "       [12, 1193, 1, ..., 12,\n",
       "        list([3745, 4551, 1235, 3308, 3810, 4559, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])],\n",
       "       ...,\n",
       "       [5780, 2845, 1, ..., 17,\n",
       "        list([4216, 2694, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])],\n",
       "       [5851, 3607, 0, ..., 20,\n",
       "        list([3745, 3888, 4262, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])],\n",
       "       [5938, 2909, 1, ..., 1,\n",
       "        list([1676, 2940, 3243, 1262, 1070, 3819, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127, 5127]),\n",
       "        list([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "movie_titles (InputLayer)       [(None, 18)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_title_embed_layer (Embedd (None, 18, 32)       164096      movie_titles[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 18, 32, 1)    0           movie_title_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 17, 1, 8)     520         reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 1, 8)     776         reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 15, 1, 8)     1032        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 1, 8)     1288        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "uid (InputLayer)                [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_gender (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_age (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_job (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d_12[0][0]           \n",
      "                                                                 max_pooling2d_13[0][0]           \n",
      "                                                                 max_pooling2d_14[0][0]           \n",
      "                                                                 max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "age_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "job_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories (InputLayer)   [(None, 1, 18)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "age_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "job_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "movie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "movie_categories_fc_layer (Dens (None, 1, 32)        608         movie_categories[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1, 128)       0           uid_fc_layer[0][0]               \n",
      "                                                                 gender_fc_layer[0][0]            \n",
      "                                                                 age_fc_layer[0][0]               \n",
      "                                                                 job_fc_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n",
      "                                                                 movie_categories_fc_layer[0][0]  \n",
      "                                                                 dropout_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1, 200)       25800       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1, 200)       19400       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "movie_combine_layer_flat (Resha (None, 200)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "inference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n",
      "                                                                 movie_combine_layer_flat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           inference[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 537,552\n",
      "Trainable params: 537,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1\tEpoch   0 Batch    0/3125   Loss: 14.088545 mae: 3.564927 (32.52925559471951 steps/sec)\n",
      "Step #2\tEpoch   0 Batch    1/3125   Loss: 14.585484 mae: 3.649956 (2016.1239773502919 steps/sec)\n",
      "Step #3\tEpoch   0 Batch    2/3125   Loss: 13.522289 mae: 3.503155 (2414.1546466518553 steps/sec)\n",
      "Step #4\tEpoch   0 Batch    3/3125   Loss: 13.986399 mae: 3.559940 (2420.004846582582 steps/sec)\n",
      "Step #5\tEpoch   0 Batch    4/3125   Loss: 14.419783 mae: 3.631360 (2155.08056560342 steps/sec)\n",
      "Step #6\tEpoch   0 Batch    5/3125   Loss: 14.060732 mae: 3.577078 (2343.2389550604485 steps/sec)\n",
      "Step #7\tEpoch   0 Batch    6/3125   Loss: 14.502630 mae: 3.647885 (2326.859577489792 steps/sec)\n",
      "Step #8\tEpoch   0 Batch    7/3125   Loss: 13.912355 mae: 3.549046 (2219.748721911153 steps/sec)\n",
      "Step #9\tEpoch   0 Batch    8/3125   Loss: 13.273201 mae: 3.464107 (1908.757622644944 steps/sec)\n",
      "Step #10\tEpoch   0 Batch    9/3125   Loss: 13.883889 mae: 3.581987 (1984.9807384691105 steps/sec)\n",
      "Step #11\tEpoch   0 Batch   10/3125   Loss: 13.456969 mae: 3.516276 (1342.6842775832154 steps/sec)\n",
      "Step #12\tEpoch   0 Batch   11/3125   Loss: 13.790930 mae: 3.536027 (1403.2277922008404 steps/sec)\n",
      "Step #13\tEpoch   0 Batch   12/3125   Loss: 13.936363 mae: 3.560839 (1733.5846311543166 steps/sec)\n",
      "Step #14\tEpoch   0 Batch   13/3125   Loss: 14.608775 mae: 3.682103 (1443.911843074614 steps/sec)\n",
      "Step #15\tEpoch   0 Batch   14/3125   Loss: 13.320181 mae: 3.475184 (1678.4601224538796 steps/sec)\n",
      "Step #16\tEpoch   0 Batch   15/3125   Loss: 13.531969 mae: 3.497664 (1547.028622012393 steps/sec)\n",
      "Step #17\tEpoch   0 Batch   16/3125   Loss: 13.739305 mae: 3.519397 (830.6736802078713 steps/sec)\n",
      "Step #18\tEpoch   0 Batch   17/3125   Loss: 13.122107 mae: 3.441289 (1286.335895186864 steps/sec)\n",
      "Step #19\tEpoch   0 Batch   18/3125   Loss: 13.802430 mae: 3.552734 (1546.3099916680799 steps/sec)\n",
      "Step #20\tEpoch   0 Batch   19/3125   Loss: 13.842540 mae: 3.531472 (2211.8356800084375 steps/sec)\n",
      "Step #21\tEpoch   0 Batch   20/3125   Loss: 14.138788 mae: 3.604659 (2059.4031404358116 steps/sec)\n",
      "Step #22\tEpoch   0 Batch   21/3125   Loss: 14.282312 mae: 3.602843 (2130.7323417052753 steps/sec)\n",
      "Step #23\tEpoch   0 Batch   22/3125   Loss: 14.172021 mae: 3.602677 (1957.412333510673 steps/sec)\n",
      "Step #24\tEpoch   0 Batch   23/3125   Loss: 13.401701 mae: 3.496443 (1997.1164376386787 steps/sec)\n",
      "Step #25\tEpoch   0 Batch   24/3125   Loss: 13.706337 mae: 3.543989 (1791.0292761247565 steps/sec)\n",
      "Step #26\tEpoch   0 Batch   25/3125   Loss: 13.257826 mae: 3.453491 (2034.0950533462658 steps/sec)\n",
      "Step #27\tEpoch   0 Batch   26/3125   Loss: 13.202996 mae: 3.473792 (2156.764984162244 steps/sec)\n",
      "Step #28\tEpoch   0 Batch   27/3125   Loss: 13.103929 mae: 3.461313 (2209.854583772392 steps/sec)\n",
      "Step #29\tEpoch   0 Batch   28/3125   Loss: 13.499615 mae: 3.492735 (2095.203460781473 steps/sec)\n",
      "Step #30\tEpoch   0 Batch   29/3125   Loss: 13.260591 mae: 3.477037 (2195.212125652916 steps/sec)\n",
      "Step #31\tEpoch   0 Batch   30/3125   Loss: 13.141903 mae: 3.438128 (1588.859846504686 steps/sec)\n",
      "Step #32\tEpoch   0 Batch   31/3125   Loss: 13.495095 mae: 3.516849 (1949.9865175225716 steps/sec)\n",
      "Step #33\tEpoch   0 Batch   32/3125   Loss: 13.260587 mae: 3.459862 (2054.9031904052677 steps/sec)\n",
      "Step #34\tEpoch   0 Batch   33/3125   Loss: 13.087361 mae: 3.423562 (2049.761513800923 steps/sec)\n",
      "Step #35\tEpoch   0 Batch   34/3125   Loss: 12.698090 mae: 3.372304 (1885.0804494382023 steps/sec)\n",
      "Step #36\tEpoch   0 Batch   35/3125   Loss: 12.710921 mae: 3.388795 (1572.832543349133 steps/sec)\n",
      "Step #37\tEpoch   0 Batch   36/3125   Loss: 12.595507 mae: 3.377945 (1178.1554243467806 steps/sec)\n",
      "Step #38\tEpoch   0 Batch   37/3125   Loss: 13.032563 mae: 3.448026 (1689.4259429326373 steps/sec)\n",
      "Step #39\tEpoch   0 Batch   38/3125   Loss: 13.315886 mae: 3.481380 (1573.4579803875963 steps/sec)\n",
      "Step #40\tEpoch   0 Batch   39/3125   Loss: 13.410456 mae: 3.477827 (1801.47578019637 steps/sec)\n",
      "Step #41\tEpoch   0 Batch   40/3125   Loss: 12.824890 mae: 3.394978 (1821.8994335754248 steps/sec)\n",
      "Step #42\tEpoch   0 Batch   41/3125   Loss: 13.084938 mae: 3.440305 (1747.087981205795 steps/sec)\n",
      "Step #43\tEpoch   0 Batch   42/3125   Loss: 12.774334 mae: 3.426645 (1866.3747608241001 steps/sec)\n",
      "Step #44\tEpoch   0 Batch   43/3125   Loss: 12.934513 mae: 3.427080 (2152.8682297868845 steps/sec)\n",
      "Step #45\tEpoch   0 Batch   44/3125   Loss: 13.109818 mae: 3.444726 (2230.0638026371757 steps/sec)\n",
      "Step #46\tEpoch   0 Batch   45/3125   Loss: 12.523218 mae: 3.368805 (1735.033217231594 steps/sec)\n",
      "Step #47\tEpoch   0 Batch   46/3125   Loss: 12.608934 mae: 3.380323 (1773.1751655097191 steps/sec)\n",
      "Step #48\tEpoch   0 Batch   47/3125   Loss: 12.512525 mae: 3.331131 (2087.757093081135 steps/sec)\n",
      "Step #49\tEpoch   0 Batch   48/3125   Loss: 12.193695 mae: 3.279048 (2124.236009116232 steps/sec)\n",
      "Step #50\tEpoch   0 Batch   49/3125   Loss: 12.813580 mae: 3.385396 (2147.643089022929 steps/sec)\n",
      "Step #51\tEpoch   0 Batch   50/3125   Loss: 12.656831 mae: 3.369219 (2170.3357204950944 steps/sec)\n",
      "Step #52\tEpoch   0 Batch   51/3125   Loss: 11.633410 mae: 3.232203 (1898.390513261519 steps/sec)\n",
      "Step #53\tEpoch   0 Batch   52/3125   Loss: 12.787701 mae: 3.414924 (1064.2146340473255 steps/sec)\n",
      "Step #54\tEpoch   0 Batch   53/3125   Loss: 12.782511 mae: 3.384863 (987.5735799655292 steps/sec)\n",
      "Step #55\tEpoch   0 Batch   54/3125   Loss: 12.114230 mae: 3.305654 (1669.9463298880412 steps/sec)\n",
      "Step #56\tEpoch   0 Batch   55/3125   Loss: 12.067392 mae: 3.266339 (1482.0966932628498 steps/sec)\n",
      "Step #57\tEpoch   0 Batch   56/3125   Loss: 11.966722 mae: 3.232109 (1922.7754907443912 steps/sec)\n",
      "Step #58\tEpoch   0 Batch   57/3125   Loss: 12.532784 mae: 3.368078 (2127.66268287239 steps/sec)\n",
      "Step #59\tEpoch   0 Batch   58/3125   Loss: 11.738688 mae: 3.261322 (2031.8680786335055 steps/sec)\n",
      "Step #60\tEpoch   0 Batch   59/3125   Loss: 12.526591 mae: 3.369802 (1994.950676826194 steps/sec)\n",
      "Step #61\tEpoch   0 Batch   60/3125   Loss: 11.513975 mae: 3.224472 (2212.3956915740946 steps/sec)\n",
      "Step #62\tEpoch   0 Batch   61/3125   Loss: 11.197837 mae: 3.148784 (1390.7489074426533 steps/sec)\n",
      "Step #63\tEpoch   0 Batch   62/3125   Loss: 11.694398 mae: 3.228536 (2170.4929570176255 steps/sec)\n",
      "Step #64\tEpoch   0 Batch   63/3125   Loss: 12.041298 mae: 3.283863 (2142.5089137030945 steps/sec)\n",
      "Step #65\tEpoch   0 Batch   64/3125   Loss: 11.831510 mae: 3.272259 (1716.3181628460827 steps/sec)\n",
      "Step #66\tEpoch   0 Batch   65/3125   Loss: 11.320226 mae: 3.149003 (1542.8289768923482 steps/sec)\n",
      "Step #67\tEpoch   0 Batch   66/3125   Loss: 11.232082 mae: 3.144865 (1753.6475231628592 steps/sec)\n",
      "Step #68\tEpoch   0 Batch   67/3125   Loss: 10.914122 mae: 3.110073 (2075.7099165619156 steps/sec)\n",
      "Step #69\tEpoch   0 Batch   68/3125   Loss: 10.610203 mae: 3.053030 (2307.4532931364565 steps/sec)\n",
      "Step #70\tEpoch   0 Batch   69/3125   Loss: 10.419170 mae: 3.024160 (1919.6777884571377 steps/sec)\n",
      "Step #71\tEpoch   0 Batch   70/3125   Loss: 11.721014 mae: 3.222919 (1753.2809978848452 steps/sec)\n",
      "Step #72\tEpoch   0 Batch   71/3125   Loss: 10.584599 mae: 3.054200 (2095.4756195043965 steps/sec)\n",
      "Step #73\tEpoch   0 Batch   72/3125   Loss: 10.776215 mae: 3.068815 (2214.2644465795947 steps/sec)\n",
      "Step #74\tEpoch   0 Batch   73/3125   Loss: 10.168020 mae: 2.968451 (2205.787010255062 steps/sec)\n",
      "Step #75\tEpoch   0 Batch   74/3125   Loss: 10.272173 mae: 2.997171 (2115.4079707878996 steps/sec)\n",
      "Step #76\tEpoch   0 Batch   75/3125   Loss: 10.704580 mae: 3.087559 (2102.0708457791234 steps/sec)\n",
      "Step #77\tEpoch   0 Batch   76/3125   Loss: 9.587107 mae: 2.915090 (2214.5684174956177 steps/sec)\n",
      "Step #78\tEpoch   0 Batch   77/3125   Loss: 10.249615 mae: 3.017470 (2331.853004948018 steps/sec)\n",
      "Step #79\tEpoch   0 Batch   78/3125   Loss: 9.719147 mae: 2.929920 (1394.475696522375 steps/sec)\n",
      "Step #80\tEpoch   0 Batch   79/3125   Loss: 10.799280 mae: 3.095569 (1616.0405637623198 steps/sec)\n",
      "Step #81\tEpoch   0 Batch   80/3125   Loss: 9.700148 mae: 2.903477 (1826.1352652798216 steps/sec)\n",
      "Step #82\tEpoch   0 Batch   81/3125   Loss: 9.419634 mae: 2.818284 (2283.4345941943775 steps/sec)\n",
      "Step #83\tEpoch   0 Batch   82/3125   Loss: 9.120360 mae: 2.782783 (2137.290312060496 steps/sec)\n",
      "Step #84\tEpoch   0 Batch   83/3125   Loss: 9.547478 mae: 2.897738 (2141.0652482414316 steps/sec)\n",
      "Step #85\tEpoch   0 Batch   84/3125   Loss: 9.633533 mae: 2.887826 (2147.4891455722127 steps/sec)\n",
      "Step #86\tEpoch   0 Batch   85/3125   Loss: 8.998806 mae: 2.786826 (2164.1988813440385 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #87\tEpoch   0 Batch   86/3125   Loss: 8.465082 mae: 2.655589 (1948.1750536475704 steps/sec)\n",
      "Step #88\tEpoch   0 Batch   87/3125   Loss: 8.619873 mae: 2.720303 (1610.728192996874 steps/sec)\n",
      "Step #89\tEpoch   0 Batch   88/3125   Loss: 8.661415 mae: 2.676000 (2125.1185602529285 steps/sec)\n",
      "Step #90\tEpoch   0 Batch   89/3125   Loss: 8.934999 mae: 2.782679 (2373.1492587982348 steps/sec)\n",
      "Step #91\tEpoch   0 Batch   90/3125   Loss: 8.004530 mae: 2.585940 (2291.593727804185 steps/sec)\n",
      "Step #92\tEpoch   0 Batch   91/3125   Loss: 8.825655 mae: 2.748163 (2037.2368638346238 steps/sec)\n",
      "Step #93\tEpoch   0 Batch   92/3125   Loss: 8.105722 mae: 2.601827 (1906.2245491564863 steps/sec)\n",
      "Step #94\tEpoch   0 Batch   93/3125   Loss: 7.905704 mae: 2.555287 (1614.3114463859595 steps/sec)\n",
      "Step #95\tEpoch   0 Batch   94/3125   Loss: 7.382788 mae: 2.465325 (1699.9294787098656 steps/sec)\n",
      "Step #96\tEpoch   0 Batch   95/3125   Loss: 7.014082 mae: 2.414817 (1598.9874575883496 steps/sec)\n",
      "Step #97\tEpoch   0 Batch   96/3125   Loss: 7.154732 mae: 2.459129 (1787.04592128024 steps/sec)\n",
      "Step #98\tEpoch   0 Batch   97/3125   Loss: 6.972139 mae: 2.401982 (1785.2508278638984 steps/sec)\n",
      "Step #99\tEpoch   0 Batch   98/3125   Loss: 6.690702 mae: 2.326832 (1675.9384015407607 steps/sec)\n",
      "Step #100\tEpoch   0 Batch   99/3125   Loss: 7.234604 mae: 2.438464 (1168.0304768694373 steps/sec)\n",
      "Step #101\tEpoch   0 Batch  100/3125   Loss: 6.787970 mae: 2.376643 (1606.7790896344593 steps/sec)\n",
      "Step #102\tEpoch   0 Batch  101/3125   Loss: 6.425120 mae: 2.319795 (1376.0839895013123 steps/sec)\n",
      "Step #103\tEpoch   0 Batch  102/3125   Loss: 6.776529 mae: 2.354935 (1627.541248234436 steps/sec)\n",
      "Step #104\tEpoch   0 Batch  103/3125   Loss: 6.189805 mae: 2.251355 (1827.9657619022714 steps/sec)\n",
      "Step #105\tEpoch   0 Batch  104/3125   Loss: 5.811124 mae: 2.165208 (1444.6180340290693 steps/sec)\n",
      "Step #106\tEpoch   0 Batch  105/3125   Loss: 5.993602 mae: 2.219409 (2089.8583942042274 steps/sec)\n",
      "Step #107\tEpoch   0 Batch  106/3125   Loss: 6.152174 mae: 2.269062 (2016.8607726411556 steps/sec)\n",
      "Step #108\tEpoch   0 Batch  107/3125   Loss: 5.933414 mae: 2.168464 (2203.5388559660405 steps/sec)\n",
      "Step #109\tEpoch   0 Batch  108/3125   Loss: 5.230309 mae: 2.015916 (2091.6092355258565 steps/sec)\n",
      "Step #110\tEpoch   0 Batch  109/3125   Loss: 5.074792 mae: 2.000009 (1997.439804937519 steps/sec)\n",
      "Step #111\tEpoch   0 Batch  110/3125   Loss: 5.024564 mae: 2.005295 (2002.169097991293 steps/sec)\n",
      "Step #112\tEpoch   0 Batch  111/3125   Loss: 4.789623 mae: 1.958811 (2118.121401878598 steps/sec)\n",
      "Step #113\tEpoch   0 Batch  112/3125   Loss: 4.821810 mae: 1.947040 (2109.004605885074 steps/sec)\n",
      "Step #114\tEpoch   0 Batch  113/3125   Loss: 4.251132 mae: 1.791130 (2286.347233578632 steps/sec)\n",
      "Step #115\tEpoch   0 Batch  114/3125   Loss: 4.092876 mae: 1.764934 (2093.383908963865 steps/sec)\n",
      "Step #116\tEpoch   0 Batch  115/3125   Loss: 3.699049 mae: 1.668725 (2315.6830052007995 steps/sec)\n",
      "Step #117\tEpoch   0 Batch  116/3125   Loss: 3.378683 mae: 1.583894 (2291.6688521723927 steps/sec)\n",
      "Step #118\tEpoch   0 Batch  117/3125   Loss: 4.023498 mae: 1.778571 (2208.5745879627193 steps/sec)\n",
      "Step #119\tEpoch   0 Batch  118/3125   Loss: 3.964305 mae: 1.735693 (2007.208966223524 steps/sec)\n",
      "Step #120\tEpoch   0 Batch  119/3125   Loss: 3.485173 mae: 1.616389 (1938.0569083902449 steps/sec)\n",
      "Step #121\tEpoch   0 Batch  120/3125   Loss: 3.604982 mae: 1.656056 (2213.1895268951107 steps/sec)\n",
      "Step #122\tEpoch   0 Batch  121/3125   Loss: 3.340291 mae: 1.569811 (2169.8641475855934 steps/sec)\n",
      "Step #123\tEpoch   0 Batch  122/3125   Loss: 3.429886 mae: 1.610012 (1948.265546905483 steps/sec)\n",
      "Step #124\tEpoch   0 Batch  123/3125   Loss: 2.677495 mae: 1.417417 (1518.058300216436 steps/sec)\n",
      "Step #125\tEpoch   0 Batch  124/3125   Loss: 2.829767 mae: 1.464216 (1591.5004705096683 steps/sec)\n",
      "Step #126\tEpoch   0 Batch  125/3125   Loss: 2.671155 mae: 1.396407 (1598.0005486299492 steps/sec)\n",
      "Step #127\tEpoch   0 Batch  126/3125   Loss: 2.577026 mae: 1.351748 (1221.0634184968676 steps/sec)\n",
      "Step #128\tEpoch   0 Batch  127/3125   Loss: 2.105964 mae: 1.195182 (1690.0386013264672 steps/sec)\n",
      "Step #129\tEpoch   0 Batch  128/3125   Loss: 2.292957 mae: 1.292127 (1846.4583498419574 steps/sec)\n",
      "Step #130\tEpoch   0 Batch  129/3125   Loss: 2.299412 mae: 1.267518 (1837.7531437584892 steps/sec)\n",
      "Step #131\tEpoch   0 Batch  130/3125   Loss: 2.107016 mae: 1.224704 (2001.442995934416 steps/sec)\n",
      "Step #132\tEpoch   0 Batch  131/3125   Loss: 2.046610 mae: 1.210373 (1918.9224800527047 steps/sec)\n",
      "Step #133\tEpoch   0 Batch  132/3125   Loss: 2.105351 mae: 1.247964 (2262.7392589715373 steps/sec)\n",
      "Step #134\tEpoch   0 Batch  133/3125   Loss: 2.176891 mae: 1.263572 (2346.3062619572393 steps/sec)\n",
      "Step #135\tEpoch   0 Batch  134/3125   Loss: 1.735645 mae: 1.091268 (2138.6852679026697 steps/sec)\n",
      "Step #136\tEpoch   0 Batch  135/3125   Loss: 2.028364 mae: 1.207017 (2127.4036803343547 steps/sec)\n",
      "Step #137\tEpoch   0 Batch  136/3125   Loss: 1.785590 mae: 1.117576 (2019.0743932143992 steps/sec)\n",
      "Step #138\tEpoch   0 Batch  137/3125   Loss: 1.800873 mae: 1.097095 (2306.4128366711757 steps/sec)\n",
      "Step #139\tEpoch   0 Batch  138/3125   Loss: 1.844044 mae: 1.144557 (2084.62341328615 steps/sec)\n",
      "Step #140\tEpoch   0 Batch  139/3125   Loss: 1.569575 mae: 1.048959 (2247.895898986001 steps/sec)\n",
      "Step #141\tEpoch   0 Batch  140/3125   Loss: 1.869717 mae: 1.147289 (2361.179040284627 steps/sec)\n",
      "Step #142\tEpoch   0 Batch  141/3125   Loss: 1.705878 mae: 1.072621 (2356.9596637333243 steps/sec)\n",
      "Step #143\tEpoch   0 Batch  142/3125   Loss: 1.677593 mae: 1.050578 (2385.7298871496178 steps/sec)\n",
      "Step #144\tEpoch   0 Batch  143/3125   Loss: 1.648373 mae: 1.045937 (2036.7620065070655 steps/sec)\n",
      "Step #145\tEpoch   0 Batch  144/3125   Loss: 1.503216 mae: 0.990106 (2031.120279706734 steps/sec)\n",
      "Step #146\tEpoch   0 Batch  145/3125   Loss: 1.761907 mae: 1.061406 (2188.5685065172243 steps/sec)\n",
      "Step #147\tEpoch   0 Batch  146/3125   Loss: 1.510374 mae: 0.989179 (2286.546659834055 steps/sec)\n",
      "Step #148\tEpoch   0 Batch  147/3125   Loss: 1.799443 mae: 1.121539 (2263.349773897277 steps/sec)\n",
      "Step #149\tEpoch   0 Batch  148/3125   Loss: 1.535042 mae: 1.039399 (2193.7654295158795 steps/sec)\n",
      "Step #150\tEpoch   0 Batch  149/3125   Loss: 1.658493 mae: 1.023382 (2168.1592142672525 steps/sec)\n",
      "Step #151\tEpoch   0 Batch  150/3125   Loss: 1.638846 mae: 1.061607 (2163.19432267115 steps/sec)\n",
      "Step #152\tEpoch   0 Batch  151/3125   Loss: 1.478252 mae: 0.982860 (2102.8928976104767 steps/sec)\n",
      "Step #153\tEpoch   0 Batch  152/3125   Loss: 1.721526 mae: 1.084309 (2040.6464984577062 steps/sec)\n",
      "Step #154\tEpoch   0 Batch  153/3125   Loss: 1.572417 mae: 1.027399 (1998.7914716786916 steps/sec)\n",
      "Step #155\tEpoch   0 Batch  154/3125   Loss: 1.619484 mae: 1.038150 (2136.13649096002 steps/sec)\n",
      "Step #156\tEpoch   0 Batch  155/3125   Loss: 1.442152 mae: 0.966374 (2162.926597840324 steps/sec)\n",
      "Step #157\tEpoch   0 Batch  156/3125   Loss: 1.727835 mae: 1.053196 (2166.568866482086 steps/sec)\n",
      "Step #158\tEpoch   0 Batch  157/3125   Loss: 1.659265 mae: 1.043500 (2258.669452551993 steps/sec)\n",
      "Step #159\tEpoch   0 Batch  158/3125   Loss: 1.622046 mae: 1.039735 (2376.7532526406453 steps/sec)\n",
      "Step #160\tEpoch   0 Batch  159/3125   Loss: 1.337244 mae: 0.936073 (2188.2716307024502 steps/sec)\n",
      "Step #161\tEpoch   0 Batch  160/3125   Loss: 1.419250 mae: 0.946671 (2211.5091375001316 steps/sec)\n",
      "Step #162\tEpoch   0 Batch  161/3125   Loss: 1.663732 mae: 1.032014 (1955.4047123982507 steps/sec)\n",
      "Step #163\tEpoch   0 Batch  162/3125   Loss: 1.636366 mae: 1.016206 (1916.2222912592972 steps/sec)\n",
      "Step #164\tEpoch   0 Batch  163/3125   Loss: 1.706273 mae: 1.070645 (2350.619276595268 steps/sec)\n",
      "Step #165\tEpoch   0 Batch  164/3125   Loss: 1.453511 mae: 0.959006 (2123.5907042681383 steps/sec)\n",
      "Step #166\tEpoch   0 Batch  165/3125   Loss: 1.695798 mae: 1.031043 (2203.42309591603 steps/sec)\n",
      "Step #167\tEpoch   0 Batch  166/3125   Loss: 1.822056 mae: 1.103795 (2194.1326637371835 steps/sec)\n",
      "Step #168\tEpoch   0 Batch  167/3125   Loss: 1.699795 mae: 1.060135 (1925.5825911302911 steps/sec)\n",
      "Step #169\tEpoch   0 Batch  168/3125   Loss: 1.832530 mae: 1.070813 (2019.3854657153036 steps/sec)\n",
      "Step #170\tEpoch   0 Batch  169/3125   Loss: 1.509456 mae: 0.961324 (2321.4762500415113 steps/sec)\n",
      "Step #171\tEpoch   0 Batch  170/3125   Loss: 1.487180 mae: 0.993596 (2046.8206794913087 steps/sec)\n",
      "Step #172\tEpoch   0 Batch  171/3125   Loss: 1.575828 mae: 1.030696 (2012.4673729464148 steps/sec)\n",
      "Step #173\tEpoch   0 Batch  172/3125   Loss: 1.615606 mae: 1.018946 (2293.14729970586 steps/sec)\n",
      "Step #174\tEpoch   0 Batch  173/3125   Loss: 1.371892 mae: 0.945922 (2065.691518178147 steps/sec)\n",
      "Step #175\tEpoch   0 Batch  174/3125   Loss: 1.416144 mae: 0.976175 (2201.087344402695 steps/sec)\n",
      "Step #176\tEpoch   0 Batch  175/3125   Loss: 1.497255 mae: 0.987124 (2293.14729970586 steps/sec)\n",
      "Step #177\tEpoch   0 Batch  176/3125   Loss: 1.550674 mae: 0.993484 (2263.4719164184257 steps/sec)\n",
      "Step #178\tEpoch   0 Batch  177/3125   Loss: 1.394514 mae: 0.945100 (2155.6343601920094 steps/sec)\n",
      "Step #179\tEpoch   0 Batch  178/3125   Loss: 1.455088 mae: 0.985171 (1779.4340503160663 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #180\tEpoch   0 Batch  179/3125   Loss: 1.503624 mae: 0.988934 (1259.0970220941404 steps/sec)\n",
      "Step #181\tEpoch   0 Batch  180/3125   Loss: 1.426303 mae: 0.962147 (1552.8937859131568 steps/sec)\n",
      "Step #182\tEpoch   0 Batch  181/3125   Loss: 1.469515 mae: 0.967577 (2041.918114989533 steps/sec)\n",
      "Step #183\tEpoch   0 Batch  182/3125   Loss: 1.562131 mae: 1.019628 (1386.4917755328713 steps/sec)\n",
      "Step #184\tEpoch   0 Batch  183/3125   Loss: 1.511000 mae: 1.006464 (1816.2821312270491 steps/sec)\n",
      "Step #185\tEpoch   0 Batch  184/3125   Loss: 1.670805 mae: 1.048408 (1514.560975257464 steps/sec)\n",
      "Step #186\tEpoch   0 Batch  185/3125   Loss: 1.538125 mae: 1.002120 (1765.7104848826734 steps/sec)\n",
      "Step #187\tEpoch   0 Batch  186/3125   Loss: 1.334828 mae: 0.939680 (1355.309688760211 steps/sec)\n",
      "Step #188\tEpoch   0 Batch  187/3125   Loss: 1.540581 mae: 1.028872 (1839.1714243117858 steps/sec)\n",
      "Step #189\tEpoch   0 Batch  188/3125   Loss: 1.346942 mae: 0.918134 (1513.2276964816579 steps/sec)\n",
      "Step #190\tEpoch   0 Batch  189/3125   Loss: 1.458313 mae: 0.977725 (1552.9397825893782 steps/sec)\n",
      "Step #191\tEpoch   0 Batch  190/3125   Loss: 1.553767 mae: 1.001981 (1405.871114358689 steps/sec)\n",
      "Step #192\tEpoch   0 Batch  191/3125   Loss: 1.655911 mae: 1.059736 (1461.8068770344967 steps/sec)\n",
      "Step #193\tEpoch   0 Batch  192/3125   Loss: 1.605739 mae: 1.029344 (1391.320962509371 steps/sec)\n",
      "Step #194\tEpoch   0 Batch  193/3125   Loss: 1.510253 mae: 0.994197 (1072.3445162016087 steps/sec)\n",
      "Step #195\tEpoch   0 Batch  194/3125   Loss: 1.405349 mae: 0.959557 (1799.0186322616066 steps/sec)\n",
      "Step #196\tEpoch   0 Batch  195/3125   Loss: 1.626412 mae: 1.050071 (1523.960119756998 steps/sec)\n",
      "Step #197\tEpoch   0 Batch  196/3125   Loss: 1.637599 mae: 1.033151 (1782.6103956819245 steps/sec)\n",
      "Step #198\tEpoch   0 Batch  197/3125   Loss: 1.352637 mae: 0.940048 (1223.2927931869222 steps/sec)\n",
      "Step #199\tEpoch   0 Batch  198/3125   Loss: 1.673405 mae: 1.049972 (1190.2719208132085 steps/sec)\n",
      "Step #200\tEpoch   0 Batch  199/3125   Loss: 1.456285 mae: 0.994426 (1194.4547598163738 steps/sec)\n",
      "Step #201\tEpoch   0 Batch  200/3125   Loss: 1.729162 mae: 1.100170 (1166.542622722848 steps/sec)\n",
      "Step #202\tEpoch   0 Batch  201/3125   Loss: 1.465549 mae: 0.991462 (1499.1114637615892 steps/sec)\n",
      "Step #203\tEpoch   0 Batch  202/3125   Loss: 1.571414 mae: 1.043035 (1384.0851642368284 steps/sec)\n",
      "Step #204\tEpoch   0 Batch  203/3125   Loss: 1.666529 mae: 1.052284 (1539.600922078494 steps/sec)\n",
      "Step #205\tEpoch   0 Batch  204/3125   Loss: 1.328098 mae: 0.925849 (1482.0966932628498 steps/sec)\n",
      "Step #206\tEpoch   0 Batch  205/3125   Loss: 1.308413 mae: 0.947366 (1669.62724710603 steps/sec)\n",
      "Step #207\tEpoch   0 Batch  206/3125   Loss: 1.430776 mae: 0.953444 (1529.3836235815759 steps/sec)\n",
      "Step #208\tEpoch   0 Batch  207/3125   Loss: 1.509939 mae: 1.002284 (1566.816089893013 steps/sec)\n",
      "Step #209\tEpoch   0 Batch  208/3125   Loss: 1.520757 mae: 1.019665 (2093.8019169329073 steps/sec)\n",
      "Step #210\tEpoch   0 Batch  209/3125   Loss: 1.327614 mae: 0.939502 (2175.8751634122555 steps/sec)\n",
      "Step #211\tEpoch   0 Batch  210/3125   Loss: 1.407540 mae: 0.972971 (2370.7883967532616 steps/sec)\n",
      "Step #212\tEpoch   0 Batch  211/3125   Loss: 1.578598 mae: 1.002063 (2339.1877572418102 steps/sec)\n",
      "Step #213\tEpoch   0 Batch  212/3125   Loss: 1.383927 mae: 0.955244 (1915.4172146719275 steps/sec)\n",
      "Step #214\tEpoch   0 Batch  213/3125   Loss: 1.320362 mae: 0.933161 (1927.2459932362888 steps/sec)\n",
      "Step #215\tEpoch   0 Batch  214/3125   Loss: 1.425371 mae: 0.988277 (2246.1864724468483 steps/sec)\n",
      "Step #216\tEpoch   0 Batch  215/3125   Loss: 1.315442 mae: 0.930045 (2108.1141938078003 steps/sec)\n",
      "Step #217\tEpoch   0 Batch  216/3125   Loss: 1.493731 mae: 1.004498 (2178.564973042602 steps/sec)\n",
      "Step #218\tEpoch   0 Batch  217/3125   Loss: 1.442787 mae: 0.997377 (1992.202758673101 steps/sec)\n",
      "Step #219\tEpoch   0 Batch  218/3125   Loss: 1.477024 mae: 0.957819 (1372.4637112079686 steps/sec)\n",
      "Step #220\tEpoch   0 Batch  219/3125   Loss: 1.567750 mae: 1.019187 (1508.709883959339 steps/sec)\n",
      "Step #221\tEpoch   0 Batch  220/3125   Loss: 1.608937 mae: 1.049271 (992.2274056340427 steps/sec)\n",
      "Step #222\tEpoch   0 Batch  221/3125   Loss: 1.409856 mae: 0.960503 (1431.9135861475645 steps/sec)\n",
      "Step #223\tEpoch   0 Batch  222/3125   Loss: 1.493837 mae: 0.996336 (1553.2388274156037 steps/sec)\n",
      "Step #224\tEpoch   0 Batch  223/3125   Loss: 1.456907 mae: 0.997901 (1350.1094429995107 steps/sec)\n",
      "Step #225\tEpoch   0 Batch  224/3125   Loss: 1.218154 mae: 0.901591 (1643.8194673062753 steps/sec)\n",
      "Step #226\tEpoch   0 Batch  225/3125   Loss: 1.332420 mae: 0.936963 (1531.0696268607683 steps/sec)\n",
      "Step #227\tEpoch   0 Batch  226/3125   Loss: 1.268844 mae: 0.895595 (1613.2683046909858 steps/sec)\n",
      "Step #228\tEpoch   0 Batch  227/3125   Loss: 1.489341 mae: 0.999676 (1156.1754701274072 steps/sec)\n",
      "Step #229\tEpoch   0 Batch  228/3125   Loss: 1.500061 mae: 0.992110 (1876.4949579004822 steps/sec)\n",
      "Step #230\tEpoch   0 Batch  229/3125   Loss: 1.394731 mae: 0.936116 (1664.9348999682438 steps/sec)\n",
      "Step #231\tEpoch   0 Batch  230/3125   Loss: 1.532253 mae: 1.006649 (1675.9384015407607 steps/sec)\n",
      "Step #232\tEpoch   0 Batch  231/3125   Loss: 1.544683 mae: 1.007491 (1717.5551387785522 steps/sec)\n",
      "Step #233\tEpoch   0 Batch  232/3125   Loss: 1.484318 mae: 0.992306 (1635.9716046493486 steps/sec)\n",
      "Step #234\tEpoch   0 Batch  233/3125   Loss: 1.240760 mae: 0.897849 (1209.2489015487874 steps/sec)\n",
      "Step #235\tEpoch   0 Batch  234/3125   Loss: 1.393804 mae: 0.964684 (1684.0806886804573 steps/sec)\n",
      "Step #236\tEpoch   0 Batch  235/3125   Loss: 1.338916 mae: 0.941739 (1451.5874938569837 steps/sec)\n",
      "Step #237\tEpoch   0 Batch  236/3125   Loss: 1.292457 mae: 0.929690 (1677.8558284662772 steps/sec)\n",
      "Step #238\tEpoch   0 Batch  237/3125   Loss: 1.365912 mae: 0.953397 (1748.0054011702537 steps/sec)\n",
      "Step #239\tEpoch   0 Batch  238/3125   Loss: 1.300282 mae: 0.905305 (1731.6092808190901 steps/sec)\n",
      "Step #240\tEpoch   0 Batch  239/3125   Loss: 1.345831 mae: 0.959394 (1602.616576747314 steps/sec)\n",
      "Step #241\tEpoch   0 Batch  240/3125   Loss: 1.400981 mae: 0.959267 (1242.521121920584 steps/sec)\n",
      "Step #242\tEpoch   0 Batch  241/3125   Loss: 1.299130 mae: 0.894140 (1583.9875525880495 steps/sec)\n",
      "Step #243\tEpoch   0 Batch  242/3125   Loss: 1.539069 mae: 0.999638 (1788.6615435789402 steps/sec)\n",
      "Step #244\tEpoch   0 Batch  243/3125   Loss: 1.471351 mae: 0.982878 (1680.787356137595 steps/sec)\n",
      "Step #245\tEpoch   0 Batch  244/3125   Loss: 1.364111 mae: 0.933782 (1906.657817457792 steps/sec)\n",
      "Step #246\tEpoch   0 Batch  245/3125   Loss: 1.532890 mae: 0.995084 (1788.4174888924895 steps/sec)\n",
      "Step #247\tEpoch   0 Batch  246/3125   Loss: 1.538796 mae: 1.043832 (1705.1541194740994 steps/sec)\n",
      "Step #248\tEpoch   0 Batch  247/3125   Loss: 1.402890 mae: 0.974507 (1473.3502413252868 steps/sec)\n",
      "Step #249\tEpoch   0 Batch  248/3125   Loss: 1.428750 mae: 0.982235 (1393.9659011598924 steps/sec)\n",
      "Step #250\tEpoch   0 Batch  249/3125   Loss: 1.456826 mae: 0.962937 (2005.5965189116816 steps/sec)\n",
      "Step #251\tEpoch   0 Batch  250/3125   Loss: 1.399229 mae: 0.967250 (2041.2423714460915 steps/sec)\n",
      "Step #252\tEpoch   0 Batch  251/3125   Loss: 1.298323 mae: 0.927748 (2048.440094551564 steps/sec)\n",
      "Step #253\tEpoch   0 Batch  252/3125   Loss: 1.363156 mae: 0.941385 (2285.798990702693 steps/sec)\n",
      "Step #254\tEpoch   0 Batch  253/3125   Loss: 1.392795 mae: 0.956515 (2168.8318941000052 steps/sec)\n",
      "Step #255\tEpoch   0 Batch  254/3125   Loss: 1.214973 mae: 0.905892 (2243.75922795455 steps/sec)\n",
      "Step #256\tEpoch   0 Batch  255/3125   Loss: 1.531235 mae: 1.024411 (2081.251240522409 steps/sec)\n",
      "Step #257\tEpoch   0 Batch  256/3125   Loss: 1.435942 mae: 0.978412 (1512.0384723534034 steps/sec)\n",
      "Step #258\tEpoch   0 Batch  257/3125   Loss: 1.162275 mae: 0.873074 (2049.340877331848 steps/sec)\n",
      "Step #259\tEpoch   0 Batch  258/3125   Loss: 1.847275 mae: 1.120149 (2163.082762604175 steps/sec)\n",
      "Step #260\tEpoch   0 Batch  259/3125   Loss: 1.421130 mae: 0.960058 (2247.39002304024 steps/sec)\n",
      "Step #261\tEpoch   0 Batch  260/3125   Loss: 1.461184 mae: 0.973237 (2330.894055928511 steps/sec)\n",
      "Step #262\tEpoch   0 Batch  261/3125   Loss: 1.734692 mae: 1.059884 (2199.9097861091586 steps/sec)\n",
      "Step #263\tEpoch   0 Batch  262/3125   Loss: 1.276520 mae: 0.886134 (2230.6568100834975 steps/sec)\n",
      "Step #264\tEpoch   0 Batch  263/3125   Loss: 1.205230 mae: 0.895041 (2248.088673541582 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #265\tEpoch   0 Batch  264/3125   Loss: 1.299898 mae: 0.943469 (1878.9664193814285 steps/sec)\n",
      "Step #266\tEpoch   0 Batch  265/3125   Loss: 1.274347 mae: 0.919950 (1698.3050572944082 steps/sec)\n",
      "Step #267\tEpoch   0 Batch  266/3125   Loss: 1.302611 mae: 0.923597 (2214.147556906964 steps/sec)\n",
      "Step #268\tEpoch   0 Batch  267/3125   Loss: 1.363627 mae: 0.934879 (2139.863679033509 steps/sec)\n",
      "Step #269\tEpoch   0 Batch  268/3125   Loss: 1.313970 mae: 0.920167 (2217.776884762217 steps/sec)\n",
      "Step #270\tEpoch   0 Batch  269/3125   Loss: 1.581587 mae: 1.026004 (2170.515421237839 steps/sec)\n",
      "Step #271\tEpoch   0 Batch  270/3125   Loss: 1.309765 mae: 0.925540 (2004.1781744856125 steps/sec)\n",
      "Step #272\tEpoch   0 Batch  271/3125   Loss: 1.278610 mae: 0.920728 (2200.6023148196728 steps/sec)\n",
      "Step #273\tEpoch   0 Batch  272/3125   Loss: 1.409507 mae: 0.957712 (2206.413601548691 steps/sec)\n",
      "Step #274\tEpoch   0 Batch  273/3125   Loss: 1.171644 mae: 0.858325 (1813.031788434439 steps/sec)\n",
      "Step #275\tEpoch   0 Batch  274/3125   Loss: 1.378165 mae: 0.953169 (2060.758996128373 steps/sec)\n",
      "Step #276\tEpoch   0 Batch  275/3125   Loss: 1.282130 mae: 0.931194 (2382.5316398173186 steps/sec)\n",
      "Step #277\tEpoch   0 Batch  276/3125   Loss: 1.346733 mae: 0.931398 (2259.9838353359555 steps/sec)\n",
      "Step #278\tEpoch   0 Batch  277/3125   Loss: 1.225308 mae: 0.893874 (2264.4006305742114 steps/sec)\n",
      "Step #279\tEpoch   0 Batch  278/3125   Loss: 1.208913 mae: 0.900573 (2456.745896934269 steps/sec)\n",
      "Step #280\tEpoch   0 Batch  279/3125   Loss: 1.298784 mae: 0.934568 (2381.638748509454 steps/sec)\n",
      "Step #281\tEpoch   0 Batch  280/3125   Loss: 1.422065 mae: 0.978694 (2418.0237518736308 steps/sec)\n",
      "Step #282\tEpoch   0 Batch  281/3125   Loss: 1.257358 mae: 0.919464 (2324.8475711150036 steps/sec)\n",
      "Step #283\tEpoch   0 Batch  282/3125   Loss: 1.553021 mae: 1.033680 (2384.1296908927616 steps/sec)\n",
      "Step #284\tEpoch   0 Batch  283/3125   Loss: 1.376867 mae: 0.983307 (2198.20341079421 steps/sec)\n",
      "Step #285\tEpoch   0 Batch  284/3125   Loss: 1.483122 mae: 1.003878 (2011.9846881505857 steps/sec)\n",
      "Step #286\tEpoch   0 Batch  285/3125   Loss: 1.417547 mae: 0.954564 (2002.322028719829 steps/sec)\n",
      "Step #287\tEpoch   0 Batch  286/3125   Loss: 1.358376 mae: 0.931240 (2114.8106690868754 steps/sec)\n",
      "Step #288\tEpoch   0 Batch  287/3125   Loss: 1.320581 mae: 0.914086 (2179.515906091186 steps/sec)\n",
      "Step #289\tEpoch   0 Batch  288/3125   Loss: 1.527283 mae: 1.012507 (2027.0172047167987 steps/sec)\n",
      "Step #290\tEpoch   0 Batch  289/3125   Loss: 1.343543 mae: 0.927505 (2040.8848059013012 steps/sec)\n",
      "Step #291\tEpoch   0 Batch  290/3125   Loss: 1.446516 mae: 1.001849 (2146.67581095882 steps/sec)\n",
      "Step #292\tEpoch   0 Batch  291/3125   Loss: 1.299783 mae: 0.925519 (2185.762824922352 steps/sec)\n",
      "Step #293\tEpoch   0 Batch  292/3125   Loss: 1.597798 mae: 1.003482 (1781.2628467562472 steps/sec)\n",
      "Step #294\tEpoch   0 Batch  293/3125   Loss: 1.307342 mae: 0.922472 (2238.3469239636256 steps/sec)\n",
      "Step #295\tEpoch   0 Batch  294/3125   Loss: 1.264600 mae: 0.895054 (2126.476105494773 steps/sec)\n",
      "Step #296\tEpoch   0 Batch  295/3125   Loss: 1.147102 mae: 0.846202 (2221.1122761308634 steps/sec)\n",
      "Step #297\tEpoch   0 Batch  296/3125   Loss: 1.520081 mae: 0.985869 (2441.5297747249547 steps/sec)\n",
      "Step #298\tEpoch   0 Batch  297/3125   Loss: 1.354339 mae: 0.951316 (2102.239419394936 steps/sec)\n",
      "Step #299\tEpoch   0 Batch  298/3125   Loss: 1.285677 mae: 0.917818 (2406.591540244658 steps/sec)\n",
      "Step #300\tEpoch   0 Batch  299/3125   Loss: 1.303874 mae: 0.913004 (2304.8919076351567 steps/sec)\n",
      "Step #301\tEpoch   0 Batch  300/3125   Loss: 1.440492 mae: 0.975274 (2420.03277250802 steps/sec)\n",
      "Step #302\tEpoch   0 Batch  301/3125   Loss: 1.412519 mae: 0.955512 (2365.200130826576 steps/sec)\n",
      "Step #303\tEpoch   0 Batch  302/3125   Loss: 1.496504 mae: 0.997010 (1937.6808648249098 steps/sec)\n",
      "Step #304\tEpoch   0 Batch  303/3125   Loss: 1.312748 mae: 0.923437 (1997.1164376386787 steps/sec)\n",
      "Step #305\tEpoch   0 Batch  304/3125   Loss: 1.475085 mae: 0.983324 (2243.2552119546035 steps/sec)\n",
      "Step #306\tEpoch   0 Batch  305/3125   Loss: 1.416122 mae: 0.956551 (2212.208989546303 steps/sec)\n",
      "Step #307\tEpoch   0 Batch  306/3125   Loss: 1.195961 mae: 0.892964 (2355.7153127246584 steps/sec)\n",
      "Step #308\tEpoch   0 Batch  307/3125   Loss: 1.370657 mae: 0.951379 (2071.424902708362 steps/sec)\n",
      "Step #309\tEpoch   0 Batch  308/3125   Loss: 1.111951 mae: 0.845810 (2085.162316679095 steps/sec)\n",
      "Step #310\tEpoch   0 Batch  309/3125   Loss: 1.183643 mae: 0.889603 (2069.8915285687494 steps/sec)\n",
      "Step #311\tEpoch   0 Batch  310/3125   Loss: 1.446530 mae: 0.974228 (1611.4584293837406 steps/sec)\n",
      "Step #312\tEpoch   0 Batch  311/3125   Loss: 1.409428 mae: 0.949017 (2100.470743775165 steps/sec)\n",
      "Step #313\tEpoch   0 Batch  312/3125   Loss: 1.332309 mae: 0.971393 (2076.223665452241 steps/sec)\n",
      "Step #314\tEpoch   0 Batch  313/3125   Loss: 1.298842 mae: 0.933906 (2070.1367158580524 steps/sec)\n",
      "Step #315\tEpoch   0 Batch  314/3125   Loss: 1.392515 mae: 0.972159 (1837.22185233206 steps/sec)\n",
      "Step #316\tEpoch   0 Batch  315/3125   Loss: 1.293065 mae: 0.934319 (1454.1541277787794 steps/sec)\n",
      "Step #317\tEpoch   0 Batch  316/3125   Loss: 1.255561 mae: 0.901288 (1519.7744780456696 steps/sec)\n",
      "Step #318\tEpoch   0 Batch  317/3125   Loss: 1.176809 mae: 0.885164 (1339.0407110384635 steps/sec)\n",
      "Step #319\tEpoch   0 Batch  318/3125   Loss: 1.249828 mae: 0.905782 (1264.3118530441964 steps/sec)\n",
      "Step #320\tEpoch   0 Batch  319/3125   Loss: 1.359128 mae: 0.941936 (1895.7306214689265 steps/sec)\n",
      "Step #321\tEpoch   0 Batch  320/3125   Loss: 1.456120 mae: 0.985618 (1503.4748757948769 steps/sec)\n",
      "Step #322\tEpoch   0 Batch  321/3125   Loss: 1.330739 mae: 0.936310 (1440.7474580928827 steps/sec)\n",
      "Step #323\tEpoch   0 Batch  322/3125   Loss: 1.232501 mae: 0.914183 (1740.447321465621 steps/sec)\n",
      "Step #324\tEpoch   0 Batch  323/3125   Loss: 1.307128 mae: 0.931618 (1829.2094061824018 steps/sec)\n",
      "Step #325\tEpoch   0 Batch  324/3125   Loss: 1.295577 mae: 0.921960 (1202.957558236249 steps/sec)\n",
      "Step #326\tEpoch   0 Batch  325/3125   Loss: 1.325908 mae: 0.926538 (1882.1873793988566 steps/sec)\n",
      "Step #327\tEpoch   0 Batch  326/3125   Loss: 1.111097 mae: 0.843872 (1774.5105007530758 steps/sec)\n",
      "Step #328\tEpoch   0 Batch  327/3125   Loss: 1.475069 mae: 1.020277 (1656.662111241893 steps/sec)\n",
      "Step #329\tEpoch   0 Batch  328/3125   Loss: 1.199542 mae: 0.894971 (1731.8809820713348 steps/sec)\n",
      "Step #330\tEpoch   0 Batch  329/3125   Loss: 1.426027 mae: 0.981408 (1422.5888290439432 steps/sec)\n",
      "Step #331\tEpoch   0 Batch  330/3125   Loss: 1.353678 mae: 0.925620 (1627.4023202576338 steps/sec)\n",
      "Step #332\tEpoch   0 Batch  331/3125   Loss: 1.353803 mae: 0.965534 (2072.448414894458 steps/sec)\n",
      "Step #333\tEpoch   0 Batch  332/3125   Loss: 1.285688 mae: 0.906392 (1541.5814582582937 steps/sec)\n",
      "Step #334\tEpoch   0 Batch  333/3125   Loss: 1.318109 mae: 0.948938 (2157.4971965885825 steps/sec)\n",
      "Step #335\tEpoch   0 Batch  334/3125   Loss: 1.286411 mae: 0.906284 (2111.2976945535083 steps/sec)\n",
      "Step #336\tEpoch   0 Batch  335/3125   Loss: 1.070894 mae: 0.845499 (2047.7600281217044 steps/sec)\n",
      "Step #337\tEpoch   0 Batch  336/3125   Loss: 1.130355 mae: 0.880641 (2122.9028110986264 steps/sec)\n",
      "Step #338\tEpoch   0 Batch  337/3125   Loss: 1.152889 mae: 0.864585 (1350.0051498609537 steps/sec)\n",
      "Step #339\tEpoch   0 Batch  338/3125   Loss: 1.267034 mae: 0.919341 (1406.861390256665 steps/sec)\n",
      "Step #340\tEpoch   0 Batch  339/3125   Loss: 1.203953 mae: 0.877358 (1211.7012855698397 steps/sec)\n",
      "Step #341\tEpoch   0 Batch  340/3125   Loss: 1.061821 mae: 0.839056 (1332.7605272188823 steps/sec)\n",
      "Step #342\tEpoch   0 Batch  341/3125   Loss: 1.181926 mae: 0.866092 (1268.1344596760052 steps/sec)\n",
      "Step #343\tEpoch   0 Batch  342/3125   Loss: 1.442246 mae: 0.953934 (1602.9105584175368 steps/sec)\n",
      "Step #344\tEpoch   0 Batch  343/3125   Loss: 1.432712 mae: 0.977100 (1620.486033303713 steps/sec)\n",
      "Step #345\tEpoch   0 Batch  344/3125   Loss: 1.361100 mae: 0.962045 (1044.211197195722 steps/sec)\n",
      "Step #346\tEpoch   0 Batch  345/3125   Loss: 1.225116 mae: 0.905414 (1527.4231609613985 steps/sec)\n",
      "Step #347\tEpoch   0 Batch  346/3125   Loss: 1.299816 mae: 0.922888 (1688.2835015859216 steps/sec)\n",
      "Step #348\tEpoch   0 Batch  347/3125   Loss: 1.176270 mae: 0.872270 (1759.4442673288925 steps/sec)\n",
      "Step #349\tEpoch   0 Batch  348/3125   Loss: 1.198550 mae: 0.891412 (1635.9716046493486 steps/sec)\n",
      "Step #350\tEpoch   0 Batch  349/3125   Loss: 1.338132 mae: 0.979944 (1718.6952958531388 steps/sec)\n",
      "Step #351\tEpoch   0 Batch  350/3125   Loss: 1.212150 mae: 0.869076 (2130.7323417052753 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #352\tEpoch   0 Batch  351/3125   Loss: 1.363397 mae: 0.960166 (1979.1734694840554 steps/sec)\n",
      "Step #353\tEpoch   0 Batch  352/3125   Loss: 1.135699 mae: 0.871050 (1694.8463272909478 steps/sec)\n",
      "Step #354\tEpoch   0 Batch  353/3125   Loss: 1.478090 mae: 0.979905 (1278.057639451761 steps/sec)\n",
      "Step #355\tEpoch   0 Batch  354/3125   Loss: 1.387046 mae: 0.950568 (1762.7421808676063 steps/sec)\n",
      "Step #356\tEpoch   0 Batch  355/3125   Loss: 1.170739 mae: 0.881125 (1688.1611887914867 steps/sec)\n",
      "Step #357\tEpoch   0 Batch  356/3125   Loss: 1.231009 mae: 0.907156 (1603.9526114922485 steps/sec)\n",
      "Step #358\tEpoch   0 Batch  357/3125   Loss: 1.083626 mae: 0.855973 (1627.9834496464032 steps/sec)\n",
      "Step #359\tEpoch   0 Batch  358/3125   Loss: 1.319636 mae: 0.947568 (1282.081504395564 steps/sec)\n",
      "Step #360\tEpoch   0 Batch  359/3125   Loss: 1.504743 mae: 1.002614 (1658.8111528574254 steps/sec)\n",
      "Step #361\tEpoch   0 Batch  360/3125   Loss: 1.217402 mae: 0.903927 (1286.3674560967681 steps/sec)\n",
      "Step #362\tEpoch   0 Batch  361/3125   Loss: 1.202274 mae: 0.889158 (1526.056045931176 steps/sec)\n",
      "Step #363\tEpoch   0 Batch  362/3125   Loss: 1.236710 mae: 0.909813 (1850.7602834626211 steps/sec)\n",
      "Step #364\tEpoch   0 Batch  363/3125   Loss: 1.191057 mae: 0.887438 (1682.027590632018 steps/sec)\n",
      "Step #365\tEpoch   0 Batch  364/3125   Loss: 1.157943 mae: 0.880782 (1578.3368831422958 steps/sec)\n",
      "Step #366\tEpoch   0 Batch  365/3125   Loss: 1.270473 mae: 0.924609 (1233.2270925705952 steps/sec)\n",
      "Step #367\tEpoch   0 Batch  366/3125   Loss: 1.314199 mae: 0.943404 (1546.4012093057552 steps/sec)\n",
      "Step #368\tEpoch   0 Batch  367/3125   Loss: 1.127304 mae: 0.838404 (1607.456463085602 steps/sec)\n",
      "Step #369\tEpoch   0 Batch  368/3125   Loss: 1.227728 mae: 0.896587 (1807.6714879238712 steps/sec)\n",
      "Step #370\tEpoch   0 Batch  369/3125   Loss: 1.350181 mae: 0.967766 (1532.5689313719042 steps/sec)\n",
      "Step #371\tEpoch   0 Batch  370/3125   Loss: 1.452201 mae: 0.996174 (1753.8968478979016 steps/sec)\n",
      "Step #372\tEpoch   0 Batch  371/3125   Loss: 1.202803 mae: 0.898664 (2063.0497870205504 steps/sec)\n",
      "Step #373\tEpoch   0 Batch  372/3125   Loss: 1.207381 mae: 0.907044 (1911.4542223032402 steps/sec)\n",
      "Step #374\tEpoch   0 Batch  373/3125   Loss: 1.365987 mae: 0.976289 (1685.8406083699094 steps/sec)\n",
      "Step #375\tEpoch   0 Batch  374/3125   Loss: 1.217093 mae: 0.897005 (2251.1292400171747 steps/sec)\n",
      "Step #376\tEpoch   0 Batch  375/3125   Loss: 1.257105 mae: 0.926779 (2106.9493143115487 steps/sec)\n",
      "Step #377\tEpoch   0 Batch  376/3125   Loss: 1.152414 mae: 0.864187 (1500.3340988274347 steps/sec)\n",
      "Step #378\tEpoch   0 Batch  377/3125   Loss: 1.099136 mae: 0.838927 (1855.771766350757 steps/sec)\n",
      "Step #379\tEpoch   0 Batch  378/3125   Loss: 1.388408 mae: 0.944986 (1810.683727476019 steps/sec)\n",
      "Step #380\tEpoch   0 Batch  379/3125   Loss: 1.190017 mae: 0.895861 (1708.5298095253613 steps/sec)\n",
      "Step #381\tEpoch   0 Batch  380/3125   Loss: 1.176858 mae: 0.890070 (1597.3919534451502 steps/sec)\n",
      "Step #382\tEpoch   0 Batch  381/3125   Loss: 1.132081 mae: 0.859083 (1634.8875462872734 steps/sec)\n",
      "Step #383\tEpoch   0 Batch  382/3125   Loss: 1.172380 mae: 0.883005 (1898.9749719294432 steps/sec)\n",
      "Step #384\tEpoch   0 Batch  383/3125   Loss: 1.272698 mae: 0.907569 (2126.756449780951 steps/sec)\n",
      "Step #385\tEpoch   0 Batch  384/3125   Loss: 1.294122 mae: 0.934066 (2216.4174214481236 steps/sec)\n",
      "Step #386\tEpoch   0 Batch  385/3125   Loss: 1.189236 mae: 0.902959 (2255.293156106164 steps/sec)\n",
      "Step #387\tEpoch   0 Batch  386/3125   Loss: 1.194989 mae: 0.900224 (2098.2850739399278 steps/sec)\n",
      "Step #388\tEpoch   0 Batch  387/3125   Loss: 1.307997 mae: 0.920113 (2075.032157204203 steps/sec)\n",
      "Step #389\tEpoch   0 Batch  388/3125   Loss: 1.250255 mae: 0.917250 (1625.9134924757527 steps/sec)\n",
      "Step #390\tEpoch   0 Batch  389/3125   Loss: 1.127243 mae: 0.862736 (1629.6786727279791 steps/sec)\n",
      "Step #391\tEpoch   0 Batch  390/3125   Loss: 1.355114 mae: 0.947239 (1612.8092531780883 steps/sec)\n",
      "Step #392\tEpoch   0 Batch  391/3125   Loss: 1.193438 mae: 0.841684 (1773.9851290423544 steps/sec)\n",
      "Step #393\tEpoch   0 Batch  392/3125   Loss: 1.179829 mae: 0.881767 (1936.3926797289062 steps/sec)\n",
      "Step #394\tEpoch   0 Batch  393/3125   Loss: 1.218394 mae: 0.886372 (1856.2645494215637 steps/sec)\n",
      "Step #395\tEpoch   0 Batch  394/3125   Loss: 1.438653 mae: 0.965394 (2059.0594010800196 steps/sec)\n",
      "Step #396\tEpoch   0 Batch  395/3125   Loss: 1.176146 mae: 0.873240 (1930.936947554508 steps/sec)\n",
      "Step #397\tEpoch   0 Batch  396/3125   Loss: 1.105750 mae: 0.836578 (1571.8305214321583 steps/sec)\n",
      "Step #398\tEpoch   0 Batch  397/3125   Loss: 1.235562 mae: 0.896024 (1572.832543349133 steps/sec)\n",
      "Step #399\tEpoch   0 Batch  398/3125   Loss: 1.257514 mae: 0.916415 (1492.454293786517 steps/sec)\n",
      "Step #400\tEpoch   0 Batch  399/3125   Loss: 1.300774 mae: 0.916644 (1698.415089449857 steps/sec)\n",
      "Step #401\tEpoch   0 Batch  400/3125   Loss: 1.061929 mae: 0.821087 (1376.4181592643884 steps/sec)\n",
      "Step #402\tEpoch   0 Batch  401/3125   Loss: 1.256825 mae: 0.901809 (920.708064062939 steps/sec)\n",
      "Step #403\tEpoch   0 Batch  402/3125   Loss: 1.181826 mae: 0.861343 (1231.6843545725244 steps/sec)\n",
      "Step #404\tEpoch   0 Batch  403/3125   Loss: 1.291070 mae: 0.951009 (1305.8231631382316 steps/sec)\n",
      "Step #405\tEpoch   0 Batch  404/3125   Loss: 1.136979 mae: 0.862647 (1649.1841180217516 steps/sec)\n",
      "Step #406\tEpoch   0 Batch  405/3125   Loss: 1.162350 mae: 0.881142 (1544.931636020745 steps/sec)\n",
      "Step #407\tEpoch   0 Batch  406/3125   Loss: 1.189953 mae: 0.900196 (1542.3977876984857 steps/sec)\n",
      "Step #408\tEpoch   0 Batch  407/3125   Loss: 1.037419 mae: 0.828722 (1532.5577316574102 steps/sec)\n",
      "Step #409\tEpoch   0 Batch  408/3125   Loss: 1.182161 mae: 0.868508 (1251.814312745853 steps/sec)\n",
      "Step #410\tEpoch   0 Batch  409/3125   Loss: 1.128898 mae: 0.865082 (1950.6031828709085 steps/sec)\n",
      "Step #411\tEpoch   0 Batch  410/3125   Loss: 1.033625 mae: 0.818376 (1790.1272716408737 steps/sec)\n",
      "Step #412\tEpoch   0 Batch  411/3125   Loss: 1.204838 mae: 0.918918 (1941.4478800222182 steps/sec)\n",
      "Step #413\tEpoch   0 Batch  412/3125   Loss: 1.088473 mae: 0.846112 (2307.681811679523 steps/sec)\n",
      "Step #414\tEpoch   0 Batch  413/3125   Loss: 1.050345 mae: 0.821554 (2103.5889822857944 steps/sec)\n",
      "Step #415\tEpoch   0 Batch  414/3125   Loss: 1.123881 mae: 0.842585 (2292.7461763001675 steps/sec)\n",
      "Step #416\tEpoch   0 Batch  415/3125   Loss: 1.292715 mae: 0.934608 (1736.843761646445 steps/sec)\n",
      "Step #417\tEpoch   0 Batch  416/3125   Loss: 1.286420 mae: 0.930160 (1850.2051223234846 steps/sec)\n",
      "Step #418\tEpoch   0 Batch  417/3125   Loss: 1.454192 mae: 0.986998 (2202.058045276996 steps/sec)\n",
      "Step #419\tEpoch   0 Batch  418/3125   Loss: 1.258641 mae: 0.934834 (2139.383428886214 steps/sec)\n",
      "Step #420\tEpoch   0 Batch  419/3125   Loss: 1.282421 mae: 0.920155 (2259.569883204758 steps/sec)\n",
      "Step #421\tEpoch   0 Batch  420/3125   Loss: 1.097556 mae: 0.847960 (2137.8567933452946 steps/sec)\n",
      "Step #422\tEpoch   0 Batch  421/3125   Loss: 1.269277 mae: 0.901215 (2141.3275881433983 steps/sec)\n",
      "Step #423\tEpoch   0 Batch  422/3125   Loss: 1.267458 mae: 0.939579 (2183.9873364992086 steps/sec)\n",
      "Step #424\tEpoch   0 Batch  423/3125   Loss: 1.182871 mae: 0.874822 (2276.6672094664277 steps/sec)\n",
      "Step #425\tEpoch   0 Batch  424/3125   Loss: 1.170049 mae: 0.875223 (1785.64604708587 steps/sec)\n",
      "Step #426\tEpoch   0 Batch  425/3125   Loss: 1.285743 mae: 0.921651 (1498.672231194706 steps/sec)\n",
      "Step #427\tEpoch   0 Batch  426/3125   Loss: 1.230121 mae: 0.902436 (2203.099032471557 steps/sec)\n",
      "Step #428\tEpoch   0 Batch  427/3125   Loss: 1.137158 mae: 0.839210 (2046.4012490241998 steps/sec)\n",
      "Step #429\tEpoch   0 Batch  428/3125   Loss: 1.081840 mae: 0.820759 (2291.9944480267545 steps/sec)\n",
      "Step #430\tEpoch   0 Batch  429/3125   Loss: 1.034517 mae: 0.825453 (2202.659384518433 steps/sec)\n",
      "Step #431\tEpoch   0 Batch  430/3125   Loss: 1.359279 mae: 0.946737 (2271.1935627105063 steps/sec)\n",
      "Step #432\tEpoch   0 Batch  431/3125   Loss: 1.165666 mae: 0.845981 (2341.486071568135 steps/sec)\n",
      "Step #433\tEpoch   0 Batch  432/3125   Loss: 1.208226 mae: 0.878461 (2227.7659156335976 steps/sec)\n",
      "Step #434\tEpoch   0 Batch  433/3125   Loss: 1.115090 mae: 0.853999 (1573.8004112446906 steps/sec)\n",
      "Step #435\tEpoch   0 Batch  434/3125   Loss: 1.331211 mae: 0.943538 (1590.2092069245293 steps/sec)\n",
      "Step #436\tEpoch   0 Batch  435/3125   Loss: 1.328129 mae: 0.926355 (1837.173568343685 steps/sec)\n",
      "Step #437\tEpoch   0 Batch  436/3125   Loss: 1.105069 mae: 0.859350 (1518.5859419691672 steps/sec)\n",
      "Step #438\tEpoch   0 Batch  437/3125   Loss: 1.168529 mae: 0.894217 (1817.588684445446 steps/sec)\n",
      "Step #439\tEpoch   0 Batch  438/3125   Loss: 1.164793 mae: 0.885267 (1781.6411659261398 steps/sec)\n",
      "Step #440\tEpoch   0 Batch  439/3125   Loss: 1.196156 mae: 0.845398 (1649.6900663918693 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #441\tEpoch   0 Batch  440/3125   Loss: 1.180954 mae: 0.897580 (1465.812079317262 steps/sec)\n",
      "Step #442\tEpoch   0 Batch  441/3125   Loss: 1.093483 mae: 0.859730 (1284.6575107506462 steps/sec)\n",
      "Step #443\tEpoch   0 Batch  442/3125   Loss: 1.257782 mae: 0.922138 (962.5880128152166 steps/sec)\n",
      "Step #444\tEpoch   0 Batch  443/3125   Loss: 1.165175 mae: 0.892591 (1587.7411344296054 steps/sec)\n",
      "Step #445\tEpoch   0 Batch  444/3125   Loss: 1.135682 mae: 0.856126 (1737.5632793404864 steps/sec)\n",
      "Step #446\tEpoch   0 Batch  445/3125   Loss: 1.169835 mae: 0.875826 (1875.8235762395013 steps/sec)\n",
      "Step #447\tEpoch   0 Batch  446/3125   Loss: 1.196860 mae: 0.881984 (2019.307688604304 steps/sec)\n",
      "Step #448\tEpoch   0 Batch  447/3125   Loss: 1.245653 mae: 0.901938 (1659.3098973786862 steps/sec)\n",
      "Step #449\tEpoch   0 Batch  448/3125   Loss: 1.132361 mae: 0.873890 (1907.594349491072 steps/sec)\n",
      "Step #450\tEpoch   0 Batch  449/3125   Loss: 1.145307 mae: 0.860823 (1639.0146304864325 steps/sec)\n",
      "Step #451\tEpoch   0 Batch  450/3125   Loss: 1.073069 mae: 0.823830 (1558.6066457083825 steps/sec)\n",
      "Step #452\tEpoch   0 Batch  451/3125   Loss: 1.117256 mae: 0.824485 (1531.8636689018422 steps/sec)\n",
      "Step #453\tEpoch   0 Batch  452/3125   Loss: 1.252387 mae: 0.903275 (1608.1837352862237 steps/sec)\n",
      "Step #454\tEpoch   0 Batch  453/3125   Loss: 1.029118 mae: 0.821942 (1437.5279327694227 steps/sec)\n",
      "Step #455\tEpoch   0 Batch  454/3125   Loss: 1.197180 mae: 0.897859 (1596.1761527103345 steps/sec)\n",
      "Step #456\tEpoch   0 Batch  455/3125   Loss: 1.174268 mae: 0.878548 (1575.6923678002013 steps/sec)\n",
      "Step #457\tEpoch   0 Batch  456/3125   Loss: 1.166557 mae: 0.891316 (1550.975853270717 steps/sec)\n",
      "Step #458\tEpoch   0 Batch  457/3125   Loss: 1.379845 mae: 0.984253 (1531.170234514179 steps/sec)\n",
      "Step #459\tEpoch   0 Batch  458/3125   Loss: 1.312187 mae: 0.934707 (1585.1369226234117 steps/sec)\n",
      "Step #460\tEpoch   0 Batch  459/3125   Loss: 1.293897 mae: 0.931126 (1512.6383058524834 steps/sec)\n",
      "Step #461\tEpoch   0 Batch  460/3125   Loss: 1.251625 mae: 0.883247 (1332.4387516519264 steps/sec)\n",
      "Step #462\tEpoch   0 Batch  461/3125   Loss: 1.145823 mae: 0.860482 (1685.2716168434588 steps/sec)\n",
      "Step #463\tEpoch   0 Batch  462/3125   Loss: 1.131284 mae: 0.879305 (1696.6425578046374 steps/sec)\n",
      "Step #464\tEpoch   0 Batch  463/3125   Loss: 0.909611 mae: 0.771769 (1631.5931971307202 steps/sec)\n",
      "Step #465\tEpoch   0 Batch  464/3125   Loss: 1.152639 mae: 0.863314 (1856.7247164649532 steps/sec)\n",
      "Step #466\tEpoch   0 Batch  465/3125   Loss: 1.035629 mae: 0.810232 (2272.3009578294977 steps/sec)\n",
      "Step #467\tEpoch   0 Batch  466/3125   Loss: 1.117994 mae: 0.874486 (1620.0103512471708 steps/sec)\n",
      "Step #468\tEpoch   0 Batch  467/3125   Loss: 1.177455 mae: 0.897969 (1969.9706921169309 steps/sec)\n",
      "Step #469\tEpoch   0 Batch  468/3125   Loss: 0.951693 mae: 0.791971 (1794.9382473017965 steps/sec)\n",
      "Step #470\tEpoch   0 Batch  469/3125   Loss: 0.963137 mae: 0.775715 (1913.3725651202044 steps/sec)\n",
      "Step #471\tEpoch   0 Batch  470/3125   Loss: 1.119226 mae: 0.853203 (2176.643002449454 steps/sec)\n",
      "Step #472\tEpoch   0 Batch  471/3125   Loss: 1.300905 mae: 0.930828 (2131.1220860516637 steps/sec)\n",
      "Step #473\tEpoch   0 Batch  472/3125   Loss: 1.227463 mae: 0.901996 (2183.5098131084387 steps/sec)\n",
      "Step #474\tEpoch   0 Batch  473/3125   Loss: 1.099997 mae: 0.845808 (2122.1509380502316 steps/sec)\n",
      "Step #475\tEpoch   0 Batch  474/3125   Loss: 1.127491 mae: 0.837931 (2214.3579672041137 steps/sec)\n",
      "Step #476\tEpoch   0 Batch  475/3125   Loss: 1.199702 mae: 0.883968 (2045.6028092079594 steps/sec)\n",
      "Step #477\tEpoch   0 Batch  476/3125   Loss: 1.124141 mae: 0.871051 (1868.6697497037255 steps/sec)\n",
      "Step #478\tEpoch   0 Batch  477/3125   Loss: 1.178833 mae: 0.886914 (1658.758670874562 steps/sec)\n",
      "Step #479\tEpoch   0 Batch  478/3125   Loss: 1.175377 mae: 0.885684 (2225.189397958534 steps/sec)\n",
      "Step #480\tEpoch   0 Batch  479/3125   Loss: 1.148043 mae: 0.868439 (2249.511407639418 steps/sec)\n",
      "Step #481\tEpoch   0 Batch  480/3125   Loss: 1.147208 mae: 0.856315 (2238.442489966698 steps/sec)\n",
      "Step #482\tEpoch   0 Batch  481/3125   Loss: 1.122755 mae: 0.855111 (2394.063791410763 steps/sec)\n",
      "Step #483\tEpoch   0 Batch  482/3125   Loss: 1.170344 mae: 0.877977 (2285.599694839518 steps/sec)\n",
      "Step #484\tEpoch   0 Batch  483/3125   Loss: 1.117395 mae: 0.864557 (2283.857337326436 steps/sec)\n",
      "Step #485\tEpoch   0 Batch  484/3125   Loss: 1.082940 mae: 0.840217 (1967.6418156912048 steps/sec)\n",
      "Step #486\tEpoch   0 Batch  485/3125   Loss: 1.271145 mae: 0.923937 (1611.6565737296733 steps/sec)\n",
      "Step #487\tEpoch   0 Batch  486/3125   Loss: 1.090669 mae: 0.856360 (1817.179200568423 steps/sec)\n",
      "Step #488\tEpoch   0 Batch  487/3125   Loss: 1.185993 mae: 0.890200 (2229.660737637815 steps/sec)\n",
      "Step #489\tEpoch   0 Batch  488/3125   Loss: 1.047541 mae: 0.822772 (2223.1843190467607 steps/sec)\n",
      "Step #490\tEpoch   0 Batch  489/3125   Loss: 1.170503 mae: 0.873448 (2338.0924243268855 steps/sec)\n",
      "Step #491\tEpoch   0 Batch  490/3125   Loss: 1.204048 mae: 0.884743 (2264.4006305742114 steps/sec)\n",
      "Step #492\tEpoch   0 Batch  491/3125   Loss: 0.936499 mae: 0.791790 (2268.0985907876666 steps/sec)\n",
      "Step #493\tEpoch   0 Batch  492/3125   Loss: 1.082440 mae: 0.831343 (2213.6567550165196 steps/sec)\n",
      "Step #494\tEpoch   0 Batch  493/3125   Loss: 1.131814 mae: 0.848568 (1970.211286791992 steps/sec)\n",
      "Step #495\tEpoch   0 Batch  494/3125   Loss: 1.174349 mae: 0.870891 (1796.3065748449653 steps/sec)\n",
      "Step #496\tEpoch   0 Batch  495/3125   Loss: 1.076763 mae: 0.844380 (1082.6524870292455 steps/sec)\n",
      "Step #497\tEpoch   0 Batch  496/3125   Loss: 1.151941 mae: 0.859940 (2083.401549771508 steps/sec)\n",
      "Step #498\tEpoch   0 Batch  497/3125   Loss: 1.178838 mae: 0.885078 (2326.420766542792 steps/sec)\n",
      "Step #499\tEpoch   0 Batch  498/3125   Loss: 1.127303 mae: 0.873241 (2013.2788049843518 steps/sec)\n",
      "Step #500\tEpoch   0 Batch  499/3125   Loss: 0.998438 mae: 0.808104 (2326.5498114044817 steps/sec)\n",
      "Step #501\tEpoch   0 Batch  500/3125   Loss: 0.857892 mae: 0.734924 (2327.32438131173 steps/sec)\n",
      "Step #502\tEpoch   0 Batch  501/3125   Loss: 1.116585 mae: 0.857890 (2119.341505563247 steps/sec)\n",
      "Step #503\tEpoch   0 Batch  502/3125   Loss: 1.186203 mae: 0.889309 (1632.9017137606963 steps/sec)\n",
      "Step #504\tEpoch   0 Batch  503/3125   Loss: 1.100036 mae: 0.826545 (2396.8273197938215 steps/sec)\n",
      "Step #505\tEpoch   0 Batch  504/3125   Loss: 1.218576 mae: 0.879003 (2162.3467546527813 steps/sec)\n",
      "Step #506\tEpoch   0 Batch  505/3125   Loss: 1.074021 mae: 0.839239 (2377.669440602254 steps/sec)\n",
      "Step #507\tEpoch   0 Batch  506/3125   Loss: 1.190983 mae: 0.863733 (1672.0100137928835 steps/sec)\n",
      "Step #508\tEpoch   0 Batch  507/3125   Loss: 1.097463 mae: 0.849259 (1946.2043876907087 steps/sec)\n",
      "Step #509\tEpoch   0 Batch  508/3125   Loss: 1.193318 mae: 0.857566 (2015.484565409603 steps/sec)\n",
      "Step #510\tEpoch   0 Batch  509/3125   Loss: 1.118217 mae: 0.847194 (1987.445034116755 steps/sec)\n",
      "Step #511\tEpoch   0 Batch  510/3125   Loss: 1.225479 mae: 0.899411 (1581.6222331158792 steps/sec)\n",
      "Step #512\tEpoch   0 Batch  511/3125   Loss: 1.335874 mae: 0.934104 (2233.9596915079464 steps/sec)\n",
      "Step #513\tEpoch   0 Batch  512/3125   Loss: 1.062578 mae: 0.821210 (2165.539745151897 steps/sec)\n",
      "Step #514\tEpoch   0 Batch  513/3125   Loss: 1.087938 mae: 0.834561 (2129.9965467508987 steps/sec)\n",
      "Step #515\tEpoch   0 Batch  514/3125   Loss: 0.978407 mae: 0.787680 (2095.999200439758 steps/sec)\n",
      "Step #516\tEpoch   0 Batch  515/3125   Loss: 1.087922 mae: 0.838342 (2052.2889632630695 steps/sec)\n",
      "Step #517\tEpoch   0 Batch  516/3125   Loss: 1.194372 mae: 0.879455 (2396.717751797122 steps/sec)\n",
      "Step #518\tEpoch   0 Batch  517/3125   Loss: 1.049644 mae: 0.836942 (2041.918114989533 steps/sec)\n",
      "Step #519\tEpoch   0 Batch  518/3125   Loss: 1.178246 mae: 0.873789 (1507.0077608508193 steps/sec)\n",
      "Step #520\tEpoch   0 Batch  519/3125   Loss: 1.092739 mae: 0.862062 (1877.0324093549455 steps/sec)\n",
      "Step #521\tEpoch   0 Batch  520/3125   Loss: 1.216326 mae: 0.872515 (2300.946863719649 steps/sec)\n",
      "Step #522\tEpoch   0 Batch  521/3125   Loss: 1.023799 mae: 0.801173 (2027.644351626252 steps/sec)\n",
      "Step #523\tEpoch   0 Batch  522/3125   Loss: 1.181558 mae: 0.894063 (1948.86301331673 steps/sec)\n",
      "Step #524\tEpoch   0 Batch  523/3125   Loss: 0.863111 mae: 0.723071 (1659.9140421557531 steps/sec)\n",
      "Step #525\tEpoch   0 Batch  524/3125   Loss: 1.017669 mae: 0.820676 (1241.4986887361547 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #526\tEpoch   0 Batch  525/3125   Loss: 1.153660 mae: 0.859918 (1222.1664053894972 steps/sec)\n",
      "Step #527\tEpoch   0 Batch  526/3125   Loss: 1.041186 mae: 0.859695 (1254.5401250261718 steps/sec)\n",
      "Step #528\tEpoch   0 Batch  527/3125   Loss: 1.139061 mae: 0.859933 (1120.0700727431984 steps/sec)\n",
      "Step #529\tEpoch   0 Batch  528/3125   Loss: 0.976172 mae: 0.800691 (1595.4475602148411 steps/sec)\n",
      "Step #530\tEpoch   0 Batch  529/3125   Loss: 1.106745 mae: 0.849870 (1703.8103439871309 steps/sec)\n",
      "Step #531\tEpoch   0 Batch  530/3125   Loss: 1.044750 mae: 0.836176 (1992.505605594193 steps/sec)\n",
      "Step #532\tEpoch   0 Batch  531/3125   Loss: 1.201076 mae: 0.869775 (2004.9637660375915 steps/sec)\n",
      "Step #533\tEpoch   0 Batch  532/3125   Loss: 1.038772 mae: 0.809601 (1953.2738483318742 steps/sec)\n",
      "Step #534\tEpoch   0 Batch  533/3125   Loss: 1.001461 mae: 0.805115 (1947.957904123204 steps/sec)\n",
      "Step #535\tEpoch   0 Batch  534/3125   Loss: 1.062507 mae: 0.836607 (1750.6903748226061 steps/sec)\n",
      "Step #536\tEpoch   0 Batch  535/3125   Loss: 1.143012 mae: 0.842296 (1790.3717932300337 steps/sec)\n",
      "Step #537\tEpoch   0 Batch  536/3125   Loss: 1.208379 mae: 0.856131 (2197.9039154858724 steps/sec)\n",
      "Step #538\tEpoch   0 Batch  537/3125   Loss: 1.049783 mae: 0.837984 (1996.3369823893383 steps/sec)\n",
      "Step #539\tEpoch   0 Batch  538/3125   Loss: 1.057908 mae: 0.825094 (2146.3901909811066 steps/sec)\n",
      "Step #540\tEpoch   0 Batch  539/3125   Loss: 1.113813 mae: 0.832224 (1707.4726026281937 steps/sec)\n",
      "Step #541\tEpoch   0 Batch  540/3125   Loss: 1.051592 mae: 0.850559 (1743.3554458243968 steps/sec)\n",
      "Step #542\tEpoch   0 Batch  541/3125   Loss: 0.907566 mae: 0.769340 (1565.9269436396762 steps/sec)\n",
      "Step #543\tEpoch   0 Batch  542/3125   Loss: 1.074433 mae: 0.841820 (1645.444559520447 steps/sec)\n",
      "Step #544\tEpoch   0 Batch  543/3125   Loss: 1.098512 mae: 0.836334 (1854.5081532312263 steps/sec)\n",
      "Step #545\tEpoch   0 Batch  544/3125   Loss: 1.033575 mae: 0.827467 (1956.4810150200578 steps/sec)\n",
      "Step #546\tEpoch   0 Batch  545/3125   Loss: 1.079774 mae: 0.843891 (1856.396002443148 steps/sec)\n",
      "Step #547\tEpoch   0 Batch  546/3125   Loss: 1.248746 mae: 0.902732 (1792.054689168981 steps/sec)\n",
      "Step #548\tEpoch   0 Batch  547/3125   Loss: 1.200831 mae: 0.902694 (1053.696967260888 steps/sec)\n",
      "Step #549\tEpoch   0 Batch  548/3125   Loss: 1.088781 mae: 0.832841 (2099.4403900251273 steps/sec)\n",
      "Step #550\tEpoch   0 Batch  549/3125   Loss: 0.972888 mae: 0.789762 (1817.0532426461032 steps/sec)\n",
      "Step #551\tEpoch   0 Batch  550/3125   Loss: 1.090656 mae: 0.857586 (1903.1626328351165 steps/sec)\n",
      "Step #552\tEpoch   0 Batch  551/3125   Loss: 1.091076 mae: 0.851609 (2222.995791772226 steps/sec)\n",
      "Step #553\tEpoch   0 Batch  552/3125   Loss: 1.133226 mae: 0.839697 (2295.783158908788 steps/sec)\n",
      "Step #554\tEpoch   0 Batch  553/3125   Loss: 1.074254 mae: 0.836321 (1995.6151035322778 steps/sec)\n",
      "Step #555\tEpoch   0 Batch  554/3125   Loss: 1.033926 mae: 0.809381 (1810.8557119419738 steps/sec)\n",
      "Step #556\tEpoch   0 Batch  555/3125   Loss: 1.218255 mae: 0.881282 (1716.3181628460827 steps/sec)\n",
      "Step #557\tEpoch   0 Batch  556/3125   Loss: 0.985809 mae: 0.785708 (1618.5974715588966 steps/sec)\n",
      "Step #558\tEpoch   0 Batch  557/3125   Loss: 1.185531 mae: 0.886107 (1778.4079441669564 steps/sec)\n",
      "Step #559\tEpoch   0 Batch  558/3125   Loss: 1.027489 mae: 0.813256 (2094.680277272818 steps/sec)\n",
      "Step #560\tEpoch   0 Batch  559/3125   Loss: 1.186136 mae: 0.891055 (2139.492557717224 steps/sec)\n",
      "Step #561\tEpoch   0 Batch  560/3125   Loss: 1.147732 mae: 0.868577 (1882.964758698092 steps/sec)\n",
      "Step #562\tEpoch   0 Batch  561/3125   Loss: 1.024227 mae: 0.814143 (1513.2604538730743 steps/sec)\n",
      "Step #563\tEpoch   0 Batch  562/3125   Loss: 0.967592 mae: 0.804178 (1708.7525462397132 steps/sec)\n",
      "Step #564\tEpoch   0 Batch  563/3125   Loss: 1.038512 mae: 0.841522 (1770.9141882420497 steps/sec)\n",
      "Step #565\tEpoch   0 Batch  564/3125   Loss: 1.141837 mae: 0.860221 (1807.6091640952266 steps/sec)\n",
      "Step #566\tEpoch   0 Batch  565/3125   Loss: 1.270550 mae: 0.887044 (1655.4459197044569 steps/sec)\n",
      "Step #567\tEpoch   0 Batch  566/3125   Loss: 0.931077 mae: 0.778525 (1624.5909766980665 steps/sec)\n",
      "Step #568\tEpoch   0 Batch  567/3125   Loss: 1.130605 mae: 0.850366 (1519.5322179794657 steps/sec)\n",
      "Step #569\tEpoch   0 Batch  568/3125   Loss: 1.463735 mae: 0.985531 (1543.2490507167456 steps/sec)\n",
      "Step #570\tEpoch   0 Batch  569/3125   Loss: 0.977091 mae: 0.766033 (1656.897709585924 steps/sec)\n",
      "Step #571\tEpoch   0 Batch  570/3125   Loss: 1.138173 mae: 0.846148 (2173.732599479669 steps/sec)\n",
      "Step #572\tEpoch   0 Batch  571/3125   Loss: 1.082048 mae: 0.830941 (2192.503998912714 steps/sec)\n",
      "Step #573\tEpoch   0 Batch  572/3125   Loss: 1.154417 mae: 0.878062 (1925.971640584821 steps/sec)\n",
      "Step #574\tEpoch   0 Batch  573/3125   Loss: 1.055081 mae: 0.821578 (2108.1141938078003 steps/sec)\n",
      "Step #575\tEpoch   0 Batch  574/3125   Loss: 1.120282 mae: 0.847014 (2042.5545177407887 steps/sec)\n",
      "Step #576\tEpoch   0 Batch  575/3125   Loss: 1.080244 mae: 0.863440 (2006.4216145883163 steps/sec)\n",
      "Step #577\tEpoch   0 Batch  576/3125   Loss: 1.045889 mae: 0.815108 (1849.0468885009434 steps/sec)\n",
      "Step #578\tEpoch   0 Batch  577/3125   Loss: 1.153540 mae: 0.873048 (1171.5147587871204 steps/sec)\n",
      "Step #579\tEpoch   0 Batch  578/3125   Loss: 1.069084 mae: 0.839803 (2235.7220528346943 steps/sec)\n",
      "Step #580\tEpoch   0 Batch  579/3125   Loss: 1.041387 mae: 0.818531 (2027.8208066216073 steps/sec)\n",
      "Step #581\tEpoch   0 Batch  580/3125   Loss: 1.170425 mae: 0.870416 (2107.478645362275 steps/sec)\n",
      "Step #582\tEpoch   0 Batch  581/3125   Loss: 1.030194 mae: 0.819027 (2048.0 steps/sec)\n",
      "Step #583\tEpoch   0 Batch  582/3125   Loss: 1.070787 mae: 0.842616 (2066.302109505089 steps/sec)\n",
      "Step #584\tEpoch   0 Batch  583/3125   Loss: 1.074634 mae: 0.832058 (2237.7259437888133 steps/sec)\n",
      "Step #585\tEpoch   0 Batch  584/3125   Loss: 0.990814 mae: 0.812532 (1316.8185157510723 steps/sec)\n",
      "Step #586\tEpoch   0 Batch  585/3125   Loss: 0.942882 mae: 0.794840 (2086.801464734915 steps/sec)\n",
      "Step #587\tEpoch   0 Batch  586/3125   Loss: 1.085238 mae: 0.841497 (2027.1543599508955 steps/sec)\n",
      "Step #588\tEpoch   0 Batch  587/3125   Loss: 0.980992 mae: 0.796302 (1904.960531933254 steps/sec)\n",
      "Step #589\tEpoch   0 Batch  588/3125   Loss: 1.027936 mae: 0.822361 (2133.2899314385695 steps/sec)\n",
      "Step #590\tEpoch   0 Batch  589/3125   Loss: 1.022489 mae: 0.827045 (2065.3456765806577 steps/sec)\n",
      "Step #591\tEpoch   0 Batch  590/3125   Loss: 1.055214 mae: 0.801072 (2265.0120424672477 steps/sec)\n",
      "Step #592\tEpoch   0 Batch  591/3125   Loss: 0.959344 mae: 0.773296 (2186.5604570904275 steps/sec)\n",
      "Step #593\tEpoch   0 Batch  592/3125   Loss: 0.939915 mae: 0.769278 (2090.295829678654 steps/sec)\n",
      "Step #594\tEpoch   0 Batch  593/3125   Loss: 0.961530 mae: 0.768370 (1324.294013639808 steps/sec)\n",
      "Step #595\tEpoch   0 Batch  594/3125   Loss: 1.070637 mae: 0.817097 (2292.545667216896 steps/sec)\n",
      "Step #596\tEpoch   0 Batch  595/3125   Loss: 1.237459 mae: 0.877856 (2258.5478277726324 steps/sec)\n",
      "Step #597\tEpoch   0 Batch  596/3125   Loss: 1.048200 mae: 0.800043 (2281.4224949142217 steps/sec)\n",
      "Step #598\tEpoch   0 Batch  597/3125   Loss: 1.118268 mae: 0.872084 (2107.8599284364573 steps/sec)\n",
      "Step #599\tEpoch   0 Batch  598/3125   Loss: 1.120780 mae: 0.848690 (2283.484320557491 steps/sec)\n",
      "Step #600\tEpoch   0 Batch  599/3125   Loss: 0.942093 mae: 0.776899 (1842.1601869257393 steps/sec)\n",
      "Step #601\tEpoch   0 Batch  600/3125   Loss: 1.093662 mae: 0.851487 (2119.9842301588105 steps/sec)\n",
      "Step #602\tEpoch   0 Batch  601/3125   Loss: 1.008023 mae: 0.821623 (2145.3801456747688 steps/sec)\n",
      "Step #603\tEpoch   0 Batch  602/3125   Loss: 1.158111 mae: 0.849558 (1911.4542223032402 steps/sec)\n",
      "Step #604\tEpoch   0 Batch  603/3125   Loss: 1.022988 mae: 0.816408 (1371.700667813483 steps/sec)\n",
      "Step #605\tEpoch   0 Batch  604/3125   Loss: 1.050829 mae: 0.822333 (2024.3561527472102 steps/sec)\n",
      "Step #606\tEpoch   0 Batch  605/3125   Loss: 1.030920 mae: 0.825825 (2057.0195486066837 steps/sec)\n",
      "Step #607\tEpoch   0 Batch  606/3125   Loss: 0.966414 mae: 0.780563 (2178.4518219969254 steps/sec)\n",
      "Step #608\tEpoch   0 Batch  607/3125   Loss: 1.058969 mae: 0.842959 (2090.149997508347 steps/sec)\n",
      "Step #609\tEpoch   0 Batch  608/3125   Loss: 1.052188 mae: 0.825103 (2234.8167092924127 steps/sec)\n",
      "Step #610\tEpoch   0 Batch  609/3125   Loss: 0.958482 mae: 0.789043 (2030.9629184865241 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #611\tEpoch   0 Batch  610/3125   Loss: 1.013023 mae: 0.785740 (1804.2810929864409 steps/sec)\n",
      "Step #612\tEpoch   0 Batch  611/3125   Loss: 1.005775 mae: 0.823190 (1399.1460290350128 steps/sec)\n",
      "Step #613\tEpoch   0 Batch  612/3125   Loss: 1.049211 mae: 0.803157 (2089.962528900582 steps/sec)\n",
      "Step #614\tEpoch   0 Batch  613/3125   Loss: 1.180486 mae: 0.860000 (2073.370441041257 steps/sec)\n",
      "Step #615\tEpoch   0 Batch  614/3125   Loss: 1.043032 mae: 0.827593 (2160.7871825253724 steps/sec)\n",
      "Step #616\tEpoch   0 Batch  615/3125   Loss: 0.983611 mae: 0.810525 (2428.355391901437 steps/sec)\n",
      "Step #617\tEpoch   0 Batch  616/3125   Loss: 1.007894 mae: 0.811371 (2041.8187128809268 steps/sec)\n",
      "Step #618\tEpoch   0 Batch  617/3125   Loss: 1.188844 mae: 0.862799 (2185.762824922352 steps/sec)\n",
      "Step #619\tEpoch   0 Batch  618/3125   Loss: 1.052307 mae: 0.843029 (2308.291415803551 steps/sec)\n",
      "Step #620\tEpoch   0 Batch  619/3125   Loss: 1.001114 mae: 0.775604 (1627.6675669802241 steps/sec)\n",
      "Step #621\tEpoch   0 Batch  620/3125   Loss: 1.101373 mae: 0.839334 (2042.4749457035168 steps/sec)\n",
      "Step #622\tEpoch   0 Batch  621/3125   Loss: 0.784091 mae: 0.713472 (2247.07697582719 steps/sec)\n",
      "Step #623\tEpoch   0 Batch  622/3125   Loss: 0.909959 mae: 0.767899 (2244.1674068208326 steps/sec)\n",
      "Step #624\tEpoch   0 Batch  623/3125   Loss: 1.095520 mae: 0.833947 (2018.8217173661917 steps/sec)\n",
      "Step #625\tEpoch   0 Batch  624/3125   Loss: 0.925982 mae: 0.779385 (2188.7740831193773 steps/sec)\n",
      "Step #626\tEpoch   0 Batch  625/3125   Loss: 1.041788 mae: 0.808230 (2194.522984837228 steps/sec)\n",
      "Step #627\tEpoch   0 Batch  626/3125   Loss: 0.977246 mae: 0.786103 (2129.26125979775 steps/sec)\n",
      "Step #628\tEpoch   0 Batch  627/3125   Loss: 1.114772 mae: 0.851592 (2163.1050736970224 steps/sec)\n",
      "Step #629\tEpoch   0 Batch  628/3125   Loss: 0.985982 mae: 0.800799 (2112.9994962216624 steps/sec)\n",
      "Step #630\tEpoch   0 Batch  629/3125   Loss: 1.080435 mae: 0.815567 (1727.8286302780639 steps/sec)\n",
      "Step #631\tEpoch   0 Batch  630/3125   Loss: 1.056283 mae: 0.848622 (2019.2882452626714 steps/sec)\n",
      "Step #632\tEpoch   0 Batch  631/3125   Loss: 1.026172 mae: 0.820262 (2268.294846141366 steps/sec)\n",
      "Step #633\tEpoch   0 Batch  632/3125   Loss: 1.013816 mae: 0.774593 (2039.4756292060529 steps/sec)\n",
      "Step #634\tEpoch   0 Batch  633/3125   Loss: 0.931672 mae: 0.772158 (2030.3730310100784 steps/sec)\n",
      "Step #635\tEpoch   0 Batch  634/3125   Loss: 0.932202 mae: 0.778002 (2135.1795477453447 steps/sec)\n",
      "Step #636\tEpoch   0 Batch  635/3125   Loss: 1.066904 mae: 0.848564 (2095.8316260755723 steps/sec)\n",
      "Step #637\tEpoch   0 Batch  636/3125   Loss: 1.085260 mae: 0.840360 (1837.7048318407262 steps/sec)\n",
      "Step #638\tEpoch   0 Batch  637/3125   Loss: 0.995424 mae: 0.799784 (1995.2923267208982 steps/sec)\n",
      "Step #639\tEpoch   0 Batch  638/3125   Loss: 1.066777 mae: 0.831389 (1989.179345145502 steps/sec)\n",
      "Step #640\tEpoch   0 Batch  639/3125   Loss: 1.056795 mae: 0.832614 (2355.1597506878543 steps/sec)\n",
      "Step #641\tEpoch   0 Batch  640/3125   Loss: 1.059138 mae: 0.835018 (2162.458238812126 steps/sec)\n",
      "Step #642\tEpoch   0 Batch  641/3125   Loss: 1.188783 mae: 0.883235 (2281.1247076739 steps/sec)\n",
      "Step #643\tEpoch   0 Batch  642/3125   Loss: 0.977530 mae: 0.781345 (2117.650860327975 steps/sec)\n",
      "Step #644\tEpoch   0 Batch  643/3125   Loss: 0.881045 mae: 0.766893 (2096.942305769423 steps/sec)\n",
      "Step #645\tEpoch   0 Batch  644/3125   Loss: 1.090528 mae: 0.852960 (1781.3838914088647 steps/sec)\n",
      "Step #646\tEpoch   0 Batch  645/3125   Loss: 1.080752 mae: 0.837722 (1380.9863096688375 steps/sec)\n",
      "Step #647\tEpoch   0 Batch  646/3125   Loss: 1.086611 mae: 0.846150 (1662.9018189891685 steps/sec)\n",
      "Step #648\tEpoch   0 Batch  647/3125   Loss: 0.954961 mae: 0.784492 (1849.6013546884922 steps/sec)\n",
      "Step #649\tEpoch   0 Batch  648/3125   Loss: 1.105002 mae: 0.830078 (1711.108754008208 steps/sec)\n",
      "Step #650\tEpoch   0 Batch  649/3125   Loss: 0.974920 mae: 0.802709 (1839.106909524603 steps/sec)\n",
      "Step #651\tEpoch   0 Batch  650/3125   Loss: 1.037114 mae: 0.832729 (1744.8204138344163 steps/sec)\n",
      "Step #652\tEpoch   0 Batch  651/3125   Loss: 1.045188 mae: 0.812485 (1818.108679821063 steps/sec)\n",
      "Step #653\tEpoch   0 Batch  652/3125   Loss: 1.090594 mae: 0.838579 (1824.816400403745 steps/sec)\n",
      "Step #654\tEpoch   0 Batch  653/3125   Loss: 1.027531 mae: 0.807915 (1545.3984068148825 steps/sec)\n",
      "Step #655\tEpoch   0 Batch  654/3125   Loss: 0.930351 mae: 0.767233 (1910.9491179472227 steps/sec)\n",
      "Step #656\tEpoch   0 Batch  655/3125   Loss: 0.934127 mae: 0.768489 (1924.999311566599 steps/sec)\n",
      "Step #657\tEpoch   0 Batch  656/3125   Loss: 1.105755 mae: 0.838923 (2011.907480069457 steps/sec)\n",
      "Step #658\tEpoch   0 Batch  657/3125   Loss: 1.121219 mae: 0.879874 (2076.1620021581807 steps/sec)\n",
      "Step #659\tEpoch   0 Batch  658/3125   Loss: 0.998603 mae: 0.807130 (1894.7201040800837 steps/sec)\n",
      "Step #660\tEpoch   0 Batch  659/3125   Loss: 1.109939 mae: 0.831333 (1816.51811622448 steps/sec)\n",
      "Step #661\tEpoch   0 Batch  660/3125   Loss: 1.082501 mae: 0.820782 (2013.5300951484835 steps/sec)\n",
      "Step #662\tEpoch   0 Batch  661/3125   Loss: 0.978369 mae: 0.806891 (2070.034547428684 steps/sec)\n",
      "Step #663\tEpoch   0 Batch  662/3125   Loss: 1.048481 mae: 0.816856 (2097.6764191047764 steps/sec)\n",
      "Step #664\tEpoch   0 Batch  663/3125   Loss: 0.981060 mae: 0.809970 (2286.546659834055 steps/sec)\n",
      "Step #665\tEpoch   0 Batch  664/3125   Loss: 0.944981 mae: 0.789631 (2330.9976880668682 steps/sec)\n",
      "Step #666\tEpoch   0 Batch  665/3125   Loss: 1.130734 mae: 0.840413 (2072.5303395659566 steps/sec)\n",
      "Step #667\tEpoch   0 Batch  666/3125   Loss: 0.935033 mae: 0.790031 (2329.910009998889 steps/sec)\n",
      "Step #668\tEpoch   0 Batch  667/3125   Loss: 0.944759 mae: 0.756654 (2362.509012256669 steps/sec)\n",
      "Step #669\tEpoch   0 Batch  668/3125   Loss: 0.897758 mae: 0.760762 (2092.3396188765837 steps/sec)\n",
      "Step #670\tEpoch   0 Batch  669/3125   Loss: 0.921056 mae: 0.765310 (2032.7543424317619 steps/sec)\n",
      "Step #671\tEpoch   0 Batch  670/3125   Loss: 1.018796 mae: 0.784111 (2211.1127511966765 steps/sec)\n",
      "Step #672\tEpoch   0 Batch  671/3125   Loss: 0.965349 mae: 0.789247 (2058.796618988249 steps/sec)\n",
      "Step #673\tEpoch   0 Batch  672/3125   Loss: 1.061597 mae: 0.813933 (2202.9370364923634 steps/sec)\n",
      "Step #674\tEpoch   0 Batch  673/3125   Loss: 1.039249 mae: 0.829217 (2199.9328633769724 steps/sec)\n",
      "Step #675\tEpoch   0 Batch  674/3125   Loss: 0.976984 mae: 0.794753 (2166.389818602537 steps/sec)\n",
      "Step #676\tEpoch   0 Batch  675/3125   Loss: 0.856827 mae: 0.730365 (2119.44859926426 steps/sec)\n",
      "Step #677\tEpoch   0 Batch  676/3125   Loss: 1.027992 mae: 0.798137 (2244.2874876931637 steps/sec)\n",
      "Step #678\tEpoch   0 Batch  677/3125   Loss: 0.968830 mae: 0.783299 (2093.6137926903534 steps/sec)\n",
      "Step #679\tEpoch   0 Batch  678/3125   Loss: 0.936634 mae: 0.776890 (2096.7116905449857 steps/sec)\n",
      "Step #680\tEpoch   0 Batch  679/3125   Loss: 1.176933 mae: 0.882684 (2092.8407480589985 steps/sec)\n",
      "Step #681\tEpoch   0 Batch  680/3125   Loss: 1.073574 mae: 0.842035 (2216.206619605191 steps/sec)\n",
      "Step #682\tEpoch   0 Batch  681/3125   Loss: 0.947241 mae: 0.777043 (2195.671793368442 steps/sec)\n",
      "Step #683\tEpoch   0 Batch  682/3125   Loss: 1.036569 mae: 0.804438 (2151.8300003078216 steps/sec)\n",
      "Step #684\tEpoch   0 Batch  683/3125   Loss: 0.926391 mae: 0.782710 (2123.3111939089586 steps/sec)\n",
      "Step #685\tEpoch   0 Batch  684/3125   Loss: 1.082117 mae: 0.839606 (1973.0101983216045 steps/sec)\n",
      "Step #686\tEpoch   0 Batch  685/3125   Loss: 0.989134 mae: 0.792619 (2022.579494054221 steps/sec)\n",
      "Step #687\tEpoch   0 Batch  686/3125   Loss: 1.079987 mae: 0.848304 (1942.4189096567438 steps/sec)\n",
      "Step #688\tEpoch   0 Batch  687/3125   Loss: 0.993715 mae: 0.803258 (2011.1935861288528 steps/sec)\n",
      "Step #689\tEpoch   0 Batch  688/3125   Loss: 1.059247 mae: 0.826148 (2067.728227325163 steps/sec)\n",
      "Step #690\tEpoch   0 Batch  689/3125   Loss: 1.160723 mae: 0.866334 (2144.985169274829 steps/sec)\n",
      "Step #691\tEpoch   0 Batch  690/3125   Loss: 1.008681 mae: 0.814135 (2384.021280707538 steps/sec)\n",
      "Step #692\tEpoch   0 Batch  691/3125   Loss: 1.041893 mae: 0.835521 (2185.011304556205 steps/sec)\n",
      "Step #693\tEpoch   0 Batch  692/3125   Loss: 1.148232 mae: 0.879476 (2163.2835790103463 steps/sec)\n",
      "Step #694\tEpoch   0 Batch  693/3125   Loss: 1.112652 mae: 0.847503 (2264.3761809642065 steps/sec)\n",
      "Step #695\tEpoch   0 Batch  694/3125   Loss: 0.989584 mae: 0.802760 (2227.340024427805 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #696\tEpoch   0 Batch  695/3125   Loss: 0.939005 mae: 0.784714 (2061.9544377476477 steps/sec)\n",
      "Step #697\tEpoch   0 Batch  696/3125   Loss: 0.996801 mae: 0.807703 (1789.9439242764354 steps/sec)\n",
      "Step #698\tEpoch   0 Batch  697/3125   Loss: 1.140513 mae: 0.848827 (2290.2173200829966 steps/sec)\n",
      "Step #699\tEpoch   0 Batch  698/3125   Loss: 1.104659 mae: 0.838269 (2265.525883674704 steps/sec)\n",
      "Step #700\tEpoch   0 Batch  699/3125   Loss: 0.869350 mae: 0.743797 (2236.9621333333334 steps/sec)\n",
      "Step #701\tEpoch   0 Batch  700/3125   Loss: 1.009218 mae: 0.791456 (2218.950175111892 steps/sec)\n",
      "Step #702\tEpoch   0 Batch  701/3125   Loss: 1.052109 mae: 0.801837 (2217.776884762217 steps/sec)\n",
      "Step #703\tEpoch   0 Batch  702/3125   Loss: 0.991733 mae: 0.817984 (2291.8942548331747 steps/sec)\n",
      "Step #704\tEpoch   0 Batch  703/3125   Loss: 1.014192 mae: 0.799680 (2352.17479082079 steps/sec)\n",
      "Step #705\tEpoch   0 Batch  704/3125   Loss: 1.000412 mae: 0.780637 (1929.1606873459175 steps/sec)\n",
      "Step #706\tEpoch   0 Batch  705/3125   Loss: 1.182871 mae: 0.896472 (1989.726657748176 steps/sec)\n",
      "Step #707\tEpoch   0 Batch  706/3125   Loss: 1.043087 mae: 0.809370 (2052.811276429131 steps/sec)\n",
      "Step #708\tEpoch   0 Batch  707/3125   Loss: 1.070828 mae: 0.834409 (2164.2212154673325 steps/sec)\n",
      "Step #709\tEpoch   0 Batch  708/3125   Loss: 0.987812 mae: 0.781697 (2135.201286933149 steps/sec)\n",
      "Step #710\tEpoch   0 Batch  709/3125   Loss: 0.942500 mae: 0.783112 (2251.4434173940117 steps/sec)\n",
      "Step #711\tEpoch   0 Batch  710/3125   Loss: 0.966289 mae: 0.786553 (2183.8963635606283 steps/sec)\n",
      "Step #712\tEpoch   0 Batch  711/3125   Loss: 1.029253 mae: 0.798495 (1848.166948674563 steps/sec)\n",
      "Step #713\tEpoch   0 Batch  712/3125   Loss: 0.858088 mae: 0.740129 (1554.2518342844437 steps/sec)\n",
      "Step #714\tEpoch   0 Batch  713/3125   Loss: 0.928285 mae: 0.766940 (1556.90571640683 steps/sec)\n",
      "Step #715\tEpoch   0 Batch  714/3125   Loss: 0.916515 mae: 0.776761 (1447.3398345031299 steps/sec)\n",
      "Step #716\tEpoch   0 Batch  715/3125   Loss: 1.001168 mae: 0.791227 (1711.6953288877644 steps/sec)\n",
      "Step #717\tEpoch   0 Batch  716/3125   Loss: 0.954775 mae: 0.780176 (1719.1461455225103 steps/sec)\n",
      "Step #718\tEpoch   0 Batch  717/3125   Loss: 0.944797 mae: 0.791418 (1998.3153240714273 steps/sec)\n",
      "Step #719\tEpoch   0 Batch  718/3125   Loss: 1.042652 mae: 0.821949 (2006.1720954704167 steps/sec)\n",
      "Step #720\tEpoch   0 Batch  719/3125   Loss: 1.026919 mae: 0.837894 (2065.7729095046247 steps/sec)\n",
      "Step #721\tEpoch   0 Batch  720/3125   Loss: 0.895196 mae: 0.747432 (1608.3934104365433 steps/sec)\n",
      "Step #722\tEpoch   0 Batch  721/3125   Loss: 0.918907 mae: 0.762258 (2222.807296469416 steps/sec)\n",
      "Step #723\tEpoch   0 Batch  722/3125   Loss: 0.981389 mae: 0.804045 (2106.5048816746353 steps/sec)\n",
      "Step #724\tEpoch   0 Batch  723/3125   Loss: 1.211974 mae: 0.882015 (2209.9477322542575 steps/sec)\n",
      "Step #725\tEpoch   0 Batch  724/3125   Loss: 0.820565 mae: 0.740551 (2193.1460004392247 steps/sec)\n",
      "Step #726\tEpoch   0 Batch  725/3125   Loss: 0.975384 mae: 0.799238 (2025.001206994776 steps/sec)\n",
      "Step #727\tEpoch   0 Batch  726/3125   Loss: 0.956426 mae: 0.807876 (2046.6608761845278 steps/sec)\n",
      "Step #728\tEpoch   0 Batch  727/3125   Loss: 0.836051 mae: 0.734970 (2227.6712590688435 steps/sec)\n",
      "Step #729\tEpoch   0 Batch  728/3125   Loss: 0.959602 mae: 0.782588 (1788.2649885310345 steps/sec)\n",
      "Step #730\tEpoch   0 Batch  729/3125   Loss: 0.945368 mae: 0.779874 (2144.4149044950714 steps/sec)\n",
      "Step #731\tEpoch   0 Batch  730/3125   Loss: 0.852750 mae: 0.730043 (2121.5283608663544 steps/sec)\n",
      "Step #732\tEpoch   0 Batch  731/3125   Loss: 0.943642 mae: 0.785098 (2295.984234727392 steps/sec)\n",
      "Step #733\tEpoch   0 Batch  732/3125   Loss: 1.172448 mae: 0.882148 (2077.6223499108382 steps/sec)\n",
      "Step #734\tEpoch   0 Batch  733/3125   Loss: 0.991720 mae: 0.797950 (2115.962910272321 steps/sec)\n",
      "Step #735\tEpoch   0 Batch  734/3125   Loss: 1.021715 mae: 0.808587 (2080.982763924308 steps/sec)\n",
      "Step #736\tEpoch   0 Batch  735/3125   Loss: 0.894216 mae: 0.735894 (2240.2358646769144 steps/sec)\n",
      "Step #737\tEpoch   0 Batch  736/3125   Loss: 1.071939 mae: 0.827012 (2305.6267727962354 steps/sec)\n",
      "Step #738\tEpoch   0 Batch  737/3125   Loss: 0.854867 mae: 0.751625 (1558.0624071322436 steps/sec)\n",
      "Step #739\tEpoch   0 Batch  738/3125   Loss: 1.130199 mae: 0.850853 (1803.0245974224722 steps/sec)\n",
      "Step #740\tEpoch   0 Batch  739/3125   Loss: 1.019230 mae: 0.838550 (1910.67055393586 steps/sec)\n",
      "Step #741\tEpoch   0 Batch  740/3125   Loss: 1.049786 mae: 0.815891 (2084.540529794742 steps/sec)\n",
      "Step #742\tEpoch   0 Batch  741/3125   Loss: 0.993250 mae: 0.801251 (2192.8020242999646 steps/sec)\n",
      "Step #743\tEpoch   0 Batch  742/3125   Loss: 1.074678 mae: 0.848506 (2174.2734803479416 steps/sec)\n",
      "Step #744\tEpoch   0 Batch  743/3125   Loss: 0.924883 mae: 0.768982 (2295.029438157981 steps/sec)\n",
      "Step #745\tEpoch   0 Batch  744/3125   Loss: 1.017589 mae: 0.803730 (2237.5110693823553 steps/sec)\n",
      "Step #746\tEpoch   0 Batch  745/3125   Loss: 0.909994 mae: 0.763005 (2048.94043164343 steps/sec)\n",
      "Step #747\tEpoch   0 Batch  746/3125   Loss: 0.970132 mae: 0.797351 (1854.8854158374684 steps/sec)\n",
      "Step #748\tEpoch   0 Batch  747/3125   Loss: 0.937867 mae: 0.783436 (1968.4914019674102 steps/sec)\n",
      "Step #749\tEpoch   0 Batch  748/3125   Loss: 0.863104 mae: 0.743028 (2199.079326797043 steps/sec)\n",
      "Step #750\tEpoch   0 Batch  749/3125   Loss: 0.904972 mae: 0.754464 (2325.9047302168246 steps/sec)\n",
      "Step #751\tEpoch   0 Batch  750/3125   Loss: 0.935210 mae: 0.781679 (2275.2099290472365 steps/sec)\n",
      "Step #752\tEpoch   0 Batch  751/3125   Loss: 0.965952 mae: 0.770677 (2378.8021778584393 steps/sec)\n",
      "Step #753\tEpoch   0 Batch  752/3125   Loss: 1.143330 mae: 0.847693 (2208.3652752622047 steps/sec)\n",
      "Step #754\tEpoch   0 Batch  753/3125   Loss: 1.036724 mae: 0.821936 (2295.808291460037 steps/sec)\n",
      "Step #755\tEpoch   0 Batch  754/3125   Loss: 1.050838 mae: 0.822289 (2150.8148300087173 steps/sec)\n",
      "Step #756\tEpoch   0 Batch  755/3125   Loss: 1.044660 mae: 0.803138 (2216.4174214481236 steps/sec)\n",
      "Step #757\tEpoch   0 Batch  756/3125   Loss: 0.951837 mae: 0.787380 (1931.3993111197067 steps/sec)\n",
      "Step #758\tEpoch   0 Batch  757/3125   Loss: 0.951645 mae: 0.780006 (2206.3671751709626 steps/sec)\n",
      "Step #759\tEpoch   0 Batch  758/3125   Loss: 0.905585 mae: 0.748243 (2260.300489319049 steps/sec)\n",
      "Step #760\tEpoch   0 Batch  759/3125   Loss: 1.112764 mae: 0.840738 (2122.9672821509557 steps/sec)\n",
      "Step #761\tEpoch   0 Batch  760/3125   Loss: 0.963552 mae: 0.782173 (2106.3144679355196 steps/sec)\n",
      "Step #762\tEpoch   0 Batch  761/3125   Loss: 0.966723 mae: 0.766375 (2124.8601767042232 steps/sec)\n",
      "Step #763\tEpoch   0 Batch  762/3125   Loss: 0.978712 mae: 0.798188 (2352.8609253691156 steps/sec)\n",
      "Step #764\tEpoch   0 Batch  763/3125   Loss: 1.033917 mae: 0.822929 (2118.6135553153445 steps/sec)\n",
      "Step #765\tEpoch   0 Batch  764/3125   Loss: 1.044971 mae: 0.847439 (2128.0944939419155 steps/sec)\n",
      "Step #766\tEpoch   0 Batch  765/3125   Loss: 0.910820 mae: 0.789110 (1819.3705104625742 steps/sec)\n",
      "Step #767\tEpoch   0 Batch  766/3125   Loss: 0.958624 mae: 0.787876 (2264.596246463512 steps/sec)\n",
      "Step #768\tEpoch   0 Batch  767/3125   Loss: 1.031400 mae: 0.818941 (2282.3660009794853 steps/sec)\n",
      "Step #769\tEpoch   0 Batch  768/3125   Loss: 0.946889 mae: 0.794170 (2222.807296469416 steps/sec)\n",
      "Step #770\tEpoch   0 Batch  769/3125   Loss: 1.048187 mae: 0.806640 (2060.819747845484 steps/sec)\n",
      "Step #771\tEpoch   0 Batch  770/3125   Loss: 0.916361 mae: 0.750821 (2192.297721095547 steps/sec)\n",
      "Step #772\tEpoch   0 Batch  771/3125   Loss: 0.851435 mae: 0.756168 (2226.8670029200957 steps/sec)\n",
      "Step #773\tEpoch   0 Batch  772/3125   Loss: 1.005862 mae: 0.826459 (2179.0176947933874 steps/sec)\n",
      "Step #774\tEpoch   0 Batch  773/3125   Loss: 0.839207 mae: 0.750539 (2153.7526188226593 steps/sec)\n",
      "Step #775\tEpoch   0 Batch  774/3125   Loss: 0.942116 mae: 0.789222 (1736.9588444304563 steps/sec)\n",
      "Step #776\tEpoch   0 Batch  775/3125   Loss: 0.927513 mae: 0.772478 (2051.6866244032244 steps/sec)\n",
      "Step #777\tEpoch   0 Batch  776/3125   Loss: 0.926199 mae: 0.784305 (2104.285527939716 steps/sec)\n",
      "Step #778\tEpoch   0 Batch  777/3125   Loss: 0.853014 mae: 0.745462 (2154.327861442689 steps/sec)\n",
      "Step #779\tEpoch   0 Batch  778/3125   Loss: 0.936718 mae: 0.754353 (1912.186226327355 steps/sec)\n",
      "Step #780\tEpoch   0 Batch  779/3125   Loss: 0.918180 mae: 0.747053 (1863.4394270583427 steps/sec)\n",
      "Step #781\tEpoch   0 Batch  780/3125   Loss: 1.037415 mae: 0.826459 (1998.0107086374117 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #782\tEpoch   0 Batch  781/3125   Loss: 0.966577 mae: 0.781492 (1729.7382898524427 steps/sec)\n",
      "Step #783\tEpoch   0 Batch  782/3125   Loss: 1.102844 mae: 0.855419 (1642.6349181483513 steps/sec)\n",
      "Step #784\tEpoch   0 Batch  783/3125   Loss: 0.837560 mae: 0.716899 (1918.3607757043542 steps/sec)\n",
      "Step #785\tEpoch   0 Batch  784/3125   Loss: 1.074556 mae: 0.827344 (1891.2002885742627 steps/sec)\n",
      "Step #786\tEpoch   0 Batch  785/3125   Loss: 1.132222 mae: 0.834034 (1768.0327108713063 steps/sec)\n",
      "Step #787\tEpoch   0 Batch  786/3125   Loss: 1.151277 mae: 0.862771 (1792.8206881812353 steps/sec)\n",
      "Step #788\tEpoch   0 Batch  787/3125   Loss: 1.042311 mae: 0.818331 (1978.0907195879984 steps/sec)\n",
      "Step #789\tEpoch   0 Batch  788/3125   Loss: 0.862526 mae: 0.749239 (2114.1710771712283 steps/sec)\n",
      "Step #790\tEpoch   0 Batch  789/3125   Loss: 0.948266 mae: 0.783833 (1765.101168232165 steps/sec)\n",
      "Step #791\tEpoch   0 Batch  790/3125   Loss: 0.911089 mae: 0.776283 (2206.25111777392 steps/sec)\n",
      "Step #792\tEpoch   0 Batch  791/3125   Loss: 1.013831 mae: 0.820448 (2246.3789539081163 steps/sec)\n",
      "Step #793\tEpoch   0 Batch  792/3125   Loss: 1.067375 mae: 0.824198 (2235.1263495582293 steps/sec)\n",
      "Step #794\tEpoch   0 Batch  793/3125   Loss: 0.944579 mae: 0.793128 (2362.4025593655656 steps/sec)\n",
      "Step #795\tEpoch   0 Batch  794/3125   Loss: 1.098130 mae: 0.845297 (2269.4239738553606 steps/sec)\n",
      "Step #796\tEpoch   0 Batch  795/3125   Loss: 0.996209 mae: 0.804653 (2275.2099290472365 steps/sec)\n",
      "Step #797\tEpoch   0 Batch  796/3125   Loss: 1.108626 mae: 0.840729 (2084.105499572675 steps/sec)\n",
      "Step #798\tEpoch   0 Batch  797/3125   Loss: 1.077724 mae: 0.827007 (2075.216956766973 steps/sec)\n",
      "Step #799\tEpoch   0 Batch  798/3125   Loss: 0.841511 mae: 0.708810 (1915.1198575407516 steps/sec)\n",
      "Step #800\tEpoch   0 Batch  799/3125   Loss: 0.980553 mae: 0.786830 (1802.9780941572956 steps/sec)\n",
      "Step #801\tEpoch   0 Batch  800/3125   Loss: 0.839138 mae: 0.738463 (1776.7806762630157 steps/sec)\n",
      "Step #802\tEpoch   0 Batch  801/3125   Loss: 1.178876 mae: 0.883125 (1899.8351240193504 steps/sec)\n",
      "Step #803\tEpoch   0 Batch  802/3125   Loss: 0.874948 mae: 0.758689 (1808.5132804415316 steps/sec)\n",
      "Step #804\tEpoch   0 Batch  803/3125   Loss: 0.955345 mae: 0.778189 (1575.3846153846155 steps/sec)\n",
      "Step #805\tEpoch   0 Batch  804/3125   Loss: 0.973426 mae: 0.800020 (1573.670506134394 steps/sec)\n",
      "Step #806\tEpoch   0 Batch  805/3125   Loss: 0.838108 mae: 0.732728 (1500.3770345197638 steps/sec)\n",
      "Step #807\tEpoch   0 Batch  806/3125   Loss: 0.991369 mae: 0.796852 (1311.342887871739 steps/sec)\n",
      "Step #808\tEpoch   0 Batch  807/3125   Loss: 0.908887 mae: 0.787536 (1954.0382393500056 steps/sec)\n",
      "Step #809\tEpoch   0 Batch  808/3125   Loss: 0.913353 mae: 0.760245 (1892.0875512008517 steps/sec)\n",
      "Step #810\tEpoch   0 Batch  809/3125   Loss: 0.985279 mae: 0.798215 (2072.8990807551645 steps/sec)\n",
      "Step #811\tEpoch   0 Batch  810/3125   Loss: 0.852520 mae: 0.728441 (1986.7295704731048 steps/sec)\n",
      "Step #812\tEpoch   0 Batch  811/3125   Loss: 1.030030 mae: 0.820260 (2204.4885473715194 steps/sec)\n",
      "Step #813\tEpoch   0 Batch  812/3125   Loss: 0.962511 mae: 0.790577 (2209.4587903115353 steps/sec)\n",
      "Step #814\tEpoch   0 Batch  813/3125   Loss: 1.089075 mae: 0.836880 (2010.4416515678774 steps/sec)\n",
      "Step #815\tEpoch   0 Batch  814/3125   Loss: 1.016219 mae: 0.810939 (1770.9740073299668 steps/sec)\n",
      "Step #816\tEpoch   0 Batch  815/3125   Loss: 0.901667 mae: 0.753147 (1980.0517400910173 steps/sec)\n",
      "Step #817\tEpoch   0 Batch  816/3125   Loss: 1.114787 mae: 0.856241 (2032.7740459643103 steps/sec)\n",
      "Step #818\tEpoch   0 Batch  817/3125   Loss: 0.861951 mae: 0.723816 (1825.880877961291 steps/sec)\n",
      "Step #819\tEpoch   0 Batch  818/3125   Loss: 0.800908 mae: 0.710256 (2048.920413466987 steps/sec)\n",
      "Step #820\tEpoch   0 Batch  819/3125   Loss: 0.847492 mae: 0.724921 (1808.778451480469 steps/sec)\n",
      "Step #821\tEpoch   0 Batch  820/3125   Loss: 0.944456 mae: 0.760872 (1697.906310215846 steps/sec)\n",
      "Step #822\tEpoch   0 Batch  821/3125   Loss: 1.135728 mae: 0.876146 (1461.5623715040388 steps/sec)\n",
      "Step #823\tEpoch   0 Batch  822/3125   Loss: 0.865933 mae: 0.744237 (1785.3876147179512 steps/sec)\n",
      "Step #824\tEpoch   0 Batch  823/3125   Loss: 0.936937 mae: 0.760170 (1625.9134924757527 steps/sec)\n",
      "Step #825\tEpoch   0 Batch  824/3125   Loss: 0.962787 mae: 0.777901 (1901.730204214879 steps/sec)\n",
      "Step #826\tEpoch   0 Batch  825/3125   Loss: 0.979230 mae: 0.803886 (1705.9586271973708 steps/sec)\n",
      "Step #827\tEpoch   0 Batch  826/3125   Loss: 1.005787 mae: 0.780089 (1732.982960649181 steps/sec)\n",
      "Step #828\tEpoch   0 Batch  827/3125   Loss: 0.904829 mae: 0.767942 (2127.8353862700137 steps/sec)\n",
      "Step #829\tEpoch   0 Batch  828/3125   Loss: 1.051167 mae: 0.817637 (2105.0036636654722 steps/sec)\n",
      "Step #830\tEpoch   0 Batch  829/3125   Loss: 1.118654 mae: 0.837049 (1777.3830207388698 steps/sec)\n",
      "Step #831\tEpoch   0 Batch  830/3125   Loss: 0.801869 mae: 0.690249 (2253.306113677877 steps/sec)\n",
      "Step #832\tEpoch   0 Batch  831/3125   Loss: 1.099250 mae: 0.854914 (2291.36838424894 steps/sec)\n",
      "Step #833\tEpoch   0 Batch  832/3125   Loss: 1.013484 mae: 0.817804 (2067.3001853239225 steps/sec)\n",
      "Step #834\tEpoch   0 Batch  833/3125   Loss: 1.177561 mae: 0.887558 (2157.674777509131 steps/sec)\n",
      "Step #835\tEpoch   0 Batch  834/3125   Loss: 1.003064 mae: 0.793010 (2026.3317068457413 steps/sec)\n",
      "Step #836\tEpoch   0 Batch  835/3125   Loss: 0.903743 mae: 0.785383 (2136.854761468077 steps/sec)\n",
      "Step #837\tEpoch   0 Batch  836/3125   Loss: 0.947458 mae: 0.767197 (2173.349638319481 steps/sec)\n",
      "Step #838\tEpoch   0 Batch  837/3125   Loss: 1.043551 mae: 0.844431 (2147.951042146771 steps/sec)\n",
      "Step #839\tEpoch   0 Batch  838/3125   Loss: 1.083048 mae: 0.828189 (1996.5650526476134 steps/sec)\n",
      "Step #840\tEpoch   0 Batch  839/3125   Loss: 1.064288 mae: 0.831960 (2085.597788253078 steps/sec)\n",
      "Step #841\tEpoch   0 Batch  840/3125   Loss: 0.894524 mae: 0.759135 (2152.7577322233287 steps/sec)\n",
      "Step #842\tEpoch   0 Batch  841/3125   Loss: 1.087665 mae: 0.846344 (2418.5256942522374 steps/sec)\n",
      "Step #843\tEpoch   0 Batch  842/3125   Loss: 0.925465 mae: 0.762688 (2135.570920866386 steps/sec)\n",
      "Step #844\tEpoch   0 Batch  843/3125   Loss: 0.970668 mae: 0.806084 (2067.5039927440503 steps/sec)\n",
      "Step #845\tEpoch   0 Batch  844/3125   Loss: 1.104673 mae: 0.824720 (2126.476105494773 steps/sec)\n",
      "Step #846\tEpoch   0 Batch  845/3125   Loss: 0.982401 mae: 0.776204 (2044.8847458949258 steps/sec)\n",
      "Step #847\tEpoch   0 Batch  846/3125   Loss: 0.833802 mae: 0.741222 (2312.8992412211046 steps/sec)\n",
      "Step #848\tEpoch   0 Batch  847/3125   Loss: 1.066728 mae: 0.824437 (2004.331412296547 steps/sec)\n",
      "Step #849\tEpoch   0 Batch  848/3125   Loss: 1.055893 mae: 0.792296 (1943.4989713268956 steps/sec)\n",
      "Step #850\tEpoch   0 Batch  849/3125   Loss: 1.051095 mae: 0.840014 (2196.7297600217876 steps/sec)\n",
      "Step #851\tEpoch   0 Batch  850/3125   Loss: 0.981468 mae: 0.795190 (2097.8652741932256 steps/sec)\n",
      "Step #852\tEpoch   0 Batch  851/3125   Loss: 1.009461 mae: 0.806931 (2312.032280113774 steps/sec)\n",
      "Step #853\tEpoch   0 Batch  852/3125   Loss: 0.994589 mae: 0.826661 (2252.9672124102963 steps/sec)\n",
      "Step #854\tEpoch   0 Batch  853/3125   Loss: 0.986743 mae: 0.780035 (2077.107908681226 steps/sec)\n",
      "Step #855\tEpoch   0 Batch  854/3125   Loss: 0.911232 mae: 0.752906 (1899.336140922882 steps/sec)\n",
      "Step #856\tEpoch   0 Batch  855/3125   Loss: 1.185542 mae: 0.876857 (1771.7220870505541 steps/sec)\n",
      "Step #857\tEpoch   0 Batch  856/3125   Loss: 0.842097 mae: 0.735157 (1431.4737582165553 steps/sec)\n",
      "Step #858\tEpoch   0 Batch  857/3125   Loss: 0.877019 mae: 0.751286 (1923.4456255560344 steps/sec)\n",
      "Step #859\tEpoch   0 Batch  858/3125   Loss: 1.047199 mae: 0.838611 (1818.439740910628 steps/sec)\n",
      "Step #860\tEpoch   0 Batch  859/3125   Loss: 1.072392 mae: 0.786641 (1824.0704177575215 steps/sec)\n",
      "Step #861\tEpoch   0 Batch  860/3125   Loss: 0.930897 mae: 0.767300 (2004.21648174164 steps/sec)\n",
      "Step #862\tEpoch   0 Batch  861/3125   Loss: 0.745378 mae: 0.715192 (2139.4052537617954 steps/sec)\n",
      "Step #863\tEpoch   0 Batch  862/3125   Loss: 1.057002 mae: 0.821404 (2031.100608220664 steps/sec)\n",
      "Step #864\tEpoch   0 Batch  863/3125   Loss: 0.944624 mae: 0.772355 (1878.2259797950849 steps/sec)\n",
      "Step #865\tEpoch   0 Batch  864/3125   Loss: 0.863233 mae: 0.759519 (2021.3318425846498 steps/sec)\n",
      "Step #866\tEpoch   0 Batch  865/3125   Loss: 1.051969 mae: 0.814005 (2292.8464439949707 steps/sec)\n",
      "Step #867\tEpoch   0 Batch  866/3125   Loss: 0.960946 mae: 0.763864 (1859.4245688699739 steps/sec)\n",
      "Step #868\tEpoch   0 Batch  867/3125   Loss: 0.876799 mae: 0.740440 (1997.1164376386787 steps/sec)\n",
      "Step #869\tEpoch   0 Batch  868/3125   Loss: 1.001085 mae: 0.793418 (1898.8374198688928 steps/sec)\n",
      "Step #870\tEpoch   0 Batch  869/3125   Loss: 1.048615 mae: 0.821379 (1831.493821230514 steps/sec)\n",
      "Step #871\tEpoch   0 Batch  870/3125   Loss: 0.886747 mae: 0.736292 (1932.3781178877146 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #872\tEpoch   0 Batch  871/3125   Loss: 0.844607 mae: 0.720518 (1624.8553077083996 steps/sec)\n",
      "Step #873\tEpoch   0 Batch  872/3125   Loss: 0.875419 mae: 0.741235 (1656.936982491625 steps/sec)\n",
      "Step #874\tEpoch   0 Batch  873/3125   Loss: 1.032327 mae: 0.817373 (2046.680850233248 steps/sec)\n",
      "Step #875\tEpoch   0 Batch  874/3125   Loss: 0.950161 mae: 0.781221 (2189.139648009353 steps/sec)\n",
      "Step #876\tEpoch   0 Batch  875/3125   Loss: 1.042726 mae: 0.840101 (2153.221898229907 steps/sec)\n",
      "Step #877\tEpoch   0 Batch  876/3125   Loss: 1.044082 mae: 0.830233 (2212.208989546303 steps/sec)\n",
      "Step #878\tEpoch   0 Batch  877/3125   Loss: 0.908109 mae: 0.745369 (2170.4255671468786 steps/sec)\n",
      "Step #879\tEpoch   0 Batch  878/3125   Loss: 1.034642 mae: 0.799999 (2187.5868399641167 steps/sec)\n",
      "Step #880\tEpoch   0 Batch  879/3125   Loss: 0.865114 mae: 0.751906 (2069.27881437043 steps/sec)\n",
      "Step #881\tEpoch   0 Batch  880/3125   Loss: 0.852205 mae: 0.750464 (1759.3261858022515 steps/sec)\n",
      "Step #882\tEpoch   0 Batch  881/3125   Loss: 0.901059 mae: 0.767039 (2136.2017683249805 steps/sec)\n",
      "Step #883\tEpoch   0 Batch  882/3125   Loss: 0.907287 mae: 0.754533 (2324.486810019951 steps/sec)\n",
      "Step #884\tEpoch   0 Batch  883/3125   Loss: 1.012595 mae: 0.771861 (2257.550998439098 steps/sec)\n",
      "Step #885\tEpoch   0 Batch  884/3125   Loss: 1.090943 mae: 0.842500 (2247.39002304024 steps/sec)\n",
      "Step #886\tEpoch   0 Batch  885/3125   Loss: 1.012956 mae: 0.800127 (2151.0795646866954 steps/sec)\n",
      "Step #887\tEpoch   0 Batch  886/3125   Loss: 0.952458 mae: 0.777134 (2171.1670859603896 steps/sec)\n",
      "Step #888\tEpoch   0 Batch  887/3125   Loss: 1.042639 mae: 0.818794 (2256.725026633236 steps/sec)\n",
      "Step #889\tEpoch   0 Batch  888/3125   Loss: 0.993160 mae: 0.800498 (2254.299197024584 steps/sec)\n",
      "Step #890\tEpoch   0 Batch  889/3125   Loss: 1.062443 mae: 0.825210 (1956.7000690440202 steps/sec)\n",
      "Step #891\tEpoch   0 Batch  890/3125   Loss: 0.888878 mae: 0.737363 (2082.801497680978 steps/sec)\n",
      "Step #892\tEpoch   0 Batch  891/3125   Loss: 0.888406 mae: 0.751435 (1939.6342986098907 steps/sec)\n",
      "Step #893\tEpoch   0 Batch  892/3125   Loss: 0.943523 mae: 0.779554 (1882.1704869774371 steps/sec)\n",
      "Step #894\tEpoch   0 Batch  893/3125   Loss: 0.996065 mae: 0.799571 (2051.184945374165 steps/sec)\n",
      "Step #895\tEpoch   0 Batch  894/3125   Loss: 1.133365 mae: 0.833889 (1934.5349888382561 steps/sec)\n",
      "Step #896\tEpoch   0 Batch  895/3125   Loss: 0.867905 mae: 0.743011 (2118.5493484190324 steps/sec)\n",
      "Step #897\tEpoch   0 Batch  896/3125   Loss: 0.960552 mae: 0.780723 (1934.8919602162641 steps/sec)\n",
      "Step #898\tEpoch   0 Batch  897/3125   Loss: 0.900300 mae: 0.755518 (1623.3711344196308 steps/sec)\n",
      "Step #899\tEpoch   0 Batch  898/3125   Loss: 0.847720 mae: 0.729614 (1657.1071865987121 steps/sec)\n",
      "Step #900\tEpoch   0 Batch  899/3125   Loss: 0.997439 mae: 0.794728 (1842.0954622911652 steps/sec)\n",
      "Step #901\tEpoch   0 Batch  900/3125   Loss: 0.922165 mae: 0.769854 (1853.213506181349 steps/sec)\n",
      "Step #902\tEpoch   0 Batch  901/3125   Loss: 0.897220 mae: 0.774646 (1913.2852841893987 steps/sec)\n",
      "Step #903\tEpoch   0 Batch  902/3125   Loss: 0.874567 mae: 0.758778 (1807.4689511923948 steps/sec)\n",
      "Step #904\tEpoch   0 Batch  903/3125   Loss: 1.011923 mae: 0.803618 (1773.1151976326357 steps/sec)\n",
      "Step #905\tEpoch   0 Batch  904/3125   Loss: 1.041870 mae: 0.841495 (2118.634958478977 steps/sec)\n",
      "Step #906\tEpoch   0 Batch  905/3125   Loss: 1.020427 mae: 0.786190 (1597.4406240002438 steps/sec)\n",
      "Step #907\tEpoch   0 Batch  906/3125   Loss: 0.960191 mae: 0.777135 (1833.3991922088367 steps/sec)\n",
      "Step #908\tEpoch   0 Batch  907/3125   Loss: 0.892922 mae: 0.756176 (1755.805795329912 steps/sec)\n",
      "Step #909\tEpoch   0 Batch  908/3125   Loss: 0.918951 mae: 0.786607 (1937.1618064087052 steps/sec)\n",
      "Step #910\tEpoch   0 Batch  909/3125   Loss: 1.001687 mae: 0.794675 (1757.159255628451 steps/sec)\n",
      "Step #911\tEpoch   0 Batch  910/3125   Loss: 0.973255 mae: 0.795364 (1806.9550232638292 steps/sec)\n",
      "Step #912\tEpoch   0 Batch  911/3125   Loss: 0.955143 mae: 0.770361 (1765.2348846409602 steps/sec)\n",
      "Step #913\tEpoch   0 Batch  912/3125   Loss: 1.114523 mae: 0.842452 (1864.9971542401822 steps/sec)\n",
      "Step #914\tEpoch   0 Batch  913/3125   Loss: 0.921004 mae: 0.769121 (1461.3892295685137 steps/sec)\n",
      "Step #915\tEpoch   0 Batch  914/3125   Loss: 1.050481 mae: 0.828877 (1833.6396464138636 steps/sec)\n",
      "Step #916\tEpoch   0 Batch  915/3125   Loss: 0.958273 mae: 0.784917 (1876.8812200186153 steps/sec)\n",
      "Step #917\tEpoch   0 Batch  916/3125   Loss: 1.013513 mae: 0.796252 (1855.4926387315968 steps/sec)\n",
      "Step #918\tEpoch   0 Batch  917/3125   Loss: 0.745184 mae: 0.702908 (1911.9595937494303 steps/sec)\n",
      "Step #919\tEpoch   0 Batch  918/3125   Loss: 0.939849 mae: 0.774238 (1803.412217941662 steps/sec)\n",
      "Step #920\tEpoch   0 Batch  919/3125   Loss: 0.971804 mae: 0.762834 (1884.6569310267355 steps/sec)\n",
      "Step #921\tEpoch   0 Batch  920/3125   Loss: 0.935483 mae: 0.755292 (2094.575671923534 steps/sec)\n",
      "Step #922\tEpoch   0 Batch  921/3125   Loss: 0.942783 mae: 0.785081 (1787.4419357863067 steps/sec)\n",
      "Step #923\tEpoch   0 Batch  922/3125   Loss: 0.930540 mae: 0.769485 (2083.5050419750632 steps/sec)\n",
      "Step #924\tEpoch   0 Batch  923/3125   Loss: 0.937772 mae: 0.754833 (2404.9908256880735 steps/sec)\n",
      "Step #925\tEpoch   0 Batch  924/3125   Loss: 0.988779 mae: 0.786515 (2314.073224019597 steps/sec)\n",
      "Step #926\tEpoch   0 Batch  925/3125   Loss: 0.914967 mae: 0.767901 (2457.3504253474257 steps/sec)\n",
      "Step #927\tEpoch   0 Batch  926/3125   Loss: 0.914058 mae: 0.777623 (2175.784865021891 steps/sec)\n",
      "Step #928\tEpoch   0 Batch  927/3125   Loss: 0.859266 mae: 0.732449 (2000.6983333492335 steps/sec)\n",
      "Step #929\tEpoch   0 Batch  928/3125   Loss: 0.982350 mae: 0.762262 (1810.4023688049792 steps/sec)\n",
      "Step #930\tEpoch   0 Batch  929/3125   Loss: 0.940032 mae: 0.774248 (1960.4131806496846 steps/sec)\n",
      "Step #931\tEpoch   0 Batch  930/3125   Loss: 0.978446 mae: 0.768108 (1654.3620084408158 steps/sec)\n",
      "Step #932\tEpoch   0 Batch  931/3125   Loss: 0.993521 mae: 0.802465 (1743.123597373452 steps/sec)\n",
      "Step #933\tEpoch   0 Batch  932/3125   Loss: 0.960407 mae: 0.792043 (2074.8473905515707 steps/sec)\n",
      "Step #934\tEpoch   0 Batch  933/3125   Loss: 0.863335 mae: 0.759034 (2226.1814784934822 steps/sec)\n",
      "Step #935\tEpoch   0 Batch  934/3125   Loss: 0.985264 mae: 0.801816 (1671.2238815485393 steps/sec)\n",
      "Step #936\tEpoch   0 Batch  935/3125   Loss: 1.008473 mae: 0.810771 (1972.76891961808 steps/sec)\n",
      "Step #937\tEpoch   0 Batch  936/3125   Loss: 0.961619 mae: 0.790476 (1934.374394687082 steps/sec)\n",
      "Step #938\tEpoch   0 Batch  937/3125   Loss: 0.887015 mae: 0.744992 (2070.402400979347 steps/sec)\n",
      "Step #939\tEpoch   0 Batch  938/3125   Loss: 0.929182 mae: 0.790357 (1936.786110084965 steps/sec)\n",
      "Step #940\tEpoch   0 Batch  939/3125   Loss: 1.012034 mae: 0.783458 (1922.9165329494504 steps/sec)\n",
      "Step #941\tEpoch   0 Batch  940/3125   Loss: 0.966660 mae: 0.787625 (2278.621409013864 steps/sec)\n",
      "Step #942\tEpoch   0 Batch  941/3125   Loss: 0.920149 mae: 0.772695 (1931.3993111197067 steps/sec)\n",
      "Step #943\tEpoch   0 Batch  942/3125   Loss: 0.900033 mae: 0.780727 (2525.167971101746 steps/sec)\n",
      "Step #944\tEpoch   0 Batch  943/3125   Loss: 0.847439 mae: 0.730522 (2264.302835301993 steps/sec)\n",
      "Step #945\tEpoch   0 Batch  944/3125   Loss: 0.897391 mae: 0.744316 (2427.0626222412534 steps/sec)\n",
      "Step #946\tEpoch   0 Batch  945/3125   Loss: 0.946403 mae: 0.791353 (2248.088673541582 steps/sec)\n",
      "Step #947\tEpoch   0 Batch  946/3125   Loss: 1.023856 mae: 0.802534 (2281.223961449348 steps/sec)\n",
      "Step #948\tEpoch   0 Batch  947/3125   Loss: 0.833476 mae: 0.723318 (1944.3278323753013 steps/sec)\n",
      "Step #949\tEpoch   0 Batch  948/3125   Loss: 0.769338 mae: 0.710728 (2033.3258999990305 steps/sec)\n",
      "Step #950\tEpoch   0 Batch  949/3125   Loss: 0.791424 mae: 0.728527 (1874.4822531484908 steps/sec)\n",
      "Step #951\tEpoch   0 Batch  950/3125   Loss: 0.930226 mae: 0.770176 (1848.2972572798421 steps/sec)\n",
      "Step #952\tEpoch   0 Batch  951/3125   Loss: 1.003827 mae: 0.795132 (1294.810021856439 steps/sec)\n",
      "Step #953\tEpoch   0 Batch  952/3125   Loss: 1.025784 mae: 0.796376 (1279.0319826303335 steps/sec)\n",
      "Step #954\tEpoch   0 Batch  953/3125   Loss: 0.971557 mae: 0.795793 (1878.360561765549 steps/sec)\n",
      "Step #955\tEpoch   0 Batch  954/3125   Loss: 0.974021 mae: 0.796408 (1490.8522194102425 steps/sec)\n",
      "Step #956\tEpoch   0 Batch  955/3125   Loss: 0.989474 mae: 0.791323 (1707.9314922346464 steps/sec)\n",
      "Step #957\tEpoch   0 Batch  956/3125   Loss: 0.880466 mae: 0.732528 (1890.2968190872791 steps/sec)\n",
      "Step #958\tEpoch   0 Batch  957/3125   Loss: 0.989741 mae: 0.789402 (2059.8076866411952 steps/sec)\n",
      "Step #959\tEpoch   0 Batch  958/3125   Loss: 1.046062 mae: 0.781172 (2154.150830987941 steps/sec)\n",
      "Step #960\tEpoch   0 Batch  959/3125   Loss: 1.073412 mae: 0.804399 (2231.15518011788 steps/sec)\n",
      "Step #961\tEpoch   0 Batch  960/3125   Loss: 0.970391 mae: 0.762575 (2284.5788487515797 steps/sec)\n",
      "Step #962\tEpoch   0 Batch  961/3125   Loss: 1.096147 mae: 0.843055 (2262.56837380919 steps/sec)\n",
      "Step #963\tEpoch   0 Batch  962/3125   Loss: 0.855657 mae: 0.736874 (2250.621907899679 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #964\tEpoch   0 Batch  963/3125   Loss: 0.819464 mae: 0.705232 (2216.886013594224 steps/sec)\n",
      "Step #965\tEpoch   0 Batch  964/3125   Loss: 0.854364 mae: 0.739061 (2200.6023148196728 steps/sec)\n",
      "Step #966\tEpoch   0 Batch  965/3125   Loss: 0.845319 mae: 0.746632 (2182.7371225762136 steps/sec)\n",
      "Step #967\tEpoch   0 Batch  966/3125   Loss: 0.911118 mae: 0.780570 (1990.444282039844 steps/sec)\n",
      "Step #968\tEpoch   0 Batch  967/3125   Loss: 0.913390 mae: 0.764071 (2068.9317706483566 steps/sec)\n",
      "Step #969\tEpoch   0 Batch  968/3125   Loss: 1.055504 mae: 0.823560 (2098.012184995848 steps/sec)\n",
      "Step #970\tEpoch   0 Batch  969/3125   Loss: 1.013141 mae: 0.800253 (2118.271163500096 steps/sec)\n",
      "Step #971\tEpoch   0 Batch  970/3125   Loss: 1.020470 mae: 0.827497 (2172.2015640374952 steps/sec)\n",
      "Step #972\tEpoch   0 Batch  971/3125   Loss: 0.948822 mae: 0.775772 (2272.104008667389 steps/sec)\n",
      "Step #973\tEpoch   0 Batch  972/3125   Loss: 0.863118 mae: 0.770704 (1839.0423992633841 steps/sec)\n",
      "Step #974\tEpoch   0 Batch  973/3125   Loss: 1.028094 mae: 0.812049 (2011.5022348404918 steps/sec)\n",
      "Step #975\tEpoch   0 Batch  974/3125   Loss: 0.853490 mae: 0.735699 (2143.516256630927 steps/sec)\n",
      "Step #976\tEpoch   0 Batch  975/3125   Loss: 0.864857 mae: 0.745100 (2374.976784217798 steps/sec)\n",
      "Step #977\tEpoch   0 Batch  976/3125   Loss: 0.961188 mae: 0.768732 (2253.6451169188445 steps/sec)\n",
      "Step #978\tEpoch   0 Batch  977/3125   Loss: 0.854766 mae: 0.735698 (2361.8438390413658 steps/sec)\n",
      "Step #979\tEpoch   0 Batch  978/3125   Loss: 0.755415 mae: 0.697725 (2332.7348972758923 steps/sec)\n",
      "Step #980\tEpoch   0 Batch  979/3125   Loss: 1.026174 mae: 0.806565 (2339.3965084499973 steps/sec)\n",
      "Step #981\tEpoch   0 Batch  980/3125   Loss: 1.003571 mae: 0.794476 (2407.9731777890047 steps/sec)\n",
      "Step #982\tEpoch   0 Batch  981/3125   Loss: 1.007801 mae: 0.828373 (2217.0735059360827 steps/sec)\n",
      "Step #983\tEpoch   0 Batch  982/3125   Loss: 1.043019 mae: 0.784310 (2036.979621967092 steps/sec)\n",
      "Step #984\tEpoch   0 Batch  983/3125   Loss: 0.885931 mae: 0.755187 (2028.1541943095879 steps/sec)\n",
      "Step #985\tEpoch   0 Batch  984/3125   Loss: 0.938416 mae: 0.781742 (2151.2560906806175 steps/sec)\n",
      "Step #986\tEpoch   0 Batch  985/3125   Loss: 0.967166 mae: 0.785510 (2087.861018467818 steps/sec)\n",
      "Step #987\tEpoch   0 Batch  986/3125   Loss: 1.083937 mae: 0.849667 (1595.3504648013754 steps/sec)\n",
      "Step #988\tEpoch   0 Batch  987/3125   Loss: 0.845736 mae: 0.733115 (1980.2761043228645 steps/sec)\n",
      "Step #989\tEpoch   0 Batch  988/3125   Loss: 0.879496 mae: 0.740986 (1861.834710890544 steps/sec)\n",
      "Step #990\tEpoch   0 Batch  989/3125   Loss: 0.942297 mae: 0.782801 (1930.8125028771349 steps/sec)\n",
      "Step #991\tEpoch   0 Batch  990/3125   Loss: 0.802663 mae: 0.704065 (1786.0870750153301 steps/sec)\n",
      "Step #992\tEpoch   0 Batch  991/3125   Loss: 0.916852 mae: 0.744259 (2218.6684722236914 steps/sec)\n",
      "Step #993\tEpoch   0 Batch  992/3125   Loss: 0.961411 mae: 0.759331 (2326.3433464968716 steps/sec)\n",
      "Step #994\tEpoch   0 Batch  993/3125   Loss: 1.033703 mae: 0.784609 (2262.6416071467106 steps/sec)\n",
      "Step #995\tEpoch   0 Batch  994/3125   Loss: 0.883981 mae: 0.748698 (2194.247449646874 steps/sec)\n",
      "Step #996\tEpoch   0 Batch  995/3125   Loss: 0.823664 mae: 0.745433 (2083.3394593842822 steps/sec)\n",
      "Step #997\tEpoch   0 Batch  996/3125   Loss: 0.942577 mae: 0.784991 (2336.2170953691225 steps/sec)\n",
      "Step #998\tEpoch   0 Batch  997/3125   Loss: 0.847170 mae: 0.750843 (2221.841759545705 steps/sec)\n",
      "Step #999\tEpoch   0 Batch  998/3125   Loss: 0.889277 mae: 0.765191 (1985.9582003617459 steps/sec)\n",
      "Step #1000\tEpoch   0 Batch  999/3125   Loss: 0.965964 mae: 0.778775 (1941.8253872721043 steps/sec)\n",
      "Step #1001\tEpoch   0 Batch 1000/3125   Loss: 1.000229 mae: 0.801197 (2120.6916776216 steps/sec)\n",
      "Step #1002\tEpoch   0 Batch 1001/3125   Loss: 0.881238 mae: 0.761872 (2257.0164770709343 steps/sec)\n",
      "Step #1003\tEpoch   0 Batch 1002/3125   Loss: 0.875228 mae: 0.748474 (2113.0846583237612 steps/sec)\n",
      "Step #1004\tEpoch   0 Batch 1003/3125   Loss: 1.216664 mae: 0.888983 (591.6186381627017 steps/sec)\n",
      "Step #1005\tEpoch   0 Batch 1004/3125   Loss: 0.921382 mae: 0.759961 (1490.4495899250921 steps/sec)\n",
      "Step #1006\tEpoch   0 Batch 1005/3125   Loss: 0.814391 mae: 0.733187 (1442.3725549533688 steps/sec)\n",
      "Step #1007\tEpoch   0 Batch 1006/3125   Loss: 0.871238 mae: 0.713248 (819.4977081517895 steps/sec)\n",
      "Step #1008\tEpoch   0 Batch 1007/3125   Loss: 0.931313 mae: 0.799430 (840.9733249923809 steps/sec)\n",
      "Step #1009\tEpoch   0 Batch 1008/3125   Loss: 0.944767 mae: 0.763075 (906.6844214631278 steps/sec)\n",
      "Step #1010\tEpoch   0 Batch 1009/3125   Loss: 0.932803 mae: 0.770514 (1255.2384600651217 steps/sec)\n",
      "Step #1011\tEpoch   0 Batch 1010/3125   Loss: 1.048817 mae: 0.805902 (1706.361165806903 steps/sec)\n",
      "Step #1012\tEpoch   0 Batch 1011/3125   Loss: 1.019075 mae: 0.808800 (1716.0372803966975 steps/sec)\n",
      "Step #1013\tEpoch   0 Batch 1012/3125   Loss: 0.869384 mae: 0.750770 (2018.5108185108186 steps/sec)\n",
      "Step #1014\tEpoch   0 Batch 1013/3125   Loss: 0.894831 mae: 0.738259 (1952.2192433720584 steps/sec)\n",
      "Step #1015\tEpoch   0 Batch 1014/3125   Loss: 0.951061 mae: 0.783064 (2061.1640752462013 steps/sec)\n",
      "Step #1016\tEpoch   0 Batch 1015/3125   Loss: 0.968398 mae: 0.773964 (2023.71150932654 steps/sec)\n",
      "Step #1017\tEpoch   0 Batch 1016/3125   Loss: 1.020677 mae: 0.792020 (1721.7713994844091 steps/sec)\n",
      "Step #1018\tEpoch   0 Batch 1017/3125   Loss: 0.907683 mae: 0.789274 (1891.8827244023455 steps/sec)\n",
      "Step #1019\tEpoch   0 Batch 1018/3125   Loss: 0.870460 mae: 0.721884 (2056.616096733385 steps/sec)\n",
      "Step #1020\tEpoch   0 Batch 1019/3125   Loss: 0.966499 mae: 0.794519 (1993.3957511525118 steps/sec)\n",
      "Step #1021\tEpoch   0 Batch 1020/3125   Loss: 0.968328 mae: 0.786103 (2109.810865191147 steps/sec)\n",
      "Step #1022\tEpoch   0 Batch 1021/3125   Loss: 0.993020 mae: 0.771822 (2039.1583368986037 steps/sec)\n",
      "Step #1023\tEpoch   0 Batch 1022/3125   Loss: 0.938884 mae: 0.762879 (2179.7197854737456 steps/sec)\n",
      "Step #1024\tEpoch   0 Batch 1023/3125   Loss: 0.912143 mae: 0.769951 (1782.474034031992 steps/sec)\n",
      "Step #1025\tEpoch   0 Batch 1024/3125   Loss: 0.939748 mae: 0.766062 (1795.3377678472061 steps/sec)\n",
      "Step #1026\tEpoch   0 Batch 1025/3125   Loss: 0.887381 mae: 0.761645 (1903.525396652507 steps/sec)\n",
      "Step #1027\tEpoch   0 Batch 1026/3125   Loss: 1.013653 mae: 0.808418 (1913.0060387134438 steps/sec)\n",
      "Step #1028\tEpoch   0 Batch 1027/3125   Loss: 0.946269 mae: 0.773512 (2007.477959547034 steps/sec)\n",
      "Step #1029\tEpoch   0 Batch 1028/3125   Loss: 1.115335 mae: 0.819694 (2143.9106921968123 steps/sec)\n",
      "Step #1030\tEpoch   0 Batch 1029/3125   Loss: 0.847986 mae: 0.736968 (1925.4411575680788 steps/sec)\n",
      "Step #1031\tEpoch   0 Batch 1030/3125   Loss: 0.858492 mae: 0.730714 (1946.746375062659 steps/sec)\n",
      "Step #1032\tEpoch   0 Batch 1031/3125   Loss: 0.871845 mae: 0.745293 (1730.6947034842458 steps/sec)\n",
      "Step #1033\tEpoch   0 Batch 1032/3125   Loss: 1.010894 mae: 0.788009 (1886.3351802547313 steps/sec)\n",
      "Step #1034\tEpoch   0 Batch 1033/3125   Loss: 0.885243 mae: 0.754834 (1952.6736748014414 steps/sec)\n",
      "Step #1035\tEpoch   0 Batch 1034/3125   Loss: 0.919038 mae: 0.744787 (1987.991392630651 steps/sec)\n",
      "Step #1036\tEpoch   0 Batch 1035/3125   Loss: 0.973009 mae: 0.805706 (2062.441116017427 steps/sec)\n",
      "Step #1037\tEpoch   0 Batch 1036/3125   Loss: 1.092454 mae: 0.815065 (1851.642695061761 steps/sec)\n",
      "Step #1038\tEpoch   0 Batch 1037/3125   Loss: 0.793676 mae: 0.719768 (1933.6431363871063 steps/sec)\n",
      "Step #1039\tEpoch   0 Batch 1038/3125   Loss: 1.021677 mae: 0.796775 (2127.943340131706 steps/sec)\n",
      "Step #1040\tEpoch   0 Batch 1039/3125   Loss: 0.941666 mae: 0.757198 (1940.998657966588 steps/sec)\n",
      "Step #1041\tEpoch   0 Batch 1040/3125   Loss: 0.797496 mae: 0.717944 (1857.0699914990082 steps/sec)\n",
      "Step #1042\tEpoch   0 Batch 1041/3125   Loss: 1.053777 mae: 0.800060 (1991.8621659099974 steps/sec)\n",
      "Step #1043\tEpoch   0 Batch 1042/3125   Loss: 0.923475 mae: 0.732690 (2185.147906181947 steps/sec)\n",
      "Step #1044\tEpoch   0 Batch 1043/3125   Loss: 0.966582 mae: 0.771078 (2273.754513026791 steps/sec)\n",
      "Step #1045\tEpoch   0 Batch 1044/3125   Loss: 0.920983 mae: 0.782423 (1370.0070553189266 steps/sec)\n",
      "Step #1046\tEpoch   0 Batch 1045/3125   Loss: 1.088654 mae: 0.833269 (1811.4657383973533 steps/sec)\n",
      "Step #1047\tEpoch   0 Batch 1046/3125   Loss: 0.848759 mae: 0.735993 (927.3360807966465 steps/sec)\n",
      "Step #1048\tEpoch   0 Batch 1047/3125   Loss: 0.884152 mae: 0.768309 (1552.8477919616148 steps/sec)\n",
      "Step #1049\tEpoch   0 Batch 1048/3125   Loss: 0.881973 mae: 0.756342 (2106.758820220204 steps/sec)\n",
      "Step #1050\tEpoch   0 Batch 1049/3125   Loss: 0.941385 mae: 0.776077 (2213.5866582225035 steps/sec)\n",
      "Step #1051\tEpoch   0 Batch 1050/3125   Loss: 0.894048 mae: 0.766222 (1927.157442037842 steps/sec)\n",
      "Step #1052\tEpoch   0 Batch 1051/3125   Loss: 0.976993 mae: 0.785617 (2008.362302601967 steps/sec)\n",
      "Step #1053\tEpoch   0 Batch 1052/3125   Loss: 0.874935 mae: 0.731888 (2016.6086504990672 steps/sec)\n",
      "Step #1054\tEpoch   0 Batch 1053/3125   Loss: 0.987739 mae: 0.802153 (1900.8511062568546 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1055\tEpoch   0 Batch 1054/3125   Loss: 0.873845 mae: 0.754963 (1494.7626514611547 steps/sec)\n",
      "Step #1056\tEpoch   0 Batch 1055/3125   Loss: 0.997998 mae: 0.800881 (1332.4387516519264 steps/sec)\n",
      "Step #1057\tEpoch   0 Batch 1056/3125   Loss: 1.072033 mae: 0.842094 (1369.863872703995 steps/sec)\n",
      "Step #1058\tEpoch   0 Batch 1057/3125   Loss: 0.923565 mae: 0.748715 (1410.7036189963676 steps/sec)\n",
      "Step #1059\tEpoch   0 Batch 1058/3125   Loss: 0.920962 mae: 0.768561 (1538.889174255377 steps/sec)\n",
      "Step #1060\tEpoch   0 Batch 1059/3125   Loss: 0.851310 mae: 0.742140 (1738.6436743491959 steps/sec)\n",
      "Step #1061\tEpoch   0 Batch 1060/3125   Loss: 0.993146 mae: 0.793070 (1741.0686414505362 steps/sec)\n",
      "Step #1062\tEpoch   0 Batch 1061/3125   Loss: 0.903898 mae: 0.745141 (1260.8077723135377 steps/sec)\n",
      "Step #1063\tEpoch   0 Batch 1062/3125   Loss: 0.835746 mae: 0.717246 (1849.6502941409935 steps/sec)\n",
      "Step #1064\tEpoch   0 Batch 1063/3125   Loss: 0.801421 mae: 0.716420 (1834.2490794433802 steps/sec)\n",
      "Step #1065\tEpoch   0 Batch 1064/3125   Loss: 0.758970 mae: 0.712331 (1749.6679459369263 steps/sec)\n",
      "Step #1066\tEpoch   0 Batch 1065/3125   Loss: 0.974922 mae: 0.770202 (1776.3291857599038 steps/sec)\n",
      "Step #1067\tEpoch   0 Batch 1066/3125   Loss: 0.842175 mae: 0.736518 (1759.931521219191 steps/sec)\n",
      "Step #1068\tEpoch   0 Batch 1067/3125   Loss: 0.927124 mae: 0.757350 (1499.5187873154339 steps/sec)\n",
      "Step #1069\tEpoch   0 Batch 1068/3125   Loss: 0.954131 mae: 0.784003 (1599.3898811793597 steps/sec)\n",
      "Step #1070\tEpoch   0 Batch 1069/3125   Loss: 0.996643 mae: 0.785603 (1619.5849776425432 steps/sec)\n",
      "Step #1071\tEpoch   0 Batch 1070/3125   Loss: 0.898348 mae: 0.774921 (1619.1723285978999 steps/sec)\n",
      "Step #1072\tEpoch   0 Batch 1071/3125   Loss: 0.859838 mae: 0.747374 (1858.9630627675888 steps/sec)\n",
      "Step #1073\tEpoch   0 Batch 1072/3125   Loss: 0.935899 mae: 0.771002 (1911.314856502283 steps/sec)\n",
      "Step #1074\tEpoch   0 Batch 1073/3125   Loss: 0.738401 mae: 0.681255 (1509.9917197681534 steps/sec)\n",
      "Step #1075\tEpoch   0 Batch 1074/3125   Loss: 0.919501 mae: 0.784556 (1731.5377946579697 steps/sec)\n",
      "Step #1076\tEpoch   0 Batch 1075/3125   Loss: 0.915342 mae: 0.768158 (1721.6865887298043 steps/sec)\n",
      "Step #1077\tEpoch   0 Batch 1076/3125   Loss: 0.869254 mae: 0.755810 (1949.3158834027365 steps/sec)\n",
      "Step #1078\tEpoch   0 Batch 1077/3125   Loss: 0.855355 mae: 0.717050 (2033.6808215591393 steps/sec)\n",
      "Step #1079\tEpoch   0 Batch 1078/3125   Loss: 0.975806 mae: 0.798280 (1859.1443414123862 steps/sec)\n",
      "Step #1080\tEpoch   0 Batch 1079/3125   Loss: 0.913824 mae: 0.755559 (1877.3012505482898 steps/sec)\n",
      "Step #1081\tEpoch   0 Batch 1080/3125   Loss: 0.956594 mae: 0.775667 (2180.7397548015433 steps/sec)\n",
      "Step #1082\tEpoch   0 Batch 1081/3125   Loss: 1.109279 mae: 0.835946 (1866.2917148705171 steps/sec)\n",
      "Step #1083\tEpoch   0 Batch 1082/3125   Loss: 1.032646 mae: 0.826322 (1630.7304707547316 steps/sec)\n",
      "Step #1084\tEpoch   0 Batch 1083/3125   Loss: 0.866125 mae: 0.733348 (1710.5086294085022 steps/sec)\n",
      "Step #1085\tEpoch   0 Batch 1084/3125   Loss: 0.884629 mae: 0.721225 (2034.3318329970512 steps/sec)\n",
      "Step #1086\tEpoch   0 Batch 1085/3125   Loss: 0.827856 mae: 0.709647 (1915.3472399810032 steps/sec)\n",
      "Step #1087\tEpoch   0 Batch 1086/3125   Loss: 0.961445 mae: 0.769018 (1847.613342025972 steps/sec)\n",
      "Step #1088\tEpoch   0 Batch 1087/3125   Loss: 0.911478 mae: 0.761657 (2028.8801818797465 steps/sec)\n",
      "Step #1089\tEpoch   0 Batch 1088/3125   Loss: 0.907846 mae: 0.777158 (1896.8794659816567 steps/sec)\n",
      "Step #1090\tEpoch   0 Batch 1089/3125   Loss: 0.880277 mae: 0.754874 (2039.1583368986037 steps/sec)\n",
      "Step #1091\tEpoch   0 Batch 1090/3125   Loss: 1.054444 mae: 0.803778 (1883.3029500246957 steps/sec)\n",
      "Step #1092\tEpoch   0 Batch 1091/3125   Loss: 1.011686 mae: 0.812066 (1844.4121966878622 steps/sec)\n",
      "Step #1093\tEpoch   0 Batch 1092/3125   Loss: 0.924704 mae: 0.760359 (2205.763810004628 steps/sec)\n",
      "Step #1094\tEpoch   0 Batch 1093/3125   Loss: 0.909255 mae: 0.758640 (2160.5868293084973 steps/sec)\n",
      "Step #1095\tEpoch   0 Batch 1094/3125   Loss: 0.977240 mae: 0.785983 (2017.6951644249457 steps/sec)\n",
      "Step #1096\tEpoch   0 Batch 1095/3125   Loss: 0.852544 mae: 0.752240 (2170.6052827688995 steps/sec)\n",
      "Step #1097\tEpoch   0 Batch 1096/3125   Loss: 0.771060 mae: 0.698765 (2244.5997581102633 steps/sec)\n",
      "Step #1098\tEpoch   0 Batch 1097/3125   Loss: 1.012063 mae: 0.779241 (2264.8897336760483 steps/sec)\n",
      "Step #1099\tEpoch   0 Batch 1098/3125   Loss: 1.026438 mae: 0.815516 (2339.3704125114336 steps/sec)\n",
      "Step #1100\tEpoch   0 Batch 1099/3125   Loss: 0.892679 mae: 0.768609 (2086.5523142436423 steps/sec)\n",
      "Step #1101\tEpoch   0 Batch 1100/3125   Loss: 0.935300 mae: 0.770912 (1847.1902194975867 steps/sec)\n",
      "Step #1102\tEpoch   0 Batch 1101/3125   Loss: 0.933464 mae: 0.758361 (2262.1290733170094 steps/sec)\n",
      "Step #1103\tEpoch   0 Batch 1102/3125   Loss: 1.042658 mae: 0.817729 (2070.484163968091 steps/sec)\n",
      "Step #1104\tEpoch   0 Batch 1103/3125   Loss: 1.006376 mae: 0.807016 (2225.590847828163 steps/sec)\n",
      "Step #1105\tEpoch   0 Batch 1104/3125   Loss: 1.017969 mae: 0.816658 (2176.2815989373626 steps/sec)\n",
      "Step #1106\tEpoch   0 Batch 1105/3125   Loss: 0.797270 mae: 0.719451 (2140.4970655779534 steps/sec)\n",
      "Step #1107\tEpoch   0 Batch 1106/3125   Loss: 0.939063 mae: 0.757185 (1880.836943166429 steps/sec)\n",
      "Step #1108\tEpoch   0 Batch 1107/3125   Loss: 0.885576 mae: 0.747870 (1729.7525569119102 steps/sec)\n",
      "Step #1109\tEpoch   0 Batch 1108/3125   Loss: 1.031161 mae: 0.819734 (1437.725035306377 steps/sec)\n",
      "Step #1110\tEpoch   0 Batch 1109/3125   Loss: 0.894385 mae: 0.749246 (1720.7118652411857 steps/sec)\n",
      "Step #1111\tEpoch   0 Batch 1110/3125   Loss: 1.036700 mae: 0.836123 (2043.6293473918086 steps/sec)\n",
      "Step #1112\tEpoch   0 Batch 1111/3125   Loss: 0.811686 mae: 0.721812 (2211.6257487555895 steps/sec)\n",
      "Step #1113\tEpoch   0 Batch 1112/3125   Loss: 0.896473 mae: 0.749773 (2028.0757402858635 steps/sec)\n",
      "Step #1114\tEpoch   0 Batch 1113/3125   Loss: 1.003396 mae: 0.814642 (1975.3145957350614 steps/sec)\n",
      "Step #1115\tEpoch   0 Batch 1114/3125   Loss: 0.828637 mae: 0.725881 (1854.754176653194 steps/sec)\n",
      "Step #1116\tEpoch   0 Batch 1115/3125   Loss: 0.914774 mae: 0.782022 (2020.1246472021808 steps/sec)\n",
      "Step #1117\tEpoch   0 Batch 1116/3125   Loss: 1.028811 mae: 0.802485 (2019.7744411591914 steps/sec)\n",
      "Step #1118\tEpoch   0 Batch 1117/3125   Loss: 0.858621 mae: 0.745921 (2181.306816999854 steps/sec)\n",
      "Step #1119\tEpoch   0 Batch 1118/3125   Loss: 0.851249 mae: 0.732958 (1697.5077503379391 steps/sec)\n",
      "Step #1120\tEpoch   0 Batch 1119/3125   Loss: 0.869976 mae: 0.729379 (2207.7840592068555 steps/sec)\n",
      "Step #1121\tEpoch   0 Batch 1120/3125   Loss: 0.907065 mae: 0.763973 (2238.6336464560204 steps/sec)\n",
      "Step #1122\tEpoch   0 Batch 1121/3125   Loss: 0.920159 mae: 0.767271 (2207.389007010084 steps/sec)\n",
      "Step #1123\tEpoch   0 Batch 1122/3125   Loss: 0.920290 mae: 0.764590 (1979.0987590242062 steps/sec)\n",
      "Step #1124\tEpoch   0 Batch 1123/3125   Loss: 0.715457 mae: 0.651376 (2146.01680259509 steps/sec)\n",
      "Step #1125\tEpoch   0 Batch 1124/3125   Loss: 0.977690 mae: 0.781746 (1905.79147772194 steps/sec)\n",
      "Step #1126\tEpoch   0 Batch 1125/3125   Loss: 0.869048 mae: 0.738933 (1843.1154038828297 steps/sec)\n",
      "Step #1127\tEpoch   0 Batch 1126/3125   Loss: 1.000159 mae: 0.831504 (1961.6417854603958 steps/sec)\n",
      "Step #1128\tEpoch   0 Batch 1127/3125   Loss: 0.840695 mae: 0.700731 (1852.60777385159 steps/sec)\n",
      "Step #1129\tEpoch   0 Batch 1128/3125   Loss: 0.963432 mae: 0.778628 (2061.6706481454175 steps/sec)\n",
      "Step #1130\tEpoch   0 Batch 1129/3125   Loss: 0.968251 mae: 0.794937 (2123.4186891852214 steps/sec)\n",
      "Step #1131\tEpoch   0 Batch 1130/3125   Loss: 0.828822 mae: 0.736426 (2123.2252055238328 steps/sec)\n",
      "Step #1132\tEpoch   0 Batch 1131/3125   Loss: 0.802363 mae: 0.700619 (1888.2884180765525 steps/sec)\n",
      "Step #1133\tEpoch   0 Batch 1132/3125   Loss: 0.941222 mae: 0.773637 (1433.4012275641464 steps/sec)\n",
      "Step #1134\tEpoch   0 Batch 1133/3125   Loss: 1.027503 mae: 0.813181 (1623.6350685949647 steps/sec)\n",
      "Step #1135\tEpoch   0 Batch 1134/3125   Loss: 0.806893 mae: 0.723007 (2155.2577489106307 steps/sec)\n",
      "Step #1136\tEpoch   0 Batch 1135/3125   Loss: 0.863171 mae: 0.756483 (1937.3049671596568 steps/sec)\n",
      "Step #1137\tEpoch   0 Batch 1136/3125   Loss: 0.870438 mae: 0.739122 (2191.1524396614773 steps/sec)\n",
      "Step #1138\tEpoch   0 Batch 1137/3125   Loss: 0.978443 mae: 0.798633 (2205.7406102422246 steps/sec)\n",
      "Step #1139\tEpoch   0 Batch 1138/3125   Loss: 0.886314 mae: 0.740850 (2273.976405274115 steps/sec)\n",
      "Step #1140\tEpoch   0 Batch 1139/3125   Loss: 0.962429 mae: 0.798538 (2474.165309926618 steps/sec)\n",
      "Step #1141\tEpoch   0 Batch 1140/3125   Loss: 0.871491 mae: 0.737597 (2242.4396659573786 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1142\tEpoch   0 Batch 1141/3125   Loss: 1.036870 mae: 0.801816 (2014.7294194502888 steps/sec)\n",
      "Step #1143\tEpoch   0 Batch 1142/3125   Loss: 0.714313 mae: 0.648402 (2172.5841206696505 steps/sec)\n",
      "Step #1144\tEpoch   0 Batch 1143/3125   Loss: 1.079158 mae: 0.834553 (2165.6515588050024 steps/sec)\n",
      "Step #1145\tEpoch   0 Batch 1144/3125   Loss: 0.973586 mae: 0.792254 (2062.339705766659 steps/sec)\n",
      "Step #1146\tEpoch   0 Batch 1145/3125   Loss: 0.924573 mae: 0.762435 (1962.7249670095182 steps/sec)\n",
      "Step #1147\tEpoch   0 Batch 1146/3125   Loss: 1.022725 mae: 0.809762 (2012.544624006756 steps/sec)\n",
      "Step #1148\tEpoch   0 Batch 1147/3125   Loss: 1.055092 mae: 0.781465 (2288.842564802183 steps/sec)\n",
      "Step #1149\tEpoch   0 Batch 1148/3125   Loss: 1.087893 mae: 0.841514 (2100.2393518472154 steps/sec)\n",
      "Step #1150\tEpoch   0 Batch 1149/3125   Loss: 0.833336 mae: 0.722017 (1158.4491054018372 steps/sec)\n",
      "Step #1151\tEpoch   0 Batch 1150/3125   Loss: 0.888833 mae: 0.768888 (1648.3674720575982 steps/sec)\n",
      "Step #1152\tEpoch   0 Batch 1151/3125   Loss: 0.826995 mae: 0.720225 (1926.1131520940485 steps/sec)\n",
      "Step #1153\tEpoch   0 Batch 1152/3125   Loss: 0.939615 mae: 0.802616 (1920.6975189354043 steps/sec)\n",
      "Step #1154\tEpoch   0 Batch 1153/3125   Loss: 0.892905 mae: 0.770449 (1963.9380800314657 steps/sec)\n",
      "Step #1155\tEpoch   0 Batch 1154/3125   Loss: 0.864567 mae: 0.759147 (1750.7780671876044 steps/sec)\n",
      "Step #1156\tEpoch   0 Batch 1155/3125   Loss: 0.902958 mae: 0.756552 (1723.200302380424 steps/sec)\n",
      "Step #1157\tEpoch   0 Batch 1156/3125   Loss: 0.953300 mae: 0.795981 (1934.374394687082 steps/sec)\n",
      "Step #1158\tEpoch   0 Batch 1157/3125   Loss: 0.909919 mae: 0.777575 (1808.638057127087 steps/sec)\n",
      "Step #1159\tEpoch   0 Batch 1158/3125   Loss: 0.884782 mae: 0.756054 (1754.1462435384847 steps/sec)\n",
      "Step #1160\tEpoch   0 Batch 1159/3125   Loss: 0.974601 mae: 0.788134 (1828.3323016834781 steps/sec)\n",
      "Step #1161\tEpoch   0 Batch 1160/3125   Loss: 0.842018 mae: 0.738444 (2167.48695157873 steps/sec)\n",
      "Step #1162\tEpoch   0 Batch 1161/3125   Loss: 0.909661 mae: 0.767966 (1930.208285396092 steps/sec)\n",
      "Step #1163\tEpoch   0 Batch 1162/3125   Loss: 0.959630 mae: 0.793982 (1997.7442463039172 steps/sec)\n",
      "Step #1164\tEpoch   0 Batch 1163/3125   Loss: 0.876152 mae: 0.780032 (1837.3667195261917 steps/sec)\n",
      "Step #1165\tEpoch   0 Batch 1164/3125   Loss: 0.936782 mae: 0.752456 (1994.2677279167735 steps/sec)\n",
      "Step #1166\tEpoch   0 Batch 1165/3125   Loss: 1.007656 mae: 0.804530 (1678.930429909535 steps/sec)\n",
      "Step #1167\tEpoch   0 Batch 1166/3125   Loss: 0.910593 mae: 0.774454 (1702.8280974690842 steps/sec)\n",
      "Step #1168\tEpoch   0 Batch 1167/3125   Loss: 0.916813 mae: 0.758934 (2086.9883666543933 steps/sec)\n",
      "Step #1169\tEpoch   0 Batch 1168/3125   Loss: 0.877062 mae: 0.760830 (1919.7656536067375 steps/sec)\n",
      "Step #1170\tEpoch   0 Batch 1169/3125   Loss: 0.785618 mae: 0.688497 (2216.1129427677743 steps/sec)\n",
      "Step #1171\tEpoch   0 Batch 1170/3125   Loss: 0.894375 mae: 0.714353 (2285.624604921856 steps/sec)\n",
      "Step #1172\tEpoch   0 Batch 1171/3125   Loss: 0.916790 mae: 0.763975 (2030.137171953805 steps/sec)\n",
      "Step #1173\tEpoch   0 Batch 1172/3125   Loss: 0.904395 mae: 0.753055 (2058.897681085433 steps/sec)\n",
      "Step #1174\tEpoch   0 Batch 1173/3125   Loss: 0.905312 mae: 0.767122 (1920.4336916906284 steps/sec)\n",
      "Step #1175\tEpoch   0 Batch 1174/3125   Loss: 0.806566 mae: 0.686034 (1783.6110189744768 steps/sec)\n",
      "Step #1176\tEpoch   0 Batch 1175/3125   Loss: 0.898923 mae: 0.763519 (1844.1364755539923 steps/sec)\n",
      "Step #1177\tEpoch   0 Batch 1176/3125   Loss: 0.933815 mae: 0.748795 (2133.0078621629596 steps/sec)\n",
      "Step #1178\tEpoch   0 Batch 1177/3125   Loss: 0.950533 mae: 0.764220 (2296.663125732371 steps/sec)\n",
      "Step #1179\tEpoch   0 Batch 1178/3125   Loss: 0.880798 mae: 0.734599 (2091.108695868939 steps/sec)\n",
      "Step #1180\tEpoch   0 Batch 1179/3125   Loss: 0.876572 mae: 0.743368 (2146.478065955661 steps/sec)\n",
      "Step #1181\tEpoch   0 Batch 1180/3125   Loss: 0.887315 mae: 0.732961 (2109.725966761901 steps/sec)\n",
      "Step #1182\tEpoch   0 Batch 1181/3125   Loss: 0.937615 mae: 0.784054 (1911.541336250114 steps/sec)\n",
      "Step #1183\tEpoch   0 Batch 1182/3125   Loss: 0.894123 mae: 0.744251 (1550.4827810554643 steps/sec)\n",
      "Step #1184\tEpoch   0 Batch 1183/3125   Loss: 0.849183 mae: 0.713425 (2098.474038644346 steps/sec)\n",
      "Step #1185\tEpoch   0 Batch 1184/3125   Loss: 0.867892 mae: 0.760558 (2145.0948703523754 steps/sec)\n",
      "Step #1186\tEpoch   0 Batch 1185/3125   Loss: 0.857380 mae: 0.734793 (2127.382099635825 steps/sec)\n",
      "Step #1187\tEpoch   0 Batch 1186/3125   Loss: 0.892269 mae: 0.772816 (2027.997292331496 steps/sec)\n",
      "Step #1188\tEpoch   0 Batch 1187/3125   Loss: 0.798878 mae: 0.718284 (2165.8081173190126 steps/sec)\n",
      "Step #1189\tEpoch   0 Batch 1188/3125   Loss: 0.887580 mae: 0.767769 (2392.998391090521 steps/sec)\n",
      "Step #1190\tEpoch   0 Batch 1189/3125   Loss: 0.760982 mae: 0.698936 (2210.4135924786037 steps/sec)\n",
      "Step #1191\tEpoch   0 Batch 1190/3125   Loss: 0.958975 mae: 0.782704 (2048.520132064782 steps/sec)\n",
      "Step #1192\tEpoch   0 Batch 1191/3125   Loss: 0.905889 mae: 0.759288 (2119.641395203105 steps/sec)\n",
      "Step #1193\tEpoch   0 Batch 1192/3125   Loss: 0.924833 mae: 0.739597 (2158.052234044743 steps/sec)\n",
      "Step #1194\tEpoch   0 Batch 1193/3125   Loss: 0.795031 mae: 0.690779 (2185.398386861466 steps/sec)\n",
      "Step #1195\tEpoch   0 Batch 1194/3125   Loss: 0.953059 mae: 0.781212 (2351.726380712083 steps/sec)\n",
      "Step #1196\tEpoch   0 Batch 1195/3125   Loss: 1.014615 mae: 0.817623 (2109.5986319283775 steps/sec)\n",
      "Step #1197\tEpoch   0 Batch 1196/3125   Loss: 0.818990 mae: 0.721985 (1939.849596240831 steps/sec)\n",
      "Step #1198\tEpoch   0 Batch 1197/3125   Loss: 0.990128 mae: 0.800187 (1879.623206331281 steps/sec)\n",
      "Step #1199\tEpoch   0 Batch 1198/3125   Loss: 0.873749 mae: 0.725826 (1554.2979114477566 steps/sec)\n",
      "Step #1200\tEpoch   0 Batch 1199/3125   Loss: 0.828205 mae: 0.734531 (1613.069763864318 steps/sec)\n",
      "Step #1201\tEpoch   0 Batch 1200/3125   Loss: 1.018132 mae: 0.803359 (1728.5976871275377 steps/sec)\n",
      "Step #1202\tEpoch   0 Batch 1201/3125   Loss: 0.951052 mae: 0.755782 (1795.3223983837277 steps/sec)\n",
      "Step #1203\tEpoch   0 Batch 1202/3125   Loss: 0.972559 mae: 0.804214 (1885.7245620976153 steps/sec)\n",
      "Step #1204\tEpoch   0 Batch 1203/3125   Loss: 0.870308 mae: 0.753520 (1745.3867536661285 steps/sec)\n",
      "Step #1205\tEpoch   0 Batch 1204/3125   Loss: 0.961129 mae: 0.801061 (1829.002014634444 steps/sec)\n",
      "Step #1206\tEpoch   0 Batch 1205/3125   Loss: 0.875400 mae: 0.746671 (1649.1322434279334 steps/sec)\n",
      "Step #1207\tEpoch   0 Batch 1206/3125   Loss: 0.930784 mae: 0.775280 (1611.0251584405607 steps/sec)\n",
      "Step #1208\tEpoch   0 Batch 1207/3125   Loss: 0.942972 mae: 0.763943 (1857.4976528316593 steps/sec)\n",
      "Step #1209\tEpoch   0 Batch 1208/3125   Loss: 0.970383 mae: 0.782001 (1772.5606869970925 steps/sec)\n",
      "Step #1210\tEpoch   0 Batch 1209/3125   Loss: 0.908284 mae: 0.745976 (1764.9674720798512 steps/sec)\n",
      "Step #1211\tEpoch   0 Batch 1210/3125   Loss: 0.820615 mae: 0.714281 (1992.1081378891074 steps/sec)\n",
      "Step #1212\tEpoch   0 Batch 1211/3125   Loss: 0.771909 mae: 0.706516 (2055.0642834744435 steps/sec)\n",
      "Step #1213\tEpoch   0 Batch 1212/3125   Loss: 0.908306 mae: 0.754262 (1910.3746686464378 steps/sec)\n",
      "Step #1214\tEpoch   0 Batch 1213/3125   Loss: 0.970603 mae: 0.776352 (1747.1607571314316 steps/sec)\n",
      "Step #1215\tEpoch   0 Batch 1214/3125   Loss: 0.948377 mae: 0.788900 (1986.5790121819525 steps/sec)\n",
      "Step #1216\tEpoch   0 Batch 1215/3125   Loss: 0.722488 mae: 0.668211 (2211.6957214119234 steps/sec)\n",
      "Step #1217\tEpoch   0 Batch 1216/3125   Loss: 0.929835 mae: 0.751390 (2327.6343537037446 steps/sec)\n",
      "Step #1218\tEpoch   0 Batch 1217/3125   Loss: 0.840302 mae: 0.717142 (2021.4292599232742 steps/sec)\n",
      "Step #1219\tEpoch   0 Batch 1218/3125   Loss: 0.887589 mae: 0.733775 (2023.6333986278503 steps/sec)\n",
      "Step #1220\tEpoch   0 Batch 1219/3125   Loss: 1.128517 mae: 0.859489 (2157.5859833948907 steps/sec)\n",
      "Step #1221\tEpoch   0 Batch 1220/3125   Loss: 0.891584 mae: 0.745290 (2178.655502342638 steps/sec)\n",
      "Step #1222\tEpoch   0 Batch 1221/3125   Loss: 0.900564 mae: 0.757541 (2291.36838424894 steps/sec)\n",
      "Step #1223\tEpoch   0 Batch 1222/3125   Loss: 0.779997 mae: 0.685997 (2192.4123150906903 steps/sec)\n",
      "Step #1224\tEpoch   0 Batch 1223/3125   Loss: 0.663406 mae: 0.671107 (1801.0116537705144 steps/sec)\n",
      "Step #1225\tEpoch   0 Batch 1224/3125   Loss: 0.831797 mae: 0.729058 (2176.846344678687 steps/sec)\n",
      "Step #1226\tEpoch   0 Batch 1225/3125   Loss: 0.945820 mae: 0.788974 (2413.015763433437 steps/sec)\n",
      "Step #1227\tEpoch   0 Batch 1226/3125   Loss: 0.890537 mae: 0.771292 (2357.595584184907 steps/sec)\n",
      "Step #1228\tEpoch   0 Batch 1227/3125   Loss: 0.955205 mae: 0.787831 (2324.924891633316 steps/sec)\n",
      "Step #1229\tEpoch   0 Batch 1228/3125   Loss: 0.923694 mae: 0.784927 (2144.743866395312 steps/sec)\n",
      "Step #1230\tEpoch   0 Batch 1229/3125   Loss: 0.819523 mae: 0.722820 (2293.7743360896006 steps/sec)\n",
      "Step #1231\tEpoch   0 Batch 1230/3125   Loss: 0.784143 mae: 0.704850 (2135.1360706977125 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1232\tEpoch   0 Batch 1231/3125   Loss: 0.902021 mae: 0.753391 (1911.4542223032402 steps/sec)\n",
      "Step #1233\tEpoch   0 Batch 1232/3125   Loss: 0.881732 mae: 0.734754 (1709.630136874628 steps/sec)\n",
      "Step #1234\tEpoch   0 Batch 1233/3125   Loss: 0.853825 mae: 0.737427 (1612.0777922976401 steps/sec)\n",
      "Step #1235\tEpoch   0 Batch 1234/3125   Loss: 0.933200 mae: 0.755911 (1546.8802785215346 steps/sec)\n",
      "Step #1236\tEpoch   0 Batch 1235/3125   Loss: 0.970884 mae: 0.783736 (1693.3136319227447 steps/sec)\n",
      "Step #1237\tEpoch   0 Batch 1236/3125   Loss: 0.856953 mae: 0.750463 (1587.2484389782403 steps/sec)\n",
      "Step #1238\tEpoch   0 Batch 1237/3125   Loss: 1.047857 mae: 0.828071 (1730.4519312490202 steps/sec)\n",
      "Step #1239\tEpoch   0 Batch 1238/3125   Loss: 0.988951 mae: 0.777826 (1762.9199976462478 steps/sec)\n",
      "Step #1240\tEpoch   0 Batch 1239/3125   Loss: 0.957294 mae: 0.794252 (1674.8810018209117 steps/sec)\n",
      "Step #1241\tEpoch   0 Batch 1240/3125   Loss: 0.839714 mae: 0.715316 (2197.6275307037768 steps/sec)\n",
      "Step #1242\tEpoch   0 Batch 1241/3125   Loss: 0.871792 mae: 0.752632 (2219.654745398546 steps/sec)\n",
      "Step #1243\tEpoch   0 Batch 1242/3125   Loss: 0.881819 mae: 0.753849 (2222.9015401248635 steps/sec)\n",
      "Step #1244\tEpoch   0 Batch 1243/3125   Loss: 0.938487 mae: 0.770173 (2014.4197796497833 steps/sec)\n",
      "Step #1245\tEpoch   0 Batch 1244/3125   Loss: 0.976517 mae: 0.761300 (2133.8542938542937 steps/sec)\n",
      "Step #1246\tEpoch   0 Batch 1245/3125   Loss: 1.049503 mae: 0.827423 (2125.0970258904595 steps/sec)\n",
      "Step #1247\tEpoch   0 Batch 1246/3125   Loss: 0.841125 mae: 0.710059 (2202.5668494129013 steps/sec)\n",
      "Step #1248\tEpoch   0 Batch 1247/3125   Loss: 1.085763 mae: 0.858907 (1712.8556960371134 steps/sec)\n",
      "Step #1249\tEpoch   0 Batch 1248/3125   Loss: 0.898799 mae: 0.749575 (1821.4405447423503 steps/sec)\n",
      "Step #1250\tEpoch   0 Batch 1249/3125   Loss: 1.001858 mae: 0.811808 (2063.4557673196696 steps/sec)\n",
      "Step #1251\tEpoch   0 Batch 1250/3125   Loss: 0.978405 mae: 0.774664 (1992.4488147831457 steps/sec)\n",
      "Step #1252\tEpoch   0 Batch 1251/3125   Loss: 0.940850 mae: 0.787508 (1963.0924187252524 steps/sec)\n",
      "Step #1253\tEpoch   0 Batch 1252/3125   Loss: 0.932144 mae: 0.761596 (2025.6858048064291 steps/sec)\n",
      "Step #1254\tEpoch   0 Batch 1253/3125   Loss: 1.089777 mae: 0.832307 (2007.0744965929102 steps/sec)\n",
      "Step #1255\tEpoch   0 Batch 1254/3125   Loss: 0.931876 mae: 0.757359 (2083.3394593842822 steps/sec)\n",
      "Step #1256\tEpoch   0 Batch 1255/3125   Loss: 0.944164 mae: 0.759380 (1569.5483291546607 steps/sec)\n",
      "Step #1257\tEpoch   0 Batch 1256/3125   Loss: 0.861033 mae: 0.744871 (1844.558200080919 steps/sec)\n",
      "Step #1258\tEpoch   0 Batch 1257/3125   Loss: 1.177855 mae: 0.820316 (2004.4846735421465 steps/sec)\n",
      "Step #1259\tEpoch   0 Batch 1258/3125   Loss: 0.901926 mae: 0.770096 (2100.3024536805206 steps/sec)\n",
      "Step #1260\tEpoch   0 Batch 1259/3125   Loss: 0.918413 mae: 0.771842 (2349.960780798279 steps/sec)\n",
      "Step #1261\tEpoch   0 Batch 1260/3125   Loss: 0.869353 mae: 0.767978 (2287.3946096877285 steps/sec)\n",
      "Step #1262\tEpoch   0 Batch 1261/3125   Loss: 0.830261 mae: 0.728415 (2268.0740612562727 steps/sec)\n",
      "Step #1263\tEpoch   0 Batch 1262/3125   Loss: 0.917006 mae: 0.762592 (2296.135064707557 steps/sec)\n",
      "Step #1264\tEpoch   0 Batch 1263/3125   Loss: 0.789646 mae: 0.710167 (2139.056109178813 steps/sec)\n",
      "Step #1265\tEpoch   0 Batch 1264/3125   Loss: 0.922445 mae: 0.749022 (1845.7108155919136 steps/sec)\n",
      "Step #1266\tEpoch   0 Batch 1265/3125   Loss: 0.890431 mae: 0.752047 (1910.078874984061 steps/sec)\n",
      "Step #1267\tEpoch   0 Batch 1266/3125   Loss: 0.906058 mae: 0.758777 (2097.844288615243 steps/sec)\n",
      "Step #1268\tEpoch   0 Batch 1267/3125   Loss: 0.846930 mae: 0.737827 (2082.015745529997 steps/sec)\n",
      "Step #1269\tEpoch   0 Batch 1268/3125   Loss: 1.017626 mae: 0.787981 (1831.0300871356976 steps/sec)\n",
      "Step #1270\tEpoch   0 Batch 1269/3125   Loss: 0.899948 mae: 0.739070 (2045.5030480370642 steps/sec)\n",
      "Step #1271\tEpoch   0 Batch 1270/3125   Loss: 0.914415 mae: 0.756299 (1877.519740729467 steps/sec)\n",
      "Step #1272\tEpoch   0 Batch 1271/3125   Loss: 0.983235 mae: 0.789684 (2082.4705823941213 steps/sec)\n",
      "Step #1273\tEpoch   0 Batch 1272/3125   Loss: 0.950982 mae: 0.790478 (1724.021933033549 steps/sec)\n",
      "Step #1274\tEpoch   0 Batch 1273/3125   Loss: 0.850802 mae: 0.750061 (1765.7104848826734 steps/sec)\n",
      "Step #1275\tEpoch   0 Batch 1274/3125   Loss: 0.951084 mae: 0.766603 (2009.3244291996818 steps/sec)\n",
      "Step #1276\tEpoch   0 Batch 1275/3125   Loss: 0.889384 mae: 0.764040 (1985.0558936836824 steps/sec)\n",
      "Step #1277\tEpoch   0 Batch 1276/3125   Loss: 0.931544 mae: 0.769175 (2161.0321091463666 steps/sec)\n",
      "Step #1278\tEpoch   0 Batch 1277/3125   Loss: 1.003551 mae: 0.815045 (1902.5065544175413 steps/sec)\n",
      "Step #1279\tEpoch   0 Batch 1278/3125   Loss: 0.919320 mae: 0.766407 (1546.1617859565308 steps/sec)\n",
      "Step #1280\tEpoch   0 Batch 1279/3125   Loss: 1.090770 mae: 0.823415 (1693.5871241793118 steps/sec)\n",
      "Step #1281\tEpoch   0 Batch 1280/3125   Loss: 0.925789 mae: 0.778223 (1774.7507743344108 steps/sec)\n",
      "Step #1282\tEpoch   0 Batch 1281/3125   Loss: 0.949526 mae: 0.763645 (1958.9299058436707 steps/sec)\n",
      "Step #1283\tEpoch   0 Batch 1282/3125   Loss: 0.825165 mae: 0.738875 (1927.5294117647059 steps/sec)\n",
      "Step #1284\tEpoch   0 Batch 1283/3125   Loss: 0.810811 mae: 0.723722 (2045.9819904196056 steps/sec)\n",
      "Step #1285\tEpoch   0 Batch 1284/3125   Loss: 0.944858 mae: 0.778782 (1943.5530059405207 steps/sec)\n",
      "Step #1286\tEpoch   0 Batch 1285/3125   Loss: 0.984276 mae: 0.798144 (2110.0656014810643 steps/sec)\n",
      "Step #1287\tEpoch   0 Batch 1286/3125   Loss: 0.781420 mae: 0.719566 (1936.7145654020908 steps/sec)\n",
      "Step #1288\tEpoch   0 Batch 1287/3125   Loss: 0.831098 mae: 0.718016 (1906.2765309554325 steps/sec)\n",
      "Step #1289\tEpoch   0 Batch 1288/3125   Loss: 0.976587 mae: 0.754158 (1924.2574666238472 steps/sec)\n",
      "Step #1290\tEpoch   0 Batch 1289/3125   Loss: 0.938008 mae: 0.761930 (1799.6670385308505 steps/sec)\n",
      "Step #1291\tEpoch   0 Batch 1290/3125   Loss: 0.809182 mae: 0.729750 (2096.250612236738 steps/sec)\n",
      "Step #1292\tEpoch   0 Batch 1291/3125   Loss: 0.960533 mae: 0.779983 (1938.7198165883963 steps/sec)\n",
      "Step #1293\tEpoch   0 Batch 1292/3125   Loss: 0.897685 mae: 0.727530 (2163.0158320870505 steps/sec)\n",
      "Step #1294\tEpoch   0 Batch 1293/3125   Loss: 0.905231 mae: 0.752889 (2032.5179298313626 steps/sec)\n",
      "Step #1295\tEpoch   0 Batch 1294/3125   Loss: 0.878431 mae: 0.756467 (2026.8996578586202 steps/sec)\n",
      "Step #1296\tEpoch   0 Batch 1295/3125   Loss: 0.889192 mae: 0.775517 (1928.8590480570247 steps/sec)\n",
      "Step #1297\tEpoch   0 Batch 1296/3125   Loss: 0.824934 mae: 0.730413 (1951.292858804373 steps/sec)\n",
      "Step #1298\tEpoch   0 Batch 1297/3125   Loss: 0.982779 mae: 0.781863 (1879.8590880162067 steps/sec)\n",
      "Step #1299\tEpoch   0 Batch 1298/3125   Loss: 0.817464 mae: 0.706050 (1757.0414806002161 steps/sec)\n",
      "Step #1300\tEpoch   0 Batch 1299/3125   Loss: 0.893268 mae: 0.749949 (1895.6620777553806 steps/sec)\n",
      "Step #1301\tEpoch   0 Batch 1300/3125   Loss: 0.819540 mae: 0.706855 (1699.7503647268602 steps/sec)\n",
      "Step #1302\tEpoch   0 Batch 1301/3125   Loss: 0.839410 mae: 0.739711 (1618.272732884746 steps/sec)\n",
      "Step #1303\tEpoch   0 Batch 1302/3125   Loss: 0.809145 mae: 0.713205 (1819.9704937950187 steps/sec)\n",
      "Step #1304\tEpoch   0 Batch 1303/3125   Loss: 0.814204 mae: 0.718809 (1834.0565831474923 steps/sec)\n",
      "Step #1305\tEpoch   0 Batch 1304/3125   Loss: 0.841575 mae: 0.746755 (1730.4090961598758 steps/sec)\n",
      "Step #1306\tEpoch   0 Batch 1305/3125   Loss: 0.799813 mae: 0.712757 (2033.8386042497066 steps/sec)\n",
      "Step #1307\tEpoch   0 Batch 1306/3125   Loss: 0.785696 mae: 0.700212 (2126.476105494773 steps/sec)\n",
      "Step #1308\tEpoch   0 Batch 1307/3125   Loss: 1.072723 mae: 0.810110 (2192.1143956181795 steps/sec)\n",
      "Step #1309\tEpoch   0 Batch 1308/3125   Loss: 0.896549 mae: 0.722503 (2174.679317675118 steps/sec)\n",
      "Step #1310\tEpoch   0 Batch 1309/3125   Loss: 0.975031 mae: 0.792188 (2097.4875980156826 steps/sec)\n",
      "Step #1311\tEpoch   0 Batch 1310/3125   Loss: 0.868232 mae: 0.724888 (2229.3289111415847 steps/sec)\n",
      "Step #1312\tEpoch   0 Batch 1311/3125   Loss: 0.943483 mae: 0.784336 (2176.1912668105597 steps/sec)\n",
      "Step #1313\tEpoch   0 Batch 1312/3125   Loss: 0.915802 mae: 0.764176 (1839.8652442448063 steps/sec)\n",
      "Step #1314\tEpoch   0 Batch 1313/3125   Loss: 0.860957 mae: 0.752155 (2234.031084550403 steps/sec)\n",
      "Step #1315\tEpoch   0 Batch 1314/3125   Loss: 1.026828 mae: 0.828301 (2114.4268675075364 steps/sec)\n",
      "Step #1316\tEpoch   0 Batch 1315/3125   Loss: 0.985378 mae: 0.788124 (2283.0120075332848 steps/sec)\n",
      "Step #1317\tEpoch   0 Batch 1316/3125   Loss: 0.852640 mae: 0.718061 (2159.852518615405 steps/sec)\n",
      "Step #1318\tEpoch   0 Batch 1317/3125   Loss: 0.799484 mae: 0.715987 (2103.377998876675 steps/sec)\n",
      "Step #1319\tEpoch   0 Batch 1318/3125   Loss: 0.913139 mae: 0.746244 (2086.9052950015425 steps/sec)\n",
      "Step #1320\tEpoch   0 Batch 1319/3125   Loss: 0.981952 mae: 0.785871 (2185.034070307779 steps/sec)\n",
      "Step #1321\tEpoch   0 Batch 1320/3125   Loss: 0.850594 mae: 0.705301 (1962.9454215299943 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1322\tEpoch   0 Batch 1321/3125   Loss: 0.934776 mae: 0.787773 (1722.7048695537885 steps/sec)\n",
      "Step #1323\tEpoch   0 Batch 1322/3125   Loss: 0.754179 mae: 0.688207 (1976.8227963840998 steps/sec)\n",
      "Step #1324\tEpoch   0 Batch 1323/3125   Loss: 0.895572 mae: 0.741789 (2268.8101780730035 steps/sec)\n",
      "Step #1325\tEpoch   0 Batch 1324/3125   Loss: 0.883686 mae: 0.756378 (2163.6406780360476 steps/sec)\n",
      "Step #1326\tEpoch   0 Batch 1325/3125   Loss: 0.829276 mae: 0.718941 (2174.093156819855 steps/sec)\n",
      "Step #1327\tEpoch   0 Batch 1326/3125   Loss: 0.867073 mae: 0.732002 (2142.1368743615935 steps/sec)\n",
      "Step #1328\tEpoch   0 Batch 1327/3125   Loss: 0.740174 mae: 0.691859 (2426.5009777037267 steps/sec)\n",
      "Step #1329\tEpoch   0 Batch 1328/3125   Loss: 0.946857 mae: 0.768937 (2128.375265139598 steps/sec)\n",
      "Step #1330\tEpoch   0 Batch 1329/3125   Loss: 0.854282 mae: 0.765610 (2281.0254625349417 steps/sec)\n",
      "Step #1331\tEpoch   0 Batch 1330/3125   Loss: 1.007675 mae: 0.797194 (2014.4197796497833 steps/sec)\n",
      "Step #1332\tEpoch   0 Batch 1331/3125   Loss: 0.686218 mae: 0.632760 (2014.4197796497833 steps/sec)\n",
      "Step #1333\tEpoch   0 Batch 1332/3125   Loss: 0.912646 mae: 0.771668 (2069.8915285687494 steps/sec)\n",
      "Step #1334\tEpoch   0 Batch 1333/3125   Loss: 0.784450 mae: 0.696353 (1767.4664778808794 steps/sec)\n",
      "Step #1335\tEpoch   0 Batch 1334/3125   Loss: 0.734901 mae: 0.671984 (1731.2947858534492 steps/sec)\n",
      "Step #1336\tEpoch   0 Batch 1335/3125   Loss: 0.899834 mae: 0.755672 (1936.106648941081 steps/sec)\n",
      "Step #1337\tEpoch   0 Batch 1336/3125   Loss: 0.872744 mae: 0.752361 (2144.45876025114 steps/sec)\n",
      "Step #1338\tEpoch   0 Batch 1337/3125   Loss: 0.793234 mae: 0.709566 (2081.519786404105 steps/sec)\n",
      "Step #1339\tEpoch   0 Batch 1338/3125   Loss: 0.938364 mae: 0.784074 (2055.084421884034 steps/sec)\n",
      "Step #1340\tEpoch   0 Batch 1339/3125   Loss: 0.954977 mae: 0.797627 (1933.01932879225 steps/sec)\n",
      "Step #1341\tEpoch   0 Batch 1340/3125   Loss: 0.794238 mae: 0.727013 (1657.8145627307294 steps/sec)\n",
      "Step #1342\tEpoch   0 Batch 1341/3125   Loss: 0.902625 mae: 0.738898 (1584.4303414928982 steps/sec)\n",
      "Step #1343\tEpoch   0 Batch 1342/3125   Loss: 0.888865 mae: 0.750299 (1507.397717144418 steps/sec)\n",
      "Step #1344\tEpoch   0 Batch 1343/3125   Loss: 0.786454 mae: 0.706453 (1551.8136478666884 steps/sec)\n",
      "Step #1345\tEpoch   0 Batch 1344/3125   Loss: 0.808041 mae: 0.719567 (1514.506286514866 steps/sec)\n",
      "Step #1346\tEpoch   0 Batch 1345/3125   Loss: 0.935826 mae: 0.771341 (1528.3916247002835 steps/sec)\n",
      "Step #1347\tEpoch   0 Batch 1346/3125   Loss: 0.804382 mae: 0.729128 (1619.9102432392767 steps/sec)\n",
      "Step #1348\tEpoch   0 Batch 1347/3125   Loss: 0.895641 mae: 0.754476 (2123.053249645677 steps/sec)\n",
      "Step #1349\tEpoch   0 Batch 1348/3125   Loss: 0.841343 mae: 0.754463 (2263.7651122625216 steps/sec)\n",
      "Step #1350\tEpoch   0 Batch 1349/3125   Loss: 0.922969 mae: 0.790934 (2214.942650134133 steps/sec)\n",
      "Step #1351\tEpoch   0 Batch 1350/3125   Loss: 0.853341 mae: 0.703377 (2230.5381833652414 steps/sec)\n",
      "Step #1352\tEpoch   0 Batch 1351/3125   Loss: 0.959295 mae: 0.777403 (1770.660002195223 steps/sec)\n",
      "Step #1353\tEpoch   0 Batch 1352/3125   Loss: 0.865861 mae: 0.765120 (2096.1877536333286 steps/sec)\n",
      "Step #1354\tEpoch   0 Batch 1353/3125   Loss: 0.767522 mae: 0.690380 (2325.1568840499367 steps/sec)\n",
      "Step #1355\tEpoch   0 Batch 1354/3125   Loss: 0.794252 mae: 0.720216 (1697.9750463528974 steps/sec)\n",
      "Step #1356\tEpoch   0 Batch 1355/3125   Loss: 0.904724 mae: 0.752397 (1788.3259855545796 steps/sec)\n",
      "Step #1357\tEpoch   0 Batch 1356/3125   Loss: 0.998311 mae: 0.797121 (1912.848998951065 steps/sec)\n",
      "Step #1358\tEpoch   0 Batch 1357/3125   Loss: 0.821988 mae: 0.736901 (1837.3023312861935 steps/sec)\n",
      "Step #1359\tEpoch   0 Batch 1358/3125   Loss: 1.067515 mae: 0.797220 (1700.398109184079 steps/sec)\n",
      "Step #1360\tEpoch   0 Batch 1359/3125   Loss: 0.998788 mae: 0.805013 (1899.8351240193504 steps/sec)\n",
      "Step #1361\tEpoch   0 Batch 1360/3125   Loss: 0.802794 mae: 0.726932 (2032.675532121118 steps/sec)\n",
      "Step #1362\tEpoch   0 Batch 1361/3125   Loss: 0.844058 mae: 0.744765 (1894.0869392436846 steps/sec)\n",
      "Step #1363\tEpoch   0 Batch 1362/3125   Loss: 0.885978 mae: 0.748593 (1587.344550663427 steps/sec)\n",
      "Step #1364\tEpoch   0 Batch 1363/3125   Loss: 1.043655 mae: 0.798701 (1997.534932896454 steps/sec)\n",
      "Step #1365\tEpoch   0 Batch 1364/3125   Loss: 0.910963 mae: 0.761090 (1984.360925021763 steps/sec)\n",
      "Step #1366\tEpoch   0 Batch 1365/3125   Loss: 0.808137 mae: 0.728165 (2208.6908899420746 steps/sec)\n",
      "Step #1367\tEpoch   0 Batch 1366/3125   Loss: 0.998874 mae: 0.796492 (2132.378899418392 steps/sec)\n",
      "Step #1368\tEpoch   0 Batch 1367/3125   Loss: 0.889570 mae: 0.743781 (2409.7439904398584 steps/sec)\n",
      "Step #1369\tEpoch   0 Batch 1368/3125   Loss: 0.941898 mae: 0.760057 (2186.3780898466416 steps/sec)\n",
      "Step #1370\tEpoch   0 Batch 1369/3125   Loss: 0.773763 mae: 0.696146 (2164.0202249509853 steps/sec)\n",
      "Step #1371\tEpoch   0 Batch 1370/3125   Loss: 0.780730 mae: 0.716456 (2193.2606831349744 steps/sec)\n",
      "Step #1372\tEpoch   0 Batch 1371/3125   Loss: 0.912135 mae: 0.771573 (1785.9501809665744 steps/sec)\n",
      "Step #1373\tEpoch   0 Batch 1372/3125   Loss: 1.001271 mae: 0.783716 (1893.0781729554071 steps/sec)\n",
      "Step #1374\tEpoch   0 Batch 1373/3125   Loss: 1.064189 mae: 0.809975 (1899.336140922882 steps/sec)\n",
      "Step #1375\tEpoch   0 Batch 1374/3125   Loss: 0.864969 mae: 0.712450 (1629.843322556578 steps/sec)\n",
      "Step #1376\tEpoch   0 Batch 1375/3125   Loss: 0.946440 mae: 0.769851 (1167.0035168944487 steps/sec)\n",
      "Step #1377\tEpoch   0 Batch 1376/3125   Loss: 0.886220 mae: 0.743172 (1943.7871906571509 steps/sec)\n",
      "Step #1378\tEpoch   0 Batch 1377/3125   Loss: 0.971864 mae: 0.787926 (2011.4250637816272 steps/sec)\n",
      "Step #1379\tEpoch   0 Batch 1378/3125   Loss: 0.850251 mae: 0.734795 (2031.7106015248835 steps/sec)\n",
      "Step #1380\tEpoch   0 Batch 1379/3125   Loss: 0.930343 mae: 0.765949 (2100.744272706328 steps/sec)\n",
      "Step #1381\tEpoch   0 Batch 1380/3125   Loss: 0.824792 mae: 0.723003 (1885.8432624432355 steps/sec)\n",
      "Step #1382\tEpoch   0 Batch 1381/3125   Loss: 0.895783 mae: 0.779323 (2245.8737604146586 steps/sec)\n",
      "Step #1383\tEpoch   0 Batch 1382/3125   Loss: 0.680905 mae: 0.659926 (2276.864950546647 steps/sec)\n",
      "Step #1384\tEpoch   0 Batch 1383/3125   Loss: 0.932443 mae: 0.775368 (2059.221146481805 steps/sec)\n",
      "Step #1385\tEpoch   0 Batch 1384/3125   Loss: 0.973223 mae: 0.781307 (1991.6351687591407 steps/sec)\n",
      "Step #1386\tEpoch   0 Batch 1385/3125   Loss: 0.763236 mae: 0.682284 (2110.5964997031087 steps/sec)\n",
      "Step #1387\tEpoch   0 Batch 1386/3125   Loss: 0.939799 mae: 0.781601 (2057.463528534568 steps/sec)\n",
      "Step #1388\tEpoch   0 Batch 1387/3125   Loss: 0.802800 mae: 0.716048 (1735.5644934372776 steps/sec)\n",
      "Step #1389\tEpoch   0 Batch 1388/3125   Loss: 0.860770 mae: 0.758137 (1945.7529620248467 steps/sec)\n",
      "Step #1390\tEpoch   0 Batch 1389/3125   Loss: 0.873860 mae: 0.741163 (2125.937189546459 steps/sec)\n",
      "Step #1391\tEpoch   0 Batch 1390/3125   Loss: 0.971045 mae: 0.787117 (2213.282956740156 steps/sec)\n",
      "Step #1392\tEpoch   0 Batch 1391/3125   Loss: 0.860802 mae: 0.719471 (2095.559374875095 steps/sec)\n",
      "Step #1393\tEpoch   0 Batch 1392/3125   Loss: 0.916319 mae: 0.786729 (1950.9298106888693 steps/sec)\n",
      "Step #1394\tEpoch   0 Batch 1393/3125   Loss: 0.841021 mae: 0.726602 (1745.9389257051516 steps/sec)\n",
      "Step #1395\tEpoch   0 Batch 1394/3125   Loss: 0.904713 mae: 0.755441 (2004.082412751806 steps/sec)\n",
      "Step #1396\tEpoch   0 Batch 1395/3125   Loss: 0.831049 mae: 0.734835 (1744.3995275407162 steps/sec)\n",
      "Step #1397\tEpoch   0 Batch 1396/3125   Loss: 0.868071 mae: 0.748697 (1510.209195981709 steps/sec)\n",
      "Step #1398\tEpoch   0 Batch 1397/3125   Loss: 0.971213 mae: 0.785650 (1732.7968139340808 steps/sec)\n",
      "Step #1399\tEpoch   0 Batch 1398/3125   Loss: 0.891817 mae: 0.764577 (1711.3880252323713 steps/sec)\n",
      "Step #1400\tEpoch   0 Batch 1399/3125   Loss: 0.882102 mae: 0.749901 (1981.4361300075586 steps/sec)\n",
      "Step #1401\tEpoch   0 Batch 1400/3125   Loss: 0.929679 mae: 0.763194 (2113.7022889222612 steps/sec)\n",
      "Step #1402\tEpoch   0 Batch 1401/3125   Loss: 0.817672 mae: 0.718063 (2076.758234140738 steps/sec)\n",
      "Step #1403\tEpoch   0 Batch 1402/3125   Loss: 0.778551 mae: 0.714773 (1961.2381932105116 steps/sec)\n",
      "Step #1404\tEpoch   0 Batch 1403/3125   Loss: 0.772996 mae: 0.720127 (1824.6893815473497 steps/sec)\n",
      "Step #1405\tEpoch   0 Batch 1404/3125   Loss: 0.887177 mae: 0.761932 (2051.0244598969184 steps/sec)\n",
      "Step #1406\tEpoch   0 Batch 1405/3125   Loss: 0.880035 mae: 0.757325 (2218.269515548974 steps/sec)\n",
      "Step #1407\tEpoch   0 Batch 1406/3125   Loss: 0.990264 mae: 0.791791 (2197.2130838379817 steps/sec)\n",
      "Step #1408\tEpoch   0 Batch 1407/3125   Loss: 1.005944 mae: 0.808892 (2016.628042272076 steps/sec)\n",
      "Step #1409\tEpoch   0 Batch 1408/3125   Loss: 0.918881 mae: 0.754004 (1983.5726311408737 steps/sec)\n",
      "Step #1410\tEpoch   0 Batch 1409/3125   Loss: 0.880940 mae: 0.728907 (2143.428623991987 steps/sec)\n",
      "Step #1411\tEpoch   0 Batch 1410/3125   Loss: 0.927749 mae: 0.748739 (1955.641762094819 steps/sec)\n",
      "Step #1412\tEpoch   0 Batch 1411/3125   Loss: 0.709667 mae: 0.669391 (1962.5412927315435 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1413\tEpoch   0 Batch 1412/3125   Loss: 1.022059 mae: 0.803481 (1648.484086246335 steps/sec)\n",
      "Step #1414\tEpoch   0 Batch 1413/3125   Loss: 0.792771 mae: 0.700662 (1785.4788175997821 steps/sec)\n",
      "Step #1415\tEpoch   0 Batch 1414/3125   Loss: 0.764894 mae: 0.683658 (2008.8432506992606 steps/sec)\n",
      "Step #1416\tEpoch   0 Batch 1415/3125   Loss: 0.897274 mae: 0.774846 (2061.609846250639 steps/sec)\n",
      "Step #1417\tEpoch   0 Batch 1416/3125   Loss: 0.875580 mae: 0.733167 (2103.4201921726744 steps/sec)\n",
      "Step #1418\tEpoch   0 Batch 1417/3125   Loss: 0.950860 mae: 0.771216 (2311.3952232423317 steps/sec)\n",
      "Step #1419\tEpoch   0 Batch 1418/3125   Loss: 0.976582 mae: 0.771011 (2145.3801456747688 steps/sec)\n",
      "Step #1420\tEpoch   0 Batch 1419/3125   Loss: 0.941752 mae: 0.748622 (2201.6650394213307 steps/sec)\n",
      "Step #1421\tEpoch   0 Batch 1420/3125   Loss: 0.901738 mae: 0.754482 (2283.832465750441 steps/sec)\n",
      "Step #1422\tEpoch   0 Batch 1421/3125   Loss: 0.834859 mae: 0.751781 (1970.5074840031195 steps/sec)\n",
      "Step #1423\tEpoch   0 Batch 1422/3125   Loss: 0.840929 mae: 0.716630 (1904.3896769037976 steps/sec)\n",
      "Step #1424\tEpoch   0 Batch 1423/3125   Loss: 0.837918 mae: 0.746982 (2101.2915443423544 steps/sec)\n",
      "Step #1425\tEpoch   0 Batch 1424/3125   Loss: 0.850504 mae: 0.729925 (1996.5650526476134 steps/sec)\n",
      "Step #1426\tEpoch   0 Batch 1425/3125   Loss: 1.070502 mae: 0.809080 (2107.0551592484676 steps/sec)\n",
      "Step #1427\tEpoch   0 Batch 1426/3125   Loss: 0.902880 mae: 0.753977 (2156.0111031150404 steps/sec)\n",
      "Step #1428\tEpoch   0 Batch 1427/3125   Loss: 0.708714 mae: 0.654838 (2122.494585349068 steps/sec)\n",
      "Step #1429\tEpoch   0 Batch 1428/3125   Loss: 0.855801 mae: 0.721035 (2121.785934701889 steps/sec)\n",
      "Step #1430\tEpoch   0 Batch 1429/3125   Loss: 0.990153 mae: 0.786304 (1710.2854346762356 steps/sec)\n",
      "Step #1431\tEpoch   0 Batch 1430/3125   Loss: 0.899415 mae: 0.744297 (1071.0082681769666 steps/sec)\n",
      "Step #1432\tEpoch   0 Batch 1431/3125   Loss: 0.972676 mae: 0.790037 (1213.8895481092595 steps/sec)\n",
      "Step #1433\tEpoch   0 Batch 1432/3125   Loss: 0.963052 mae: 0.766112 (770.9862541312971 steps/sec)\n",
      "Step #1434\tEpoch   0 Batch 1433/3125   Loss: 0.857305 mae: 0.736867 (910.0681742239811 steps/sec)\n",
      "Step #1435\tEpoch   0 Batch 1434/3125   Loss: 0.836430 mae: 0.732847 (1237.9297317718172 steps/sec)\n",
      "Step #1436\tEpoch   0 Batch 1435/3125   Loss: 0.896379 mae: 0.759130 (1185.6287560563317 steps/sec)\n",
      "Step #1437\tEpoch   0 Batch 1436/3125   Loss: 0.943890 mae: 0.762478 (2192.206054524168 steps/sec)\n",
      "Step #1438\tEpoch   0 Batch 1437/3125   Loss: 0.922457 mae: 0.762539 (2211.579102777719 steps/sec)\n",
      "Step #1439\tEpoch   0 Batch 1438/3125   Loss: 1.040568 mae: 0.807178 (2027.9188504457811 steps/sec)\n",
      "Step #1440\tEpoch   0 Batch 1439/3125   Loss: 0.941393 mae: 0.758106 (2017.1905659651418 steps/sec)\n",
      "Step #1441\tEpoch   0 Batch 1440/3125   Loss: 0.758122 mae: 0.692870 (1771.9765781446713 steps/sec)\n",
      "Step #1442\tEpoch   0 Batch 1441/3125   Loss: 0.823737 mae: 0.720525 (1916.3798854093372 steps/sec)\n",
      "Step #1443\tEpoch   0 Batch 1442/3125   Loss: 0.889210 mae: 0.769320 (2143.7134562701885 steps/sec)\n",
      "Step #1444\tEpoch   0 Batch 1443/3125   Loss: 0.845399 mae: 0.752204 (2116.4113432233326 steps/sec)\n",
      "Step #1445\tEpoch   0 Batch 1444/3125   Loss: 0.853665 mae: 0.745358 (2049.9218016890836 steps/sec)\n",
      "Step #1446\tEpoch   0 Batch 1445/3125   Loss: 0.991472 mae: 0.802593 (1312.746552490407 steps/sec)\n",
      "Step #1447\tEpoch   0 Batch 1446/3125   Loss: 0.766812 mae: 0.686443 (1095.7994785271264 steps/sec)\n",
      "Step #1448\tEpoch   0 Batch 1447/3125   Loss: 0.927233 mae: 0.776481 (1781.5806240602142 steps/sec)\n",
      "Step #1449\tEpoch   0 Batch 1448/3125   Loss: 0.955837 mae: 0.798156 (1605.4997971260805 steps/sec)\n",
      "Step #1450\tEpoch   0 Batch 1449/3125   Loss: 0.860919 mae: 0.742382 (2302.336202353768 steps/sec)\n",
      "Step #1451\tEpoch   0 Batch 1450/3125   Loss: 0.916997 mae: 0.774280 (1914.8225927210972 steps/sec)\n",
      "Step #1452\tEpoch   0 Batch 1451/3125   Loss: 0.933182 mae: 0.761711 (1920.6975189354043 steps/sec)\n",
      "Step #1453\tEpoch   0 Batch 1452/3125   Loss: 0.814541 mae: 0.747477 (2189.73395146807 steps/sec)\n",
      "Step #1454\tEpoch   0 Batch 1453/3125   Loss: 0.935052 mae: 0.772396 (2132.205457724999 steps/sec)\n",
      "Step #1455\tEpoch   0 Batch 1454/3125   Loss: 0.906541 mae: 0.745874 (2142.684035759898 steps/sec)\n",
      "Step #1456\tEpoch   0 Batch 1455/3125   Loss: 0.896386 mae: 0.757290 (2277.8046899607903 steps/sec)\n",
      "Step #1457\tEpoch   0 Batch 1456/3125   Loss: 0.897140 mae: 0.743221 (2012.2163479529077 steps/sec)\n",
      "Step #1458\tEpoch   0 Batch 1457/3125   Loss: 0.862367 mae: 0.749617 (2091.6926820997196 steps/sec)\n",
      "Step #1459\tEpoch   0 Batch 1458/3125   Loss: 0.897948 mae: 0.741295 (1790.2648068156595 steps/sec)\n",
      "Step #1460\tEpoch   0 Batch 1459/3125   Loss: 0.855101 mae: 0.702401 (1347.2732061750364 steps/sec)\n",
      "Step #1461\tEpoch   0 Batch 1460/3125   Loss: 0.848464 mae: 0.757664 (2259.3507934626864 steps/sec)\n",
      "Step #1462\tEpoch   0 Batch 1461/3125   Loss: 0.862118 mae: 0.755994 (2123.6767222610406 steps/sec)\n",
      "Step #1463\tEpoch   0 Batch 1462/3125   Loss: 0.942424 mae: 0.748268 (2172.5841206696505 steps/sec)\n",
      "Step #1464\tEpoch   0 Batch 1463/3125   Loss: 0.851694 mae: 0.753448 (2110.766443560968 steps/sec)\n",
      "Step #1465\tEpoch   0 Batch 1464/3125   Loss: 0.915094 mae: 0.782516 (1962.10061469083 steps/sec)\n",
      "Step #1466\tEpoch   0 Batch 1465/3125   Loss: 0.871714 mae: 0.748487 (1799.0803651087776 steps/sec)\n",
      "Step #1467\tEpoch   0 Batch 1466/3125   Loss: 0.769258 mae: 0.683261 (1976.8414305374883 steps/sec)\n",
      "Step #1468\tEpoch   0 Batch 1467/3125   Loss: 0.842954 mae: 0.706846 (2255.1233937308457 steps/sec)\n",
      "Step #1469\tEpoch   0 Batch 1468/3125   Loss: 0.986697 mae: 0.792965 (2189.3224762501304 steps/sec)\n",
      "Step #1470\tEpoch   0 Batch 1469/3125   Loss: 0.978765 mae: 0.774138 (2267.9759484362157 steps/sec)\n",
      "Step #1471\tEpoch   0 Batch 1470/3125   Loss: 0.922180 mae: 0.761656 (2245.9699702272583 steps/sec)\n",
      "Step #1472\tEpoch   0 Batch 1471/3125   Loss: 0.881609 mae: 0.743623 (2308.4184572032405 steps/sec)\n",
      "Step #1473\tEpoch   0 Batch 1472/3125   Loss: 0.870182 mae: 0.757302 (2125.592426668829 steps/sec)\n",
      "Step #1474\tEpoch   0 Batch 1473/3125   Loss: 0.944805 mae: 0.764874 (2023.8677488153946 steps/sec)\n",
      "Step #1475\tEpoch   0 Batch 1474/3125   Loss: 0.771106 mae: 0.701249 (2186.925282861463 steps/sec)\n",
      "Step #1476\tEpoch   0 Batch 1475/3125   Loss: 0.964750 mae: 0.774233 (2073.2269608715424 steps/sec)\n",
      "Step #1477\tEpoch   0 Batch 1476/3125   Loss: 0.793781 mae: 0.704854 (2246.8603019167103 steps/sec)\n",
      "Step #1478\tEpoch   0 Batch 1477/3125   Loss: 0.900602 mae: 0.774390 (1795.9835230240903 steps/sec)\n",
      "Step #1479\tEpoch   0 Batch 1478/3125   Loss: 0.872589 mae: 0.744027 (1546.6064883441375 steps/sec)\n",
      "Step #1480\tEpoch   0 Batch 1479/3125   Loss: 0.753127 mae: 0.702829 (1789.1040625159958 steps/sec)\n",
      "Step #1481\tEpoch   0 Batch 1480/3125   Loss: 0.798321 mae: 0.683295 (1701.7779328588933 steps/sec)\n",
      "Step #1482\tEpoch   0 Batch 1481/3125   Loss: 0.860658 mae: 0.768275 (1842.5810079426442 steps/sec)\n",
      "Step #1483\tEpoch   0 Batch 1482/3125   Loss: 0.925862 mae: 0.748317 (1549.7609388047679 steps/sec)\n",
      "Step #1484\tEpoch   0 Batch 1483/3125   Loss: 0.987388 mae: 0.795121 (1647.87528287654 steps/sec)\n",
      "Step #1485\tEpoch   0 Batch 1484/3125   Loss: 0.846289 mae: 0.746870 (1996.1659638869587 steps/sec)\n",
      "Step #1486\tEpoch   0 Batch 1485/3125   Loss: 0.912280 mae: 0.733250 (1512.5401187153357 steps/sec)\n",
      "Step #1487\tEpoch   0 Batch 1486/3125   Loss: 0.912739 mae: 0.756299 (820.5590509280996 steps/sec)\n",
      "Step #1488\tEpoch   0 Batch 1487/3125   Loss: 0.877101 mae: 0.737732 (1726.633678854593 steps/sec)\n",
      "Step #1489\tEpoch   0 Batch 1488/3125   Loss: 0.922702 mae: 0.745893 (2221.7946816400045 steps/sec)\n",
      "Step #1490\tEpoch   0 Batch 1489/3125   Loss: 0.779137 mae: 0.693272 (1775.9531189133343 steps/sec)\n",
      "Step #1491\tEpoch   0 Batch 1490/3125   Loss: 0.925183 mae: 0.766071 (2021.994465709575 steps/sec)\n",
      "Step #1492\tEpoch   0 Batch 1491/3125   Loss: 0.853839 mae: 0.714425 (2192.9854648122973 steps/sec)\n",
      "Step #1493\tEpoch   0 Batch 1492/3125   Loss: 0.827943 mae: 0.735141 (2318.7810972778134 steps/sec)\n",
      "Step #1494\tEpoch   0 Batch 1493/3125   Loss: 0.893959 mae: 0.779128 (2139.121565107407 steps/sec)\n",
      "Step #1495\tEpoch   0 Batch 1494/3125   Loss: 0.818494 mae: 0.734255 (2195.9936753264433 steps/sec)\n",
      "Step #1496\tEpoch   0 Batch 1495/3125   Loss: 0.888718 mae: 0.751109 (2121.871806546264 steps/sec)\n",
      "Step #1497\tEpoch   0 Batch 1496/3125   Loss: 0.830293 mae: 0.743900 (2274.1736791879935 steps/sec)\n",
      "Step #1498\tEpoch   0 Batch 1497/3125   Loss: 0.979127 mae: 0.777031 (2115.5146673122704 steps/sec)\n",
      "Step #1499\tEpoch   0 Batch 1498/3125   Loss: 0.852430 mae: 0.746626 (2023.3210161217185 steps/sec)\n",
      "Step #1500\tEpoch   0 Batch 1499/3125   Loss: 0.875610 mae: 0.731707 (2176.9593290011835 steps/sec)\n",
      "Step #1501\tEpoch   0 Batch 1500/3125   Loss: 0.951891 mae: 0.772023 (1924.7166365330079 steps/sec)\n",
      "Step #1502\tEpoch   0 Batch 1501/3125   Loss: 0.796207 mae: 0.728853 (2159.652338681441 steps/sec)\n",
      "Step #1503\tEpoch   0 Batch 1502/3125   Loss: 0.811418 mae: 0.711117 (2015.4070884908126 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1504\tEpoch   0 Batch 1503/3125   Loss: 0.996046 mae: 0.777665 (1811.9822357393423 steps/sec)\n",
      "Step #1505\tEpoch   0 Batch 1504/3125   Loss: 0.964611 mae: 0.780837 (1744.2834567079765 steps/sec)\n",
      "Step #1506\tEpoch   0 Batch 1505/3125   Loss: 0.697690 mae: 0.654659 (1664.459189180609 steps/sec)\n",
      "Step #1507\tEpoch   0 Batch 1506/3125   Loss: 0.827082 mae: 0.705370 (1740.6495630016352 steps/sec)\n",
      "Step #1508\tEpoch   0 Batch 1507/3125   Loss: 0.909620 mae: 0.758752 (1678.1780644335258 steps/sec)\n",
      "Step #1509\tEpoch   0 Batch 1508/3125   Loss: 0.886740 mae: 0.743652 (1778.785051485182 steps/sec)\n",
      "Step #1510\tEpoch   0 Batch 1509/3125   Loss: 0.795346 mae: 0.710361 (1821.56711167473 steps/sec)\n",
      "Step #1511\tEpoch   0 Batch 1510/3125   Loss: 0.970387 mae: 0.773569 (2101.818036040009 steps/sec)\n",
      "Step #1512\tEpoch   0 Batch 1511/3125   Loss: 0.930166 mae: 0.774538 (1979.565791957712 steps/sec)\n",
      "Step #1513\tEpoch   0 Batch 1512/3125   Loss: 0.929405 mae: 0.753377 (2041.3218474716505 steps/sec)\n",
      "Step #1514\tEpoch   0 Batch 1513/3125   Loss: 0.883859 mae: 0.738635 (1883.032387246231 steps/sec)\n",
      "Step #1515\tEpoch   0 Batch 1514/3125   Loss: 0.751960 mae: 0.703257 (1874.0467360707744 steps/sec)\n",
      "Step #1516\tEpoch   0 Batch 1515/3125   Loss: 0.826005 mae: 0.741008 (1955.8059070942952 steps/sec)\n",
      "Step #1517\tEpoch   0 Batch 1516/3125   Loss: 0.860760 mae: 0.760038 (1925.4411575680788 steps/sec)\n",
      "Step #1518\tEpoch   0 Batch 1517/3125   Loss: 0.902253 mae: 0.743546 (2029.3513707047541 steps/sec)\n",
      "Step #1519\tEpoch   0 Batch 1518/3125   Loss: 0.978199 mae: 0.809480 (1473.4434061687627 steps/sec)\n",
      "Step #1520\tEpoch   0 Batch 1519/3125   Loss: 0.793795 mae: 0.721491 (1816.4551808961223 steps/sec)\n",
      "Step #1521\tEpoch   0 Batch 1520/3125   Loss: 0.850644 mae: 0.744711 (1878.00732522007 steps/sec)\n",
      "Step #1522\tEpoch   0 Batch 1521/3125   Loss: 0.917283 mae: 0.754335 (1913.721768490213 steps/sec)\n",
      "Step #1523\tEpoch   0 Batch 1522/3125   Loss: 0.977428 mae: 0.801347 (2082.4705823941213 steps/sec)\n",
      "Step #1524\tEpoch   0 Batch 1523/3125   Loss: 0.829578 mae: 0.740766 (1996.736139542412 steps/sec)\n",
      "Step #1525\tEpoch   0 Batch 1524/3125   Loss: 0.785788 mae: 0.726523 (1836.6265271270306 steps/sec)\n",
      "Step #1526\tEpoch   0 Batch 1525/3125   Loss: 0.783674 mae: 0.701690 (1854.0490840936418 steps/sec)\n",
      "Step #1527\tEpoch   0 Batch 1526/3125   Loss: 0.970616 mae: 0.799743 (1769.9725703675572 steps/sec)\n",
      "Step #1528\tEpoch   0 Batch 1527/3125   Loss: 0.759437 mae: 0.675937 (1798.8797488441512 steps/sec)\n",
      "Step #1529\tEpoch   0 Batch 1528/3125   Loss: 0.918685 mae: 0.764825 (1954.3295933201625 steps/sec)\n",
      "Step #1530\tEpoch   0 Batch 1529/3125   Loss: 0.915323 mae: 0.733185 (1965.3368570009466 steps/sec)\n",
      "Step #1531\tEpoch   0 Batch 1530/3125   Loss: 0.970424 mae: 0.785272 (2029.0568520453578 steps/sec)\n",
      "Step #1532\tEpoch   0 Batch 1531/3125   Loss: 0.825828 mae: 0.733164 (2167.3973480503105 steps/sec)\n",
      "Step #1533\tEpoch   0 Batch 1532/3125   Loss: 0.820227 mae: 0.706404 (2136.180008760046 steps/sec)\n",
      "Step #1534\tEpoch   0 Batch 1533/3125   Loss: 0.965594 mae: 0.782253 (2126.1096129280804 steps/sec)\n",
      "Step #1535\tEpoch   0 Batch 1534/3125   Loss: 0.991304 mae: 0.787181 (1925.8301498677638 steps/sec)\n",
      "Step #1536\tEpoch   0 Batch 1535/3125   Loss: 0.747345 mae: 0.698198 (1847.6784550051982 steps/sec)\n",
      "Step #1537\tEpoch   0 Batch 1536/3125   Loss: 0.886127 mae: 0.752471 (2013.2788049843518 steps/sec)\n",
      "Step #1538\tEpoch   0 Batch 1537/3125   Loss: 1.044922 mae: 0.800737 (2323.405198200793 steps/sec)\n",
      "Step #1539\tEpoch   0 Batch 1538/3125   Loss: 0.875657 mae: 0.738360 (2223.090051412519 steps/sec)\n",
      "Step #1540\tEpoch   0 Batch 1539/3125   Loss: 0.869638 mae: 0.716755 (2106.0394866335937 steps/sec)\n",
      "Step #1541\tEpoch   0 Batch 1540/3125   Loss: 0.986126 mae: 0.783640 (1975.1843654344243 steps/sec)\n",
      "Step #1542\tEpoch   0 Batch 1541/3125   Loss: 0.848847 mae: 0.746800 (2253.475602548811 steps/sec)\n",
      "Step #1543\tEpoch   0 Batch 1542/3125   Loss: 0.880814 mae: 0.751323 (2261.8363010817634 steps/sec)\n",
      "Step #1544\tEpoch   0 Batch 1543/3125   Loss: 0.930068 mae: 0.770695 (1555.1162359571392 steps/sec)\n",
      "Step #1545\tEpoch   0 Batch 1544/3125   Loss: 0.980519 mae: 0.805064 (1641.0153682431376 steps/sec)\n",
      "Step #1546\tEpoch   0 Batch 1545/3125   Loss: 0.851540 mae: 0.741813 (1472.0161720526714 steps/sec)\n",
      "Step #1547\tEpoch   0 Batch 1546/3125   Loss: 0.939005 mae: 0.781789 (1654.7013942038363 steps/sec)\n",
      "Step #1548\tEpoch   0 Batch 1547/3125   Loss: 0.763859 mae: 0.682690 (1686.2879427491657 steps/sec)\n",
      "Step #1549\tEpoch   0 Batch 1548/3125   Loss: 1.008334 mae: 0.820143 (1537.5127383631846 steps/sec)\n",
      "Step #1550\tEpoch   0 Batch 1549/3125   Loss: 0.900830 mae: 0.731878 (1758.2641648640945 steps/sec)\n",
      "Step #1551\tEpoch   0 Batch 1550/3125   Loss: 0.833804 mae: 0.742541 (1847.3366630550638 steps/sec)\n",
      "Step #1552\tEpoch   0 Batch 1551/3125   Loss: 1.004396 mae: 0.800788 (1899.6802391412655 steps/sec)\n",
      "Step #1553\tEpoch   0 Batch 1552/3125   Loss: 0.742444 mae: 0.696260 (2015.000432372186 steps/sec)\n",
      "Step #1554\tEpoch   0 Batch 1553/3125   Loss: 1.033562 mae: 0.817843 (2161.8786466816487 steps/sec)\n",
      "Step #1555\tEpoch   0 Batch 1554/3125   Loss: 0.795841 mae: 0.712245 (2095.559374875095 steps/sec)\n",
      "Step #1556\tEpoch   0 Batch 1555/3125   Loss: 0.842178 mae: 0.741780 (2090.462519936204 steps/sec)\n",
      "Step #1557\tEpoch   0 Batch 1556/3125   Loss: 0.950122 mae: 0.790817 (2183.873621510169 steps/sec)\n",
      "Step #1558\tEpoch   0 Batch 1557/3125   Loss: 1.076538 mae: 0.806174 (2086.1164440111806 steps/sec)\n",
      "Step #1559\tEpoch   0 Batch 1558/3125   Loss: 0.807383 mae: 0.713427 (1902.299465725715 steps/sec)\n",
      "Step #1560\tEpoch   0 Batch 1559/3125   Loss: 0.801668 mae: 0.741703 (1904.770208900999 steps/sec)\n",
      "Step #1561\tEpoch   0 Batch 1560/3125   Loss: 0.773701 mae: 0.715103 (2083.1532104259377 steps/sec)\n",
      "Step #1562\tEpoch   0 Batch 1561/3125   Loss: 1.059772 mae: 0.788060 (2052.0279063395924 steps/sec)\n",
      "Step #1563\tEpoch   0 Batch 1562/3125   Loss: 0.829710 mae: 0.710322 (1910.1310672095162 steps/sec)\n",
      "Step #1564\tEpoch   0 Batch 1563/3125   Loss: 0.973379 mae: 0.785238 (2171.3694063075936 steps/sec)\n",
      "Step #1565\tEpoch   0 Batch 1564/3125   Loss: 0.815344 mae: 0.726655 (2095.1197338581574 steps/sec)\n",
      "Step #1566\tEpoch   0 Batch 1565/3125   Loss: 0.880568 mae: 0.758442 (2022.65752341271 steps/sec)\n",
      "Step #1567\tEpoch   0 Batch 1566/3125   Loss: 0.901813 mae: 0.770270 (2185.717263517739 steps/sec)\n",
      "Step #1568\tEpoch   0 Batch 1567/3125   Loss: 0.842369 mae: 0.746419 (1632.380596550221 steps/sec)\n",
      "Step #1569\tEpoch   0 Batch 1568/3125   Loss: 0.854049 mae: 0.707970 (1639.9246174177556 steps/sec)\n",
      "Step #1570\tEpoch   0 Batch 1569/3125   Loss: 0.845152 mae: 0.737155 (1660.5843693087338 steps/sec)\n",
      "Step #1571\tEpoch   0 Batch 1570/3125   Loss: 0.906973 mae: 0.720767 (1529.3836235815759 steps/sec)\n",
      "Step #1572\tEpoch   0 Batch 1571/3125   Loss: 1.048512 mae: 0.812014 (1681.6364496547965 steps/sec)\n",
      "Step #1573\tEpoch   0 Batch 1572/3125   Loss: 0.977492 mae: 0.783782 (1731.2947858534492 steps/sec)\n",
      "Step #1574\tEpoch   0 Batch 1573/3125   Loss: 0.758333 mae: 0.690830 (1908.4796971406731 steps/sec)\n",
      "Step #1575\tEpoch   0 Batch 1574/3125   Loss: 0.834203 mae: 0.732382 (2052.0279063395924 steps/sec)\n",
      "Step #1576\tEpoch   0 Batch 1575/3125   Loss: 0.809712 mae: 0.698863 (2027.1543599508955 steps/sec)\n",
      "Step #1577\tEpoch   0 Batch 1576/3125   Loss: 0.859598 mae: 0.715658 (1998.639079758694 steps/sec)\n",
      "Step #1578\tEpoch   0 Batch 1577/3125   Loss: 0.752493 mae: 0.677288 (2138.9470248658795 steps/sec)\n",
      "Step #1579\tEpoch   0 Batch 1578/3125   Loss: 0.724751 mae: 0.661645 (2058.210654418404 steps/sec)\n",
      "Step #1580\tEpoch   0 Batch 1579/3125   Loss: 0.827515 mae: 0.740486 (2170.8074984214395 steps/sec)\n",
      "Step #1581\tEpoch   0 Batch 1580/3125   Loss: 0.935339 mae: 0.789768 (2231.8437716170915 steps/sec)\n",
      "Step #1582\tEpoch   0 Batch 1581/3125   Loss: 1.000151 mae: 0.792998 (2402.6763209752075 steps/sec)\n",
      "Step #1583\tEpoch   0 Batch 1582/3125   Loss: 0.864614 mae: 0.744007 (2264.180603089945 steps/sec)\n",
      "Step #1584\tEpoch   0 Batch 1583/3125   Loss: 0.783894 mae: 0.714280 (1916.5024765595015 steps/sec)\n",
      "Step #1585\tEpoch   0 Batch 1584/3125   Loss: 0.761408 mae: 0.722731 (1739.552244996143 steps/sec)\n",
      "Step #1586\tEpoch   0 Batch 1585/3125   Loss: 0.781027 mae: 0.738337 (2116.304556233917 steps/sec)\n",
      "Step #1587\tEpoch   0 Batch 1586/3125   Loss: 0.880435 mae: 0.730562 (2178.3613096227355 steps/sec)\n",
      "Step #1588\tEpoch   0 Batch 1587/3125   Loss: 0.820647 mae: 0.745563 (2020.6891235643259 steps/sec)\n",
      "Step #1589\tEpoch   0 Batch 1588/3125   Loss: 0.820958 mae: 0.742074 (2017.015956065517 steps/sec)\n",
      "Step #1590\tEpoch   0 Batch 1589/3125   Loss: 0.807295 mae: 0.728540 (2116.3259127696933 steps/sec)\n",
      "Step #1591\tEpoch   0 Batch 1590/3125   Loss: 0.940779 mae: 0.783485 (2193.949031259154 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1592\tEpoch   0 Batch 1591/3125   Loss: 0.806581 mae: 0.722978 (1949.1528259273373 steps/sec)\n",
      "Step #1593\tEpoch   0 Batch 1592/3125   Loss: 0.795630 mae: 0.697685 (1933.696624344186 steps/sec)\n",
      "Step #1594\tEpoch   0 Batch 1593/3125   Loss: 0.928174 mae: 0.766168 (2017.3652060987927 steps/sec)\n",
      "Step #1595\tEpoch   0 Batch 1594/3125   Loss: 0.754958 mae: 0.697741 (2039.6343123905856 steps/sec)\n",
      "Step #1596\tEpoch   0 Batch 1595/3125   Loss: 0.955735 mae: 0.783831 (2063.983780645034 steps/sec)\n",
      "Step #1597\tEpoch   0 Batch 1596/3125   Loss: 0.990487 mae: 0.773350 (1750.6319181261165 steps/sec)\n",
      "Step #1598\tEpoch   0 Batch 1597/3125   Loss: 0.733394 mae: 0.696123 (2044.4860396194042 steps/sec)\n",
      "Step #1599\tEpoch   0 Batch 1598/3125   Loss: 0.920403 mae: 0.762092 (1806.4880696011714 steps/sec)\n",
      "Step #1600\tEpoch   0 Batch 1599/3125   Loss: 0.856506 mae: 0.726050 (1590.0765789673212 steps/sec)\n",
      "Step #1601\tEpoch   0 Batch 1600/3125   Loss: 0.810603 mae: 0.713485 (1745.9970694018916 steps/sec)\n",
      "Step #1602\tEpoch   0 Batch 1601/3125   Loss: 0.760325 mae: 0.679290 (1845.5646296817797 steps/sec)\n",
      "Step #1603\tEpoch   0 Batch 1602/3125   Loss: 0.822537 mae: 0.729639 (1973.9201641519912 steps/sec)\n",
      "Step #1604\tEpoch   0 Batch 1603/3125   Loss: 0.774410 mae: 0.718681 (1972.4534903406632 steps/sec)\n",
      "Step #1605\tEpoch   0 Batch 1604/3125   Loss: 0.844494 mae: 0.739219 (1934.3387107187987 steps/sec)\n",
      "Step #1606\tEpoch   0 Batch 1605/3125   Loss: 0.853384 mae: 0.725377 (2064.9186203365466 steps/sec)\n",
      "Step #1607\tEpoch   0 Batch 1606/3125   Loss: 0.860470 mae: 0.722797 (2138.6852679026697 steps/sec)\n",
      "Step #1608\tEpoch   0 Batch 1607/3125   Loss: 1.058857 mae: 0.799480 (1865.129847029527 steps/sec)\n",
      "Step #1609\tEpoch   0 Batch 1608/3125   Loss: 0.834411 mae: 0.734995 (1789.4857201368682 steps/sec)\n",
      "Step #1610\tEpoch   0 Batch 1609/3125   Loss: 0.917671 mae: 0.789907 (2105.532017429369 steps/sec)\n",
      "Step #1611\tEpoch   0 Batch 1610/3125   Loss: 0.816804 mae: 0.701043 (1903.542674569533 steps/sec)\n",
      "Step #1612\tEpoch   0 Batch 1611/3125   Loss: 0.760376 mae: 0.700560 (2090.733448313677 steps/sec)\n",
      "Step #1613\tEpoch   0 Batch 1612/3125   Loss: 0.909498 mae: 0.758612 (1789.5620712018294 steps/sec)\n",
      "Step #1614\tEpoch   0 Batch 1613/3125   Loss: 0.811336 mae: 0.718016 (1815.3545181479012 steps/sec)\n",
      "Step #1615\tEpoch   0 Batch 1614/3125   Loss: 0.858456 mae: 0.721735 (1938.343515754254 steps/sec)\n",
      "Step #1616\tEpoch   0 Batch 1615/3125   Loss: 0.914276 mae: 0.759797 (1811.0746485198108 steps/sec)\n",
      "Step #1617\tEpoch   0 Batch 1616/3125   Loss: 0.994794 mae: 0.792502 (1703.0078607159098 steps/sec)\n",
      "Step #1618\tEpoch   0 Batch 1617/3125   Loss: 0.941061 mae: 0.768020 (679.4970077729861 steps/sec)\n",
      "Step #1619\tEpoch   0 Batch 1618/3125   Loss: 0.753673 mae: 0.681694 (1428.130149953012 steps/sec)\n",
      "Step #1620\tEpoch   0 Batch 1619/3125   Loss: 0.919589 mae: 0.755061 (1259.2633513072092 steps/sec)\n",
      "Step #1621\tEpoch   0 Batch 1620/3125   Loss: 0.804907 mae: 0.714204 (631.4478588935258 steps/sec)\n",
      "Step #1622\tEpoch   0 Batch 1621/3125   Loss: 0.901009 mae: 0.748199 (1098.2498402756685 steps/sec)\n",
      "Step #1623\tEpoch   0 Batch 1622/3125   Loss: 0.842295 mae: 0.726417 (956.9220098924967 steps/sec)\n",
      "Step #1624\tEpoch   0 Batch 1623/3125   Loss: 0.933189 mae: 0.782283 (1300.9627791563275 steps/sec)\n",
      "Step #1625\tEpoch   0 Batch 1624/3125   Loss: 0.946651 mae: 0.781306 (1861.834710890544 steps/sec)\n",
      "Step #1626\tEpoch   0 Batch 1625/3125   Loss: 0.889452 mae: 0.764855 (2180.9438632251085 steps/sec)\n",
      "Step #1627\tEpoch   0 Batch 1626/3125   Loss: 0.796555 mae: 0.710333 (1719.2166121508735 steps/sec)\n",
      "Step #1628\tEpoch   0 Batch 1627/3125   Loss: 0.904804 mae: 0.763108 (1824.546506468536 steps/sec)\n",
      "Step #1629\tEpoch   0 Batch 1628/3125   Loss: 0.964537 mae: 0.802427 (2045.5629035719162 steps/sec)\n",
      "Step #1630\tEpoch   0 Batch 1629/3125   Loss: 0.823574 mae: 0.726952 (2001.7486589160605 steps/sec)\n",
      "Step #1631\tEpoch   0 Batch 1630/3125   Loss: 0.963988 mae: 0.770274 (1912.2559702377152 steps/sec)\n",
      "Step #1632\tEpoch   0 Batch 1631/3125   Loss: 0.946183 mae: 0.767704 (2103.652285562388 steps/sec)\n",
      "Step #1633\tEpoch   0 Batch 1632/3125   Loss: 0.896799 mae: 0.735508 (2124.946297572245 steps/sec)\n",
      "Step #1634\tEpoch   0 Batch 1633/3125   Loss: 1.009559 mae: 0.775470 (2074.2727713322056 steps/sec)\n",
      "Step #1635\tEpoch   0 Batch 1634/3125   Loss: 1.056624 mae: 0.816884 (1620.536121349808 steps/sec)\n",
      "Step #1636\tEpoch   0 Batch 1635/3125   Loss: 0.918086 mae: 0.767232 (1750.6465319342533 steps/sec)\n",
      "Step #1637\tEpoch   0 Batch 1636/3125   Loss: 0.865612 mae: 0.728359 (2063.232458384164 steps/sec)\n",
      "Step #1638\tEpoch   0 Batch 1637/3125   Loss: 0.872837 mae: 0.729908 (1931.1681016621392 steps/sec)\n",
      "Step #1639\tEpoch   0 Batch 1638/3125   Loss: 0.908397 mae: 0.731210 (2081.0034135111537 steps/sec)\n",
      "Step #1640\tEpoch   0 Batch 1639/3125   Loss: 0.825452 mae: 0.697452 (2072.7761524472207 steps/sec)\n",
      "Step #1641\tEpoch   0 Batch 1640/3125   Loss: 0.924670 mae: 0.780664 (1969.360215609124 steps/sec)\n",
      "Step #1642\tEpoch   0 Batch 1641/3125   Loss: 0.711920 mae: 0.655133 (2036.4060087587272 steps/sec)\n",
      "Step #1643\tEpoch   0 Batch 1642/3125   Loss: 0.873178 mae: 0.756791 (1839.5263365641858 steps/sec)\n",
      "Step #1644\tEpoch   0 Batch 1643/3125   Loss: 0.861539 mae: 0.725665 (2069.605550127799 steps/sec)\n",
      "Step #1645\tEpoch   0 Batch 1644/3125   Loss: 0.807506 mae: 0.700421 (1299.6486183325794 steps/sec)\n",
      "Step #1646\tEpoch   0 Batch 1645/3125   Loss: 0.914247 mae: 0.762398 (1742.718010935864 steps/sec)\n",
      "Step #1647\tEpoch   0 Batch 1646/3125   Loss: 0.782011 mae: 0.710391 (945.3955316731882 steps/sec)\n",
      "Step #1648\tEpoch   0 Batch 1647/3125   Loss: 0.862681 mae: 0.731315 (1972.0083501024956 steps/sec)\n",
      "Step #1649\tEpoch   0 Batch 1648/3125   Loss: 0.937910 mae: 0.759029 (2082.2017911396174 steps/sec)\n",
      "Step #1650\tEpoch   0 Batch 1649/3125   Loss: 0.888494 mae: 0.764430 (1419.5651585303117 steps/sec)\n",
      "Step #1651\tEpoch   0 Batch 1650/3125   Loss: 0.792903 mae: 0.711428 (1280.5080140436576 steps/sec)\n",
      "Step #1652\tEpoch   0 Batch 1651/3125   Loss: 0.864922 mae: 0.708898 (1382.5886882511554 steps/sec)\n",
      "Step #1653\tEpoch   0 Batch 1652/3125   Loss: 0.757297 mae: 0.691623 (1247.3469776183763 steps/sec)\n",
      "Step #1654\tEpoch   0 Batch 1653/3125   Loss: 0.807453 mae: 0.735927 (1815.8103451261538 steps/sec)\n",
      "Step #1655\tEpoch   0 Batch 1654/3125   Loss: 0.804527 mae: 0.720163 (1839.5263365641858 steps/sec)\n",
      "Step #1656\tEpoch   0 Batch 1655/3125   Loss: 0.951737 mae: 0.741534 (1747.0297648303497 steps/sec)\n",
      "Step #1657\tEpoch   0 Batch 1656/3125   Loss: 0.915393 mae: 0.740213 (1556.6745843230403 steps/sec)\n",
      "Step #1658\tEpoch   0 Batch 1657/3125   Loss: 0.805629 mae: 0.717301 (1281.1338159381778 steps/sec)\n",
      "Step #1659\tEpoch   0 Batch 1658/3125   Loss: 0.696146 mae: 0.671014 (2092.6527964875518 steps/sec)\n",
      "Step #1660\tEpoch   0 Batch 1659/3125   Loss: 0.712254 mae: 0.670601 (2121.5068992028487 steps/sec)\n",
      "Step #1661\tEpoch   0 Batch 1660/3125   Loss: 0.967835 mae: 0.769880 (2017.0353556726811 steps/sec)\n",
      "Step #1662\tEpoch   0 Batch 1661/3125   Loss: 0.996451 mae: 0.792749 (1670.3453549127055 steps/sec)\n",
      "Step #1663\tEpoch   0 Batch 1662/3125   Loss: 0.907688 mae: 0.749106 (1789.1651167949221 steps/sec)\n",
      "Step #1664\tEpoch   0 Batch 1663/3125   Loss: 0.869020 mae: 0.751892 (1722.7190208239208 steps/sec)\n",
      "Step #1665\tEpoch   0 Batch 1664/3125   Loss: 0.876225 mae: 0.746568 (2079.78578866465 steps/sec)\n",
      "Step #1666\tEpoch   0 Batch 1665/3125   Loss: 0.874323 mae: 0.741119 (1847.7598526833308 steps/sec)\n",
      "Step #1667\tEpoch   0 Batch 1666/3125   Loss: 0.869798 mae: 0.751560 (1833.2549499541065 steps/sec)\n",
      "Step #1668\tEpoch   0 Batch 1667/3125   Loss: 0.817354 mae: 0.739239 (1717.8083762686042 steps/sec)\n",
      "Step #1669\tEpoch   0 Batch 1668/3125   Loss: 0.782568 mae: 0.685381 (1685.59670781893 steps/sec)\n",
      "Step #1670\tEpoch   0 Batch 1669/3125   Loss: 0.938410 mae: 0.773056 (1914.9100139704337 steps/sec)\n",
      "Step #1671\tEpoch   0 Batch 1670/3125   Loss: 0.762390 mae: 0.663208 (1735.3347124534546 steps/sec)\n",
      "Step #1672\tEpoch   0 Batch 1671/3125   Loss: 0.881324 mae: 0.766544 (2040.725928088357 steps/sec)\n",
      "Step #1673\tEpoch   0 Batch 1672/3125   Loss: 0.865005 mae: 0.738593 (1960.99978493216 steps/sec)\n",
      "Step #1674\tEpoch   0 Batch 1673/3125   Loss: 0.783981 mae: 0.726765 (1741.5457693553342 steps/sec)\n",
      "Step #1675\tEpoch   0 Batch 1674/3125   Loss: 0.868807 mae: 0.736300 (1992.7517365235321 steps/sec)\n",
      "Step #1676\tEpoch   0 Batch 1675/3125   Loss: 0.837370 mae: 0.723172 (2103.483485290725 steps/sec)\n",
      "Step #1677\tEpoch   0 Batch 1676/3125   Loss: 0.788764 mae: 0.692468 (2048.400078140262 steps/sec)\n",
      "Step #1678\tEpoch   0 Batch 1677/3125   Loss: 0.756469 mae: 0.694898 (1858.3865021976465 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1679\tEpoch   0 Batch 1678/3125   Loss: 0.925125 mae: 0.748835 (1613.6779495387077 steps/sec)\n",
      "Step #1680\tEpoch   0 Batch 1679/3125   Loss: 0.893032 mae: 0.717703 (1714.508085482104 steps/sec)\n",
      "Step #1681\tEpoch   0 Batch 1680/3125   Loss: 0.855872 mae: 0.724006 (1822.4852482380443 steps/sec)\n",
      "Step #1682\tEpoch   0 Batch 1681/3125   Loss: 0.929854 mae: 0.767859 (1845.9220139072265 steps/sec)\n",
      "Step #1683\tEpoch   0 Batch 1682/3125   Loss: 0.943493 mae: 0.783927 (1847.2064898573958 steps/sec)\n",
      "Step #1684\tEpoch   0 Batch 1683/3125   Loss: 0.790700 mae: 0.714248 (1878.6297835746022 steps/sec)\n",
      "Step #1685\tEpoch   0 Batch 1684/3125   Loss: 0.682967 mae: 0.662140 (1783.8537635140308 steps/sec)\n",
      "Step #1686\tEpoch   0 Batch 1685/3125   Loss: 0.962828 mae: 0.780687 (1801.2746293783175 steps/sec)\n",
      "Step #1687\tEpoch   0 Batch 1686/3125   Loss: 0.726891 mae: 0.675239 (1566.757562400544 steps/sec)\n",
      "Step #1688\tEpoch   0 Batch 1687/3125   Loss: 0.983930 mae: 0.778173 (2017.7922315337767 steps/sec)\n",
      "Step #1689\tEpoch   0 Batch 1688/3125   Loss: 0.911727 mae: 0.767281 (2253.8873245491477 steps/sec)\n",
      "Step #1690\tEpoch   0 Batch 1689/3125   Loss: 0.825445 mae: 0.727865 (2065.4066990358197 steps/sec)\n",
      "Step #1691\tEpoch   0 Batch 1690/3125   Loss: 0.881732 mae: 0.742487 (1823.7055846391986 steps/sec)\n",
      "Step #1692\tEpoch   0 Batch 1691/3125   Loss: 0.960641 mae: 0.775672 (1816.8643384996578 steps/sec)\n",
      "Step #1693\tEpoch   0 Batch 1692/3125   Loss: 0.895994 mae: 0.767672 (2123.7627472227005 steps/sec)\n",
      "Step #1694\tEpoch   0 Batch 1693/3125   Loss: 1.090205 mae: 0.809733 (2050.8439437501224 steps/sec)\n",
      "Step #1695\tEpoch   0 Batch 1694/3125   Loss: 0.922537 mae: 0.752904 (2084.0226572592665 steps/sec)\n",
      "Step #1696\tEpoch   0 Batch 1695/3125   Loss: 0.926341 mae: 0.734387 (2104.560051380861 steps/sec)\n",
      "Step #1697\tEpoch   0 Batch 1696/3125   Loss: 0.866904 mae: 0.721317 (1820.0178777543458 steps/sec)\n",
      "Step #1698\tEpoch   0 Batch 1697/3125   Loss: 0.852608 mae: 0.747629 (1621.0622328378515 steps/sec)\n",
      "Step #1699\tEpoch   0 Batch 1698/3125   Loss: 0.816787 mae: 0.728478 (1979.5844778599005 steps/sec)\n",
      "Step #1700\tEpoch   0 Batch 1699/3125   Loss: 0.929510 mae: 0.740909 (1855.3448992772023 steps/sec)\n",
      "Step #1701\tEpoch   0 Batch 1700/3125   Loss: 0.794442 mae: 0.710128 (2010.8657506400361 steps/sec)\n",
      "Step #1702\tEpoch   0 Batch 1701/3125   Loss: 0.895193 mae: 0.765113 (1931.3993111197067 steps/sec)\n",
      "Step #1703\tEpoch   0 Batch 1702/3125   Loss: 0.848743 mae: 0.726403 (1868.9362005507483 steps/sec)\n",
      "Step #1704\tEpoch   0 Batch 1703/3125   Loss: 0.953937 mae: 0.768716 (1923.3045057273087 steps/sec)\n",
      "Step #1705\tEpoch   0 Batch 1704/3125   Loss: 0.871189 mae: 0.748437 (1975.5006688144088 steps/sec)\n",
      "Step #1706\tEpoch   0 Batch 1705/3125   Loss: 0.873010 mae: 0.754973 (1562.9393352213444 steps/sec)\n",
      "Step #1707\tEpoch   0 Batch 1706/3125   Loss: 0.804006 mae: 0.723408 (1461.205947520241 steps/sec)\n",
      "Step #1708\tEpoch   0 Batch 1707/3125   Loss: 0.941713 mae: 0.779369 (1203.8068997187302 steps/sec)\n",
      "Step #1709\tEpoch   0 Batch 1708/3125   Loss: 0.874182 mae: 0.731481 (1762.9644574464508 steps/sec)\n",
      "Step #1710\tEpoch   0 Batch 1709/3125   Loss: 0.742151 mae: 0.691281 (1779.554846538308 steps/sec)\n",
      "Step #1711\tEpoch   0 Batch 1710/3125   Loss: 0.818580 mae: 0.698693 (1962.320928970441 steps/sec)\n",
      "Step #1712\tEpoch   0 Batch 1711/3125   Loss: 0.808516 mae: 0.712846 (1991.6919131962582 steps/sec)\n",
      "Step #1713\tEpoch   0 Batch 1712/3125   Loss: 0.738658 mae: 0.679154 (2206.413601548691 steps/sec)\n",
      "Step #1714\tEpoch   0 Batch 1713/3125   Loss: 0.813545 mae: 0.725693 (2337.28462207164 steps/sec)\n",
      "Step #1715\tEpoch   0 Batch 1714/3125   Loss: 0.889989 mae: 0.771493 (2033.4244768941378 steps/sec)\n",
      "Step #1716\tEpoch   0 Batch 1715/3125   Loss: 0.766886 mae: 0.674664 (1999.0201031370045 steps/sec)\n",
      "Step #1717\tEpoch   0 Batch 1716/3125   Loss: 1.036765 mae: 0.786000 (1884.860196111915 steps/sec)\n",
      "Step #1718\tEpoch   0 Batch 1717/3125   Loss: 0.879068 mae: 0.734377 (1983.666442807011 steps/sec)\n",
      "Step #1719\tEpoch   0 Batch 1718/3125   Loss: 0.868070 mae: 0.718140 (1669.7867732534994 steps/sec)\n",
      "Step #1720\tEpoch   0 Batch 1719/3125   Loss: 0.863448 mae: 0.752420 (2054.9031904052677 steps/sec)\n",
      "Step #1721\tEpoch   0 Batch 1720/3125   Loss: 0.924482 mae: 0.756927 (1900.9372563949166 steps/sec)\n",
      "Step #1722\tEpoch   0 Batch 1721/3125   Loss: 0.905858 mae: 0.745202 (2303.4994837547506 steps/sec)\n",
      "Step #1723\tEpoch   0 Batch 1722/3125   Loss: 0.767655 mae: 0.703663 (2158.607557152121 steps/sec)\n",
      "Step #1724\tEpoch   0 Batch 1723/3125   Loss: 0.933961 mae: 0.739015 (1821.836124817569 steps/sec)\n",
      "Step #1725\tEpoch   0 Batch 1724/3125   Loss: 1.008197 mae: 0.813173 (1953.6013712411968 steps/sec)\n",
      "Step #1726\tEpoch   0 Batch 1725/3125   Loss: 0.810962 mae: 0.699072 (2102.7874704207275 steps/sec)\n",
      "Step #1727\tEpoch   0 Batch 1726/3125   Loss: 0.906485 mae: 0.775193 (1884.2336028751124 steps/sec)\n",
      "Step #1728\tEpoch   0 Batch 1727/3125   Loss: 0.769184 mae: 0.702083 (1941.4478800222182 steps/sec)\n",
      "Step #1729\tEpoch   0 Batch 1728/3125   Loss: 0.970819 mae: 0.788525 (2004.7337730618488 steps/sec)\n",
      "Step #1730\tEpoch   0 Batch 1729/3125   Loss: 0.805223 mae: 0.712794 (2044.326600639476 steps/sec)\n",
      "Step #1731\tEpoch   0 Batch 1730/3125   Loss: 0.873239 mae: 0.750492 (2111.85047933618 steps/sec)\n",
      "Step #1732\tEpoch   0 Batch 1731/3125   Loss: 0.874505 mae: 0.728052 (2024.3561527472102 steps/sec)\n",
      "Step #1733\tEpoch   0 Batch 1732/3125   Loss: 0.932848 mae: 0.741823 (2220.641895826936 steps/sec)\n",
      "Step #1734\tEpoch   0 Batch 1733/3125   Loss: 0.866139 mae: 0.747577 (2115.578690393326 steps/sec)\n",
      "Step #1735\tEpoch   0 Batch 1734/3125   Loss: 0.887055 mae: 0.759930 (1742.4718540982926 steps/sec)\n",
      "Step #1736\tEpoch   0 Batch 1735/3125   Loss: 0.952617 mae: 0.801706 (2063.394860088945 steps/sec)\n",
      "Step #1737\tEpoch   0 Batch 1736/3125   Loss: 0.760350 mae: 0.686947 (2249.680326110277 steps/sec)\n",
      "Step #1738\tEpoch   0 Batch 1737/3125   Loss: 0.788866 mae: 0.710543 (2121.8932755934193 steps/sec)\n",
      "Step #1739\tEpoch   0 Batch 1738/3125   Loss: 0.897206 mae: 0.734935 (1829.0977279665083 steps/sec)\n",
      "Step #1740\tEpoch   0 Batch 1739/3125   Loss: 0.840056 mae: 0.715507 (2071.7727834033094 steps/sec)\n",
      "Step #1741\tEpoch   0 Batch 1740/3125   Loss: 0.965939 mae: 0.770247 (2046.0817983140805 steps/sec)\n",
      "Step #1742\tEpoch   0 Batch 1741/3125   Loss: 0.866693 mae: 0.732285 (2046.740774718679 steps/sec)\n",
      "Step #1743\tEpoch   0 Batch 1742/3125   Loss: 0.948066 mae: 0.770022 (1876.8644227068921 steps/sec)\n",
      "Step #1744\tEpoch   0 Batch 1743/3125   Loss: 0.888182 mae: 0.768163 (1713.6674892546046 steps/sec)\n",
      "Step #1745\tEpoch   0 Batch 1744/3125   Loss: 0.890324 mae: 0.760281 (1913.7916244604448 steps/sec)\n",
      "Step #1746\tEpoch   0 Batch 1745/3125   Loss: 0.728540 mae: 0.685746 (2222.807296469416 steps/sec)\n",
      "Step #1747\tEpoch   0 Batch 1746/3125   Loss: 0.856894 mae: 0.734677 (2151.101628851598 steps/sec)\n",
      "Step #1748\tEpoch   0 Batch 1747/3125   Loss: 0.852724 mae: 0.751969 (1842.5810079426442 steps/sec)\n",
      "Step #1749\tEpoch   0 Batch 1748/3125   Loss: 0.723793 mae: 0.674963 (2021.760549123196 steps/sec)\n",
      "Step #1750\tEpoch   0 Batch 1749/3125   Loss: 0.872332 mae: 0.750937 (1882.2549521168223 steps/sec)\n",
      "Step #1751\tEpoch   0 Batch 1750/3125   Loss: 0.755000 mae: 0.702166 (1124.1527922210191 steps/sec)\n",
      "Step #1752\tEpoch   0 Batch 1751/3125   Loss: 0.935419 mae: 0.776205 (772.6194949785215 steps/sec)\n",
      "Step #1753\tEpoch   0 Batch 1752/3125   Loss: 0.833556 mae: 0.723687 (1417.1861062305718 steps/sec)\n",
      "Step #1754\tEpoch   0 Batch 1753/3125   Loss: 0.681572 mae: 0.656066 (692.7993075812584 steps/sec)\n",
      "Step #1755\tEpoch   0 Batch 1754/3125   Loss: 0.926916 mae: 0.756808 (1105.39321104786 steps/sec)\n",
      "Step #1756\tEpoch   0 Batch 1755/3125   Loss: 0.790062 mae: 0.723305 (1089.1354023848103 steps/sec)\n",
      "Step #1757\tEpoch   0 Batch 1756/3125   Loss: 0.965896 mae: 0.792857 (877.6861233526269 steps/sec)\n",
      "Step #1758\tEpoch   0 Batch 1757/3125   Loss: 0.808551 mae: 0.740020 (1417.4064086187202 steps/sec)\n",
      "Step #1759\tEpoch   0 Batch 1758/3125   Loss: 0.942993 mae: 0.790472 (1532.9722301411518 steps/sec)\n",
      "Step #1760\tEpoch   0 Batch 1759/3125   Loss: 1.062588 mae: 0.814881 (1686.844052636659 steps/sec)\n",
      "Step #1761\tEpoch   0 Batch 1760/3125   Loss: 0.872580 mae: 0.758387 (1690.161186331399 steps/sec)\n",
      "Step #1762\tEpoch   0 Batch 1761/3125   Loss: 0.806408 mae: 0.732376 (1975.4820599289744 steps/sec)\n",
      "Step #1763\tEpoch   0 Batch 1762/3125   Loss: 0.742319 mae: 0.697877 (2228.2866705626097 steps/sec)\n",
      "Step #1764\tEpoch   0 Batch 1763/3125   Loss: 0.804116 mae: 0.736085 (1833.2389244379174 steps/sec)\n",
      "Step #1765\tEpoch   0 Batch 1764/3125   Loss: 1.032277 mae: 0.824851 (1713.2614964830443 steps/sec)\n",
      "Step #1766\tEpoch   0 Batch 1765/3125   Loss: 0.847275 mae: 0.749331 (2226.157847248023 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1767\tEpoch   0 Batch 1766/3125   Loss: 0.839022 mae: 0.762567 (1699.984598299328 steps/sec)\n",
      "Step #1768\tEpoch   0 Batch 1767/3125   Loss: 0.762001 mae: 0.695169 (1883.810464855154 steps/sec)\n",
      "Step #1769\tEpoch   0 Batch 1768/3125   Loss: 0.837673 mae: 0.713148 (1533.3196853157078 steps/sec)\n",
      "Step #1770\tEpoch   0 Batch 1769/3125   Loss: 0.928194 mae: 0.755583 (1618.4225960796418 steps/sec)\n",
      "Step #1771\tEpoch   0 Batch 1770/3125   Loss: 0.997715 mae: 0.779854 (1980.8933682192144 steps/sec)\n",
      "Step #1772\tEpoch   0 Batch 1771/3125   Loss: 0.899838 mae: 0.759181 (2117.9502716677775 steps/sec)\n",
      "Step #1773\tEpoch   0 Batch 1772/3125   Loss: 0.956371 mae: 0.771887 (1777.2022745184445 steps/sec)\n",
      "Step #1774\tEpoch   0 Batch 1773/3125   Loss: 0.764408 mae: 0.687640 (2049.601250977326 steps/sec)\n",
      "Step #1775\tEpoch   0 Batch 1774/3125   Loss: 0.784529 mae: 0.688021 (1653.4880787183045 steps/sec)\n",
      "Step #1776\tEpoch   0 Batch 1775/3125   Loss: 0.919280 mae: 0.760454 (1678.5810334891464 steps/sec)\n",
      "Step #1777\tEpoch   0 Batch 1776/3125   Loss: 0.900457 mae: 0.742296 (1892.5827324495303 steps/sec)\n",
      "Step #1778\tEpoch   0 Batch 1777/3125   Loss: 0.869386 mae: 0.763507 (2165.9870690545536 steps/sec)\n",
      "Step #1779\tEpoch   0 Batch 1778/3125   Loss: 0.874693 mae: 0.742171 (2316.9622043242407 steps/sec)\n",
      "Step #1780\tEpoch   0 Batch 1779/3125   Loss: 0.797545 mae: 0.694890 (2102.3447916353393 steps/sec)\n",
      "Step #1781\tEpoch   0 Batch 1780/3125   Loss: 0.890043 mae: 0.757246 (1997.8394032637586 steps/sec)\n",
      "Step #1782\tEpoch   0 Batch 1781/3125   Loss: 0.785612 mae: 0.702637 (1876.9316137577975 steps/sec)\n",
      "Step #1783\tEpoch   0 Batch 1782/3125   Loss: 0.852660 mae: 0.755838 (1938.2718559664315 steps/sec)\n",
      "Step #1784\tEpoch   0 Batch 1783/3125   Loss: 0.948870 mae: 0.754415 (1918.7293570846944 steps/sec)\n",
      "Step #1785\tEpoch   0 Batch 1784/3125   Loss: 0.997998 mae: 0.786180 (1863.323530195737 steps/sec)\n",
      "Step #1786\tEpoch   0 Batch 1785/3125   Loss: 0.959530 mae: 0.757683 (1869.9360683364393 steps/sec)\n",
      "Step #1787\tEpoch   0 Batch 1786/3125   Loss: 0.785900 mae: 0.708904 (1860.810463083735 steps/sec)\n",
      "Step #1788\tEpoch   0 Batch 1787/3125   Loss: 0.785539 mae: 0.693996 (1539.4087982911378 steps/sec)\n",
      "Step #1789\tEpoch   0 Batch 1788/3125   Loss: 0.904985 mae: 0.752773 (1785.5852327393166 steps/sec)\n",
      "Step #1790\tEpoch   0 Batch 1789/3125   Loss: 0.770513 mae: 0.699359 (1829.959598956379 steps/sec)\n",
      "Step #1791\tEpoch   0 Batch 1790/3125   Loss: 0.894996 mae: 0.748379 (1483.9949617175448 steps/sec)\n",
      "Step #1792\tEpoch   0 Batch 1791/3125   Loss: 0.959552 mae: 0.768726 (1927.6002794220376 steps/sec)\n",
      "Step #1793\tEpoch   0 Batch 1792/3125   Loss: 0.866019 mae: 0.730449 (1882.6097885022803 steps/sec)\n",
      "Step #1794\tEpoch   0 Batch 1793/3125   Loss: 0.752267 mae: 0.674110 (2116.4113432233326 steps/sec)\n",
      "Step #1795\tEpoch   0 Batch 1794/3125   Loss: 0.820374 mae: 0.699878 (2187.1533607967876 steps/sec)\n",
      "Step #1796\tEpoch   0 Batch 1795/3125   Loss: 0.942388 mae: 0.765899 (2277.186353074033 steps/sec)\n",
      "Step #1797\tEpoch   0 Batch 1796/3125   Loss: 0.848085 mae: 0.733957 (2128.5696886037917 steps/sec)\n",
      "Step #1798\tEpoch   0 Batch 1797/3125   Loss: 0.892486 mae: 0.747710 (2294.1005305475032 steps/sec)\n",
      "Step #1799\tEpoch   0 Batch 1798/3125   Loss: 1.055065 mae: 0.798812 (1863.9362912400454 steps/sec)\n",
      "Step #1800\tEpoch   0 Batch 1799/3125   Loss: 0.989338 mae: 0.786993 (1830.1033230941077 steps/sec)\n",
      "Step #1801\tEpoch   0 Batch 1800/3125   Loss: 0.805489 mae: 0.724959 (1976.357056694813 steps/sec)\n",
      "Step #1802\tEpoch   0 Batch 1801/3125   Loss: 0.830655 mae: 0.723062 (1812.123149772313 steps/sec)\n",
      "Step #1803\tEpoch   0 Batch 1802/3125   Loss: 0.856338 mae: 0.745914 (1718.3291545810596 steps/sec)\n",
      "Step #1804\tEpoch   0 Batch 1803/3125   Loss: 0.777015 mae: 0.706241 (1254.9229867036872 steps/sec)\n",
      "Step #1805\tEpoch   0 Batch 1804/3125   Loss: 0.915944 mae: 0.777793 (1332.0917469653757 steps/sec)\n",
      "Step #1806\tEpoch   0 Batch 1805/3125   Loss: 0.898136 mae: 0.750245 (727.4200485605272 steps/sec)\n",
      "Step #1807\tEpoch   0 Batch 1806/3125   Loss: 0.822206 mae: 0.716529 (1443.1268923754474 steps/sec)\n",
      "Step #1808\tEpoch   0 Batch 1807/3125   Loss: 0.918249 mae: 0.770044 (1757.1739786171531 steps/sec)\n",
      "Step #1809\tEpoch   0 Batch 1808/3125   Loss: 0.813064 mae: 0.702589 (1541.2642301219253 steps/sec)\n",
      "Step #1810\tEpoch   0 Batch 1809/3125   Loss: 0.855557 mae: 0.735606 (2037.3358203155358 steps/sec)\n",
      "Step #1811\tEpoch   0 Batch 1810/3125   Loss: 0.968739 mae: 0.777589 (2149.1396890788164 steps/sec)\n",
      "Step #1812\tEpoch   0 Batch 1811/3125   Loss: 0.920279 mae: 0.771238 (2190.8777501514805 steps/sec)\n",
      "Step #1813\tEpoch   0 Batch 1812/3125   Loss: 0.812709 mae: 0.724324 (2137.59530313532 steps/sec)\n",
      "Step #1814\tEpoch   0 Batch 1813/3125   Loss: 0.815737 mae: 0.715231 (1815.8103451261538 steps/sec)\n",
      "Step #1815\tEpoch   0 Batch 1814/3125   Loss: 0.837281 mae: 0.728960 (1906.0686207680073 steps/sec)\n",
      "Step #1816\tEpoch   0 Batch 1815/3125   Loss: 0.836550 mae: 0.713388 (1928.5752384105351 steps/sec)\n",
      "Step #1817\tEpoch   0 Batch 1816/3125   Loss: 0.913623 mae: 0.744989 (2130.104720018689 steps/sec)\n",
      "Step #1818\tEpoch   0 Batch 1817/3125   Loss: 0.841662 mae: 0.727111 (1848.4927546451363 steps/sec)\n",
      "Step #1819\tEpoch   0 Batch 1818/3125   Loss: 0.970528 mae: 0.810631 (2184.4650687999333 steps/sec)\n",
      "Step #1820\tEpoch   0 Batch 1819/3125   Loss: 0.836889 mae: 0.731666 (2071.3226070895926 steps/sec)\n",
      "Step #1821\tEpoch   0 Batch 1820/3125   Loss: 0.825009 mae: 0.721553 (2115.471987410979 steps/sec)\n",
      "Step #1822\tEpoch   0 Batch 1821/3125   Loss: 0.854028 mae: 0.729884 (1827.2331230613738 steps/sec)\n",
      "Step #1823\tEpoch   0 Batch 1822/3125   Loss: 0.868588 mae: 0.738931 (1619.0723241306898 steps/sec)\n",
      "Step #1824\tEpoch   0 Batch 1823/3125   Loss: 0.887822 mae: 0.750802 (1617.7983491475738 steps/sec)\n",
      "Step #1825\tEpoch   0 Batch 1824/3125   Loss: 0.795501 mae: 0.719175 (1495.487477893776 steps/sec)\n",
      "Step #1826\tEpoch   0 Batch 1825/3125   Loss: 0.998178 mae: 0.802992 (1777.337830737156 steps/sec)\n",
      "Step #1827\tEpoch   0 Batch 1826/3125   Loss: 0.924039 mae: 0.765397 (1926.555509622893 steps/sec)\n",
      "Step #1828\tEpoch   0 Batch 1827/3125   Loss: 0.899505 mae: 0.759309 (1960.7797671918097 steps/sec)\n",
      "Step #1829\tEpoch   0 Batch 1828/3125   Loss: 0.769168 mae: 0.687620 (1965.4657919400188 steps/sec)\n",
      "Step #1830\tEpoch   0 Batch 1829/3125   Loss: 0.869218 mae: 0.759125 (2065.6711713486466 steps/sec)\n",
      "Step #1831\tEpoch   0 Batch 1830/3125   Loss: 0.876971 mae: 0.745677 (2114.256333739956 steps/sec)\n",
      "Step #1832\tEpoch   0 Batch 1831/3125   Loss: 0.775554 mae: 0.697847 (1985.0558936836824 steps/sec)\n",
      "Step #1833\tEpoch   0 Batch 1832/3125   Loss: 0.756327 mae: 0.709166 (2082.6360268925587 steps/sec)\n",
      "Step #1834\tEpoch   0 Batch 1833/3125   Loss: 0.830462 mae: 0.733427 (2001.366594774111 steps/sec)\n",
      "Step #1835\tEpoch   0 Batch 1834/3125   Loss: 0.864530 mae: 0.755089 (1833.8480910823903 steps/sec)\n",
      "Step #1836\tEpoch   0 Batch 1835/3125   Loss: 0.981281 mae: 0.778487 (1918.7820231298492 steps/sec)\n",
      "Step #1837\tEpoch   0 Batch 1836/3125   Loss: 0.910519 mae: 0.768805 (1923.2868672046955 steps/sec)\n",
      "Step #1838\tEpoch   0 Batch 1837/3125   Loss: 0.807281 mae: 0.749078 (1845.9707588440854 steps/sec)\n",
      "Step #1839\tEpoch   0 Batch 1838/3125   Loss: 0.873917 mae: 0.751994 (1728.198831469563 steps/sec)\n",
      "Step #1840\tEpoch   0 Batch 1839/3125   Loss: 0.784630 mae: 0.703686 (1940.3880494823231 steps/sec)\n",
      "Step #1841\tEpoch   0 Batch 1840/3125   Loss: 0.868759 mae: 0.720711 (1745.2705514222466 steps/sec)\n",
      "Step #1842\tEpoch   0 Batch 1841/3125   Loss: 0.872741 mae: 0.730394 (1618.2227846538474 steps/sec)\n",
      "Step #1843\tEpoch   0 Batch 1842/3125   Loss: 0.788224 mae: 0.693619 (1560.5551214793318 steps/sec)\n",
      "Step #1844\tEpoch   0 Batch 1843/3125   Loss: 0.645030 mae: 0.638368 (1539.4992035118885 steps/sec)\n",
      "Step #1845\tEpoch   0 Batch 1844/3125   Loss: 0.871564 mae: 0.736211 (1471.8818649504144 steps/sec)\n",
      "Step #1846\tEpoch   0 Batch 1845/3125   Loss: 0.880432 mae: 0.764346 (1157.6753223812048 steps/sec)\n",
      "Step #1847\tEpoch   0 Batch 1846/3125   Loss: 1.077744 mae: 0.819680 (1485.9296838465573 steps/sec)\n",
      "Step #1848\tEpoch   0 Batch 1847/3125   Loss: 0.876362 mae: 0.743195 (1667.9009989183687 steps/sec)\n",
      "Step #1849\tEpoch   0 Batch 1848/3125   Loss: 1.002238 mae: 0.802031 (1904.4761480970242 steps/sec)\n",
      "Step #1850\tEpoch   0 Batch 1849/3125   Loss: 0.804255 mae: 0.704571 (1934.6063725761517 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1851\tEpoch   0 Batch 1850/3125   Loss: 0.807011 mae: 0.722036 (1837.9142018316463 steps/sec)\n",
      "Step #1852\tEpoch   0 Batch 1851/3125   Loss: 0.874264 mae: 0.747875 (2006.6135946111451 steps/sec)\n",
      "Step #1853\tEpoch   0 Batch 1852/3125   Loss: 0.959777 mae: 0.796580 (2130.840589723529 steps/sec)\n",
      "Step #1854\tEpoch   0 Batch 1853/3125   Loss: 0.865727 mae: 0.742832 (1974.719397363465 steps/sec)\n",
      "Step #1855\tEpoch   0 Batch 1854/3125   Loss: 0.953872 mae: 0.777718 (1658.1029261774681 steps/sec)\n",
      "Step #1856\tEpoch   0 Batch 1855/3125   Loss: 0.811270 mae: 0.702450 (1747.8160134014518 steps/sec)\n",
      "Step #1857\tEpoch   0 Batch 1856/3125   Loss: 0.924133 mae: 0.753933 (2037.810945273632 steps/sec)\n",
      "Step #1858\tEpoch   0 Batch 1857/3125   Loss: 0.929256 mae: 0.762365 (2109.78963994326 steps/sec)\n",
      "Step #1859\tEpoch   0 Batch 1858/3125   Loss: 0.792148 mae: 0.694171 (1518.5089713698173 steps/sec)\n",
      "Step #1860\tEpoch   0 Batch 1859/3125   Loss: 0.961029 mae: 0.760149 (1649.8328259108039 steps/sec)\n",
      "Step #1861\tEpoch   0 Batch 1860/3125   Loss: 0.917672 mae: 0.765713 (1841.0120003862596 steps/sec)\n",
      "Step #1862\tEpoch   0 Batch 1861/3125   Loss: 0.923664 mae: 0.745509 (1868.103793837575 steps/sec)\n",
      "Step #1863\tEpoch   0 Batch 1862/3125   Loss: 0.739968 mae: 0.678848 (1838.4942447115343 steps/sec)\n",
      "Step #1864\tEpoch   0 Batch 1863/3125   Loss: 0.792569 mae: 0.691743 (1531.7629701046665 steps/sec)\n",
      "Step #1865\tEpoch   0 Batch 1864/3125   Loss: 0.741804 mae: 0.684649 (1378.5896938661479 steps/sec)\n",
      "Step #1866\tEpoch   0 Batch 1865/3125   Loss: 0.727554 mae: 0.694417 (1606.0161891852567 steps/sec)\n",
      "Step #1867\tEpoch   0 Batch 1866/3125   Loss: 0.841124 mae: 0.743707 (1566.4649905137512 steps/sec)\n",
      "Step #1868\tEpoch   0 Batch 1867/3125   Loss: 0.737774 mae: 0.697846 (1772.0963639590004 steps/sec)\n",
      "Step #1869\tEpoch   0 Batch 1868/3125   Loss: 0.946928 mae: 0.791020 (2030.3730310100784 steps/sec)\n",
      "Step #1870\tEpoch   0 Batch 1869/3125   Loss: 0.896929 mae: 0.756893 (2026.370865662412 steps/sec)\n",
      "Step #1871\tEpoch   0 Batch 1870/3125   Loss: 0.948204 mae: 0.773163 (1918.3607757043542 steps/sec)\n",
      "Step #1872\tEpoch   0 Batch 1871/3125   Loss: 0.771433 mae: 0.709833 (2026.0184907884186 steps/sec)\n",
      "Step #1873\tEpoch   0 Batch 1872/3125   Loss: 0.891946 mae: 0.749013 (1942.4189096567438 steps/sec)\n",
      "Step #1874\tEpoch   0 Batch 1873/3125   Loss: 0.724064 mae: 0.689489 (1790.3106565703993 steps/sec)\n",
      "Step #1875\tEpoch   0 Batch 1874/3125   Loss: 0.935585 mae: 0.767357 (2005.212984653631 steps/sec)\n",
      "Step #1876\tEpoch   0 Batch 1875/3125   Loss: 0.879927 mae: 0.747304 (1992.3352428724788 steps/sec)\n",
      "Step #1877\tEpoch   0 Batch 1876/3125   Loss: 0.941575 mae: 0.775254 (1923.2163163493635 steps/sec)\n",
      "Step #1878\tEpoch   0 Batch 1877/3125   Loss: 0.857034 mae: 0.728607 (1829.7041451093642 steps/sec)\n",
      "Step #1879\tEpoch   0 Batch 1878/3125   Loss: 0.861774 mae: 0.749768 (1822.56444127718 steps/sec)\n",
      "Step #1880\tEpoch   0 Batch 1879/3125   Loss: 0.774936 mae: 0.684011 (1758.0872860184766 steps/sec)\n",
      "Step #1881\tEpoch   0 Batch 1880/3125   Loss: 0.863128 mae: 0.738497 (1933.1975184594537 steps/sec)\n",
      "Step #1882\tEpoch   0 Batch 1881/3125   Loss: 0.777974 mae: 0.706774 (1643.6004545632666 steps/sec)\n",
      "Step #1883\tEpoch   0 Batch 1882/3125   Loss: 0.829109 mae: 0.724631 (1125.1902007704607 steps/sec)\n",
      "Step #1884\tEpoch   0 Batch 1883/3125   Loss: 0.918617 mae: 0.759772 (1580.6685509704164 steps/sec)\n",
      "Step #1885\tEpoch   0 Batch 1884/3125   Loss: 0.984082 mae: 0.794083 (1648.0306792820545 steps/sec)\n",
      "Step #1886\tEpoch   0 Batch 1885/3125   Loss: 0.916067 mae: 0.742845 (1806.7526470410862 steps/sec)\n",
      "Step #1887\tEpoch   0 Batch 1886/3125   Loss: 0.842452 mae: 0.737748 (1619.3848791147695 steps/sec)\n",
      "Step #1888\tEpoch   0 Batch 1887/3125   Loss: 0.779863 mae: 0.689211 (1861.7851245538964 steps/sec)\n",
      "Step #1889\tEpoch   0 Batch 1888/3125   Loss: 0.870489 mae: 0.729183 (1708.9335625870906 steps/sec)\n",
      "Step #1890\tEpoch   0 Batch 1889/3125   Loss: 0.953866 mae: 0.771834 (1928.5575030806865 steps/sec)\n",
      "Step #1891\tEpoch   0 Batch 1890/3125   Loss: 0.750103 mae: 0.671856 (1917.8520151075913 steps/sec)\n",
      "Step #1892\tEpoch   0 Batch 1891/3125   Loss: 0.884920 mae: 0.743522 (1749.2301276169821 steps/sec)\n",
      "Step #1893\tEpoch   0 Batch 1892/3125   Loss: 0.842484 mae: 0.713478 (2087.7778773307846 steps/sec)\n",
      "Step #1894\tEpoch   0 Batch 1893/3125   Loss: 0.849777 mae: 0.759163 (2095.8316260755723 steps/sec)\n",
      "Step #1895\tEpoch   0 Batch 1894/3125   Loss: 0.769372 mae: 0.699210 (2009.3821861106853 steps/sec)\n",
      "Step #1896\tEpoch   0 Batch 1895/3125   Loss: 0.784171 mae: 0.710751 (2105.2572403754457 steps/sec)\n",
      "Step #1897\tEpoch   0 Batch 1896/3125   Loss: 0.979827 mae: 0.789978 (1830.646484750083 steps/sec)\n",
      "Step #1898\tEpoch   0 Batch 1897/3125   Loss: 0.874863 mae: 0.763142 (1784.2483643448445 steps/sec)\n",
      "Step #1899\tEpoch   0 Batch 1898/3125   Loss: 0.805922 mae: 0.739343 (1851.6590439527452 steps/sec)\n",
      "Step #1900\tEpoch   0 Batch 1899/3125   Loss: 0.924376 mae: 0.749002 (1596.7230339345672 steps/sec)\n",
      "Step #1901\tEpoch   0 Batch 1900/3125   Loss: 0.727770 mae: 0.668604 (1633.9322165952474 steps/sec)\n",
      "Step #1902\tEpoch   0 Batch 1901/3125   Loss: 0.930624 mae: 0.770380 (1757.2917714094185 steps/sec)\n",
      "Step #1903\tEpoch   0 Batch 1902/3125   Loss: 0.843862 mae: 0.720431 (1818.6447439166102 steps/sec)\n",
      "Step #1904\tEpoch   0 Batch 1903/3125   Loss: 0.905847 mae: 0.776371 (1463.4900696450752 steps/sec)\n",
      "Step #1905\tEpoch   0 Batch 1904/3125   Loss: 0.911573 mae: 0.763891 (1785.5396246977489 steps/sec)\n",
      "Step #1906\tEpoch   0 Batch 1905/3125   Loss: 0.786716 mae: 0.711235 (2043.808595653445 steps/sec)\n",
      "Step #1907\tEpoch   0 Batch 1906/3125   Loss: 0.979612 mae: 0.821650 (2096.0829976711875 steps/sec)\n",
      "Step #1908\tEpoch   0 Batch 1907/3125   Loss: 0.934475 mae: 0.762596 (2130.7323417052753 steps/sec)\n",
      "Step #1909\tEpoch   0 Batch 1908/3125   Loss: 0.860875 mae: 0.744900 (2077.0050510052492 steps/sec)\n",
      "Step #1910\tEpoch   0 Batch 1909/3125   Loss: 0.772876 mae: 0.681365 (2067.3205642577605 steps/sec)\n",
      "Step #1911\tEpoch   0 Batch 1910/3125   Loss: 0.799062 mae: 0.731813 (2166.747943959995 steps/sec)\n",
      "Step #1912\tEpoch   0 Batch 1911/3125   Loss: 0.858480 mae: 0.742679 (2010.5380220117345 steps/sec)\n",
      "Step #1913\tEpoch   0 Batch 1912/3125   Loss: 0.916257 mae: 0.770779 (1750.8219166645795 steps/sec)\n",
      "Step #1914\tEpoch   0 Batch 1913/3125   Loss: 0.879846 mae: 0.738434 (1993.111575746056 steps/sec)\n",
      "Step #1915\tEpoch   0 Batch 1914/3125   Loss: 0.889804 mae: 0.733566 (1968.2881732944147 steps/sec)\n",
      "Step #1916\tEpoch   0 Batch 1915/3125   Loss: 0.731226 mae: 0.684501 (2115.578690393326 steps/sec)\n",
      "Step #1917\tEpoch   0 Batch 1916/3125   Loss: 0.835281 mae: 0.717834 (1633.0161498808616 steps/sec)\n",
      "Step #1918\tEpoch   0 Batch 1917/3125   Loss: 0.824154 mae: 0.733201 (1971.4522072647965 steps/sec)\n",
      "Step #1919\tEpoch   0 Batch 1918/3125   Loss: 0.784962 mae: 0.700471 (1970.6926524897337 steps/sec)\n",
      "Step #1920\tEpoch   0 Batch 1919/3125   Loss: 0.773326 mae: 0.696343 (2234.3405071382913 steps/sec)\n",
      "Step #1921\tEpoch   0 Batch 1920/3125   Loss: 0.828789 mae: 0.692787 (1963.0189173757171 steps/sec)\n",
      "Step #1922\tEpoch   0 Batch 1921/3125   Loss: 0.730573 mae: 0.677824 (1855.4926387315968 steps/sec)\n",
      "Step #1923\tEpoch   0 Batch 1922/3125   Loss: 0.956955 mae: 0.776266 (2006.2488639733667 steps/sec)\n",
      "Step #1924\tEpoch   0 Batch 1923/3125   Loss: 0.862205 mae: 0.738690 (2125.4847112003 steps/sec)\n",
      "Step #1925\tEpoch   0 Batch 1924/3125   Loss: 1.012059 mae: 0.806194 (2103.8633240035715 steps/sec)\n",
      "Step #1926\tEpoch   0 Batch 1925/3125   Loss: 0.800225 mae: 0.704415 (1818.1717297822167 steps/sec)\n",
      "Step #1927\tEpoch   0 Batch 1926/3125   Loss: 0.820505 mae: 0.723579 (1893.6593646723131 steps/sec)\n",
      "Step #1928\tEpoch   0 Batch 1927/3125   Loss: 0.925595 mae: 0.765631 (2000.7365076942158 steps/sec)\n",
      "Step #1929\tEpoch   0 Batch 1928/3125   Loss: 0.853904 mae: 0.739881 (1631.326669518883 steps/sec)\n",
      "Step #1930\tEpoch   0 Batch 1929/3125   Loss: 0.839449 mae: 0.735658 (1666.5490551343792 steps/sec)\n",
      "Step #1931\tEpoch   0 Batch 1930/3125   Loss: 0.948654 mae: 0.779748 (2079.0023098351394 steps/sec)\n",
      "Step #1932\tEpoch   0 Batch 1931/3125   Loss: 0.682087 mae: 0.675489 (2017.2681800692574 steps/sec)\n",
      "Step #1933\tEpoch   0 Batch 1932/3125   Loss: 0.940673 mae: 0.779165 (2270.677147621213 steps/sec)\n",
      "Step #1934\tEpoch   0 Batch 1933/3125   Loss: 0.673845 mae: 0.643422 (2076.8404999108716 steps/sec)\n",
      "Step #1935\tEpoch   0 Batch 1934/3125   Loss: 0.793589 mae: 0.709380 (2155.1691536153244 steps/sec)\n",
      "Step #1936\tEpoch   0 Batch 1935/3125   Loss: 0.785521 mae: 0.700380 (2289.392268812157 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1937\tEpoch   0 Batch 1936/3125   Loss: 0.819418 mae: 0.719883 (1991.729744617401 steps/sec)\n",
      "Step #1938\tEpoch   0 Batch 1937/3125   Loss: 0.981747 mae: 0.753397 (1822.089386251477 steps/sec)\n",
      "Step #1939\tEpoch   0 Batch 1938/3125   Loss: 0.878883 mae: 0.739349 (2046.0817983140805 steps/sec)\n",
      "Step #1940\tEpoch   0 Batch 1939/3125   Loss: 0.838553 mae: 0.712932 (2121.785934701889 steps/sec)\n",
      "Step #1941\tEpoch   0 Batch 1940/3125   Loss: 0.759593 mae: 0.701468 (2124.322079395468 steps/sec)\n",
      "Step #1942\tEpoch   0 Batch 1941/3125   Loss: 0.775595 mae: 0.717669 (2018.0640691307653 steps/sec)\n",
      "Step #1943\tEpoch   0 Batch 1942/3125   Loss: 0.709342 mae: 0.673133 (1962.633125573212 steps/sec)\n",
      "Step #1944\tEpoch   0 Batch 1943/3125   Loss: 0.826654 mae: 0.719754 (1606.5821427203432 steps/sec)\n",
      "Step #1945\tEpoch   0 Batch 1944/3125   Loss: 0.841966 mae: 0.738207 (904.5335541667206 steps/sec)\n",
      "Step #1946\tEpoch   0 Batch 1945/3125   Loss: 0.880869 mae: 0.737490 (1806.3635893813846 steps/sec)\n",
      "Step #1947\tEpoch   0 Batch 1946/3125   Loss: 0.918157 mae: 0.789569 (1500.151649546482 steps/sec)\n",
      "Step #1948\tEpoch   0 Batch 1947/3125   Loss: 0.783374 mae: 0.723750 (1694.8463272909478 steps/sec)\n",
      "Step #1949\tEpoch   0 Batch 1948/3125   Loss: 0.840827 mae: 0.741593 (1624.6916640842887 steps/sec)\n",
      "Step #1950\tEpoch   0 Batch 1949/3125   Loss: 0.861600 mae: 0.720656 (1793.0046253943556 steps/sec)\n",
      "Step #1951\tEpoch   0 Batch 1950/3125   Loss: 0.800410 mae: 0.708236 (1899.9900341556665 steps/sec)\n",
      "Step #1952\tEpoch   0 Batch 1951/3125   Loss: 0.846440 mae: 0.734067 (1778.9359391965256 steps/sec)\n",
      "Step #1953\tEpoch   0 Batch 1952/3125   Loss: 0.866683 mae: 0.730686 (1452.975369799425 steps/sec)\n",
      "Step #1954\tEpoch   0 Batch 1953/3125   Loss: 1.119457 mae: 0.843941 (1788.5242546223647 steps/sec)\n",
      "Step #1955\tEpoch   0 Batch 1954/3125   Loss: 0.756507 mae: 0.701256 (1727.9567588945833 steps/sec)\n",
      "Step #1956\tEpoch   0 Batch 1955/3125   Loss: 0.789254 mae: 0.709611 (1833.1908495703633 steps/sec)\n",
      "Step #1957\tEpoch   0 Batch 1956/3125   Loss: 0.857512 mae: 0.767472 (1638.6048256032004 steps/sec)\n",
      "Step #1958\tEpoch   0 Batch 1957/3125   Loss: 0.924169 mae: 0.750164 (1842.7914905582454 steps/sec)\n",
      "Step #1959\tEpoch   0 Batch 1958/3125   Loss: 0.883543 mae: 0.731664 (1642.2361610324117 steps/sec)\n",
      "Step #1960\tEpoch   0 Batch 1959/3125   Loss: 0.937756 mae: 0.749079 (1778.9812105017602 steps/sec)\n",
      "Step #1961\tEpoch   0 Batch 1960/3125   Loss: 0.765442 mae: 0.702809 (1878.293268369577 steps/sec)\n",
      "Step #1962\tEpoch   0 Batch 1961/3125   Loss: 1.030402 mae: 0.817779 (2095.4756195043965 steps/sec)\n",
      "Step #1963\tEpoch   0 Batch 1962/3125   Loss: 0.811818 mae: 0.721496 (2069.8710989162832 steps/sec)\n",
      "Step #1964\tEpoch   0 Batch 1963/3125   Loss: 0.785981 mae: 0.713018 (2093.174967561633 steps/sec)\n",
      "Step #1965\tEpoch   0 Batch 1964/3125   Loss: 0.997343 mae: 0.779767 (2193.375377825191 steps/sec)\n",
      "Step #1966\tEpoch   0 Batch 1965/3125   Loss: 0.814664 mae: 0.712823 (2105.595437705198 steps/sec)\n",
      "Step #1967\tEpoch   0 Batch 1966/3125   Loss: 0.888027 mae: 0.767013 (2099.7767209011263 steps/sec)\n",
      "Step #1968\tEpoch   0 Batch 1967/3125   Loss: 0.872315 mae: 0.747691 (1994.5712030282568 steps/sec)\n",
      "Step #1969\tEpoch   0 Batch 1968/3125   Loss: 0.850216 mae: 0.727786 (1781.8833745422412 steps/sec)\n",
      "Step #1970\tEpoch   0 Batch 1969/3125   Loss: 0.771365 mae: 0.715536 (1927.1397327746229 steps/sec)\n",
      "Step #1971\tEpoch   0 Batch 1970/3125   Loss: 0.901371 mae: 0.754440 (2032.6952341258686 steps/sec)\n",
      "Step #1972\tEpoch   0 Batch 1971/3125   Loss: 0.790488 mae: 0.695672 (1994.6660579428942 steps/sec)\n",
      "Step #1973\tEpoch   0 Batch 1972/3125   Loss: 0.801606 mae: 0.707649 (1900.8511062568546 steps/sec)\n",
      "Step #1974\tEpoch   0 Batch 1973/3125   Loss: 0.788157 mae: 0.732142 (1668.44504554676 steps/sec)\n",
      "Step #1975\tEpoch   0 Batch 1974/3125   Loss: 0.867618 mae: 0.737654 (1610.5797513266928 steps/sec)\n",
      "Step #1976\tEpoch   0 Batch 1975/3125   Loss: 0.918521 mae: 0.742594 (1919.080518672389 steps/sec)\n",
      "Step #1977\tEpoch   0 Batch 1976/3125   Loss: 0.818912 mae: 0.718292 (2220.641895826936 steps/sec)\n",
      "Step #1978\tEpoch   0 Batch 1977/3125   Loss: 0.936975 mae: 0.784609 (2086.365490414557 steps/sec)\n",
      "Step #1979\tEpoch   0 Batch 1978/3125   Loss: 1.013798 mae: 0.805885 (2182.6462537597704 steps/sec)\n",
      "Step #1980\tEpoch   0 Batch 1979/3125   Loss: 0.813055 mae: 0.727167 (2202.9370364923634 steps/sec)\n",
      "Step #1981\tEpoch   0 Batch 1980/3125   Loss: 0.786022 mae: 0.712254 (2170.8074984214395 steps/sec)\n",
      "Step #1982\tEpoch   0 Batch 1981/3125   Loss: 0.903980 mae: 0.754131 (2203.2147584728846 steps/sec)\n",
      "Step #1983\tEpoch   0 Batch 1982/3125   Loss: 0.965061 mae: 0.786271 (2146.961506961507 steps/sec)\n",
      "Step #1984\tEpoch   0 Batch 1983/3125   Loss: 0.793481 mae: 0.697558 (1532.8937943132812 steps/sec)\n",
      "Step #1985\tEpoch   0 Batch 1984/3125   Loss: 0.770788 mae: 0.714728 (1714.03164639728 steps/sec)\n",
      "Step #1986\tEpoch   0 Batch 1985/3125   Loss: 0.859689 mae: 0.751658 (1284.0675724493483 steps/sec)\n",
      "Step #1987\tEpoch   0 Batch 1986/3125   Loss: 0.870619 mae: 0.729484 (1582.6726135223046 steps/sec)\n",
      "Step #1988\tEpoch   0 Batch 1987/3125   Loss: 0.835097 mae: 0.749884 (1486.2772056895415 steps/sec)\n",
      "Step #1989\tEpoch   0 Batch 1988/3125   Loss: 0.774875 mae: 0.702269 (1529.6959794596487 steps/sec)\n",
      "Step #1990\tEpoch   0 Batch 1989/3125   Loss: 0.798965 mae: 0.701918 (1924.3810677384427 steps/sec)\n",
      "Step #1991\tEpoch   0 Batch 1990/3125   Loss: 0.798962 mae: 0.697793 (1491.2444624584907 steps/sec)\n",
      "Step #1992\tEpoch   0 Batch 1991/3125   Loss: 0.778347 mae: 0.702961 (1539.2280196995164 steps/sec)\n",
      "Step #1993\tEpoch   0 Batch 1992/3125   Loss: 0.934427 mae: 0.779674 (1439.3828329832943 steps/sec)\n",
      "Step #1994\tEpoch   0 Batch 1993/3125   Loss: 0.870260 mae: 0.735223 (1610.7653076899444 steps/sec)\n",
      "Step #1995\tEpoch   0 Batch 1994/3125   Loss: 0.872000 mae: 0.744177 (1804.0017204301075 steps/sec)\n",
      "Step #1996\tEpoch   0 Batch 1995/3125   Loss: 0.951072 mae: 0.776655 (1699.8743626946366 steps/sec)\n",
      "Step #1997\tEpoch   0 Batch 1996/3125   Loss: 0.839227 mae: 0.721839 (1638.6432361053594 steps/sec)\n",
      "Step #1998\tEpoch   0 Batch 1997/3125   Loss: 0.954586 mae: 0.749568 (1237.6594175101066 steps/sec)\n",
      "Step #1999\tEpoch   0 Batch 1998/3125   Loss: 0.899843 mae: 0.738367 (1583.0788160606312 steps/sec)\n",
      "Step #2000\tEpoch   0 Batch 1999/3125   Loss: 0.944396 mae: 0.772406 (1714.3679288470341 steps/sec)\n",
      "Step #2001\tEpoch   0 Batch 2000/3125   Loss: 0.999596 mae: 0.801288 (1784.2483643448445 steps/sec)\n",
      "Step #2002\tEpoch   0 Batch 2001/3125   Loss: 0.785599 mae: 0.696384 (1710.1041318405323 steps/sec)\n",
      "Step #2003\tEpoch   0 Batch 2002/3125   Loss: 0.842292 mae: 0.743796 (1703.8103439871309 steps/sec)\n",
      "Step #2004\tEpoch   0 Batch 2003/3125   Loss: 0.738937 mae: 0.687712 (1395.8307819280637 steps/sec)\n",
      "Step #2005\tEpoch   0 Batch 2004/3125   Loss: 0.914514 mae: 0.777016 (1616.2398366151594 steps/sec)\n",
      "Step #2006\tEpoch   0 Batch 2005/3125   Loss: 0.845128 mae: 0.737032 (1935.5169772314052 steps/sec)\n",
      "Step #2007\tEpoch   0 Batch 2006/3125   Loss: 0.947192 mae: 0.777197 (2259.76466531615 steps/sec)\n",
      "Step #2008\tEpoch   0 Batch 2007/3125   Loss: 0.822635 mae: 0.717798 (2012.0039910967841 steps/sec)\n",
      "Step #2009\tEpoch   0 Batch 2008/3125   Loss: 0.875591 mae: 0.766821 (2301.3761165858264 steps/sec)\n",
      "Step #2010\tEpoch   0 Batch 2009/3125   Loss: 0.809517 mae: 0.722065 (2075.5661124307203 steps/sec)\n",
      "Step #2011\tEpoch   0 Batch 2010/3125   Loss: 0.749685 mae: 0.656813 (1868.2036434902677 steps/sec)\n",
      "Step #2012\tEpoch   0 Batch 2011/3125   Loss: 0.764748 mae: 0.690522 (2074.539519240281 steps/sec)\n",
      "Step #2013\tEpoch   0 Batch 2012/3125   Loss: 0.841480 mae: 0.750736 (2090.733448313677 steps/sec)\n",
      "Step #2014\tEpoch   0 Batch 2013/3125   Loss: 0.754539 mae: 0.696508 (1807.2041639378167 steps/sec)\n",
      "Step #2015\tEpoch   0 Batch 2014/3125   Loss: 0.895130 mae: 0.724821 (1910.3050618959564 steps/sec)\n",
      "Step #2016\tEpoch   0 Batch 2015/3125   Loss: 0.978544 mae: 0.786743 (2054.3194396826175 steps/sec)\n",
      "Step #2017\tEpoch   0 Batch 2016/3125   Loss: 0.813343 mae: 0.728980 (2103.0405134376256 steps/sec)\n",
      "Step #2018\tEpoch   0 Batch 2017/3125   Loss: 0.820077 mae: 0.738284 (2109.874543497289 steps/sec)\n",
      "Step #2019\tEpoch   0 Batch 2018/3125   Loss: 0.781932 mae: 0.710621 (1986.6354688669326 steps/sec)\n",
      "Step #2020\tEpoch   0 Batch 2019/3125   Loss: 0.912812 mae: 0.734740 (1998.8295732898712 steps/sec)\n",
      "Step #2021\tEpoch   0 Batch 2020/3125   Loss: 0.855058 mae: 0.740576 (1918.8522490209714 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2022\tEpoch   0 Batch 2021/3125   Loss: 0.746334 mae: 0.692427 (1613.528963707847 steps/sec)\n",
      "Step #2023\tEpoch   0 Batch 2022/3125   Loss: 0.958027 mae: 0.801940 (1212.4718873760587 steps/sec)\n",
      "Step #2024\tEpoch   0 Batch 2023/3125   Loss: 1.108956 mae: 0.829307 (905.4552205618855 steps/sec)\n",
      "Step #2025\tEpoch   0 Batch 2024/3125   Loss: 0.890124 mae: 0.725307 (1127.010280469258 steps/sec)\n",
      "Step #2026\tEpoch   0 Batch 2025/3125   Loss: 0.868307 mae: 0.758960 (1423.6607900507104 steps/sec)\n",
      "Step #2027\tEpoch   0 Batch 2026/3125   Loss: 0.908092 mae: 0.747776 (1331.8717888466203 steps/sec)\n",
      "Step #2028\tEpoch   0 Batch 2027/3125   Loss: 0.846835 mae: 0.717307 (1554.1942416719162 steps/sec)\n",
      "Step #2029\tEpoch   0 Batch 2028/3125   Loss: 0.838583 mae: 0.747208 (1600.3540822478117 steps/sec)\n",
      "Step #2030\tEpoch   0 Batch 2029/3125   Loss: 1.015727 mae: 0.814755 (1547.5080801074396 steps/sec)\n",
      "Step #2031\tEpoch   0 Batch 2030/3125   Loss: 0.842021 mae: 0.729186 (1623.6979227154127 steps/sec)\n",
      "Step #2032\tEpoch   0 Batch 2031/3125   Loss: 0.950625 mae: 0.764697 (1531.5392423921537 steps/sec)\n",
      "Step #2033\tEpoch   0 Batch 2032/3125   Loss: 0.813611 mae: 0.742153 (1783.6110189744768 steps/sec)\n",
      "Step #2034\tEpoch   0 Batch 2033/3125   Loss: 0.947638 mae: 0.771840 (1719.9215962864853 steps/sec)\n",
      "Step #2035\tEpoch   0 Batch 2034/3125   Loss: 0.838339 mae: 0.717101 (1865.3289216209484 steps/sec)\n",
      "Step #2036\tEpoch   0 Batch 2035/3125   Loss: 1.034014 mae: 0.811359 (2076.0592381404927 steps/sec)\n",
      "Step #2037\tEpoch   0 Batch 2036/3125   Loss: 0.784785 mae: 0.703942 (1896.9652564833157 steps/sec)\n",
      "Step #2038\tEpoch   0 Batch 2037/3125   Loss: 0.866520 mae: 0.729396 (1437.5673489532635 steps/sec)\n",
      "Step #2039\tEpoch   0 Batch 2038/3125   Loss: 0.931732 mae: 0.771328 (1442.4221581803552 steps/sec)\n",
      "Step #2040\tEpoch   0 Batch 2039/3125   Loss: 0.993224 mae: 0.793798 (1302.2876872252168 steps/sec)\n",
      "Step #2041\tEpoch   0 Batch 2040/3125   Loss: 0.824611 mae: 0.748926 (1128.7937261487618 steps/sec)\n",
      "Step #2042\tEpoch   0 Batch 2041/3125   Loss: 0.795028 mae: 0.706458 (1168.2777369253738 steps/sec)\n",
      "Step #2043\tEpoch   0 Batch 2042/3125   Loss: 0.827052 mae: 0.718537 (1572.184030406849 steps/sec)\n",
      "Step #2044\tEpoch   0 Batch 2043/3125   Loss: 0.825720 mae: 0.714067 (1369.4523893481696 steps/sec)\n",
      "Step #2045\tEpoch   0 Batch 2044/3125   Loss: 1.028284 mae: 0.808431 (1370.4188721165785 steps/sec)\n",
      "Step #2046\tEpoch   0 Batch 2045/3125   Loss: 0.717023 mae: 0.674013 (1419.3730034111213 steps/sec)\n",
      "Step #2047\tEpoch   0 Batch 2046/3125   Loss: 0.760970 mae: 0.693622 (1475.1327663944517 steps/sec)\n",
      "Step #2048\tEpoch   0 Batch 2047/3125   Loss: 0.948716 mae: 0.768595 (1823.9593661396093 steps/sec)\n",
      "Step #2049\tEpoch   0 Batch 2048/3125   Loss: 0.949965 mae: 0.786813 (1961.5683926967974 steps/sec)\n",
      "Step #2050\tEpoch   0 Batch 2049/3125   Loss: 0.837387 mae: 0.708653 (1750.032961989402 steps/sec)\n",
      "Step #2051\tEpoch   0 Batch 2050/3125   Loss: 0.900912 mae: 0.744492 (1318.6069176260507 steps/sec)\n",
      "Step #2052\tEpoch   0 Batch 2051/3125   Loss: 0.847100 mae: 0.755563 (1370.508430270553 steps/sec)\n",
      "Step #2053\tEpoch   0 Batch 2052/3125   Loss: 0.837617 mae: 0.731194 (1162.9523651084123 steps/sec)\n",
      "Step #2054\tEpoch   0 Batch 2053/3125   Loss: 0.939393 mae: 0.748544 (1132.885325957778 steps/sec)\n",
      "Step #2055\tEpoch   0 Batch 2054/3125   Loss: 0.689729 mae: 0.678163 (1281.2903619978615 steps/sec)\n",
      "Step #2056\tEpoch   0 Batch 2055/3125   Loss: 0.796715 mae: 0.702885 (1254.3600356482782 steps/sec)\n",
      "Step #2057\tEpoch   0 Batch 2056/3125   Loss: 0.869274 mae: 0.716723 (1507.5602585023255 steps/sec)\n",
      "Step #2058\tEpoch   0 Batch 2057/3125   Loss: 0.785510 mae: 0.717788 (1543.6239041947902 steps/sec)\n",
      "Step #2059\tEpoch   0 Batch 2058/3125   Loss: 0.780658 mae: 0.693351 (1549.0054436541175 steps/sec)\n",
      "Step #2060\tEpoch   0 Batch 2059/3125   Loss: 0.937049 mae: 0.757242 (1819.2284670836334 steps/sec)\n",
      "Step #2061\tEpoch   0 Batch 2060/3125   Loss: 0.824517 mae: 0.712544 (1828.4279449331718 steps/sec)\n",
      "Step #2062\tEpoch   0 Batch 2061/3125   Loss: 0.753167 mae: 0.694297 (1195.1422726003432 steps/sec)\n",
      "Step #2063\tEpoch   0 Batch 2062/3125   Loss: 0.797777 mae: 0.708688 (1330.4944741216327 steps/sec)\n",
      "Step #2064\tEpoch   0 Batch 2063/3125   Loss: 0.810015 mae: 0.708753 (1363.8859803723913 steps/sec)\n",
      "Step #2065\tEpoch   0 Batch 2064/3125   Loss: 0.715852 mae: 0.655307 (1316.4795982423102 steps/sec)\n",
      "Step #2066\tEpoch   0 Batch 2065/3125   Loss: 0.691643 mae: 0.673489 (1381.3683579572776 steps/sec)\n",
      "Step #2067\tEpoch   0 Batch 2066/3125   Loss: 0.768026 mae: 0.691512 (1515.3379818635067 steps/sec)\n",
      "Step #2068\tEpoch   0 Batch 2067/3125   Loss: 0.893941 mae: 0.740948 (1280.3751098954772 steps/sec)\n",
      "Step #2069\tEpoch   0 Batch 2068/3125   Loss: 0.776993 mae: 0.699492 (1808.7004519267257 steps/sec)\n",
      "Step #2070\tEpoch   0 Batch 2069/3125   Loss: 0.919194 mae: 0.747565 (1909.1399024106038 steps/sec)\n",
      "Step #2071\tEpoch   0 Batch 2070/3125   Loss: 0.820861 mae: 0.716256 (2001.6913399956095 steps/sec)\n",
      "Step #2072\tEpoch   0 Batch 2071/3125   Loss: 1.004091 mae: 0.793535 (1370.087608694232 steps/sec)\n",
      "Step #2073\tEpoch   0 Batch 2072/3125   Loss: 0.779599 mae: 0.706310 (1059.686816268576 steps/sec)\n",
      "Step #2074\tEpoch   0 Batch 2073/3125   Loss: 0.746763 mae: 0.668276 (1217.6037529900834 steps/sec)\n",
      "Step #2075\tEpoch   0 Batch 2074/3125   Loss: 0.942262 mae: 0.739287 (1121.2797810000427 steps/sec)\n",
      "Step #2076\tEpoch   0 Batch 2075/3125   Loss: 0.944772 mae: 0.772435 (1008.6923062123632 steps/sec)\n",
      "Step #2077\tEpoch   0 Batch 2076/3125   Loss: 0.975495 mae: 0.775500 (1044.7574116603812 steps/sec)\n",
      "Step #2078\tEpoch   0 Batch 2077/3125   Loss: 0.851818 mae: 0.726791 (1191.8683754369015 steps/sec)\n",
      "Step #2079\tEpoch   0 Batch 2078/3125   Loss: 0.902557 mae: 0.748749 (1738.8887506944272 steps/sec)\n",
      "Step #2080\tEpoch   0 Batch 2079/3125   Loss: 0.847674 mae: 0.717086 (2103.377998876675 steps/sec)\n",
      "Step #2081\tEpoch   0 Batch 2080/3125   Loss: 0.984227 mae: 0.775398 (1784.7494553376907 steps/sec)\n",
      "Step #2082\tEpoch   0 Batch 2081/3125   Loss: 0.861165 mae: 0.751536 (1188.154443216906 steps/sec)\n",
      "Step #2083\tEpoch   0 Batch 2082/3125   Loss: 0.882100 mae: 0.741468 (888.3037888895949 steps/sec)\n",
      "Step #2084\tEpoch   0 Batch 2083/3125   Loss: 0.843148 mae: 0.741402 (835.0895356569731 steps/sec)\n",
      "Step #2085\tEpoch   0 Batch 2084/3125   Loss: 0.910363 mae: 0.769025 (836.515211347382 steps/sec)\n",
      "Step #2086\tEpoch   0 Batch 2085/3125   Loss: 0.929339 mae: 0.780331 (985.105643395996 steps/sec)\n",
      "Step #2087\tEpoch   0 Batch 2086/3125   Loss: 0.952076 mae: 0.764050 (660.0920347742882 steps/sec)\n",
      "Step #2088\tEpoch   0 Batch 2087/3125   Loss: 0.867548 mae: 0.746823 (1396.993052178605 steps/sec)\n",
      "Step #2089\tEpoch   0 Batch 2088/3125   Loss: 0.886279 mae: 0.751383 (1868.8029656297063 steps/sec)\n",
      "Step #2090\tEpoch   0 Batch 2089/3125   Loss: 0.849178 mae: 0.718900 (1231.257705811209 steps/sec)\n",
      "Step #2091\tEpoch   0 Batch 2090/3125   Loss: 0.766593 mae: 0.684724 (1025.9185879843653 steps/sec)\n",
      "Step #2092\tEpoch   0 Batch 2091/3125   Loss: 0.746702 mae: 0.694083 (801.7157078250498 steps/sec)\n",
      "Step #2093\tEpoch   0 Batch 2092/3125   Loss: 1.004059 mae: 0.797148 (906.8294834430213 steps/sec)\n",
      "Step #2094\tEpoch   0 Batch 2093/3125   Loss: 0.846334 mae: 0.729272 (873.609491118739 steps/sec)\n",
      "Step #2095\tEpoch   0 Batch 2094/3125   Loss: 0.836562 mae: 0.736960 (971.8575639053145 steps/sec)\n",
      "Step #2096\tEpoch   0 Batch 2095/3125   Loss: 0.833574 mae: 0.709282 (912.0947083204669 steps/sec)\n",
      "Step #2097\tEpoch   0 Batch 2096/3125   Loss: 0.667142 mae: 0.633779 (1134.4112253674264 steps/sec)\n",
      "Step #2098\tEpoch   0 Batch 2097/3125   Loss: 0.735036 mae: 0.666154 (1258.2056420163428 steps/sec)\n",
      "Step #2099\tEpoch   0 Batch 2098/3125   Loss: 0.849432 mae: 0.737043 (1293.6678407738004 steps/sec)\n",
      "Step #2100\tEpoch   0 Batch 2099/3125   Loss: 0.859395 mae: 0.715902 (1310.5889411058895 steps/sec)\n",
      "Step #2101\tEpoch   0 Batch 2100/3125   Loss: 0.839176 mae: 0.739946 (1150.7448845771853 steps/sec)\n",
      "Step #2102\tEpoch   0 Batch 2101/3125   Loss: 0.804722 mae: 0.705185 (1373.0208196935969 steps/sec)\n",
      "Step #2103\tEpoch   0 Batch 2102/3125   Loss: 0.897424 mae: 0.744460 (1361.1064597571344 steps/sec)\n",
      "Step #2104\tEpoch   0 Batch 2103/3125   Loss: 1.008661 mae: 0.812655 (1573.021302130213 steps/sec)\n",
      "Step #2105\tEpoch   0 Batch 2104/3125   Loss: 0.817526 mae: 0.677337 (1673.8516549737008 steps/sec)\n",
      "Step #2106\tEpoch   0 Batch 2105/3125   Loss: 0.812114 mae: 0.723100 (1481.1336878756417 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2107\tEpoch   0 Batch 2106/3125   Loss: 0.813439 mae: 0.703369 (1201.8958432433362 steps/sec)\n",
      "Step #2108\tEpoch   0 Batch 2107/3125   Loss: 0.942482 mae: 0.766988 (1414.6241433273974 steps/sec)\n",
      "Step #2109\tEpoch   0 Batch 2108/3125   Loss: 0.889967 mae: 0.731613 (1542.0121910868302 steps/sec)\n",
      "Step #2110\tEpoch   0 Batch 2109/3125   Loss: 0.794889 mae: 0.718938 (1474.4897313487404 steps/sec)\n",
      "Step #2111\tEpoch   0 Batch 2110/3125   Loss: 0.892319 mae: 0.736152 (1570.4886359381435 steps/sec)\n",
      "Step #2112\tEpoch   0 Batch 2111/3125   Loss: 0.888982 mae: 0.729246 (1624.578391652271 steps/sec)\n",
      "Step #2113\tEpoch   0 Batch 2112/3125   Loss: 0.920835 mae: 0.751690 (1500.7313477694609 steps/sec)\n",
      "Step #2114\tEpoch   0 Batch 2113/3125   Loss: 0.934153 mae: 0.756432 (1501.5479787493018 steps/sec)\n",
      "Step #2115\tEpoch   0 Batch 2114/3125   Loss: 0.906748 mae: 0.755816 (1698.55264969587 steps/sec)\n",
      "Step #2116\tEpoch   0 Batch 2115/3125   Loss: 1.005928 mae: 0.785121 (1921.9648994180452 steps/sec)\n",
      "Step #2117\tEpoch   0 Batch 2116/3125   Loss: 0.866888 mae: 0.734064 (1911.15809426603 steps/sec)\n",
      "Step #2118\tEpoch   0 Batch 2117/3125   Loss: 0.752917 mae: 0.688049 (1910.67055393586 steps/sec)\n",
      "Step #2119\tEpoch   0 Batch 2118/3125   Loss: 0.834123 mae: 0.721359 (2115.0452830949835 steps/sec)\n",
      "Step #2120\tEpoch   0 Batch 2119/3125   Loss: 0.886582 mae: 0.730647 (1788.4632440730002 steps/sec)\n",
      "Step #2121\tEpoch   0 Batch 2120/3125   Loss: 0.771895 mae: 0.694280 (1438.0306510782734 steps/sec)\n",
      "Step #2122\tEpoch   0 Batch 2121/3125   Loss: 0.853079 mae: 0.721746 (2002.9531150014805 steps/sec)\n",
      "Step #2123\tEpoch   0 Batch 2122/3125   Loss: 0.890993 mae: 0.738966 (1961.2565347099478 steps/sec)\n",
      "Step #2124\tEpoch   0 Batch 2123/3125   Loss: 0.738878 mae: 0.703289 (1702.413404011787 steps/sec)\n",
      "Step #2125\tEpoch   0 Batch 2124/3125   Loss: 0.685301 mae: 0.639529 (1964.0300436419486 steps/sec)\n",
      "Step #2126\tEpoch   0 Batch 2125/3125   Loss: 0.946220 mae: 0.766583 (2107.224533269026 steps/sec)\n",
      "Step #2127\tEpoch   0 Batch 2126/3125   Loss: 0.946886 mae: 0.769495 (2057.725970407002 steps/sec)\n",
      "Step #2128\tEpoch   0 Batch 2127/3125   Loss: 0.839772 mae: 0.730425 (2025.8423493044822 steps/sec)\n",
      "Step #2129\tEpoch   0 Batch 2128/3125   Loss: 0.773602 mae: 0.691426 (1723.6110197909134 steps/sec)\n",
      "Step #2130\tEpoch   0 Batch 2129/3125   Loss: 0.763116 mae: 0.716546 (1868.8029656297063 steps/sec)\n",
      "Step #2131\tEpoch   0 Batch 2130/3125   Loss: 0.829587 mae: 0.733070 (1981.8293501167086 steps/sec)\n",
      "Step #2132\tEpoch   0 Batch 2131/3125   Loss: 0.895114 mae: 0.740307 (1987.2190425652882 steps/sec)\n",
      "Step #2133\tEpoch   0 Batch 2132/3125   Loss: 0.783760 mae: 0.701594 (2219.560776842885 steps/sec)\n",
      "Step #2134\tEpoch   0 Batch 2133/3125   Loss: 0.812878 mae: 0.716339 (2068.25842973658 steps/sec)\n",
      "Step #2135\tEpoch   0 Batch 2134/3125   Loss: 0.766597 mae: 0.691919 (2008.7470426528482 steps/sec)\n",
      "Step #2136\tEpoch   0 Batch 2135/3125   Loss: 0.826244 mae: 0.708303 (2060.3135929579125 steps/sec)\n",
      "Step #2137\tEpoch   0 Batch 2136/3125   Loss: 0.951883 mae: 0.765232 (1966.958984796331 steps/sec)\n",
      "Step #2138\tEpoch   0 Batch 2137/3125   Loss: 0.835281 mae: 0.723814 (1755.2326749246736 steps/sec)\n",
      "Step #2139\tEpoch   0 Batch 2138/3125   Loss: 0.757372 mae: 0.703332 (1774.7507743344108 steps/sec)\n",
      "Step #2140\tEpoch   0 Batch 2139/3125   Loss: 0.917909 mae: 0.754224 (1856.1659718718745 steps/sec)\n",
      "Step #2141\tEpoch   0 Batch 2140/3125   Loss: 0.827846 mae: 0.721395 (1898.6826976179914 steps/sec)\n",
      "Step #2142\tEpoch   0 Batch 2141/3125   Loss: 0.916487 mae: 0.761191 (2007.401096955136 steps/sec)\n",
      "Step #2143\tEpoch   0 Batch 2142/3125   Loss: 0.723525 mae: 0.681160 (2033.4441934201468 steps/sec)\n",
      "Step #2144\tEpoch   0 Batch 2143/3125   Loss: 0.759008 mae: 0.684159 (1977.3073985725196 steps/sec)\n",
      "Step #2145\tEpoch   0 Batch 2144/3125   Loss: 0.698032 mae: 0.665138 (2000.94649263415 steps/sec)\n",
      "Step #2146\tEpoch   0 Batch 2145/3125   Loss: 0.961220 mae: 0.775961 (1751.7286312114202 steps/sec)\n",
      "Step #2147\tEpoch   0 Batch 2146/3125   Loss: 0.685515 mae: 0.675361 (1870.703358458588 steps/sec)\n",
      "Step #2148\tEpoch   0 Batch 2147/3125   Loss: 0.857448 mae: 0.751456 (1664.3270955351331 steps/sec)\n",
      "Step #2149\tEpoch   0 Batch 2148/3125   Loss: 0.795539 mae: 0.703717 (1735.507042486635 steps/sec)\n",
      "Step #2150\tEpoch   0 Batch 2149/3125   Loss: 0.798368 mae: 0.703325 (1714.7884675137777 steps/sec)\n",
      "Step #2151\tEpoch   0 Batch 2150/3125   Loss: 0.854590 mae: 0.728891 (2044.645503470868 steps/sec)\n",
      "Step #2152\tEpoch   0 Batch 2151/3125   Loss: 0.874840 mae: 0.745675 (1938.4330979406218 steps/sec)\n",
      "Step #2153\tEpoch   0 Batch 2152/3125   Loss: 0.994005 mae: 0.778742 (1669.574078496935 steps/sec)\n",
      "Step #2154\tEpoch   0 Batch 2153/3125   Loss: 0.934010 mae: 0.796330 (1786.5435401155164 steps/sec)\n",
      "Step #2155\tEpoch   0 Batch 2154/3125   Loss: 0.801219 mae: 0.719393 (1836.546107364918 steps/sec)\n",
      "Step #2156\tEpoch   0 Batch 2155/3125   Loss: 0.852433 mae: 0.728217 (1974.161724559917 steps/sec)\n",
      "Step #2157\tEpoch   0 Batch 2156/3125   Loss: 0.893349 mae: 0.744318 (1750.0767741504774 steps/sec)\n",
      "Step #2158\tEpoch   0 Batch 2157/3125   Loss: 0.788806 mae: 0.708544 (1388.6952375907188 steps/sec)\n",
      "Step #2159\tEpoch   0 Batch 2158/3125   Loss: 0.768900 mae: 0.680476 (1248.5054145606734 steps/sec)\n",
      "Step #2160\tEpoch   0 Batch 2159/3125   Loss: 0.851254 mae: 0.749347 (1481.7511234208519 steps/sec)\n",
      "Step #2161\tEpoch   0 Batch 2160/3125   Loss: 0.822604 mae: 0.725454 (1310.72 steps/sec)\n",
      "Step #2162\tEpoch   0 Batch 2161/3125   Loss: 0.891297 mae: 0.766304 (1363.3185331573782 steps/sec)\n",
      "Step #2163\tEpoch   0 Batch 2162/3125   Loss: 0.921482 mae: 0.752970 (1273.2699476643231 steps/sec)\n",
      "Step #2164\tEpoch   0 Batch 2163/3125   Loss: 0.946704 mae: 0.768067 (1555.4507290878614 steps/sec)\n",
      "Step #2165\tEpoch   0 Batch 2164/3125   Loss: 0.755607 mae: 0.697737 (1542.1596023178515 steps/sec)\n",
      "Step #2166\tEpoch   0 Batch 2165/3125   Loss: 0.769932 mae: 0.703661 (1163.6815616728722 steps/sec)\n",
      "Step #2167\tEpoch   0 Batch 2166/3125   Loss: 0.762281 mae: 0.698986 (1455.6075350167275 steps/sec)\n",
      "Step #2168\tEpoch   0 Batch 2167/3125   Loss: 0.853987 mae: 0.757540 (1450.192238541753 steps/sec)\n",
      "Step #2169\tEpoch   0 Batch 2168/3125   Loss: 0.906710 mae: 0.767654 (1917.7643250361211 steps/sec)\n",
      "Step #2170\tEpoch   0 Batch 2169/3125   Loss: 0.729556 mae: 0.677126 (2039.713663243075 steps/sec)\n",
      "Step #2171\tEpoch   0 Batch 2170/3125   Loss: 0.787278 mae: 0.703703 (1979.8274266941073 steps/sec)\n",
      "Step #2172\tEpoch   0 Batch 2171/3125   Loss: 0.825413 mae: 0.723335 (1519.7524512112934 steps/sec)\n",
      "Step #2173\tEpoch   0 Batch 2172/3125   Loss: 0.829887 mae: 0.714797 (1744.3559991682262 steps/sec)\n",
      "Step #2174\tEpoch   0 Batch 2173/3125   Loss: 0.821510 mae: 0.708253 (1135.916629654104 steps/sec)\n",
      "Step #2175\tEpoch   0 Batch 2174/3125   Loss: 0.756360 mae: 0.674403 (1218.2332541374524 steps/sec)\n",
      "Step #2176\tEpoch   0 Batch 2175/3125   Loss: 0.815317 mae: 0.710452 (1168.5251016883044 steps/sec)\n",
      "Step #2177\tEpoch   0 Batch 2176/3125   Loss: 0.782954 mae: 0.726687 (1407.295665011408 steps/sec)\n",
      "Step #2178\tEpoch   0 Batch 2177/3125   Loss: 0.900754 mae: 0.771240 (1383.1631710856088 steps/sec)\n",
      "Step #2179\tEpoch   0 Batch 2178/3125   Loss: 0.798787 mae: 0.708421 (1378.7347064895107 steps/sec)\n",
      "Step #2180\tEpoch   0 Batch 2179/3125   Loss: 0.882422 mae: 0.746773 (1376.9151981511147 steps/sec)\n",
      "Step #2181\tEpoch   0 Batch 2180/3125   Loss: 0.884256 mae: 0.732731 (1381.7779300529742 steps/sec)\n",
      "Step #2182\tEpoch   0 Batch 2181/3125   Loss: 0.986796 mae: 0.780831 (1612.1769345489768 steps/sec)\n",
      "Step #2183\tEpoch   0 Batch 2182/3125   Loss: 0.728417 mae: 0.681816 (1830.3588884234046 steps/sec)\n",
      "Step #2184\tEpoch   0 Batch 2183/3125   Loss: 0.752619 mae: 0.668099 (1911.8898714559211 steps/sec)\n",
      "Step #2185\tEpoch   0 Batch 2184/3125   Loss: 0.856688 mae: 0.724127 (1460.2292191786544 steps/sec)\n",
      "Step #2186\tEpoch   0 Batch 2185/3125   Loss: 0.913903 mae: 0.762773 (1495.8821641285354 steps/sec)\n",
      "Step #2187\tEpoch   0 Batch 2186/3125   Loss: 0.839579 mae: 0.722604 (1103.3170767636275 steps/sec)\n",
      "Step #2188\tEpoch   0 Batch 2187/3125   Loss: 0.856670 mae: 0.729045 (1231.2215627219941 steps/sec)\n",
      "Step #2189\tEpoch   0 Batch 2188/3125   Loss: 0.809150 mae: 0.710780 (986.3751810810302 steps/sec)\n",
      "Step #2190\tEpoch   0 Batch 2189/3125   Loss: 0.916396 mae: 0.730230 (879.2610884941282 steps/sec)\n",
      "Step #2191\tEpoch   0 Batch 2190/3125   Loss: 0.890142 mae: 0.731399 (926.3570788075288 steps/sec)\n",
      "Step #2192\tEpoch   0 Batch 2191/3125   Loss: 0.881525 mae: 0.733046 (1272.4587557869318 steps/sec)\n",
      "Step #2193\tEpoch   0 Batch 2192/3125   Loss: 0.893173 mae: 0.735923 (1827.6152961271657 steps/sec)\n",
      "Step #2194\tEpoch   0 Batch 2193/3125   Loss: 0.929053 mae: 0.770243 (1175.8961563262217 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2195\tEpoch   0 Batch 2194/3125   Loss: 0.830028 mae: 0.720759 (826.4312736443884 steps/sec)\n",
      "Step #2196\tEpoch   0 Batch 2195/3125   Loss: 0.889965 mae: 0.741095 (805.6054087277197 steps/sec)\n",
      "Step #2197\tEpoch   0 Batch 2196/3125   Loss: 0.759129 mae: 0.687480 (914.9598397954687 steps/sec)\n",
      "Step #2198\tEpoch   0 Batch 2197/3125   Loss: 0.848017 mae: 0.702314 (1066.0106034189164 steps/sec)\n",
      "Step #2199\tEpoch   0 Batch 2198/3125   Loss: 0.864867 mae: 0.729857 (878.8741838419566 steps/sec)\n",
      "Step #2200\tEpoch   0 Batch 2199/3125   Loss: 0.758528 mae: 0.680539 (1075.5949450188741 steps/sec)\n",
      "Step #2201\tEpoch   0 Batch 2200/3125   Loss: 0.800961 mae: 0.705686 (1834.521851709297 steps/sec)\n",
      "Step #2202\tEpoch   0 Batch 2201/3125   Loss: 0.933951 mae: 0.780008 (1769.7933281011333 steps/sec)\n",
      "Step #2203\tEpoch   0 Batch 2202/3125   Loss: 0.924012 mae: 0.757491 (1504.9853961692752 steps/sec)\n",
      "Step #2204\tEpoch   0 Batch 2203/3125   Loss: 0.830035 mae: 0.732961 (1568.573950246077 steps/sec)\n",
      "Step #2205\tEpoch   0 Batch 2204/3125   Loss: 0.829933 mae: 0.733930 (1824.2925615664988 steps/sec)\n",
      "Step #2206\tEpoch   0 Batch 2205/3125   Loss: 0.925029 mae: 0.742736 (1685.366422090603 steps/sec)\n",
      "Step #2207\tEpoch   0 Batch 2206/3125   Loss: 0.730337 mae: 0.679209 (1368.880301823736 steps/sec)\n",
      "Step #2208\tEpoch   0 Batch 2207/3125   Loss: 0.851050 mae: 0.715163 (1972.230895104106 steps/sec)\n",
      "Step #2209\tEpoch   0 Batch 2208/3125   Loss: 0.859116 mae: 0.759076 (1697.5077503379391 steps/sec)\n",
      "Step #2210\tEpoch   0 Batch 2209/3125   Loss: 0.760830 mae: 0.688753 (1910.9491179472227 steps/sec)\n",
      "Step #2211\tEpoch   0 Batch 2210/3125   Loss: 0.891729 mae: 0.737417 (1924.5576683062918 steps/sec)\n",
      "Step #2212\tEpoch   0 Batch 2211/3125   Loss: 0.806141 mae: 0.726144 (1793.5873423134487 steps/sec)\n",
      "Step #2213\tEpoch   0 Batch 2212/3125   Loss: 0.816512 mae: 0.722154 (1946.4934100612586 steps/sec)\n",
      "Step #2214\tEpoch   0 Batch 2213/3125   Loss: 0.772012 mae: 0.699564 (1549.474679709781 steps/sec)\n",
      "Step #2215\tEpoch   0 Batch 2214/3125   Loss: 0.868639 mae: 0.736036 (1751.187414409299 steps/sec)\n",
      "Step #2216\tEpoch   0 Batch 2215/3125   Loss: 0.884368 mae: 0.744670 (1556.5012802909414 steps/sec)\n",
      "Step #2217\tEpoch   0 Batch 2216/3125   Loss: 0.876629 mae: 0.748000 (1698.6489551271668 steps/sec)\n",
      "Step #2218\tEpoch   0 Batch 2217/3125   Loss: 0.869379 mae: 0.762875 (1842.3058339848726 steps/sec)\n",
      "Step #2219\tEpoch   0 Batch 2218/3125   Loss: 0.914694 mae: 0.772693 (1846.1820166558093 steps/sec)\n",
      "Step #2220\tEpoch   0 Batch 2219/3125   Loss: 1.002943 mae: 0.791151 (1864.2842538514192 steps/sec)\n",
      "Step #2221\tEpoch   0 Batch 2220/3125   Loss: 0.816387 mae: 0.701227 (1419.9400106978665 steps/sec)\n",
      "Step #2222\tEpoch   0 Batch 2221/3125   Loss: 0.876308 mae: 0.741764 (1844.0553967905034 steps/sec)\n",
      "Step #2223\tEpoch   0 Batch 2222/3125   Loss: 0.974860 mae: 0.787543 (2097.2358894355775 steps/sec)\n",
      "Step #2224\tEpoch   0 Batch 2223/3125   Loss: 0.807146 mae: 0.706203 (2055.9305916376647 steps/sec)\n",
      "Step #2225\tEpoch   0 Batch 2224/3125   Loss: 0.754717 mae: 0.659301 (1595.617505630288 steps/sec)\n",
      "Step #2226\tEpoch   0 Batch 2225/3125   Loss: 0.819118 mae: 0.709038 (1491.6793513052137 steps/sec)\n",
      "Step #2227\tEpoch   0 Batch 2226/3125   Loss: 0.818894 mae: 0.751100 (1397.1512704694142 steps/sec)\n",
      "Step #2228\tEpoch   0 Batch 2227/3125   Loss: 0.956813 mae: 0.748644 (1518.5199666920098 steps/sec)\n",
      "Step #2229\tEpoch   0 Batch 2228/3125   Loss: 0.855564 mae: 0.733022 (1675.791088665857 steps/sec)\n",
      "Step #2230\tEpoch   0 Batch 2229/3125   Loss: 0.866091 mae: 0.745796 (1524.1262527526037 steps/sec)\n",
      "Step #2231\tEpoch   0 Batch 2230/3125   Loss: 0.757308 mae: 0.672252 (1782.9741287695224 steps/sec)\n",
      "Step #2232\tEpoch   0 Batch 2231/3125   Loss: 0.926151 mae: 0.754713 (1934.0889597993194 steps/sec)\n",
      "Step #2233\tEpoch   0 Batch 2232/3125   Loss: 0.779140 mae: 0.702073 (2016.9383613683794 steps/sec)\n",
      "Step #2234\tEpoch   0 Batch 2233/3125   Loss: 0.790479 mae: 0.711000 (1811.5439766425375 steps/sec)\n",
      "Step #2235\tEpoch   0 Batch 2234/3125   Loss: 0.838147 mae: 0.737589 (1398.9966912157113 steps/sec)\n",
      "Step #2236\tEpoch   0 Batch 2235/3125   Loss: 0.992034 mae: 0.789400 (1773.7300607270329 steps/sec)\n",
      "Step #2237\tEpoch   0 Batch 2236/3125   Loss: 0.815515 mae: 0.719223 (1638.592022502637 steps/sec)\n",
      "Step #2238\tEpoch   0 Batch 2237/3125   Loss: 0.750019 mae: 0.701846 (1550.9643829132647 steps/sec)\n",
      "Step #2239\tEpoch   0 Batch 2238/3125   Loss: 0.879896 mae: 0.734173 (1552.9512821841931 steps/sec)\n",
      "Step #2240\tEpoch   0 Batch 2239/3125   Loss: 0.634459 mae: 0.629472 (1674.0387148273799 steps/sec)\n",
      "Step #2241\tEpoch   0 Batch 2240/3125   Loss: 0.855218 mae: 0.732751 (1410.4284782330906 steps/sec)\n",
      "Step #2242\tEpoch   0 Batch 2241/3125   Loss: 0.928601 mae: 0.767217 (1637.248809430869 steps/sec)\n",
      "Step #2243\tEpoch   0 Batch 2242/3125   Loss: 0.775775 mae: 0.725246 (1726.9322617302657 steps/sec)\n",
      "Step #2244\tEpoch   0 Batch 2243/3125   Loss: 0.725440 mae: 0.669243 (1677.2922131934224 steps/sec)\n",
      "Step #2245\tEpoch   0 Batch 2244/3125   Loss: 0.909569 mae: 0.772122 (1543.976205200695 steps/sec)\n",
      "Step #2246\tEpoch   0 Batch 2245/3125   Loss: 0.766533 mae: 0.692241 (1679.939119637922 steps/sec)\n",
      "Step #2247\tEpoch   0 Batch 2246/3125   Loss: 0.824991 mae: 0.717392 (1615.1444436742836 steps/sec)\n",
      "Step #2248\tEpoch   0 Batch 2247/3125   Loss: 0.835597 mae: 0.721978 (1123.622764436729 steps/sec)\n",
      "Step #2249\tEpoch   0 Batch 2248/3125   Loss: 0.864668 mae: 0.726938 (1547.2112376792775 steps/sec)\n",
      "Step #2250\tEpoch   0 Batch 2249/3125   Loss: 0.842650 mae: 0.740884 (1513.4461058830323 steps/sec)\n",
      "Step #2251\tEpoch   0 Batch 2250/3125   Loss: 0.939852 mae: 0.773979 (1399.1553637073262 steps/sec)\n",
      "Step #2252\tEpoch   0 Batch 2251/3125   Loss: 0.884148 mae: 0.735385 (1529.8075660534264 steps/sec)\n",
      "Step #2253\tEpoch   0 Batch 2252/3125   Loss: 0.929216 mae: 0.754470 (1743.2540045386156 steps/sec)\n",
      "Step #2254\tEpoch   0 Batch 2253/3125   Loss: 0.775704 mae: 0.691659 (1869.5859929394144 steps/sec)\n",
      "Step #2255\tEpoch   0 Batch 2254/3125   Loss: 0.928227 mae: 0.772572 (1521.1082904185103 steps/sec)\n",
      "Step #2256\tEpoch   0 Batch 2255/3125   Loss: 0.972062 mae: 0.777377 (1218.5376284107283 steps/sec)\n",
      "Step #2257\tEpoch   0 Batch 2256/3125   Loss: 0.744735 mae: 0.699260 (1470.0659624133411 steps/sec)\n",
      "Step #2258\tEpoch   0 Batch 2257/3125   Loss: 0.856913 mae: 0.749173 (1622.1531226311474 steps/sec)\n",
      "Step #2259\tEpoch   0 Batch 2258/3125   Loss: 0.869025 mae: 0.758191 (2034.9042781319438 steps/sec)\n",
      "Step #2260\tEpoch   0 Batch 2259/3125   Loss: 0.846972 mae: 0.721742 (1868.320148243176 steps/sec)\n",
      "Step #2261\tEpoch   0 Batch 2260/3125   Loss: 0.896306 mae: 0.713180 (1981.5297396891388 steps/sec)\n",
      "Step #2262\tEpoch   0 Batch 2261/3125   Loss: 0.893065 mae: 0.719373 (1403.3592526666578 steps/sec)\n",
      "Step #2263\tEpoch   0 Batch 2262/3125   Loss: 0.938975 mae: 0.764873 (1695.53146248191 steps/sec)\n",
      "Step #2264\tEpoch   0 Batch 2263/3125   Loss: 0.810309 mae: 0.725822 (1617.2494100590711 steps/sec)\n",
      "Step #2265\tEpoch   0 Batch 2264/3125   Loss: 0.779028 mae: 0.706897 (1824.8640370341366 steps/sec)\n",
      "Step #2266\tEpoch   0 Batch 2265/3125   Loss: 0.875730 mae: 0.741771 (1868.2535723194242 steps/sec)\n",
      "Step #2267\tEpoch   0 Batch 2266/3125   Loss: 0.911409 mae: 0.771249 (2004.9829345009896 steps/sec)\n",
      "Step #2268\tEpoch   0 Batch 2267/3125   Loss: 0.755784 mae: 0.691072 (1898.167138835839 steps/sec)\n",
      "Step #2269\tEpoch   0 Batch 2268/3125   Loss: 0.897829 mae: 0.733709 (1989.5756448812697 steps/sec)\n",
      "Step #2270\tEpoch   0 Batch 2269/3125   Loss: 0.852066 mae: 0.720932 (1628.5649942146258 steps/sec)\n",
      "Step #2271\tEpoch   0 Batch 2270/3125   Loss: 0.851658 mae: 0.697737 (1307.8753710679273 steps/sec)\n",
      "Step #2272\tEpoch   0 Batch 2271/3125   Loss: 0.917582 mae: 0.754333 (1342.3920627300367 steps/sec)\n",
      "Step #2273\tEpoch   0 Batch 2272/3125   Loss: 0.906647 mae: 0.761441 (1507.419387300355 steps/sec)\n",
      "Step #2274\tEpoch   0 Batch 2273/3125   Loss: 0.875842 mae: 0.736825 (1908.6186497752053 steps/sec)\n",
      "Step #2275\tEpoch   0 Batch 2274/3125   Loss: 0.896874 mae: 0.738426 (1989.8965746275737 steps/sec)\n",
      "Step #2276\tEpoch   0 Batch 2275/3125   Loss: 0.758856 mae: 0.689046 (2169.7518985246343 steps/sec)\n",
      "Step #2277\tEpoch   0 Batch 2276/3125   Loss: 0.847906 mae: 0.716217 (1983.4225509296914 steps/sec)\n",
      "Step #2278\tEpoch   0 Batch 2277/3125   Loss: 0.840722 mae: 0.734585 (1658.942372345054 steps/sec)\n",
      "Step #2279\tEpoch   0 Batch 2278/3125   Loss: 0.899832 mae: 0.754760 (1492.7623711633735 steps/sec)\n",
      "Step #2280\tEpoch   0 Batch 2279/3125   Loss: 0.807275 mae: 0.742032 (1149.237733036683 steps/sec)\n",
      "Step #2281\tEpoch   0 Batch 2280/3125   Loss: 0.856882 mae: 0.742974 (1652.5499590241443 steps/sec)\n",
      "Step #2282\tEpoch   0 Batch 2281/3125   Loss: 0.970036 mae: 0.789629 (1865.8765959339828 steps/sec)\n",
      "Step #2283\tEpoch   0 Batch 2282/3125   Loss: 0.863075 mae: 0.758749 (1845.9545102457573 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2284\tEpoch   0 Batch 2283/3125   Loss: 0.781432 mae: 0.719373 (1348.6161127688033 steps/sec)\n",
      "Step #2285\tEpoch   0 Batch 2284/3125   Loss: 0.889657 mae: 0.767196 (1200.1075841073093 steps/sec)\n",
      "Step #2286\tEpoch   0 Batch 2285/3125   Loss: 0.987009 mae: 0.778735 (1612.8464638385578 steps/sec)\n",
      "Step #2287\tEpoch   0 Batch 2286/3125   Loss: 0.880949 mae: 0.760600 (1480.0465789195102 steps/sec)\n",
      "Step #2288\tEpoch   0 Batch 2287/3125   Loss: 0.801681 mae: 0.710624 (1494.6241616956377 steps/sec)\n",
      "Step #2289\tEpoch   0 Batch 2288/3125   Loss: 0.833258 mae: 0.708000 (1495.2635594247538 steps/sec)\n",
      "Step #2290\tEpoch   0 Batch 2289/3125   Loss: 1.054350 mae: 0.806572 (1309.4187651021798 steps/sec)\n",
      "Step #2291\tEpoch   0 Batch 2290/3125   Loss: 0.782410 mae: 0.705039 (1655.7988235758557 steps/sec)\n",
      "Step #2292\tEpoch   0 Batch 2291/3125   Loss: 0.697646 mae: 0.670170 (1506.6071826260622 steps/sec)\n",
      "Step #2293\tEpoch   0 Batch 2292/3125   Loss: 0.814534 mae: 0.739838 (1538.1893662121624 steps/sec)\n",
      "Step #2294\tEpoch   0 Batch 2293/3125   Loss: 0.808587 mae: 0.729823 (1440.0549337361808 steps/sec)\n",
      "Step #2295\tEpoch   0 Batch 2294/3125   Loss: 0.765915 mae: 0.676343 (1603.0698435266509 steps/sec)\n",
      "Step #2296\tEpoch   0 Batch 2295/3125   Loss: 1.057000 mae: 0.806794 (1365.706768777921 steps/sec)\n",
      "Step #2297\tEpoch   0 Batch 2296/3125   Loss: 0.891725 mae: 0.758038 (1333.15025300684 steps/sec)\n",
      "Step #2298\tEpoch   0 Batch 2297/3125   Loss: 0.956526 mae: 0.809179 (1356.4054304027527 steps/sec)\n",
      "Step #2299\tEpoch   0 Batch 2298/3125   Loss: 0.892592 mae: 0.735222 (1208.5729269318765 steps/sec)\n",
      "Step #2300\tEpoch   0 Batch 2299/3125   Loss: 0.868497 mae: 0.717197 (1391.0441029178635 steps/sec)\n",
      "Step #2301\tEpoch   0 Batch 2300/3125   Loss: 0.836799 mae: 0.728207 (1372.7242379216223 steps/sec)\n",
      "Step #2302\tEpoch   0 Batch 2301/3125   Loss: 0.879475 mae: 0.728788 (1349.136666580891 steps/sec)\n",
      "Step #2303\tEpoch   0 Batch 2302/3125   Loss: 0.883776 mae: 0.743320 (1732.1384619196683 steps/sec)\n",
      "Step #2304\tEpoch   0 Batch 2303/3125   Loss: 0.867621 mae: 0.735319 (2278.0026286918455 steps/sec)\n",
      "Step #2305\tEpoch   0 Batch 2304/3125   Loss: 0.789769 mae: 0.690641 (2053.2132367338945 steps/sec)\n",
      "Step #2306\tEpoch   0 Batch 2305/3125   Loss: 0.815520 mae: 0.712596 (2297.2920865831215 steps/sec)\n",
      "Step #2307\tEpoch   0 Batch 2306/3125   Loss: 0.812596 mae: 0.726869 (2143.428623991987 steps/sec)\n",
      "Step #2308\tEpoch   0 Batch 2307/3125   Loss: 0.987484 mae: 0.790231 (2010.1333282213 steps/sec)\n",
      "Step #2309\tEpoch   0 Batch 2308/3125   Loss: 0.926597 mae: 0.760835 (1910.4964926664845 steps/sec)\n",
      "Step #2310\tEpoch   0 Batch 2309/3125   Loss: 0.904979 mae: 0.752518 (1874.9179726963068 steps/sec)\n",
      "Step #2311\tEpoch   0 Batch 2310/3125   Loss: 0.858121 mae: 0.758872 (1542.4885443405733 steps/sec)\n",
      "Step #2312\tEpoch   0 Batch 2311/3125   Loss: 0.877185 mae: 0.743377 (1903.9055832955062 steps/sec)\n",
      "Step #2313\tEpoch   0 Batch 2312/3125   Loss: 0.923706 mae: 0.730967 (2084.3747825827677 steps/sec)\n",
      "Step #2314\tEpoch   0 Batch 2313/3125   Loss: 0.784911 mae: 0.711771 (2096.1877536333286 steps/sec)\n",
      "Step #2315\tEpoch   0 Batch 2314/3125   Loss: 0.841357 mae: 0.717297 (2066.4446327572277 steps/sec)\n",
      "Step #2316\tEpoch   0 Batch 2315/3125   Loss: 0.927622 mae: 0.762247 (1978.7066216292717 steps/sec)\n",
      "Step #2317\tEpoch   0 Batch 2316/3125   Loss: 0.956613 mae: 0.788523 (2095.4756195043965 steps/sec)\n",
      "Step #2318\tEpoch   0 Batch 2317/3125   Loss: 0.923433 mae: 0.780195 (1915.329747107121 steps/sec)\n",
      "Step #2319\tEpoch   0 Batch 2318/3125   Loss: 0.865803 mae: 0.732198 (1556.3742151900612 steps/sec)\n",
      "Step #2320\tEpoch   0 Batch 2319/3125   Loss: 0.892918 mae: 0.753132 (1890.2968190872791 steps/sec)\n",
      "Step #2321\tEpoch   0 Batch 2320/3125   Loss: 0.863902 mae: 0.757277 (2138.5762214086863 steps/sec)\n",
      "Step #2322\tEpoch   0 Batch 2321/3125   Loss: 0.808792 mae: 0.705436 (1804.4052863436123 steps/sec)\n",
      "Step #2323\tEpoch   0 Batch 2322/3125   Loss: 0.837112 mae: 0.720888 (2040.2494430337877 steps/sec)\n",
      "Step #2324\tEpoch   0 Batch 2323/3125   Loss: 0.812973 mae: 0.710771 (2008.1315293058708 steps/sec)\n",
      "Step #2325\tEpoch   0 Batch 2324/3125   Loss: 0.715861 mae: 0.662483 (2139.798177681186 steps/sec)\n",
      "Step #2326\tEpoch   0 Batch 2325/3125   Loss: 0.790484 mae: 0.712513 (2004.4846735421465 steps/sec)\n",
      "Step #2327\tEpoch   0 Batch 2326/3125   Loss: 0.782128 mae: 0.689731 (1641.2722263961934 steps/sec)\n",
      "Step #2328\tEpoch   0 Batch 2327/3125   Loss: 0.856636 mae: 0.722606 (2179.674475648035 steps/sec)\n",
      "Step #2329\tEpoch   0 Batch 2328/3125   Loss: 0.903309 mae: 0.759514 (2137.0071839812504 steps/sec)\n",
      "Step #2330\tEpoch   0 Batch 2329/3125   Loss: 0.847242 mae: 0.725518 (2245.6813655151736 steps/sec)\n",
      "Step #2331\tEpoch   0 Batch 2330/3125   Loss: 0.942601 mae: 0.786235 (2119.8985110232798 steps/sec)\n",
      "Step #2332\tEpoch   0 Batch 2331/3125   Loss: 0.783212 mae: 0.722825 (2021.5072005552236 steps/sec)\n",
      "Step #2333\tEpoch   0 Batch 2332/3125   Loss: 0.880364 mae: 0.745899 (2168.8991850411617 steps/sec)\n",
      "Step #2334\tEpoch   0 Batch 2333/3125   Loss: 0.796131 mae: 0.718654 (1953.055560729386 steps/sec)\n",
      "Step #2335\tEpoch   0 Batch 2334/3125   Loss: 0.776405 mae: 0.703552 (1580.4779525363438 steps/sec)\n",
      "Step #2336\tEpoch   0 Batch 2335/3125   Loss: 0.930296 mae: 0.785471 (2028.4092118117014 steps/sec)\n",
      "Step #2337\tEpoch   0 Batch 2336/3125   Loss: 0.738884 mae: 0.684442 (2207.2263795480617 steps/sec)\n",
      "Step #2338\tEpoch   0 Batch 2337/3125   Loss: 0.890082 mae: 0.745178 (1846.2470287877454 steps/sec)\n",
      "Step #2339\tEpoch   0 Batch 2338/3125   Loss: 0.834540 mae: 0.725524 (1843.9905389126784 steps/sec)\n",
      "Step #2340\tEpoch   0 Batch 2339/3125   Loss: 0.770714 mae: 0.701712 (1848.4275842617403 steps/sec)\n",
      "Step #2341\tEpoch   0 Batch 2340/3125   Loss: 0.846729 mae: 0.718751 (1694.1754317935793 steps/sec)\n",
      "Step #2342\tEpoch   0 Batch 2341/3125   Loss: 0.827054 mae: 0.737519 (2195.212125652916 steps/sec)\n",
      "Step #2343\tEpoch   0 Batch 2342/3125   Loss: 0.868083 mae: 0.752066 (2050.783778762187 steps/sec)\n",
      "Step #2344\tEpoch   0 Batch 2343/3125   Loss: 0.757922 mae: 0.696205 (1580.0849883969743 steps/sec)\n",
      "Step #2345\tEpoch   0 Batch 2344/3125   Loss: 0.915999 mae: 0.770999 (2188.088977922457 steps/sec)\n",
      "Step #2346\tEpoch   0 Batch 2345/3125   Loss: 0.849941 mae: 0.701974 (2129.1964059089296 steps/sec)\n",
      "Step #2347\tEpoch   0 Batch 2346/3125   Loss: 0.761855 mae: 0.677744 (2105.3417795223418 steps/sec)\n",
      "Step #2348\tEpoch   0 Batch 2347/3125   Loss: 0.926864 mae: 0.758961 (2030.9432500484215 steps/sec)\n",
      "Step #2349\tEpoch   0 Batch 2348/3125   Loss: 1.024538 mae: 0.791560 (1983.1787190180337 steps/sec)\n",
      "Step #2350\tEpoch   0 Batch 2349/3125   Loss: 0.851716 mae: 0.715483 (2068.5032302608865 steps/sec)\n",
      "Step #2351\tEpoch   0 Batch 2350/3125   Loss: 0.946746 mae: 0.777249 (2036.9004836923793 steps/sec)\n",
      "Step #2352\tEpoch   0 Batch 2351/3125   Loss: 0.842104 mae: 0.731563 (1538.4150411901496 steps/sec)\n",
      "Step #2353\tEpoch   0 Batch 2352/3125   Loss: 0.848214 mae: 0.734017 (1809.558817184817 steps/sec)\n",
      "Step #2354\tEpoch   0 Batch 2353/3125   Loss: 0.878430 mae: 0.730485 (2221.135800377047 steps/sec)\n",
      "Step #2355\tEpoch   0 Batch 2354/3125   Loss: 0.864855 mae: 0.727457 (1931.3281638516935 steps/sec)\n",
      "Step #2356\tEpoch   0 Batch 2355/3125   Loss: 0.866034 mae: 0.755349 (1934.374394687082 steps/sec)\n",
      "Step #2357\tEpoch   0 Batch 2356/3125   Loss: 0.942742 mae: 0.798862 (1960.9631118799382 steps/sec)\n",
      "Step #2358\tEpoch   0 Batch 2357/3125   Loss: 0.826996 mae: 0.727448 (1944.9950381643991 steps/sec)\n",
      "Step #2359\tEpoch   0 Batch 2358/3125   Loss: 0.783551 mae: 0.700856 (1859.5729587855571 steps/sec)\n",
      "Step #2360\tEpoch   0 Batch 2359/3125   Loss: 0.895543 mae: 0.743885 (1797.0762138168607 steps/sec)\n",
      "Step #2361\tEpoch   0 Batch 2360/3125   Loss: 0.843210 mae: 0.722445 (1394.3088133609915 steps/sec)\n",
      "Step #2362\tEpoch   0 Batch 2361/3125   Loss: 0.742801 mae: 0.684185 (2015.8139087806987 steps/sec)\n",
      "Step #2363\tEpoch   0 Batch 2362/3125   Loss: 0.907681 mae: 0.762134 (2189.4139018228134 steps/sec)\n",
      "Step #2364\tEpoch   0 Batch 2363/3125   Loss: 0.776812 mae: 0.711306 (2149.44807158158 steps/sec)\n",
      "Step #2365\tEpoch   0 Batch 2364/3125   Loss: 0.872381 mae: 0.733234 (2098.894082088133 steps/sec)\n",
      "Step #2366\tEpoch   0 Batch 2365/3125   Loss: 0.690518 mae: 0.642956 (2307.9611735962844 steps/sec)\n",
      "Step #2367\tEpoch   0 Batch 2366/3125   Loss: 0.822356 mae: 0.734169 (2248.835987346523 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2368\tEpoch   0 Batch 2367/3125   Loss: 0.917843 mae: 0.741880 (2083.6913538541025 steps/sec)\n",
      "Step #2369\tEpoch   0 Batch 2368/3125   Loss: 0.915830 mae: 0.762950 (1559.870280562911 steps/sec)\n",
      "Step #2370\tEpoch   0 Batch 2369/3125   Loss: 0.842306 mae: 0.743201 (2085.3281891674205 steps/sec)\n",
      "Step #2371\tEpoch   0 Batch 2370/3125   Loss: 0.838322 mae: 0.729922 (1893.8645763722072 steps/sec)\n",
      "Step #2372\tEpoch   0 Batch 2371/3125   Loss: 0.865791 mae: 0.733719 (2063.4963741378124 steps/sec)\n",
      "Step #2373\tEpoch   0 Batch 2372/3125   Loss: 0.819086 mae: 0.737967 (2074.2727713322056 steps/sec)\n",
      "Step #2374\tEpoch   0 Batch 2373/3125   Loss: 0.780161 mae: 0.692689 (2284.5788487515797 steps/sec)\n",
      "Step #2375\tEpoch   0 Batch 2374/3125   Loss: 0.815359 mae: 0.715487 (2174.769524323091 steps/sec)\n",
      "Step #2376\tEpoch   0 Batch 2375/3125   Loss: 0.942175 mae: 0.788487 (2112.1907984852146 steps/sec)\n",
      "Step #2377\tEpoch   0 Batch 2376/3125   Loss: 0.819160 mae: 0.714206 (1845.2236192621401 steps/sec)\n",
      "Step #2378\tEpoch   0 Batch 2377/3125   Loss: 0.792981 mae: 0.704403 (1841.2059595613734 steps/sec)\n",
      "Step #2379\tEpoch   0 Batch 2378/3125   Loss: 0.980714 mae: 0.797536 (2054.5609514759044 steps/sec)\n",
      "Step #2380\tEpoch   0 Batch 2379/3125   Loss: 0.938875 mae: 0.789323 (2087.2583952067203 steps/sec)\n",
      "Step #2381\tEpoch   0 Batch 2380/3125   Loss: 0.834733 mae: 0.718196 (2127.8353862700137 steps/sec)\n",
      "Step #2382\tEpoch   0 Batch 2381/3125   Loss: 0.828606 mae: 0.719677 (2185.125138058224 steps/sec)\n",
      "Step #2383\tEpoch   0 Batch 2382/3125   Loss: 0.845190 mae: 0.728373 (2053.2132367338945 steps/sec)\n",
      "Step #2384\tEpoch   0 Batch 2383/3125   Loss: 0.814693 mae: 0.724140 (2074.1701942477353 steps/sec)\n",
      "Step #2385\tEpoch   0 Batch 2384/3125   Loss: 0.937342 mae: 0.787586 (1797.3996588873556 steps/sec)\n",
      "Step #2386\tEpoch   0 Batch 2385/3125   Loss: 0.891772 mae: 0.753110 (2138.0965489116584 steps/sec)\n",
      "Step #2387\tEpoch   0 Batch 2386/3125   Loss: 0.786936 mae: 0.690870 (1863.0255758792541 steps/sec)\n",
      "Step #2388\tEpoch   0 Batch 2387/3125   Loss: 0.831690 mae: 0.736199 (1916.4324225532305 steps/sec)\n",
      "Step #2389\tEpoch   0 Batch 2388/3125   Loss: 0.801071 mae: 0.699880 (1832.7102395371803 steps/sec)\n",
      "Step #2390\tEpoch   0 Batch 2389/3125   Loss: 0.894630 mae: 0.779594 (1883.2353020411463 steps/sec)\n",
      "Step #2391\tEpoch   0 Batch 2390/3125   Loss: 0.909358 mae: 0.761754 (2048.8403446726197 steps/sec)\n",
      "Step #2392\tEpoch   0 Batch 2391/3125   Loss: 0.880458 mae: 0.742320 (1826.6760737585687 steps/sec)\n",
      "Step #2393\tEpoch   0 Batch 2392/3125   Loss: 0.829270 mae: 0.715852 (1731.8523779244051 steps/sec)\n",
      "Step #2394\tEpoch   0 Batch 2393/3125   Loss: 0.871048 mae: 0.746584 (2046.1616516411036 steps/sec)\n",
      "Step #2395\tEpoch   0 Batch 2394/3125   Loss: 0.825719 mae: 0.710695 (2043.4899538128739 steps/sec)\n",
      "Step #2396\tEpoch   0 Batch 2395/3125   Loss: 0.821973 mae: 0.740895 (1815.8732357779895 steps/sec)\n",
      "Step #2397\tEpoch   0 Batch 2396/3125   Loss: 0.972859 mae: 0.804876 (2005.366380752938 steps/sec)\n",
      "Step #2398\tEpoch   0 Batch 2397/3125   Loss: 0.739986 mae: 0.678653 (2184.5333333333333 steps/sec)\n",
      "Step #2399\tEpoch   0 Batch 2398/3125   Loss: 0.758679 mae: 0.697565 (1994.6660579428942 steps/sec)\n",
      "Step #2400\tEpoch   0 Batch 2399/3125   Loss: 0.776915 mae: 0.699927 (1965.5763210677264 steps/sec)\n",
      "Step #2401\tEpoch   0 Batch 2400/3125   Loss: 0.905613 mae: 0.747254 (1885.5889228556016 steps/sec)\n",
      "Step #2402\tEpoch   0 Batch 2401/3125   Loss: 0.832103 mae: 0.695787 (2090.2333276853615 steps/sec)\n",
      "Step #2403\tEpoch   0 Batch 2402/3125   Loss: 0.811278 mae: 0.714634 (2269.006556596629 steps/sec)\n",
      "Step #2404\tEpoch   0 Batch 2403/3125   Loss: 0.850393 mae: 0.738754 (2297.5941101713483 steps/sec)\n",
      "Step #2405\tEpoch   0 Batch 2404/3125   Loss: 0.856445 mae: 0.741296 (2220.524331879209 steps/sec)\n",
      "Step #2406\tEpoch   0 Batch 2405/3125   Loss: 0.856091 mae: 0.732510 (1985.5445413317427 steps/sec)\n",
      "Step #2407\tEpoch   0 Batch 2406/3125   Loss: 0.882503 mae: 0.740904 (2195.189147319279 steps/sec)\n",
      "Step #2408\tEpoch   0 Batch 2407/3125   Loss: 0.751909 mae: 0.710438 (1969.0643631754378 steps/sec)\n",
      "Step #2409\tEpoch   0 Batch 2408/3125   Loss: 0.955833 mae: 0.782719 (1807.1418723286915 steps/sec)\n",
      "Step #2410\tEpoch   0 Batch 2409/3125   Loss: 0.724594 mae: 0.681100 (1946.2585728471597 steps/sec)\n",
      "Step #2411\tEpoch   0 Batch 2410/3125   Loss: 0.942584 mae: 0.758952 (2131.8369877913656 steps/sec)\n",
      "Step #2412\tEpoch   0 Batch 2411/3125   Loss: 0.934184 mae: 0.785047 (2238.8487365353203 steps/sec)\n",
      "Step #2413\tEpoch   0 Batch 2412/3125   Loss: 0.892058 mae: 0.782144 (2322.9934203238886 steps/sec)\n",
      "Step #2414\tEpoch   0 Batch 2413/3125   Loss: 0.877157 mae: 0.745669 (2022.501470715877 steps/sec)\n",
      "Step #2415\tEpoch   0 Batch 2414/3125   Loss: 0.878332 mae: 0.743735 (2107.288055547182 steps/sec)\n",
      "Step #2416\tEpoch   0 Batch 2415/3125   Loss: 0.886189 mae: 0.765924 (2123.6767222610406 steps/sec)\n",
      "Step #2417\tEpoch   0 Batch 2416/3125   Loss: 0.816905 mae: 0.722727 (2058.4530820573223 steps/sec)\n",
      "Step #2418\tEpoch   0 Batch 2417/3125   Loss: 0.793200 mae: 0.696762 (2000.144969003338 steps/sec)\n",
      "Step #2419\tEpoch   0 Batch 2418/3125   Loss: 0.747584 mae: 0.681156 (1873.1428469350387 steps/sec)\n",
      "Step #2420\tEpoch   0 Batch 2419/3125   Loss: 0.800822 mae: 0.719464 (1847.4017565341485 steps/sec)\n",
      "Step #2421\tEpoch   0 Batch 2420/3125   Loss: 0.813426 mae: 0.715292 (2187.6324793457397 steps/sec)\n",
      "Step #2422\tEpoch   0 Batch 2421/3125   Loss: 0.779098 mae: 0.711859 (2076.0797901301785 steps/sec)\n",
      "Step #2423\tEpoch   0 Batch 2422/3125   Loss: 0.788101 mae: 0.699821 (2096.1668016032463 steps/sec)\n",
      "Step #2424\tEpoch   0 Batch 2423/3125   Loss: 0.698444 mae: 0.642832 (1994.8178445733854 steps/sec)\n",
      "Step #2425\tEpoch   0 Batch 2424/3125   Loss: 0.792819 mae: 0.712694 (2087.341494973624 steps/sec)\n",
      "Step #2426\tEpoch   0 Batch 2425/3125   Loss: 0.807682 mae: 0.715661 (2083.0704437999125 steps/sec)\n",
      "Step #2427\tEpoch   0 Batch 2426/3125   Loss: 0.982965 mae: 0.789897 (1974.1059746029953 steps/sec)\n",
      "Step #2428\tEpoch   0 Batch 2427/3125   Loss: 0.924974 mae: 0.748349 (1843.3746165408247 steps/sec)\n",
      "Step #2429\tEpoch   0 Batch 2428/3125   Loss: 0.801116 mae: 0.693027 (2194.1326637371835 steps/sec)\n",
      "Step #2430\tEpoch   0 Batch 2429/3125   Loss: 0.893173 mae: 0.726184 (2054.8226533411716 steps/sec)\n",
      "Step #2431\tEpoch   0 Batch 2430/3125   Loss: 0.837674 mae: 0.742780 (2011.4250637816272 steps/sec)\n",
      "Step #2432\tEpoch   0 Batch 2431/3125   Loss: 0.783700 mae: 0.719603 (2034.7660722255641 steps/sec)\n",
      "Step #2433\tEpoch   0 Batch 2432/3125   Loss: 0.820947 mae: 0.716874 (2041.918114989533 steps/sec)\n",
      "Step #2434\tEpoch   0 Batch 2433/3125   Loss: 0.756032 mae: 0.698786 (1849.1121025622938 steps/sec)\n",
      "Step #2435\tEpoch   0 Batch 2434/3125   Loss: 0.962604 mae: 0.757161 (1691.2516129032258 steps/sec)\n",
      "Step #2436\tEpoch   0 Batch 2435/3125   Loss: 0.789354 mae: 0.682879 (1786.4674463970832 steps/sec)\n",
      "Step #2437\tEpoch   0 Batch 2436/3125   Loss: 0.790493 mae: 0.707503 (1580.120704334657 steps/sec)\n",
      "Step #2438\tEpoch   0 Batch 2437/3125   Loss: 0.816985 mae: 0.706485 (1813.6432821364328 steps/sec)\n",
      "Step #2439\tEpoch   0 Batch 2438/3125   Loss: 0.843515 mae: 0.744677 (1796.2911887896255 steps/sec)\n",
      "Step #2440\tEpoch   0 Batch 2439/3125   Loss: 0.787102 mae: 0.696759 (1651.4178170106543 steps/sec)\n",
      "Step #2441\tEpoch   0 Batch 2440/3125   Loss: 0.807042 mae: 0.725135 (1928.2737821585538 steps/sec)\n",
      "Step #2442\tEpoch   0 Batch 2441/3125   Loss: 0.765421 mae: 0.689144 (1383.2087854104145 steps/sec)\n",
      "Step #2443\tEpoch   0 Batch 2442/3125   Loss: 0.790876 mae: 0.685233 (1454.7593612563992 steps/sec)\n",
      "Step #2444\tEpoch   0 Batch 2443/3125   Loss: 0.798194 mae: 0.711989 (1731.7808716906968 steps/sec)\n",
      "Step #2445\tEpoch   0 Batch 2444/3125   Loss: 0.797939 mae: 0.718408 (1967.19884434272 steps/sec)\n",
      "Step #2446\tEpoch   0 Batch 2445/3125   Loss: 0.905176 mae: 0.757203 (2031.0415960486175 steps/sec)\n",
      "Step #2447\tEpoch   0 Batch 2446/3125   Loss: 0.778011 mae: 0.698275 (1736.642403464752 steps/sec)\n",
      "Step #2448\tEpoch   0 Batch 2447/3125   Loss: 0.783662 mae: 0.709470 (1951.0024095040515 steps/sec)\n",
      "Step #2449\tEpoch   0 Batch 2448/3125   Loss: 0.798447 mae: 0.708809 (1915.627169425262 steps/sec)\n",
      "Step #2450\tEpoch   0 Batch 2449/3125   Loss: 0.847174 mae: 0.744042 (2201.780614816059 steps/sec)\n",
      "Step #2451\tEpoch   0 Batch 2450/3125   Loss: 0.910482 mae: 0.768653 (2264.596246463512 steps/sec)\n",
      "Step #2452\tEpoch   0 Batch 2451/3125   Loss: 0.883038 mae: 0.751506 (2177.9767159280914 steps/sec)\n",
      "Step #2453\tEpoch   0 Batch 2452/3125   Loss: 0.885053 mae: 0.742238 (1943.9493516003745 steps/sec)\n",
      "Step #2454\tEpoch   0 Batch 2453/3125   Loss: 0.891800 mae: 0.743482 (2002.7235830587786 steps/sec)\n",
      "Step #2455\tEpoch   0 Batch 2454/3125   Loss: 0.769144 mae: 0.685042 (2082.284488750323 steps/sec)\n",
      "Step #2456\tEpoch   0 Batch 2455/3125   Loss: 0.911230 mae: 0.752276 (2084.2919188606297 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2457\tEpoch   0 Batch 2456/3125   Loss: 0.790827 mae: 0.734083 (1740.2740089787314 steps/sec)\n",
      "Step #2458\tEpoch   0 Batch 2457/3125   Loss: 0.808490 mae: 0.702104 (1872.239829305527 steps/sec)\n",
      "Step #2459\tEpoch   0 Batch 2458/3125   Loss: 0.836004 mae: 0.735548 (1520.8325174951956 steps/sec)\n",
      "Step #2460\tEpoch   0 Batch 2459/3125   Loss: 0.788625 mae: 0.715262 (2118.827606413611 steps/sec)\n",
      "Step #2461\tEpoch   0 Batch 2460/3125   Loss: 0.860658 mae: 0.741944 (2211.206005778031 steps/sec)\n",
      "Step #2462\tEpoch   0 Batch 2461/3125   Loss: 0.737847 mae: 0.687143 (2096.8584398184253 steps/sec)\n",
      "Step #2463\tEpoch   0 Batch 2462/3125   Loss: 0.865846 mae: 0.740539 (2097.466619992999 steps/sec)\n",
      "Step #2464\tEpoch   0 Batch 2463/3125   Loss: 0.853728 mae: 0.717323 (2038.0683971661533 steps/sec)\n",
      "Step #2465\tEpoch   0 Batch 2464/3125   Loss: 0.853458 mae: 0.728950 (1975.4262353761233 steps/sec)\n",
      "Step #2466\tEpoch   0 Batch 2465/3125   Loss: 0.734031 mae: 0.698500 (1932.7342936400416 steps/sec)\n",
      "Step #2467\tEpoch   0 Batch 2466/3125   Loss: 0.775937 mae: 0.673083 (2055.6686075006373 steps/sec)\n",
      "Step #2468\tEpoch   0 Batch 2467/3125   Loss: 0.798986 mae: 0.714188 (2258.7667618073133 steps/sec)\n",
      "Step #2469\tEpoch   0 Batch 2468/3125   Loss: 1.039079 mae: 0.824158 (2028.7427930193862 steps/sec)\n",
      "Step #2470\tEpoch   0 Batch 2469/3125   Loss: 0.887571 mae: 0.715024 (2099.167200512492 steps/sec)\n",
      "Step #2471\tEpoch   0 Batch 2470/3125   Loss: 0.907466 mae: 0.768620 (1773.9851290423544 steps/sec)\n",
      "Step #2472\tEpoch   0 Batch 2471/3125   Loss: 0.832172 mae: 0.724792 (2029.3906463193955 steps/sec)\n",
      "Step #2473\tEpoch   0 Batch 2472/3125   Loss: 0.752287 mae: 0.660016 (1891.4220262092228 steps/sec)\n",
      "Step #2474\tEpoch   0 Batch 2473/3125   Loss: 0.868034 mae: 0.738686 (1562.6016139007072 steps/sec)\n",
      "Step #2475\tEpoch   0 Batch 2474/3125   Loss: 0.925831 mae: 0.752390 (1684.9195764305111 steps/sec)\n",
      "Step #2476\tEpoch   0 Batch 2475/3125   Loss: 0.884029 mae: 0.754613 (1854.4753550395274 steps/sec)\n",
      "Step #2477\tEpoch   0 Batch 2476/3125   Loss: 0.841563 mae: 0.722614 (2009.6517622707322 steps/sec)\n",
      "Step #2478\tEpoch   0 Batch 2477/3125   Loss: 0.859864 mae: 0.744499 (1723.8377063194582 steps/sec)\n",
      "Step #2479\tEpoch   0 Batch 2478/3125   Loss: 0.820263 mae: 0.731239 (1906.2245491564863 steps/sec)\n",
      "Step #2480\tEpoch   0 Batch 2479/3125   Loss: 0.851619 mae: 0.745028 (1932.5739983043975 steps/sec)\n",
      "Step #2481\tEpoch   0 Batch 2480/3125   Loss: 0.925754 mae: 0.755732 (1637.159340187514 steps/sec)\n",
      "Step #2482\tEpoch   0 Batch 2481/3125   Loss: 0.807851 mae: 0.713613 (1678.688524590164 steps/sec)\n",
      "Step #2483\tEpoch   0 Batch 2482/3125   Loss: 0.913005 mae: 0.754857 (1975.1843654344243 steps/sec)\n",
      "Step #2484\tEpoch   0 Batch 2483/3125   Loss: 0.923041 mae: 0.785580 (1905.341292122071 steps/sec)\n",
      "Step #2485\tEpoch   0 Batch 2484/3125   Loss: 0.770315 mae: 0.711370 (2117.864716931591 steps/sec)\n",
      "Step #2486\tEpoch   0 Batch 2485/3125   Loss: 0.799821 mae: 0.716543 (2008.3430694681197 steps/sec)\n",
      "Step #2487\tEpoch   0 Batch 2486/3125   Loss: 0.868478 mae: 0.709891 (2028.3895928039462 steps/sec)\n",
      "Step #2488\tEpoch   0 Batch 2487/3125   Loss: 0.757187 mae: 0.665653 (1975.8915364104882 steps/sec)\n",
      "Step #2489\tEpoch   0 Batch 2488/3125   Loss: 0.843420 mae: 0.737109 (1910.2876610008927 steps/sec)\n",
      "Step #2490\tEpoch   0 Batch 2489/3125   Loss: 0.914926 mae: 0.775000 (1781.9136552497641 steps/sec)\n",
      "Step #2491\tEpoch   0 Batch 2490/3125   Loss: 0.942625 mae: 0.762976 (2088.984072277395 steps/sec)\n",
      "Step #2492\tEpoch   0 Batch 2491/3125   Loss: 0.842755 mae: 0.722886 (1919.6075021281658 steps/sec)\n",
      "Step #2493\tEpoch   0 Batch 2492/3125   Loss: 0.963545 mae: 0.761648 (2007.3050269918451 steps/sec)\n",
      "Step #2494\tEpoch   0 Batch 2493/3125   Loss: 0.907162 mae: 0.740378 (2159.652338681441 steps/sec)\n",
      "Step #2495\tEpoch   0 Batch 2494/3125   Loss: 0.815907 mae: 0.704819 (2216.487697640991 steps/sec)\n",
      "Step #2496\tEpoch   0 Batch 2495/3125   Loss: 0.864172 mae: 0.715733 (1891.5755673413428 steps/sec)\n",
      "Step #2497\tEpoch   0 Batch 2496/3125   Loss: 0.841047 mae: 0.739653 (1937.9852697919844 steps/sec)\n",
      "Step #2498\tEpoch   0 Batch 2497/3125   Loss: 0.727771 mae: 0.691216 (2035.3387618040122 steps/sec)\n",
      "Step #2499\tEpoch   0 Batch 2498/3125   Loss: 0.882285 mae: 0.734414 (1901.2819350510417 steps/sec)\n",
      "Step #2500\tEpoch   0 Batch 2499/3125   Loss: 0.932641 mae: 0.778812 (1821.234910985671 steps/sec)\n",
      "Step #2501\tEpoch   0 Batch 2500/3125   Loss: 0.816633 mae: 0.727093 (1863.3731985143852 steps/sec)\n",
      "Step #2502\tEpoch   0 Batch 2501/3125   Loss: 0.910074 mae: 0.745341 (1792.575497260473 steps/sec)\n",
      "Step #2503\tEpoch   0 Batch 2502/3125   Loss: 0.784811 mae: 0.708171 (2033.7597098441577 steps/sec)\n",
      "Step #2504\tEpoch   0 Batch 2503/3125   Loss: 0.861955 mae: 0.742033 (1986.560194000019 steps/sec)\n",
      "Step #2505\tEpoch   0 Batch 2504/3125   Loss: 0.750542 mae: 0.697831 (2012.1391220916287 steps/sec)\n",
      "Step #2506\tEpoch   0 Batch 2505/3125   Loss: 0.829888 mae: 0.709681 (1775.0662321723305 steps/sec)\n",
      "Step #2507\tEpoch   0 Batch 2506/3125   Loss: 1.017454 mae: 0.793767 (2123.6122081130893 steps/sec)\n",
      "Step #2508\tEpoch   0 Batch 2507/3125   Loss: 0.679828 mae: 0.650404 (1985.0746833765595 steps/sec)\n",
      "Step #2509\tEpoch   0 Batch 2508/3125   Loss: 0.895240 mae: 0.752876 (2067.8301682147153 steps/sec)\n",
      "Step #2510\tEpoch   0 Batch 2509/3125   Loss: 0.802099 mae: 0.721113 (1868.320148243176 steps/sec)\n",
      "Step #2511\tEpoch   0 Batch 2510/3125   Loss: 0.948227 mae: 0.775429 (1864.2676812573338 steps/sec)\n",
      "Step #2512\tEpoch   0 Batch 2511/3125   Loss: 0.812613 mae: 0.713701 (1967.1065837483936 steps/sec)\n",
      "Step #2513\tEpoch   0 Batch 2512/3125   Loss: 0.899140 mae: 0.760287 (2051.786989658647 steps/sec)\n",
      "Step #2514\tEpoch   0 Batch 2513/3125   Loss: 0.920341 mae: 0.756715 (1661.4131683395788 steps/sec)\n",
      "Step #2515\tEpoch   0 Batch 2514/3125   Loss: 0.861566 mae: 0.739307 (1871.5881911969443 steps/sec)\n",
      "Step #2516\tEpoch   0 Batch 2515/3125   Loss: 0.917990 mae: 0.753028 (2113.1698272908648 steps/sec)\n",
      "Step #2517\tEpoch   0 Batch 2516/3125   Loss: 0.723270 mae: 0.681155 (2045.5629035719162 steps/sec)\n",
      "Step #2518\tEpoch   0 Batch 2517/3125   Loss: 0.786233 mae: 0.703982 (2067.422464953962 steps/sec)\n",
      "Step #2519\tEpoch   0 Batch 2518/3125   Loss: 0.901455 mae: 0.767172 (2009.4592004905905 steps/sec)\n",
      "Step #2520\tEpoch   0 Batch 2519/3125   Loss: 0.792998 mae: 0.713418 (2088.547185595347 steps/sec)\n",
      "Step #2521\tEpoch   0 Batch 2520/3125   Loss: 0.895449 mae: 0.768759 (1848.4275842617403 steps/sec)\n",
      "Step #2522\tEpoch   0 Batch 2521/3125   Loss: 0.962540 mae: 0.784254 (1564.84028145683 steps/sec)\n",
      "Step #2523\tEpoch   0 Batch 2522/3125   Loss: 0.769279 mae: 0.688280 (1944.6162976150736 steps/sec)\n",
      "Step #2524\tEpoch   0 Batch 2523/3125   Loss: 0.914244 mae: 0.766052 (2134.831780933476 steps/sec)\n",
      "Step #2525\tEpoch   0 Batch 2524/3125   Loss: 0.803236 mae: 0.707649 (2266.8727638277865 steps/sec)\n",
      "Step #2526\tEpoch   0 Batch 2525/3125   Loss: 0.828226 mae: 0.724295 (2102.1551292075137 steps/sec)\n",
      "Step #2527\tEpoch   0 Batch 2526/3125   Loss: 0.974461 mae: 0.753216 (2142.5089137030945 steps/sec)\n",
      "Step #2528\tEpoch   0 Batch 2527/3125   Loss: 0.759835 mae: 0.688012 (2081.8710663728234 steps/sec)\n",
      "Step #2529\tEpoch   0 Batch 2528/3125   Loss: 0.811727 mae: 0.711653 (2139.2088459106026 steps/sec)\n",
      "Step #2530\tEpoch   0 Batch 2529/3125   Loss: 0.886252 mae: 0.754570 (1905.4451622282188 steps/sec)\n",
      "Step #2531\tEpoch   0 Batch 2530/3125   Loss: 0.732830 mae: 0.676376 (2040.9046673673556 steps/sec)\n",
      "Step #2532\tEpoch   0 Batch 2531/3125   Loss: 0.921802 mae: 0.761796 (2101.986569108951 steps/sec)\n",
      "Step #2533\tEpoch   0 Batch 2532/3125   Loss: 0.847217 mae: 0.739982 (2025.7640740311426 steps/sec)\n",
      "Step #2534\tEpoch   0 Batch 2533/3125   Loss: 0.906916 mae: 0.757731 (1797.6615806617522 steps/sec)\n",
      "Step #2535\tEpoch   0 Batch 2534/3125   Loss: 0.943935 mae: 0.765775 (1989.1982129814944 steps/sec)\n",
      "Step #2536\tEpoch   0 Batch 2535/3125   Loss: 0.974116 mae: 0.772616 (1873.6448998918956 steps/sec)\n",
      "Step #2537\tEpoch   0 Batch 2536/3125   Loss: 0.809795 mae: 0.714236 (1661.518471862398 steps/sec)\n",
      "Step #2538\tEpoch   0 Batch 2537/3125   Loss: 0.882312 mae: 0.743699 (1751.7432633355052 steps/sec)\n",
      "Step #2539\tEpoch   0 Batch 2538/3125   Loss: 0.823451 mae: 0.741285 (1736.7143117412259 steps/sec)\n",
      "Step #2540\tEpoch   0 Batch 2539/3125   Loss: 0.886779 mae: 0.750929 (1874.5492737430168 steps/sec)\n",
      "Step #2541\tEpoch   0 Batch 2540/3125   Loss: 0.828094 mae: 0.733577 (1835.3245934923775 steps/sec)\n",
      "Step #2542\tEpoch   0 Batch 2541/3125   Loss: 0.803336 mae: 0.717639 (1890.654694289681 steps/sec)\n",
      "Step #2543\tEpoch   0 Batch 2542/3125   Loss: 1.007990 mae: 0.791151 (1889.3601686516874 steps/sec)\n",
      "Step #2544\tEpoch   0 Batch 2543/3125   Loss: 0.819262 mae: 0.715463 (2071.69091861028 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2545\tEpoch   0 Batch 2544/3125   Loss: 0.931406 mae: 0.767259 (1549.3373129035595 steps/sec)\n",
      "Step #2546\tEpoch   0 Batch 2545/3125   Loss: 0.922309 mae: 0.774239 (1663.231526937322 steps/sec)\n",
      "Step #2547\tEpoch   0 Batch 2546/3125   Loss: 0.814977 mae: 0.736041 (1900.5582541869064 steps/sec)\n",
      "Step #2548\tEpoch   0 Batch 2547/3125   Loss: 1.058536 mae: 0.810634 (1715.7845647851948 steps/sec)\n",
      "Step #2549\tEpoch   0 Batch 2548/3125   Loss: 0.876620 mae: 0.741534 (1944.6343295346012 steps/sec)\n",
      "Step #2550\tEpoch   0 Batch 2549/3125   Loss: 0.687517 mae: 0.662765 (1728.5549437868847 steps/sec)\n",
      "Step #2551\tEpoch   0 Batch 2550/3125   Loss: 0.925642 mae: 0.759957 (2080.1158511788453 steps/sec)\n",
      "Step #2552\tEpoch   0 Batch 2551/3125   Loss: 0.820263 mae: 0.723373 (1941.4478800222182 steps/sec)\n",
      "Step #2553\tEpoch   0 Batch 2552/3125   Loss: 0.854823 mae: 0.724137 (1868.2535723194242 steps/sec)\n",
      "Step #2554\tEpoch   0 Batch 2553/3125   Loss: 0.862179 mae: 0.740839 (1841.5616575488018 steps/sec)\n",
      "Step #2555\tEpoch   0 Batch 2554/3125   Loss: 0.872492 mae: 0.728746 (1879.4884433729758 steps/sec)\n",
      "Step #2556\tEpoch   0 Batch 2555/3125   Loss: 0.704632 mae: 0.656668 (2071.588596716518 steps/sec)\n",
      "Step #2557\tEpoch   0 Batch 2556/3125   Loss: 1.050738 mae: 0.818547 (2081.168624960305 steps/sec)\n",
      "Step #2558\tEpoch   0 Batch 2557/3125   Loss: 0.952157 mae: 0.761444 (2022.1699386739692 steps/sec)\n",
      "Step #2559\tEpoch   0 Batch 2558/3125   Loss: 0.894535 mae: 0.766739 (2040.408246660375 steps/sec)\n",
      "Step #2560\tEpoch   0 Batch 2559/3125   Loss: 0.945154 mae: 0.776780 (2131.8369877913656 steps/sec)\n",
      "Step #2561\tEpoch   0 Batch 2560/3125   Loss: 0.653887 mae: 0.631358 (1930.4925759207608 steps/sec)\n",
      "Step #2562\tEpoch   0 Batch 2561/3125   Loss: 0.662988 mae: 0.645713 (1650.4950339204483 steps/sec)\n",
      "Step #2563\tEpoch   0 Batch 2562/3125   Loss: 0.905215 mae: 0.755565 (1554.770359936242 steps/sec)\n",
      "Step #2564\tEpoch   0 Batch 2563/3125   Loss: 0.843529 mae: 0.713430 (1613.6903662665436 steps/sec)\n",
      "Step #2565\tEpoch   0 Batch 2564/3125   Loss: 0.853471 mae: 0.731999 (1774.8108528968703 steps/sec)\n",
      "Step #2566\tEpoch   0 Batch 2565/3125   Loss: 0.859857 mae: 0.727118 (1703.2844936811669 steps/sec)\n",
      "Step #2567\tEpoch   0 Batch 2566/3125   Loss: 0.784568 mae: 0.682439 (1723.2427813111144 steps/sec)\n",
      "Step #2568\tEpoch   0 Batch 2567/3125   Loss: 0.853549 mae: 0.731333 (1817.1949465365751 steps/sec)\n",
      "Step #2569\tEpoch   0 Batch 2568/3125   Loss: 0.891699 mae: 0.725901 (1875.0520810049622 steps/sec)\n",
      "Step #2570\tEpoch   0 Batch 2569/3125   Loss: 0.848365 mae: 0.723935 (1593.48367880372 steps/sec)\n",
      "Step #2571\tEpoch   0 Batch 2570/3125   Loss: 0.944646 mae: 0.786962 (1886.4369883961501 steps/sec)\n",
      "Step #2572\tEpoch   0 Batch 2571/3125   Loss: 0.775502 mae: 0.692908 (2010.8657506400361 steps/sec)\n",
      "Step #2573\tEpoch   0 Batch 2572/3125   Loss: 0.905137 mae: 0.752818 (1914.6302940666283 steps/sec)\n",
      "Step #2574\tEpoch   0 Batch 2573/3125   Loss: 0.845504 mae: 0.752184 (1706.555562788881 steps/sec)\n",
      "Step #2575\tEpoch   0 Batch 2574/3125   Loss: 0.907019 mae: 0.734929 (1873.6448998918956 steps/sec)\n",
      "Step #2576\tEpoch   0 Batch 2575/3125   Loss: 0.807117 mae: 0.694008 (1861.1737768350802 steps/sec)\n",
      "Step #2577\tEpoch   0 Batch 2576/3125   Loss: 0.914781 mae: 0.757434 (1789.302504159379 steps/sec)\n",
      "Step #2578\tEpoch   0 Batch 2577/3125   Loss: 0.835359 mae: 0.724103 (1763.1719661683846 steps/sec)\n",
      "Step #2579\tEpoch   0 Batch 2578/3125   Loss: 0.885229 mae: 0.763505 (1893.6422657047144 steps/sec)\n",
      "Step #2580\tEpoch   0 Batch 2579/3125   Loss: 0.798671 mae: 0.697652 (1713.7515117837413 steps/sec)\n",
      "Step #2581\tEpoch   0 Batch 2580/3125   Loss: 0.869913 mae: 0.730413 (1754.8067509559949 steps/sec)\n",
      "Step #2582\tEpoch   0 Batch 2581/3125   Loss: 0.835654 mae: 0.733419 (2131.273691805811 steps/sec)\n",
      "Step #2583\tEpoch   0 Batch 2582/3125   Loss: 0.917824 mae: 0.762693 (1912.7792117768313 steps/sec)\n",
      "Step #2584\tEpoch   0 Batch 2583/3125   Loss: 0.856977 mae: 0.745050 (1728.497955954108 steps/sec)\n",
      "Step #2585\tEpoch   0 Batch 2584/3125   Loss: 1.030694 mae: 0.796788 (1823.4201648523633 steps/sec)\n",
      "Step #2586\tEpoch   0 Batch 2585/3125   Loss: 0.781846 mae: 0.719262 (1852.5423129924739 steps/sec)\n",
      "Step #2587\tEpoch   0 Batch 2586/3125   Loss: 0.940701 mae: 0.747339 (1921.5422534566012 steps/sec)\n",
      "Step #2588\tEpoch   0 Batch 2587/3125   Loss: 0.821152 mae: 0.737038 (1786.5435401155164 steps/sec)\n",
      "Step #2589\tEpoch   0 Batch 2588/3125   Loss: 0.841438 mae: 0.720480 (2065.7729095046247 steps/sec)\n",
      "Step #2590\tEpoch   0 Batch 2589/3125   Loss: 0.853271 mae: 0.726125 (2047.0804130956797 steps/sec)\n",
      "Step #2591\tEpoch   0 Batch 2590/3125   Loss: 0.906231 mae: 0.751368 (2091.7135447835626 steps/sec)\n",
      "Step #2592\tEpoch   0 Batch 2591/3125   Loss: 1.059423 mae: 0.803330 (1814.2237986072062 steps/sec)\n",
      "Step #2593\tEpoch   0 Batch 2592/3125   Loss: 0.827639 mae: 0.722199 (1776.7054119082313 steps/sec)\n",
      "Step #2594\tEpoch   0 Batch 2593/3125   Loss: 0.808964 mae: 0.705470 (1827.0261793788386 steps/sec)\n",
      "Step #2595\tEpoch   0 Batch 2594/3125   Loss: 0.778608 mae: 0.728178 (1802.6526384555214 steps/sec)\n",
      "Step #2596\tEpoch   0 Batch 2595/3125   Loss: 0.803108 mae: 0.704760 (1816.0147556740935 steps/sec)\n",
      "Step #2597\tEpoch   0 Batch 2596/3125   Loss: 0.859133 mae: 0.728299 (2188.751239367531 steps/sec)\n",
      "Step #2598\tEpoch   0 Batch 2597/3125   Loss: 0.868436 mae: 0.713398 (2107.923489028938 steps/sec)\n",
      "Step #2599\tEpoch   0 Batch 2598/3125   Loss: 0.865617 mae: 0.742927 (2039.416129377328 steps/sec)\n",
      "Step #2600\tEpoch   0 Batch 2599/3125   Loss: 1.019063 mae: 0.760379 (1740.5773284862971 steps/sec)\n",
      "Step #2601\tEpoch   0 Batch 2600/3125   Loss: 0.838252 mae: 0.715729 (1743.8483286213204 steps/sec)\n",
      "Step #2602\tEpoch   0 Batch 2601/3125   Loss: 0.896954 mae: 0.736426 (1999.916080181572 steps/sec)\n",
      "Step #2603\tEpoch   0 Batch 2602/3125   Loss: 0.828757 mae: 0.727594 (1930.5814338844498 steps/sec)\n",
      "Step #2604\tEpoch   0 Batch 2603/3125   Loss: 1.010838 mae: 0.788617 (1801.1353974320436 steps/sec)\n",
      "Step #2605\tEpoch   0 Batch 2604/3125   Loss: 0.967853 mae: 0.752457 (1819.9704937950187 steps/sec)\n",
      "Step #2606\tEpoch   0 Batch 2605/3125   Loss: 0.756494 mae: 0.698953 (1815.8575127066179 steps/sec)\n",
      "Step #2607\tEpoch   0 Batch 2606/3125   Loss: 0.819986 mae: 0.715460 (1948.1026650936824 steps/sec)\n",
      "Step #2608\tEpoch   0 Batch 2607/3125   Loss: 0.798410 mae: 0.732925 (1980.8372374187697 steps/sec)\n",
      "Step #2609\tEpoch   0 Batch 2608/3125   Loss: 0.813032 mae: 0.698509 (1842.2410991154018 steps/sec)\n",
      "Step #2610\tEpoch   0 Batch 2609/3125   Loss: 0.904704 mae: 0.734744 (2039.7335019209258 steps/sec)\n",
      "Step #2611\tEpoch   0 Batch 2610/3125   Loss: 0.900129 mae: 0.754772 (1634.0977270779276 steps/sec)\n",
      "Step #2612\tEpoch   0 Batch 2611/3125   Loss: 0.806610 mae: 0.708474 (1938.0031789450338 steps/sec)\n",
      "Step #2613\tEpoch   0 Batch 2612/3125   Loss: 0.748971 mae: 0.689903 (2002.1499832927586 steps/sec)\n",
      "Step #2614\tEpoch   0 Batch 2613/3125   Loss: 0.991009 mae: 0.777877 (1825.547102143143 steps/sec)\n",
      "Step #2615\tEpoch   0 Batch 2614/3125   Loss: 0.929374 mae: 0.776495 (1721.1637736468465 steps/sec)\n",
      "Step #2616\tEpoch   0 Batch 2615/3125   Loss: 0.990830 mae: 0.781514 (1533.3196853157078 steps/sec)\n",
      "Step #2617\tEpoch   0 Batch 2616/3125   Loss: 0.819777 mae: 0.718607 (1699.6952603254879 steps/sec)\n",
      "Step #2618\tEpoch   0 Batch 2617/3125   Loss: 0.769957 mae: 0.706214 (1714.5501369415035 steps/sec)\n",
      "Step #2619\tEpoch   0 Batch 2618/3125   Loss: 0.811536 mae: 0.740108 (1845.889518712812 steps/sec)\n",
      "Step #2620\tEpoch   0 Batch 2619/3125   Loss: 1.008831 mae: 0.802431 (1742.5297670979053 steps/sec)\n",
      "Step #2621\tEpoch   0 Batch 2620/3125   Loss: 0.785539 mae: 0.710016 (1878.6634417271343 steps/sec)\n",
      "Step #2622\tEpoch   0 Batch 2621/3125   Loss: 0.798923 mae: 0.700950 (2021.994465709575 steps/sec)\n",
      "Step #2623\tEpoch   0 Batch 2622/3125   Loss: 0.747473 mae: 0.672403 (2274.0750379527217 steps/sec)\n",
      "Step #2624\tEpoch   0 Batch 2623/3125   Loss: 0.892743 mae: 0.755766 (2022.0724499339524 steps/sec)\n",
      "Step #2625\tEpoch   0 Batch 2624/3125   Loss: 1.005182 mae: 0.795375 (1804.3276634918996 steps/sec)\n",
      "Step #2626\tEpoch   0 Batch 2625/3125   Loss: 0.949198 mae: 0.789002 (1899.9900341556665 steps/sec)\n",
      "Step #2627\tEpoch   0 Batch 2626/3125   Loss: 0.790862 mae: 0.705817 (1854.2458001768346 steps/sec)\n",
      "Step #2628\tEpoch   0 Batch 2627/3125   Loss: 0.763545 mae: 0.706843 (1950.6939018491646 steps/sec)\n",
      "Step #2629\tEpoch   0 Batch 2628/3125   Loss: 0.993042 mae: 0.803927 (1777.9706999457405 steps/sec)\n",
      "Step #2630\tEpoch   0 Batch 2629/3125   Loss: 0.816168 mae: 0.720775 (2111.574050766737 steps/sec)\n",
      "Step #2631\tEpoch   0 Batch 2630/3125   Loss: 0.617404 mae: 0.619482 (2051.766915822017 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2632\tEpoch   0 Batch 2631/3125   Loss: 0.658739 mae: 0.645142 (1943.8592588473018 steps/sec)\n",
      "Step #2633\tEpoch   0 Batch 2632/3125   Loss: 0.942674 mae: 0.759951 (1765.6510208377183 steps/sec)\n",
      "Step #2634\tEpoch   0 Batch 2633/3125   Loss: 0.705247 mae: 0.663721 (1693.1495789635155 steps/sec)\n",
      "Step #2635\tEpoch   0 Batch 2634/3125   Loss: 0.901429 mae: 0.762639 (1793.5413245758073 steps/sec)\n",
      "Step #2636\tEpoch   0 Batch 2635/3125   Loss: 0.807186 mae: 0.718473 (1896.9824155148708 steps/sec)\n",
      "Step #2637\tEpoch   0 Batch 2636/3125   Loss: 0.994572 mae: 0.774302 (2085.390398154409 steps/sec)\n",
      "Step #2638\tEpoch   0 Batch 2637/3125   Loss: 0.753448 mae: 0.685202 (2047.3402126267902 steps/sec)\n",
      "Step #2639\tEpoch   0 Batch 2638/3125   Loss: 0.875175 mae: 0.724059 (2038.6429474093516 steps/sec)\n",
      "Step #2640\tEpoch   0 Batch 2639/3125   Loss: 0.862398 mae: 0.746429 (1934.410079971959 steps/sec)\n",
      "Step #2641\tEpoch   0 Batch 2640/3125   Loss: 0.797069 mae: 0.714758 (1647.008191249578 steps/sec)\n",
      "Step #2642\tEpoch   0 Batch 2641/3125   Loss: 0.760718 mae: 0.709288 (2013.9361579533668 steps/sec)\n",
      "Step #2643\tEpoch   0 Batch 2642/3125   Loss: 0.816824 mae: 0.732272 (2181.397574320248 steps/sec)\n",
      "Step #2644\tEpoch   0 Batch 2643/3125   Loss: 0.781549 mae: 0.690880 (2174.386197743862 steps/sec)\n",
      "Step #2645\tEpoch   0 Batch 2644/3125   Loss: 0.991758 mae: 0.777274 (2099.7977451589004 steps/sec)\n",
      "Step #2646\tEpoch   0 Batch 2645/3125   Loss: 0.837158 mae: 0.746863 (2154.549190433139 steps/sec)\n",
      "Step #2647\tEpoch   0 Batch 2646/3125   Loss: 0.760132 mae: 0.674946 (2210.040888589133 steps/sec)\n",
      "Step #2648\tEpoch   0 Batch 2647/3125   Loss: 0.803241 mae: 0.716193 (2182.3509823509826 steps/sec)\n",
      "Step #2649\tEpoch   0 Batch 2648/3125   Loss: 0.867069 mae: 0.729253 (1966.6454105555347 steps/sec)\n",
      "Step #2650\tEpoch   0 Batch 2649/3125   Loss: 0.929797 mae: 0.774300 (1769.2856721026567 steps/sec)\n",
      "Step #2651\tEpoch   0 Batch 2650/3125   Loss: 0.784402 mae: 0.726151 (1836.0155135130403 steps/sec)\n",
      "Step #2652\tEpoch   0 Batch 2651/3125   Loss: 0.925699 mae: 0.749272 (2113.276298154922 steps/sec)\n",
      "Step #2653\tEpoch   0 Batch 2652/3125   Loss: 0.871650 mae: 0.754493 (1783.6261885726922 steps/sec)\n",
      "Step #2654\tEpoch   0 Batch 2653/3125   Loss: 0.865293 mae: 0.755412 (2091.0044469260374 steps/sec)\n",
      "Step #2655\tEpoch   0 Batch 2654/3125   Loss: 0.925296 mae: 0.785962 (2023.8091561800354 steps/sec)\n",
      "Step #2656\tEpoch   0 Batch 2655/3125   Loss: 0.902848 mae: 0.738974 (2131.446981939405 steps/sec)\n",
      "Step #2657\tEpoch   0 Batch 2656/3125   Loss: 0.980388 mae: 0.800897 (1893.6422657047144 steps/sec)\n",
      "Step #2658\tEpoch   0 Batch 2657/3125   Loss: 0.898294 mae: 0.760589 (1836.2888114459836 steps/sec)\n",
      "Step #2659\tEpoch   0 Batch 2658/3125   Loss: 0.920572 mae: 0.743156 (2230.8703699763846 steps/sec)\n",
      "Step #2660\tEpoch   0 Batch 2659/3125   Loss: 0.727467 mae: 0.686554 (1896.0905573035334 steps/sec)\n",
      "Step #2661\tEpoch   0 Batch 2660/3125   Loss: 0.914461 mae: 0.752956 (2154.349992295444 steps/sec)\n",
      "Step #2662\tEpoch   0 Batch 2661/3125   Loss: 0.820729 mae: 0.713198 (2085.162316679095 steps/sec)\n",
      "Step #2663\tEpoch   0 Batch 2662/3125   Loss: 0.729140 mae: 0.699278 (2263.667371875135 steps/sec)\n",
      "Step #2664\tEpoch   0 Batch 2663/3125   Loss: 0.804186 mae: 0.717171 (2123.1392241030208 steps/sec)\n",
      "Step #2665\tEpoch   0 Batch 2664/3125   Loss: 0.795096 mae: 0.721039 (2194.614845278833 steps/sec)\n",
      "Step #2666\tEpoch   0 Batch 2665/3125   Loss: 0.875422 mae: 0.748787 (1801.0735234758113 steps/sec)\n",
      "Step #2667\tEpoch   0 Batch 2666/3125   Loss: 0.933001 mae: 0.771610 (2201.9886811075294 steps/sec)\n",
      "Step #2668\tEpoch   0 Batch 2667/3125   Loss: 0.715788 mae: 0.687068 (1970.6741340750625 steps/sec)\n",
      "Step #2669\tEpoch   0 Batch 2668/3125   Loss: 0.868402 mae: 0.753939 (2142.1587554520474 steps/sec)\n",
      "Step #2670\tEpoch   0 Batch 2669/3125   Loss: 0.819791 mae: 0.723107 (2018.6468249766579 steps/sec)\n",
      "Step #2671\tEpoch   0 Batch 2670/3125   Loss: 0.875747 mae: 0.738139 (2247.5104490408316 steps/sec)\n",
      "Step #2672\tEpoch   0 Batch 2671/3125   Loss: 0.775718 mae: 0.676956 (2219.1614992275295 steps/sec)\n",
      "Step #2673\tEpoch   0 Batch 2672/3125   Loss: 0.890684 mae: 0.759416 (2187.883529988628 steps/sec)\n",
      "Step #2674\tEpoch   0 Batch 2673/3125   Loss: 0.852166 mae: 0.738461 (2073.308947108255 steps/sec)\n",
      "Step #2675\tEpoch   0 Batch 2674/3125   Loss: 0.865780 mae: 0.739514 (1766.841063229285 steps/sec)\n",
      "Step #2676\tEpoch   0 Batch 2675/3125   Loss: 0.835449 mae: 0.725647 (1824.8799164636268 steps/sec)\n",
      "Step #2677\tEpoch   0 Batch 2676/3125   Loss: 0.792824 mae: 0.697971 (2056.091845838603 steps/sec)\n",
      "Step #2678\tEpoch   0 Batch 2677/3125   Loss: 0.827607 mae: 0.750060 (2261.0071911420655 steps/sec)\n",
      "Step #2679\tEpoch   0 Batch 2678/3125   Loss: 0.884006 mae: 0.732911 (2280.380579568314 steps/sec)\n",
      "Step #2680\tEpoch   0 Batch 2679/3125   Loss: 0.775792 mae: 0.668646 (1887.489649710192 steps/sec)\n",
      "Step #2681\tEpoch   0 Batch 2680/3125   Loss: 0.795451 mae: 0.707504 (2133.2899314385695 steps/sec)\n",
      "Step #2682\tEpoch   0 Batch 2681/3125   Loss: 0.738715 mae: 0.678999 (2132.378899418392 steps/sec)\n",
      "Step #2683\tEpoch   0 Batch 2682/3125   Loss: 0.943643 mae: 0.782404 (2142.618361633871 steps/sec)\n",
      "Step #2684\tEpoch   0 Batch 2683/3125   Loss: 0.883846 mae: 0.717950 (1796.2296472039263 steps/sec)\n",
      "Step #2685\tEpoch   0 Batch 2684/3125   Loss: 0.915657 mae: 0.745933 (1999.3440872516494 steps/sec)\n",
      "Step #2686\tEpoch   0 Batch 2685/3125   Loss: 0.851478 mae: 0.722815 (2310.4021152363116 steps/sec)\n",
      "Step #2687\tEpoch   0 Batch 2686/3125   Loss: 0.787655 mae: 0.713750 (2143.8011122015046 steps/sec)\n",
      "Step #2688\tEpoch   0 Batch 2687/3125   Loss: 0.829476 mae: 0.725297 (2075.812645999129 steps/sec)\n",
      "Step #2689\tEpoch   0 Batch 2688/3125   Loss: 0.707403 mae: 0.673489 (2106.3990920139413 steps/sec)\n",
      "Step #2690\tEpoch   0 Batch 2689/3125   Loss: 0.853361 mae: 0.741099 (2115.8775160167484 steps/sec)\n",
      "Step #2691\tEpoch   0 Batch 2690/3125   Loss: 0.896507 mae: 0.754853 (2261.4216701173223 steps/sec)\n",
      "Step #2692\tEpoch   0 Batch 2691/3125   Loss: 0.845435 mae: 0.731508 (2155.2577489106307 steps/sec)\n",
      "Step #2693\tEpoch   0 Batch 2692/3125   Loss: 0.958324 mae: 0.774194 (1714.1297151497813 steps/sec)\n",
      "Step #2694\tEpoch   0 Batch 2693/3125   Loss: 0.791284 mae: 0.708240 (1898.0984188185034 steps/sec)\n",
      "Step #2695\tEpoch   0 Batch 2694/3125   Loss: 0.810132 mae: 0.715490 (2168.630046326936 steps/sec)\n",
      "Step #2696\tEpoch   0 Batch 2695/3125   Loss: 0.874515 mae: 0.746134 (2059.46381223608 steps/sec)\n",
      "Step #2697\tEpoch   0 Batch 2696/3125   Loss: 0.706452 mae: 0.681764 (1856.7904732391871 steps/sec)\n",
      "Step #2698\tEpoch   0 Batch 2697/3125   Loss: 0.748474 mae: 0.691823 (1806.8927488282327 steps/sec)\n",
      "Step #2699\tEpoch   0 Batch 2698/3125   Loss: 0.841008 mae: 0.722703 (1788.844628310658 steps/sec)\n",
      "Step #2700\tEpoch   0 Batch 2699/3125   Loss: 0.804138 mae: 0.719920 (1794.554260580856 steps/sec)\n",
      "Step #2701\tEpoch   0 Batch 2700/3125   Loss: 0.868050 mae: 0.736329 (1759.3409451262992 steps/sec)\n",
      "Step #2702\tEpoch   0 Batch 2701/3125   Loss: 0.911860 mae: 0.750690 (1818.6289728135976 steps/sec)\n",
      "Step #2703\tEpoch   0 Batch 2702/3125   Loss: 0.831928 mae: 0.741565 (1949.406482677846 steps/sec)\n",
      "Step #2704\tEpoch   0 Batch 2703/3125   Loss: 0.846185 mae: 0.750659 (2045.2437145253466 steps/sec)\n",
      "Step #2705\tEpoch   0 Batch 2704/3125   Loss: 0.832678 mae: 0.712210 (2073.9035413020047 steps/sec)\n",
      "Step #2706\tEpoch   0 Batch 2705/3125   Loss: 0.786191 mae: 0.702146 (2106.695329844194 steps/sec)\n",
      "Step #2707\tEpoch   0 Batch 2706/3125   Loss: 0.723438 mae: 0.684381 (1984.9243757926818 steps/sec)\n",
      "Step #2708\tEpoch   0 Batch 2707/3125   Loss: 0.883076 mae: 0.737418 (2122.9672821509557 steps/sec)\n",
      "Step #2709\tEpoch   0 Batch 2708/3125   Loss: 0.761968 mae: 0.710591 (1746.1133684140411 steps/sec)\n",
      "Step #2710\tEpoch   0 Batch 2709/3125   Loss: 0.748500 mae: 0.688189 (1848.3787094897716 steps/sec)\n",
      "Step #2711\tEpoch   0 Batch 2710/3125   Loss: 0.757764 mae: 0.704544 (1951.9830226085985 steps/sec)\n",
      "Step #2712\tEpoch   0 Batch 2711/3125   Loss: 0.859073 mae: 0.742336 (2007.7662466970473 steps/sec)\n",
      "Step #2713\tEpoch   0 Batch 2712/3125   Loss: 0.874123 mae: 0.733122 (2020.8448967005859 steps/sec)\n",
      "Step #2714\tEpoch   0 Batch 2713/3125   Loss: 0.883966 mae: 0.775359 (2078.4665853972783 steps/sec)\n",
      "Step #2715\tEpoch   0 Batch 2714/3125   Loss: 0.872019 mae: 0.752696 (1750.1644050539949 steps/sec)\n",
      "Step #2716\tEpoch   0 Batch 2715/3125   Loss: 0.815284 mae: 0.698370 (1839.8006807734148 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2717\tEpoch   0 Batch 2716/3125   Loss: 0.767533 mae: 0.696326 (1953.201080376269 steps/sec)\n",
      "Step #2718\tEpoch   0 Batch 2717/3125   Loss: 0.843505 mae: 0.735091 (1652.8364937500985 steps/sec)\n",
      "Step #2719\tEpoch   0 Batch 2718/3125   Loss: 0.787628 mae: 0.714999 (1979.2481855847184 steps/sec)\n",
      "Step #2720\tEpoch   0 Batch 2719/3125   Loss: 0.829423 mae: 0.730134 (1913.0583910310793 steps/sec)\n",
      "Step #2721\tEpoch   0 Batch 2720/3125   Loss: 0.730883 mae: 0.681967 (2017.1905659651418 steps/sec)\n",
      "Step #2722\tEpoch   0 Batch 2721/3125   Loss: 0.833535 mae: 0.733922 (2029.0568520453578 steps/sec)\n",
      "Step #2723\tEpoch   0 Batch 2722/3125   Loss: 0.835005 mae: 0.736306 (1728.611935377514 steps/sec)\n",
      "Step #2724\tEpoch   0 Batch 2723/3125   Loss: 0.794884 mae: 0.707436 (2003.4506147482255 steps/sec)\n",
      "Step #2725\tEpoch   0 Batch 2724/3125   Loss: 0.829961 mae: 0.730675 (1514.7797697297142 steps/sec)\n",
      "Step #2726\tEpoch   0 Batch 2725/3125   Loss: 0.896988 mae: 0.732193 (2029.9406645952513 steps/sec)\n",
      "Step #2727\tEpoch   0 Batch 2726/3125   Loss: 0.798579 mae: 0.713517 (2144.9193540138895 steps/sec)\n",
      "Step #2728\tEpoch   0 Batch 2727/3125   Loss: 0.831148 mae: 0.722969 (2040.4678043939364 steps/sec)\n",
      "Step #2729\tEpoch   0 Batch 2728/3125   Loss: 0.922110 mae: 0.761182 (2238.442489966698 steps/sec)\n",
      "Step #2730\tEpoch   0 Batch 2729/3125   Loss: 0.948200 mae: 0.771816 (1802.900594045787 steps/sec)\n",
      "Step #2731\tEpoch   0 Batch 2730/3125   Loss: 0.957295 mae: 0.758804 (1934.4636103680473 steps/sec)\n",
      "Step #2732\tEpoch   0 Batch 2731/3125   Loss: 0.747778 mae: 0.677340 (1962.10061469083 steps/sec)\n",
      "Step #2733\tEpoch   0 Batch 2732/3125   Loss: 0.810188 mae: 0.724856 (1990.5198515523412 steps/sec)\n",
      "Step #2734\tEpoch   0 Batch 2733/3125   Loss: 0.863017 mae: 0.737341 (1723.186143202248 steps/sec)\n",
      "Step #2735\tEpoch   0 Batch 2734/3125   Loss: 0.758352 mae: 0.705685 (1680.6122530752896 steps/sec)\n",
      "Step #2736\tEpoch   0 Batch 2735/3125   Loss: 0.697285 mae: 0.657872 (1944.904848462366 steps/sec)\n",
      "Step #2737\tEpoch   0 Batch 2736/3125   Loss: 0.883866 mae: 0.750560 (1746.3460129238558 steps/sec)\n",
      "Step #2738\tEpoch   0 Batch 2737/3125   Loss: 0.827798 mae: 0.724854 (1861.834710890544 steps/sec)\n",
      "Step #2739\tEpoch   0 Batch 2738/3125   Loss: 0.862865 mae: 0.767341 (1992.808544604508 steps/sec)\n",
      "Step #2740\tEpoch   0 Batch 2739/3125   Loss: 0.895920 mae: 0.760963 (1883.1676589172346 steps/sec)\n",
      "Step #2741\tEpoch   0 Batch 2740/3125   Loss: 0.878682 mae: 0.736623 (1625.220477688743 steps/sec)\n",
      "Step #2742\tEpoch   0 Batch 2741/3125   Loss: 0.781355 mae: 0.705142 (1847.9389528223746 steps/sec)\n",
      "Step #2743\tEpoch   0 Batch 2742/3125   Loss: 0.796089 mae: 0.699303 (2085.162316679095 steps/sec)\n",
      "Step #2744\tEpoch   0 Batch 2743/3125   Loss: 0.747403 mae: 0.677849 (1892.5144161786072 steps/sec)\n",
      "Step #2745\tEpoch   0 Batch 2744/3125   Loss: 0.886851 mae: 0.739831 (1932.9658782974175 steps/sec)\n",
      "Step #2746\tEpoch   0 Batch 2745/3125   Loss: 0.873899 mae: 0.729319 (1894.80569936483 steps/sec)\n",
      "Step #2747\tEpoch   0 Batch 2746/3125   Loss: 0.702383 mae: 0.667245 (1967.1065837483936 steps/sec)\n",
      "Step #2748\tEpoch   0 Batch 2747/3125   Loss: 0.920936 mae: 0.754643 (1933.8571059717458 steps/sec)\n",
      "Step #2749\tEpoch   0 Batch 2748/3125   Loss: 0.835773 mae: 0.722573 (1673.1438783488377 steps/sec)\n",
      "Step #2750\tEpoch   0 Batch 2749/3125   Loss: 1.012554 mae: 0.784021 (1747.218982237478 steps/sec)\n",
      "Step #2751\tEpoch   0 Batch 2750/3125   Loss: 0.962576 mae: 0.778690 (1915.644667732359 steps/sec)\n",
      "Step #2752\tEpoch   0 Batch 2751/3125   Loss: 0.957920 mae: 0.768733 (2236.2465344423117 steps/sec)\n",
      "Step #2753\tEpoch   0 Batch 2752/3125   Loss: 0.837648 mae: 0.732707 (2385.1600796133066 steps/sec)\n",
      "Step #2754\tEpoch   0 Batch 2753/3125   Loss: 0.818987 mae: 0.714783 (2132.183779497138 steps/sec)\n",
      "Step #2755\tEpoch   0 Batch 2754/3125   Loss: 0.778456 mae: 0.710002 (1978.7252913148086 steps/sec)\n",
      "Step #2756\tEpoch   0 Batch 2755/3125   Loss: 0.893049 mae: 0.736365 (1872.37355475202 steps/sec)\n",
      "Step #2757\tEpoch   0 Batch 2756/3125   Loss: 0.982254 mae: 0.786528 (1915.5571793934964 steps/sec)\n",
      "Step #2758\tEpoch   0 Batch 2757/3125   Loss: 0.798175 mae: 0.732442 (1944.23770453808 steps/sec)\n",
      "Step #2759\tEpoch   0 Batch 2758/3125   Loss: 0.836269 mae: 0.738583 (2073.063007848797 steps/sec)\n",
      "Step #2760\tEpoch   0 Batch 2759/3125   Loss: 0.829263 mae: 0.708463 (2032.124031007752 steps/sec)\n",
      "Step #2761\tEpoch   0 Batch 2760/3125   Loss: 0.755865 mae: 0.686176 (2135.7449105333376 steps/sec)\n",
      "Step #2762\tEpoch   0 Batch 2761/3125   Loss: 0.900882 mae: 0.762991 (2088.3808006373233 steps/sec)\n",
      "Step #2763\tEpoch   0 Batch 2762/3125   Loss: 0.719768 mae: 0.659032 (2066.037475617205 steps/sec)\n",
      "Step #2764\tEpoch   0 Batch 2763/3125   Loss: 0.741659 mae: 0.682997 (2165.629195150664 steps/sec)\n",
      "Step #2765\tEpoch   0 Batch 2764/3125   Loss: 0.787198 mae: 0.712515 (2205.856613934702 steps/sec)\n",
      "Step #2766\tEpoch   0 Batch 2765/3125   Loss: 0.794135 mae: 0.694078 (1882.0184688282434 steps/sec)\n",
      "Step #2767\tEpoch   0 Batch 2766/3125   Loss: 0.737091 mae: 0.679215 (1809.2309815897993 steps/sec)\n",
      "Step #2768\tEpoch   0 Batch 2767/3125   Loss: 0.761929 mae: 0.718347 (1859.5729587855571 steps/sec)\n",
      "Step #2769\tEpoch   0 Batch 2768/3125   Loss: 0.891988 mae: 0.746578 (2076.490915391851 steps/sec)\n",
      "Step #2770\tEpoch   0 Batch 2769/3125   Loss: 0.810230 mae: 0.710951 (1995.7480419866579 steps/sec)\n",
      "Step #2771\tEpoch   0 Batch 2770/3125   Loss: 0.851877 mae: 0.725926 (2040.5472201140367 steps/sec)\n",
      "Step #2772\tEpoch   0 Batch 2771/3125   Loss: 0.929634 mae: 0.756979 (2142.990568254974 steps/sec)\n",
      "Step #2773\tEpoch   0 Batch 2772/3125   Loss: 0.915034 mae: 0.767056 (1972.082526189088 steps/sec)\n",
      "Step #2774\tEpoch   0 Batch 2773/3125   Loss: 0.967180 mae: 0.773876 (1819.0548886267434 steps/sec)\n",
      "Step #2775\tEpoch   0 Batch 2774/3125   Loss: 0.823361 mae: 0.718313 (2070.668154306421 steps/sec)\n",
      "Step #2776\tEpoch   0 Batch 2775/3125   Loss: 0.881911 mae: 0.748732 (2017.3652060987927 steps/sec)\n",
      "Step #2777\tEpoch   0 Batch 2776/3125   Loss: 0.763827 mae: 0.693079 (2068.686868686869 steps/sec)\n",
      "Step #2778\tEpoch   0 Batch 2777/3125   Loss: 0.963379 mae: 0.768410 (2265.942021155903 steps/sec)\n",
      "Step #2779\tEpoch   0 Batch 2778/3125   Loss: 0.741390 mae: 0.676014 (2376.5647133483676 steps/sec)\n",
      "Step #2780\tEpoch   0 Batch 2779/3125   Loss: 0.833623 mae: 0.701523 (2345.7551285206146 steps/sec)\n",
      "Step #2781\tEpoch   0 Batch 2780/3125   Loss: 0.820154 mae: 0.693290 (2391.770260714856 steps/sec)\n",
      "Step #2782\tEpoch   0 Batch 2781/3125   Loss: 0.952268 mae: 0.766740 (2036.0896707735026 steps/sec)\n",
      "Step #2783\tEpoch   0 Batch 2782/3125   Loss: 0.969541 mae: 0.774778 (1971.915637840735 steps/sec)\n",
      "Step #2784\tEpoch   0 Batch 2783/3125   Loss: 0.826371 mae: 0.723610 (2106.9493143115487 steps/sec)\n",
      "Step #2785\tEpoch   0 Batch 2784/3125   Loss: 0.842644 mae: 0.738752 (2189.73395146807 steps/sec)\n",
      "Step #2786\tEpoch   0 Batch 2785/3125   Loss: 0.914333 mae: 0.782769 (2213.0727506806525 steps/sec)\n",
      "Step #2787\tEpoch   0 Batch 2786/3125   Loss: 0.951989 mae: 0.760411 (2054.7421225897474 steps/sec)\n",
      "Step #2788\tEpoch   0 Batch 2787/3125   Loss: 0.800220 mae: 0.705704 (2245.6813655151736 steps/sec)\n",
      "Step #2789\tEpoch   0 Batch 2788/3125   Loss: 0.914229 mae: 0.750177 (2262.6171955074606 steps/sec)\n",
      "Step #2790\tEpoch   0 Batch 2789/3125   Loss: 0.817857 mae: 0.725064 (2078.5077851670517 steps/sec)\n",
      "Step #2791\tEpoch   0 Batch 2790/3125   Loss: 0.821913 mae: 0.730583 (2133.745739431246 steps/sec)\n",
      "Step #2792\tEpoch   0 Batch 2791/3125   Loss: 0.814439 mae: 0.715145 (1995.4442086834067 steps/sec)\n",
      "Step #2793\tEpoch   0 Batch 2792/3125   Loss: 0.712772 mae: 0.677583 (2045.9021511145797 steps/sec)\n",
      "Step #2794\tEpoch   0 Batch 2793/3125   Loss: 1.002374 mae: 0.808740 (2436.735452686373 steps/sec)\n",
      "Step #2795\tEpoch   0 Batch 2794/3125   Loss: 0.775647 mae: 0.719376 (2222.6188331301996 steps/sec)\n",
      "Step #2796\tEpoch   0 Batch 2795/3125   Loss: 0.907749 mae: 0.753836 (2080.8175819814455 steps/sec)\n",
      "Step #2797\tEpoch   0 Batch 2796/3125   Loss: 0.824566 mae: 0.706545 (2191.450097704212 steps/sec)\n",
      "Step #2798\tEpoch   0 Batch 2797/3125   Loss: 0.991067 mae: 0.794021 (2200.694684925757 steps/sec)\n",
      "Step #2799\tEpoch   0 Batch 2798/3125   Loss: 0.789015 mae: 0.706713 (2197.8117795011526 steps/sec)\n",
      "Step #2800\tEpoch   0 Batch 2799/3125   Loss: 0.859685 mae: 0.737581 (2145.182639293788 steps/sec)\n",
      "Step #2801\tEpoch   0 Batch 2800/3125   Loss: 1.038482 mae: 0.785044 (1952.1283824665593 steps/sec)\n",
      "Step #2802\tEpoch   0 Batch 2801/3125   Loss: 0.759347 mae: 0.704000 (2056.8581488637587 steps/sec)\n",
      "Step #2803\tEpoch   0 Batch 2802/3125   Loss: 0.806375 mae: 0.712677 (2119.9842301588105 steps/sec)\n",
      "Step #2804\tEpoch   0 Batch 2803/3125   Loss: 0.918789 mae: 0.765765 (2349.276336425147 steps/sec)\n",
      "Step #2805\tEpoch   0 Batch 2804/3125   Loss: 0.718772 mae: 0.658994 (2097.7813344003202 steps/sec)\n",
      "Step #2806\tEpoch   0 Batch 2805/3125   Loss: 0.826862 mae: 0.715732 (1512.9984344450938 steps/sec)\n",
      "Step #2807\tEpoch   0 Batch 2806/3125   Loss: 0.856963 mae: 0.721097 (1581.9204948329184 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2808\tEpoch   0 Batch 2807/3125   Loss: 0.904124 mae: 0.765102 (1718.4417968173848 steps/sec)\n",
      "Step #2809\tEpoch   0 Batch 2808/3125   Loss: 0.935587 mae: 0.762278 (1621.78932960073 steps/sec)\n",
      "Step #2810\tEpoch   0 Batch 2809/3125   Loss: 0.877884 mae: 0.754719 (1884.860196111915 steps/sec)\n",
      "Step #2811\tEpoch   0 Batch 2810/3125   Loss: 0.827458 mae: 0.730093 (2048.0200001953144 steps/sec)\n",
      "Step #2812\tEpoch   0 Batch 2811/3125   Loss: 0.812463 mae: 0.705625 (1599.9511733650706 steps/sec)\n",
      "Step #2813\tEpoch   0 Batch 2812/3125   Loss: 0.904329 mae: 0.773345 (1532.9049989401283 steps/sec)\n",
      "Step #2814\tEpoch   0 Batch 2813/3125   Loss: 0.955290 mae: 0.789706 (1928.3447045625908 steps/sec)\n",
      "Step #2815\tEpoch   0 Batch 2814/3125   Loss: 0.839564 mae: 0.734337 (1602.0534131882907 steps/sec)\n",
      "Step #2816\tEpoch   0 Batch 2815/3125   Loss: 0.797642 mae: 0.694914 (1595.908924875198 steps/sec)\n",
      "Step #2817\tEpoch   0 Batch 2816/3125   Loss: 0.858536 mae: 0.718459 (1576.9008662175168 steps/sec)\n",
      "Step #2818\tEpoch   0 Batch 2817/3125   Loss: 0.769252 mae: 0.688026 (1538.0765535500811 steps/sec)\n",
      "Step #2819\tEpoch   0 Batch 2818/3125   Loss: 0.957225 mae: 0.754180 (1663.7197347126582 steps/sec)\n",
      "Step #2820\tEpoch   0 Batch 2819/3125   Loss: 0.949390 mae: 0.782889 (1592.0562379484688 steps/sec)\n",
      "Step #2821\tEpoch   0 Batch 2820/3125   Loss: 1.043513 mae: 0.803537 (1631.0602289696367 steps/sec)\n",
      "Step #2822\tEpoch   0 Batch 2821/3125   Loss: 0.833854 mae: 0.726522 (1456.456698381832 steps/sec)\n",
      "Step #2823\tEpoch   0 Batch 2822/3125   Loss: 0.828963 mae: 0.725682 (1729.1963159326842 steps/sec)\n",
      "Step #2824\tEpoch   0 Batch 2823/3125   Loss: 0.770549 mae: 0.695347 (2086.718407960199 steps/sec)\n",
      "Step #2825\tEpoch   0 Batch 2824/3125   Loss: 0.888130 mae: 0.776756 (2080.549217246374 steps/sec)\n",
      "Step #2826\tEpoch   0 Batch 2825/3125   Loss: 0.832297 mae: 0.702842 (2035.9117738428083 steps/sec)\n",
      "Step #2827\tEpoch   0 Batch 2826/3125   Loss: 0.851805 mae: 0.742733 (2059.4840369639296 steps/sec)\n",
      "Step #2828\tEpoch   0 Batch 2827/3125   Loss: 0.846092 mae: 0.756926 (2052.148385897273 steps/sec)\n",
      "Step #2829\tEpoch   0 Batch 2828/3125   Loss: 0.992357 mae: 0.778087 (1852.4604934236677 steps/sec)\n",
      "Step #2830\tEpoch   0 Batch 2829/3125   Loss: 0.715060 mae: 0.661920 (1983.1037058751217 steps/sec)\n",
      "Step #2831\tEpoch   0 Batch 2830/3125   Loss: 0.735624 mae: 0.670626 (1742.5152884871045 steps/sec)\n",
      "Step #2832\tEpoch   0 Batch 2831/3125   Loss: 0.894567 mae: 0.740806 (1854.6557594516914 steps/sec)\n",
      "Step #2833\tEpoch   0 Batch 2832/3125   Loss: 0.730832 mae: 0.678376 (1812.123149772313 steps/sec)\n",
      "Step #2834\tEpoch   0 Batch 2833/3125   Loss: 0.743302 mae: 0.675416 (2056.7774584898443 steps/sec)\n",
      "Step #2835\tEpoch   0 Batch 2834/3125   Loss: 0.778766 mae: 0.690585 (1990.0476362187092 steps/sec)\n",
      "Step #2836\tEpoch   0 Batch 2835/3125   Loss: 0.905429 mae: 0.730088 (2099.50344385712 steps/sec)\n",
      "Step #2837\tEpoch   0 Batch 2836/3125   Loss: 0.834106 mae: 0.702947 (2120.8846997906576 steps/sec)\n",
      "Step #2838\tEpoch   0 Batch 2837/3125   Loss: 0.765093 mae: 0.691378 (2189.436759409093 steps/sec)\n",
      "Step #2839\tEpoch   0 Batch 2838/3125   Loss: 0.653007 mae: 0.620508 (1979.7339777779875 steps/sec)\n",
      "Step #2840\tEpoch   0 Batch 2839/3125   Loss: 0.833295 mae: 0.728140 (2029.9603136192043 steps/sec)\n",
      "Step #2841\tEpoch   0 Batch 2840/3125   Loss: 0.845969 mae: 0.723339 (2265.207764012054 steps/sec)\n",
      "Step #2842\tEpoch   0 Batch 2841/3125   Loss: 0.889274 mae: 0.731017 (2044.1472615090697 steps/sec)\n",
      "Step #2843\tEpoch   0 Batch 2842/3125   Loss: 0.810205 mae: 0.726220 (1941.609650868893 steps/sec)\n",
      "Step #2844\tEpoch   0 Batch 2843/3125   Loss: 0.895833 mae: 0.753054 (2110.830179563572 steps/sec)\n",
      "Step #2845\tEpoch   0 Batch 2844/3125   Loss: 0.873919 mae: 0.729078 (2151.7416865887567 steps/sec)\n",
      "Step #2846\tEpoch   0 Batch 2845/3125   Loss: 0.842887 mae: 0.729580 (2010.036996568712 steps/sec)\n",
      "Step #2847\tEpoch   0 Batch 2846/3125   Loss: 0.789841 mae: 0.701833 (1984.5111472803665 steps/sec)\n",
      "Step #2848\tEpoch   0 Batch 2847/3125   Loss: 0.817477 mae: 0.712095 (1904.6145183409167 steps/sec)\n",
      "Step #2849\tEpoch   0 Batch 2848/3125   Loss: 0.739845 mae: 0.709740 (1951.292858804373 steps/sec)\n",
      "Step #2850\tEpoch   0 Batch 2849/3125   Loss: 0.841512 mae: 0.733491 (2238.944345393789 steps/sec)\n",
      "Step #2851\tEpoch   0 Batch 2850/3125   Loss: 0.846547 mae: 0.731226 (2122.666450737869 steps/sec)\n",
      "Step #2852\tEpoch   0 Batch 2851/3125   Loss: 0.829473 mae: 0.702331 (2188.066148468882 steps/sec)\n",
      "Step #2853\tEpoch   0 Batch 2852/3125   Loss: 0.882623 mae: 0.747087 (2219.091053383419 steps/sec)\n",
      "Step #2854\tEpoch   0 Batch 2853/3125   Loss: 0.861638 mae: 0.752113 (2088.567985579269 steps/sec)\n",
      "Step #2855\tEpoch   0 Batch 2854/3125   Loss: 0.899696 mae: 0.736408 (2004.8870958490277 steps/sec)\n",
      "Step #2856\tEpoch   0 Batch 2855/3125   Loss: 0.909214 mae: 0.768545 (1842.2896498409966 steps/sec)\n",
      "Step #2857\tEpoch   0 Batch 2856/3125   Loss: 0.841884 mae: 0.728209 (1986.8048581768574 steps/sec)\n",
      "Step #2858\tEpoch   0 Batch 2857/3125   Loss: 0.702471 mae: 0.654200 (2249.6320611014567 steps/sec)\n",
      "Step #2859\tEpoch   0 Batch 2858/3125   Loss: 1.051579 mae: 0.804532 (2067.0556694527677 steps/sec)\n",
      "Step #2860\tEpoch   0 Batch 2859/3125   Loss: 0.804451 mae: 0.712349 (2036.247827480071 steps/sec)\n",
      "Step #2861\tEpoch   0 Batch 2860/3125   Loss: 0.800568 mae: 0.707964 (2079.6001745269923 steps/sec)\n",
      "Step #2862\tEpoch   0 Batch 2861/3125   Loss: 0.765777 mae: 0.698627 (2001.366594774111 steps/sec)\n",
      "Step #2863\tEpoch   0 Batch 2862/3125   Loss: 0.801475 mae: 0.711330 (2175.4914470067115 steps/sec)\n",
      "Step #2864\tEpoch   0 Batch 2863/3125   Loss: 0.801801 mae: 0.700178 (2209.482068355177 steps/sec)\n",
      "Step #2865\tEpoch   0 Batch 2864/3125   Loss: 0.867319 mae: 0.740793 (1732.0240169803685 steps/sec)\n",
      "Step #2866\tEpoch   0 Batch 2865/3125   Loss: 0.804564 mae: 0.728112 (2051.285261551704 steps/sec)\n",
      "Step #2867\tEpoch   0 Batch 2866/3125   Loss: 0.886866 mae: 0.726967 (2128.6561104344296 steps/sec)\n",
      "Step #2868\tEpoch   0 Batch 2867/3125   Loss: 0.917893 mae: 0.785478 (2071.9365324006835 steps/sec)\n",
      "Step #2869\tEpoch   0 Batch 2868/3125   Loss: 0.756630 mae: 0.675340 (2122.494585349068 steps/sec)\n",
      "Step #2870\tEpoch   0 Batch 2869/3125   Loss: 0.737633 mae: 0.703608 (1977.7735863291712 steps/sec)\n",
      "Step #2871\tEpoch   0 Batch 2870/3125   Loss: 0.704208 mae: 0.676254 (2178.4291931982257 steps/sec)\n",
      "Step #2872\tEpoch   0 Batch 2871/3125   Loss: 0.814119 mae: 0.721726 (2008.9972027436104 steps/sec)\n",
      "Step #2873\tEpoch   0 Batch 2872/3125   Loss: 0.894329 mae: 0.743897 (2154.6155979986233 steps/sec)\n",
      "Step #2874\tEpoch   0 Batch 2873/3125   Loss: 0.793326 mae: 0.718729 (1575.9410248510217 steps/sec)\n",
      "Step #2875\tEpoch   0 Batch 2874/3125   Loss: 0.897400 mae: 0.732392 (1665.503466569248 steps/sec)\n",
      "Step #2876\tEpoch   0 Batch 2875/3125   Loss: 0.879694 mae: 0.743931 (2033.6808215591393 steps/sec)\n",
      "Step #2877\tEpoch   0 Batch 2876/3125   Loss: 0.764899 mae: 0.702286 (1937.1618064087052 steps/sec)\n",
      "Step #2878\tEpoch   0 Batch 2877/3125   Loss: 0.794636 mae: 0.709958 (2220.453799485426 steps/sec)\n",
      "Step #2879\tEpoch   0 Batch 2878/3125   Loss: 0.880736 mae: 0.724333 (2364.8534055029318 steps/sec)\n",
      "Step #2880\tEpoch   0 Batch 2879/3125   Loss: 0.713403 mae: 0.682405 (2159.630098757041 steps/sec)\n",
      "Step #2881\tEpoch   0 Batch 2880/3125   Loss: 0.821994 mae: 0.736303 (2187.7922321791834 steps/sec)\n",
      "Step #2882\tEpoch   0 Batch 2881/3125   Loss: 0.788948 mae: 0.703982 (1935.4276643656096 steps/sec)\n",
      "Step #2883\tEpoch   0 Batch 2882/3125   Loss: 0.827989 mae: 0.732736 (1767.6601483479433 steps/sec)\n",
      "Step #2884\tEpoch   0 Batch 2883/3125   Loss: 0.980573 mae: 0.796745 (2116.859966286124 steps/sec)\n",
      "Step #2885\tEpoch   0 Batch 2884/3125   Loss: 0.793915 mae: 0.697044 (2098.999119224918 steps/sec)\n",
      "Step #2886\tEpoch   0 Batch 2885/3125   Loss: 0.860580 mae: 0.758312 (2323.4309391653096 steps/sec)\n",
      "Step #2887\tEpoch   0 Batch 2886/3125   Loss: 0.774382 mae: 0.693990 (2152.6693423389206 steps/sec)\n",
      "Step #2888\tEpoch   0 Batch 2887/3125   Loss: 0.934220 mae: 0.773646 (1807.0795850136146 steps/sec)\n",
      "Step #2889\tEpoch   0 Batch 2888/3125   Loss: 0.880945 mae: 0.755673 (1631.961402280067 steps/sec)\n",
      "Step #2890\tEpoch   0 Batch 2889/3125   Loss: 0.803923 mae: 0.714912 (1760.9364110401116 steps/sec)\n",
      "Step #2891\tEpoch   0 Batch 2890/3125   Loss: 0.790267 mae: 0.697183 (1789.409375586614 steps/sec)\n",
      "Step #2892\tEpoch   0 Batch 2891/3125   Loss: 0.920388 mae: 0.760996 (1754.8067509559949 steps/sec)\n",
      "Step #2893\tEpoch   0 Batch 2892/3125   Loss: 0.782081 mae: 0.709683 (1916.7126693110572 steps/sec)\n",
      "Step #2894\tEpoch   0 Batch 2893/3125   Loss: 0.757276 mae: 0.695319 (1955.1312649164677 steps/sec)\n",
      "Step #2895\tEpoch   0 Batch 2894/3125   Loss: 0.862313 mae: 0.751136 (2029.2924601331476 steps/sec)\n",
      "Step #2896\tEpoch   0 Batch 2895/3125   Loss: 0.815997 mae: 0.708503 (1998.3153240714273 steps/sec)\n",
      "Step #2897\tEpoch   0 Batch 2896/3125   Loss: 0.816163 mae: 0.730001 (2021.1759943715726 steps/sec)\n",
      "Step #2898\tEpoch   0 Batch 2897/3125   Loss: 0.777942 mae: 0.709171 (2167.12858190987 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2899\tEpoch   0 Batch 2898/3125   Loss: 0.841608 mae: 0.715312 (1923.022328183027 steps/sec)\n",
      "Step #2900\tEpoch   0 Batch 2899/3125   Loss: 0.880521 mae: 0.750711 (1875.2700479290363 steps/sec)\n",
      "Step #2901\tEpoch   0 Batch 2900/3125   Loss: 0.753913 mae: 0.698002 (2037.7317423918535 steps/sec)\n",
      "Step #2902\tEpoch   0 Batch 2901/3125   Loss: 0.730351 mae: 0.686967 (1947.8674394412246 steps/sec)\n",
      "Step #2903\tEpoch   0 Batch 2902/3125   Loss: 0.818195 mae: 0.700432 (1947.1078677139622 steps/sec)\n",
      "Step #2904\tEpoch   0 Batch 2903/3125   Loss: 0.732674 mae: 0.676661 (2170.627749314289 steps/sec)\n",
      "Step #2905\tEpoch   0 Batch 2904/3125   Loss: 0.813043 mae: 0.720382 (2028.487691638052 steps/sec)\n",
      "Step #2906\tEpoch   0 Batch 2905/3125   Loss: 0.944353 mae: 0.760123 (1832.3739624290083 steps/sec)\n",
      "Step #2907\tEpoch   0 Batch 2906/3125   Loss: 0.994179 mae: 0.785376 (2115.792128653437 steps/sec)\n",
      "Step #2908\tEpoch   0 Batch 2907/3125   Loss: 0.695536 mae: 0.669364 (1740.5195451904722 steps/sec)\n",
      "Step #2909\tEpoch   0 Batch 2908/3125   Loss: 0.910750 mae: 0.751136 (2070.402400979347 steps/sec)\n",
      "Step #2910\tEpoch   0 Batch 2909/3125   Loss: 0.856833 mae: 0.734953 (2176.1686849505545 steps/sec)\n",
      "Step #2911\tEpoch   0 Batch 2910/3125   Loss: 0.901755 mae: 0.759682 (2269.129310438104 steps/sec)\n",
      "Step #2912\tEpoch   0 Batch 2911/3125   Loss: 0.879308 mae: 0.760422 (2095.4546817078167 steps/sec)\n",
      "Step #2913\tEpoch   0 Batch 2912/3125   Loss: 0.760004 mae: 0.694036 (2055.829820605823 steps/sec)\n",
      "Step #2914\tEpoch   0 Batch 2913/3125   Loss: 0.750411 mae: 0.680722 (1985.8641718116737 steps/sec)\n",
      "Step #2915\tEpoch   0 Batch 2914/3125   Loss: 0.801768 mae: 0.704437 (2227.481970068721 steps/sec)\n",
      "Step #2916\tEpoch   0 Batch 2915/3125   Loss: 0.856141 mae: 0.737468 (1798.8951792760336 steps/sec)\n",
      "Step #2917\tEpoch   0 Batch 2916/3125   Loss: 1.120187 mae: 0.825811 (2131.902002643082 steps/sec)\n",
      "Step #2918\tEpoch   0 Batch 2917/3125   Loss: 0.937114 mae: 0.768563 (1862.4795737122558 steps/sec)\n",
      "Step #2919\tEpoch   0 Batch 2918/3125   Loss: 0.771183 mae: 0.704268 (1905.9820049077525 steps/sec)\n",
      "Step #2920\tEpoch   0 Batch 2919/3125   Loss: 0.802530 mae: 0.703328 (1751.2605322711293 steps/sec)\n",
      "Step #2921\tEpoch   0 Batch 2920/3125   Loss: 0.826154 mae: 0.735296 (1799.7288158865833 steps/sec)\n",
      "Step #2922\tEpoch   0 Batch 2921/3125   Loss: 0.790659 mae: 0.716854 (1851.9860824104983 steps/sec)\n",
      "Step #2923\tEpoch   0 Batch 2922/3125   Loss: 1.047408 mae: 0.816471 (2100.3865953568497 steps/sec)\n",
      "Step #2924\tEpoch   0 Batch 2923/3125   Loss: 0.805767 mae: 0.725294 (2027.6639562203293 steps/sec)\n",
      "Step #2925\tEpoch   0 Batch 2924/3125   Loss: 0.863717 mae: 0.754106 (1680.3429349785665 steps/sec)\n",
      "Step #2926\tEpoch   0 Batch 2925/3125   Loss: 0.884011 mae: 0.749320 (1699.8743626946366 steps/sec)\n",
      "Step #2927\tEpoch   0 Batch 2926/3125   Loss: 0.733988 mae: 0.685555 (1770.1070258955401 steps/sec)\n",
      "Step #2928\tEpoch   0 Batch 2927/3125   Loss: 0.783542 mae: 0.704306 (1767.6005529146016 steps/sec)\n",
      "Step #2929\tEpoch   0 Batch 2928/3125   Loss: 0.897268 mae: 0.761831 (2043.7289258775606 steps/sec)\n",
      "Step #2930\tEpoch   0 Batch 2929/3125   Loss: 0.854399 mae: 0.737788 (2161.900932941601 steps/sec)\n",
      "Step #2931\tEpoch   0 Batch 2930/3125   Loss: 0.656243 mae: 0.633238 (1968.1958104962835 steps/sec)\n",
      "Step #2932\tEpoch   0 Batch 2931/3125   Loss: 0.790122 mae: 0.707581 (1893.0781729554071 steps/sec)\n",
      "Step #2933\tEpoch   0 Batch 2932/3125   Loss: 0.665260 mae: 0.667073 (1932.680858907013 steps/sec)\n",
      "Step #2934\tEpoch   0 Batch 2933/3125   Loss: 0.822821 mae: 0.687933 (2024.7861432405816 steps/sec)\n",
      "Step #2935\tEpoch   0 Batch 2934/3125   Loss: 0.717069 mae: 0.683512 (2198.4108015179154 steps/sec)\n",
      "Step #2936\tEpoch   0 Batch 2935/3125   Loss: 0.920478 mae: 0.779955 (2204.002017824113 steps/sec)\n",
      "Step #2937\tEpoch   0 Batch 2936/3125   Loss: 0.766864 mae: 0.696060 (1911.2451812224886 steps/sec)\n",
      "Step #2938\tEpoch   0 Batch 2937/3125   Loss: 0.873738 mae: 0.736587 (1599.81691548361 steps/sec)\n",
      "Step #2939\tEpoch   0 Batch 2938/3125   Loss: 0.860944 mae: 0.753059 (1739.6099640821878 steps/sec)\n",
      "Step #2940\tEpoch   0 Batch 2939/3125   Loss: 0.823696 mae: 0.712220 (1416.8318503955627 steps/sec)\n",
      "Step #2941\tEpoch   0 Batch 2940/3125   Loss: 0.887197 mae: 0.747145 (1886.8782840279277 steps/sec)\n",
      "Step #2942\tEpoch   0 Batch 2941/3125   Loss: 0.834807 mae: 0.741175 (1978.6319464100386 steps/sec)\n",
      "Step #2943\tEpoch   0 Batch 2942/3125   Loss: 0.961102 mae: 0.777256 (1854.8033891709267 steps/sec)\n",
      "Step #2944\tEpoch   0 Batch 2943/3125   Loss: 1.071142 mae: 0.838822 (2079.4145935172974 steps/sec)\n",
      "Step #2945\tEpoch   0 Batch 2944/3125   Loss: 0.876673 mae: 0.738405 (2001.442995934416 steps/sec)\n",
      "Step #2946\tEpoch   0 Batch 2945/3125   Loss: 0.854154 mae: 0.750583 (2020.8643700313178 steps/sec)\n",
      "Step #2947\tEpoch   0 Batch 2946/3125   Loss: 0.787266 mae: 0.700485 (2005.7116078003805 steps/sec)\n",
      "Step #2948\tEpoch   0 Batch 2947/3125   Loss: 0.897465 mae: 0.747696 (1927.6002794220376 steps/sec)\n",
      "Step #2949\tEpoch   0 Batch 2948/3125   Loss: 0.818161 mae: 0.724027 (1892.292422355765 steps/sec)\n",
      "Step #2950\tEpoch   0 Batch 2949/3125   Loss: 0.892000 mae: 0.753948 (1902.6619005280252 steps/sec)\n",
      "Step #2951\tEpoch   0 Batch 2950/3125   Loss: 0.959183 mae: 0.784355 (1912.5698808036407 steps/sec)\n",
      "Step #2952\tEpoch   0 Batch 2951/3125   Loss: 0.872416 mae: 0.752799 (1799.0031997117685 steps/sec)\n",
      "Step #2953\tEpoch   0 Batch 2952/3125   Loss: 0.861881 mae: 0.744892 (1831.3818639094593 steps/sec)\n",
      "Step #2954\tEpoch   0 Batch 2953/3125   Loss: 0.883761 mae: 0.722480 (1884.4367766515707 steps/sec)\n",
      "Step #2955\tEpoch   0 Batch 2954/3125   Loss: 0.819221 mae: 0.700682 (1945.5905000463865 steps/sec)\n",
      "Step #2956\tEpoch   0 Batch 2955/3125   Loss: 0.844626 mae: 0.712949 (1590.3418570085237 steps/sec)\n",
      "Step #2957\tEpoch   0 Batch 2956/3125   Loss: 0.916770 mae: 0.762244 (1752.35803335673 steps/sec)\n",
      "Step #2958\tEpoch   0 Batch 2957/3125   Loss: 0.905308 mae: 0.744923 (1863.654702343396 steps/sec)\n",
      "Step #2959\tEpoch   0 Batch 2958/3125   Loss: 1.014882 mae: 0.818354 (1879.6400530599076 steps/sec)\n",
      "Step #2960\tEpoch   0 Batch 2959/3125   Loss: 0.798870 mae: 0.711873 (2085.5148272638676 steps/sec)\n",
      "Step #2961\tEpoch   0 Batch 2960/3125   Loss: 0.795846 mae: 0.715220 (2135.3752163730783 steps/sec)\n",
      "Step #2962\tEpoch   0 Batch 2961/3125   Loss: 0.816355 mae: 0.726376 (2101.7969713065877 steps/sec)\n",
      "Step #2963\tEpoch   0 Batch 2962/3125   Loss: 0.838893 mae: 0.714984 (2070.1367158580524 steps/sec)\n",
      "Step #2964\tEpoch   0 Batch 2963/3125   Loss: 0.797310 mae: 0.716650 (1926.555509622893 steps/sec)\n",
      "Step #2965\tEpoch   0 Batch 2964/3125   Loss: 1.013313 mae: 0.791285 (1930.3326521971244 steps/sec)\n",
      "Step #2966\tEpoch   0 Batch 2965/3125   Loss: 0.894592 mae: 0.768760 (1964.7844702399354 steps/sec)\n",
      "Step #2967\tEpoch   0 Batch 2966/3125   Loss: 0.702709 mae: 0.687195 (2008.6893222481897 steps/sec)\n",
      "Step #2968\tEpoch   0 Batch 2967/3125   Loss: 0.995537 mae: 0.813489 (2012.5639376985307 steps/sec)\n",
      "Step #2969\tEpoch   0 Batch 2968/3125   Loss: 0.724181 mae: 0.682910 (2064.0650374496818 steps/sec)\n",
      "Step #2970\tEpoch   0 Batch 2969/3125   Loss: 0.709911 mae: 0.663314 (1961.4032790564997 steps/sec)\n",
      "Step #2971\tEpoch   0 Batch 2970/3125   Loss: 0.885287 mae: 0.741811 (2085.141584473433 steps/sec)\n",
      "Step #2972\tEpoch   0 Batch 2971/3125   Loss: 0.759361 mae: 0.705693 (2083.1532104259377 steps/sec)\n",
      "Step #2973\tEpoch   0 Batch 2972/3125   Loss: 0.739508 mae: 0.687834 (1951.7650234064533 steps/sec)\n",
      "Step #2974\tEpoch   0 Batch 2973/3125   Loss: 0.893174 mae: 0.733492 (1749.1134130677742 steps/sec)\n",
      "Step #2975\tEpoch   0 Batch 2974/3125   Loss: 0.766621 mae: 0.694248 (2167.8678492422832 steps/sec)\n",
      "Step #2976\tEpoch   0 Batch 2975/3125   Loss: 0.767003 mae: 0.689015 (2118.8918301776225 steps/sec)\n",
      "Step #2977\tEpoch   0 Batch 2976/3125   Loss: 0.925677 mae: 0.775848 (1929.7464918334483 steps/sec)\n",
      "Step #2978\tEpoch   0 Batch 2977/3125   Loss: 0.923486 mae: 0.761967 (1928.7880878146584 steps/sec)\n",
      "Step #2979\tEpoch   0 Batch 2978/3125   Loss: 0.745933 mae: 0.655759 (2103.0616031047243 steps/sec)\n",
      "Step #2980\tEpoch   0 Batch 2979/3125   Loss: 0.942498 mae: 0.759979 (1952.4191671399178 steps/sec)\n",
      "Step #2981\tEpoch   0 Batch 2980/3125   Loss: 0.791931 mae: 0.704972 (1794.3085952873937 steps/sec)\n",
      "Step #2982\tEpoch   0 Batch 2981/3125   Loss: 0.692358 mae: 0.691607 (2119.8985110232798 steps/sec)\n",
      "Step #2983\tEpoch   0 Batch 2982/3125   Loss: 0.875058 mae: 0.734659 (2203.4462469530135 steps/sec)\n",
      "Step #2984\tEpoch   0 Batch 2983/3125   Loss: 0.799316 mae: 0.708408 (2235.936583753585 steps/sec)\n",
      "Step #2985\tEpoch   0 Batch 2984/3125   Loss: 0.745931 mae: 0.696110 (2129.109940202439 steps/sec)\n",
      "Step #2986\tEpoch   0 Batch 2985/3125   Loss: 0.813044 mae: 0.714900 (2298.2235811114397 steps/sec)\n",
      "Step #2987\tEpoch   0 Batch 2986/3125   Loss: 0.872954 mae: 0.750776 (2161.52212900167 steps/sec)\n",
      "Step #2988\tEpoch   0 Batch 2987/3125   Loss: 0.813102 mae: 0.706369 (2127.8353862700137 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2989\tEpoch   0 Batch 2988/3125   Loss: 0.793643 mae: 0.701611 (1558.201325526793 steps/sec)\n",
      "Step #2990\tEpoch   0 Batch 2989/3125   Loss: 0.858766 mae: 0.715467 (1799.2810261250054 steps/sec)\n",
      "Step #2991\tEpoch   0 Batch 2990/3125   Loss: 0.794838 mae: 0.699935 (1819.5757233959482 steps/sec)\n",
      "Step #2992\tEpoch   0 Batch 2991/3125   Loss: 0.889399 mae: 0.751345 (1889.0538300785472 steps/sec)\n",
      "Step #2993\tEpoch   0 Batch 2992/3125   Loss: 0.843387 mae: 0.732071 (1924.4163852591396 steps/sec)\n",
      "Step #2994\tEpoch   0 Batch 2993/3125   Loss: 0.697621 mae: 0.653495 (1835.8226462992952 steps/sec)\n",
      "Step #2995\tEpoch   0 Batch 2994/3125   Loss: 0.792882 mae: 0.695445 (2166.927051043604 steps/sec)\n",
      "Step #2996\tEpoch   0 Batch 2995/3125   Loss: 0.784401 mae: 0.723454 (2129.0450955310553 steps/sec)\n",
      "Step #2997\tEpoch   0 Batch 2996/3125   Loss: 0.813816 mae: 0.718034 (1759.828141783029 steps/sec)\n",
      "Step #2998\tEpoch   0 Batch 2997/3125   Loss: 1.008612 mae: 0.787882 (1769.9725703675572 steps/sec)\n",
      "Step #2999\tEpoch   0 Batch 2998/3125   Loss: 0.839100 mae: 0.746777 (2154.0402017276265 steps/sec)\n",
      "Step #3000\tEpoch   0 Batch 2999/3125   Loss: 0.769357 mae: 0.712619 (2025.509721162483 steps/sec)\n",
      "Step #3001\tEpoch   0 Batch 3000/3125   Loss: 0.911917 mae: 0.733902 (1938.5047696517045 steps/sec)\n",
      "Step #3002\tEpoch   0 Batch 3001/3125   Loss: 0.844315 mae: 0.741897 (1788.7835959023873 steps/sec)\n",
      "Step #3003\tEpoch   0 Batch 3002/3125   Loss: 0.763398 mae: 0.689222 (2086.19945287242 steps/sec)\n",
      "Step #3004\tEpoch   0 Batch 3003/3125   Loss: 0.786849 mae: 0.697134 (1967.254204853522 steps/sec)\n",
      "Step #3005\tEpoch   0 Batch 3004/3125   Loss: 0.809605 mae: 0.725083 (1992.808544604508 steps/sec)\n",
      "Step #3006\tEpoch   0 Batch 3005/3125   Loss: 0.746250 mae: 0.687329 (1954.639252127392 steps/sec)\n",
      "Step #3007\tEpoch   0 Batch 3006/3125   Loss: 0.709691 mae: 0.674665 (1770.0472653612424 steps/sec)\n",
      "Step #3008\tEpoch   0 Batch 3007/3125   Loss: 0.826972 mae: 0.704373 (1791.0139803404131 steps/sec)\n",
      "Step #3009\tEpoch   0 Batch 3008/3125   Loss: 0.815546 mae: 0.689946 (1893.8645763722072 steps/sec)\n",
      "Step #3010\tEpoch   0 Batch 3009/3125   Loss: 0.847514 mae: 0.712885 (1881.5963250071777 steps/sec)\n",
      "Step #3011\tEpoch   0 Batch 3010/3125   Loss: 0.839614 mae: 0.712814 (1962.9454215299943 steps/sec)\n",
      "Step #3012\tEpoch   0 Batch 3011/3125   Loss: 0.821444 mae: 0.706544 (2047.0004880429478 steps/sec)\n",
      "Step #3013\tEpoch   0 Batch 3012/3125   Loss: 0.869167 mae: 0.744740 (1772.5606869970925 steps/sec)\n",
      "Step #3014\tEpoch   0 Batch 3013/3125   Loss: 0.961187 mae: 0.772582 (1868.3700832999243 steps/sec)\n",
      "Step #3015\tEpoch   0 Batch 3014/3125   Loss: 0.929802 mae: 0.756559 (2148.0610468093823 steps/sec)\n",
      "Step #3016\tEpoch   0 Batch 3015/3125   Loss: 0.900707 mae: 0.751523 (2180.5583571614243 steps/sec)\n",
      "Step #3017\tEpoch   0 Batch 3016/3125   Loss: 0.845239 mae: 0.736956 (1998.9629403690712 steps/sec)\n",
      "Step #3018\tEpoch   0 Batch 3017/3125   Loss: 0.872977 mae: 0.731569 (1884.5891857403462 steps/sec)\n",
      "Step #3019\tEpoch   0 Batch 3018/3125   Loss: 0.860136 mae: 0.732596 (2118.634958478977 steps/sec)\n",
      "Step #3020\tEpoch   0 Batch 3019/3125   Loss: 0.942407 mae: 0.745459 (2001.194713488239 steps/sec)\n",
      "Step #3021\tEpoch   0 Batch 3020/3125   Loss: 0.944575 mae: 0.761476 (2237.9408594692077 steps/sec)\n",
      "Step #3022\tEpoch   0 Batch 3021/3125   Loss: 0.767563 mae: 0.697101 (1672.6901480347117 steps/sec)\n",
      "Step #3023\tEpoch   0 Batch 3022/3125   Loss: 0.627026 mae: 0.634914 (2004.7529371277806 steps/sec)\n",
      "Step #3024\tEpoch   0 Batch 3023/3125   Loss: 0.909251 mae: 0.746823 (2140.606308053486 steps/sec)\n",
      "Step #3025\tEpoch   0 Batch 3024/3125   Loss: 0.737940 mae: 0.709476 (2061.934164470838 steps/sec)\n",
      "Step #3026\tEpoch   0 Batch 3025/3125   Loss: 0.845724 mae: 0.738358 (1941.7534698110237 steps/sec)\n",
      "Step #3027\tEpoch   0 Batch 3026/3125   Loss: 0.722760 mae: 0.665057 (2072.448414894458 steps/sec)\n",
      "Step #3028\tEpoch   0 Batch 3027/3125   Loss: 0.855286 mae: 0.747777 (2065.427040655531 steps/sec)\n",
      "Step #3029\tEpoch   0 Batch 3028/3125   Loss: 0.768615 mae: 0.687582 (2147.049428723535 steps/sec)\n",
      "Step #3030\tEpoch   0 Batch 3029/3125   Loss: 0.784759 mae: 0.715088 (2263.8872996167756 steps/sec)\n",
      "Step #3031\tEpoch   0 Batch 3030/3125   Loss: 0.861891 mae: 0.717337 (1657.6704186164159 steps/sec)\n",
      "Step #3032\tEpoch   0 Batch 3031/3125   Loss: 0.857107 mae: 0.735245 (1669.1089975725258 steps/sec)\n",
      "Step #3033\tEpoch   0 Batch 3032/3125   Loss: 0.789042 mae: 0.694309 (1919.150766415008 steps/sec)\n",
      "Step #3034\tEpoch   0 Batch 3033/3125   Loss: 0.780861 mae: 0.722027 (2192.5956904032537 steps/sec)\n",
      "Step #3035\tEpoch   0 Batch 3034/3125   Loss: 0.771898 mae: 0.702095 (2109.2803620819714 steps/sec)\n",
      "Step #3036\tEpoch   0 Batch 3035/3125   Loss: 0.905611 mae: 0.766068 (1872.0225661899917 steps/sec)\n",
      "Step #3037\tEpoch   0 Batch 3036/3125   Loss: 0.630279 mae: 0.634420 (2244.239453802196 steps/sec)\n",
      "Step #3038\tEpoch   0 Batch 3037/3125   Loss: 0.868850 mae: 0.751205 (1965.742138070019 steps/sec)\n",
      "Step #3039\tEpoch   0 Batch 3038/3125   Loss: 0.842642 mae: 0.757517 (1752.8413697416481 steps/sec)\n",
      "Step #3040\tEpoch   0 Batch 3039/3125   Loss: 0.735771 mae: 0.689343 (1801.8627349898616 steps/sec)\n",
      "Step #3041\tEpoch   0 Batch 3040/3125   Loss: 0.909404 mae: 0.739423 (2109.344008368369 steps/sec)\n",
      "Step #3042\tEpoch   0 Batch 3041/3125   Loss: 0.841455 mae: 0.734316 (1951.9830226085985 steps/sec)\n",
      "Step #3043\tEpoch   0 Batch 3042/3125   Loss: 0.812884 mae: 0.707584 (2008.8240083527305 steps/sec)\n",
      "Step #3044\tEpoch   0 Batch 3043/3125   Loss: 0.933021 mae: 0.767997 (2204.4885473715194 steps/sec)\n",
      "Step #3045\tEpoch   0 Batch 3044/3125   Loss: 0.861209 mae: 0.720063 (2231.630025326154 steps/sec)\n",
      "Step #3046\tEpoch   0 Batch 3045/3125   Loss: 0.900018 mae: 0.734991 (2054.7219908881593 steps/sec)\n",
      "Step #3047\tEpoch   0 Batch 3046/3125   Loss: 0.652641 mae: 0.644284 (2011.7530816825747 steps/sec)\n",
      "Step #3048\tEpoch   0 Batch 3047/3125   Loss: 0.664716 mae: 0.649251 (1993.8506003936072 steps/sec)\n",
      "Step #3049\tEpoch   0 Batch 3048/3125   Loss: 0.748034 mae: 0.693552 (1916.2923299036897 steps/sec)\n",
      "Step #3050\tEpoch   0 Batch 3049/3125   Loss: 0.811713 mae: 0.737522 (2202.7519273995335 steps/sec)\n",
      "Step #3051\tEpoch   0 Batch 3050/3125   Loss: 0.815418 mae: 0.725487 (2101.7337796396146 steps/sec)\n",
      "Step #3052\tEpoch   0 Batch 3051/3125   Loss: 0.840267 mae: 0.709915 (1942.04117162251 steps/sec)\n",
      "Step #3053\tEpoch   0 Batch 3052/3125   Loss: 0.742179 mae: 0.684853 (2016.298432842996 steps/sec)\n",
      "Step #3054\tEpoch   0 Batch 3053/3125   Loss: 0.868992 mae: 0.715091 (2102.59772811582 steps/sec)\n",
      "Step #3055\tEpoch   0 Batch 3054/3125   Loss: 0.791540 mae: 0.693349 (2010.2104001917087 steps/sec)\n",
      "Step #3056\tEpoch   0 Batch 3055/3125   Loss: 0.917662 mae: 0.753563 (1843.6014874332984 steps/sec)\n",
      "Step #3057\tEpoch   0 Batch 3056/3125   Loss: 0.876656 mae: 0.742951 (1958.2713928211258 steps/sec)\n",
      "Step #3058\tEpoch   0 Batch 3057/3125   Loss: 0.985366 mae: 0.772632 (2046.6608761845278 steps/sec)\n",
      "Step #3059\tEpoch   0 Batch 3058/3125   Loss: 0.933932 mae: 0.768864 (2124.5803320872465 steps/sec)\n",
      "Step #3060\tEpoch   0 Batch 3059/3125   Loss: 0.707048 mae: 0.657895 (2085.4318728744456 steps/sec)\n",
      "Step #3061\tEpoch   0 Batch 3060/3125   Loss: 0.732459 mae: 0.697461 (2271.7840390844194 steps/sec)\n",
      "Step #3062\tEpoch   0 Batch 3061/3125   Loss: 0.888115 mae: 0.740307 (2164.0425553870127 steps/sec)\n",
      "Step #3063\tEpoch   0 Batch 3062/3125   Loss: 0.745908 mae: 0.678491 (2126.0233977413272 steps/sec)\n",
      "Step #3064\tEpoch   0 Batch 3063/3125   Loss: 0.902108 mae: 0.790410 (1908.2539422560715 steps/sec)\n",
      "Step #3065\tEpoch   0 Batch 3064/3125   Loss: 0.840072 mae: 0.721700 (1631.8598118478287 steps/sec)\n",
      "Step #3066\tEpoch   0 Batch 3065/3125   Loss: 0.813097 mae: 0.715514 (1642.5191300057174 steps/sec)\n",
      "Step #3067\tEpoch   0 Batch 3066/3125   Loss: 0.861219 mae: 0.723223 (1922.8636396977922 steps/sec)\n",
      "Step #3068\tEpoch   0 Batch 3067/3125   Loss: 0.756228 mae: 0.679700 (1585.2327787562456 steps/sec)\n",
      "Step #3069\tEpoch   0 Batch 3068/3125   Loss: 0.949679 mae: 0.760505 (2095.391870828504 steps/sec)\n",
      "Step #3070\tEpoch   0 Batch 3069/3125   Loss: 0.732437 mae: 0.691355 (1953.1283178423082 steps/sec)\n",
      "Step #3071\tEpoch   0 Batch 3070/3125   Loss: 0.813820 mae: 0.717336 (1678.0840661582902 steps/sec)\n",
      "Step #3072\tEpoch   0 Batch 3071/3125   Loss: 0.872631 mae: 0.756669 (1763.290564514773 steps/sec)\n",
      "Step #3073\tEpoch   0 Batch 3072/3125   Loss: 0.852047 mae: 0.714758 (1503.3994293661376 steps/sec)\n",
      "Step #3074\tEpoch   0 Batch 3073/3125   Loss: 1.039611 mae: 0.793823 (1613.119394489485 steps/sec)\n",
      "Step #3075\tEpoch   0 Batch 3074/3125   Loss: 0.851425 mae: 0.739988 (1860.4472911473258 steps/sec)\n",
      "Step #3076\tEpoch   0 Batch 3075/3125   Loss: 0.809628 mae: 0.714332 (1948.265546905483 steps/sec)\n",
      "Step #3077\tEpoch   0 Batch 3076/3125   Loss: 0.927381 mae: 0.776474 (2013.6074278197582 steps/sec)\n",
      "Step #3078\tEpoch   0 Batch 3077/3125   Loss: 0.733206 mae: 0.688395 (1886.7255044848093 steps/sec)\n",
      "Step #3079\tEpoch   0 Batch 3078/3125   Loss: 0.685937 mae: 0.655713 (1819.7809826278615 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3080\tEpoch   0 Batch 3079/3125   Loss: 0.850570 mae: 0.721333 (1616.3768652114934 steps/sec)\n",
      "Step #3081\tEpoch   0 Batch 3080/3125   Loss: 0.977821 mae: 0.794020 (1736.1679581429235 steps/sec)\n",
      "Step #3082\tEpoch   0 Batch 3081/3125   Loss: 0.806516 mae: 0.681177 (1921.3486028401283 steps/sec)\n",
      "Step #3083\tEpoch   0 Batch 3082/3125   Loss: 0.812441 mae: 0.715639 (1935.7492292639702 steps/sec)\n",
      "Step #3084\tEpoch   0 Batch 3083/3125   Loss: 0.793207 mae: 0.728283 (1872.5908993499536 steps/sec)\n",
      "Step #3085\tEpoch   0 Batch 3084/3125   Loss: 0.875462 mae: 0.739542 (2052.3893874595083 steps/sec)\n",
      "Step #3086\tEpoch   0 Batch 3085/3125   Loss: 0.842559 mae: 0.732317 (1840.801924055966 steps/sec)\n",
      "Step #3087\tEpoch   0 Batch 3086/3125   Loss: 0.832776 mae: 0.716798 (1957.7229700715072 steps/sec)\n",
      "Step #3088\tEpoch   0 Batch 3087/3125   Loss: 0.944859 mae: 0.752851 (1696.821017371534 steps/sec)\n",
      "Step #3089\tEpoch   0 Batch 3088/3125   Loss: 0.681814 mae: 0.666312 (1486.266672336324 steps/sec)\n",
      "Step #3090\tEpoch   0 Batch 3089/3125   Loss: 0.803882 mae: 0.726779 (1922.3348671787633 steps/sec)\n",
      "Step #3091\tEpoch   0 Batch 3090/3125   Loss: 0.686247 mae: 0.670750 (1892.9927336733313 steps/sec)\n",
      "Step #3092\tEpoch   0 Batch 3091/3125   Loss: 0.945833 mae: 0.785001 (1571.1004397563734 steps/sec)\n",
      "Step #3093\tEpoch   0 Batch 3092/3125   Loss: 0.677931 mae: 0.657469 (1886.3691150808643 steps/sec)\n",
      "Step #3094\tEpoch   0 Batch 3093/3125   Loss: 0.829245 mae: 0.734154 (1776.0734429783702 steps/sec)\n",
      "Step #3095\tEpoch   0 Batch 3094/3125   Loss: 0.896385 mae: 0.775234 (2100.5969790456347 steps/sec)\n",
      "Step #3096\tEpoch   0 Batch 3095/3125   Loss: 0.774362 mae: 0.682066 (1875.538384488803 steps/sec)\n",
      "Step #3097\tEpoch   0 Batch 3096/3125   Loss: 0.773260 mae: 0.703795 (1564.2100081300207 steps/sec)\n",
      "Step #3098\tEpoch   0 Batch 3097/3125   Loss: 0.725855 mae: 0.657984 (1673.023749311932 steps/sec)\n",
      "Step #3099\tEpoch   0 Batch 3098/3125   Loss: 0.992695 mae: 0.771810 (1659.0211140029587 steps/sec)\n",
      "Step #3100\tEpoch   0 Batch 3099/3125   Loss: 0.913684 mae: 0.773987 (1879.134782530779 steps/sec)\n",
      "Step #3101\tEpoch   0 Batch 3100/3125   Loss: 1.014245 mae: 0.778849 (1667.1187249095751 steps/sec)\n",
      "Step #3102\tEpoch   0 Batch 3101/3125   Loss: 0.867161 mae: 0.754064 (1851.2340665936938 steps/sec)\n",
      "Step #3103\tEpoch   0 Batch 3102/3125   Loss: 0.850156 mae: 0.705581 (1640.4890602877101 steps/sec)\n",
      "Step #3104\tEpoch   0 Batch 3103/3125   Loss: 0.874725 mae: 0.738771 (1885.0804494382023 steps/sec)\n",
      "Step #3105\tEpoch   0 Batch 3104/3125   Loss: 0.794005 mae: 0.684507 (1773.9851290423544 steps/sec)\n",
      "Step #3106\tEpoch   0 Batch 3105/3125   Loss: 0.824229 mae: 0.734744 (1825.4835396319702 steps/sec)\n",
      "Step #3107\tEpoch   0 Batch 3106/3125   Loss: 0.822631 mae: 0.723513 (1866.9230495317452 steps/sec)\n",
      "Step #3108\tEpoch   0 Batch 3107/3125   Loss: 0.724946 mae: 0.680385 (2050.7035642693004 steps/sec)\n",
      "Step #3109\tEpoch   0 Batch 3108/3125   Loss: 0.824962 mae: 0.703063 (1856.741155221872 steps/sec)\n",
      "Step #3110\tEpoch   0 Batch 3109/3125   Loss: 0.689061 mae: 0.655835 (1631.1109728401207 steps/sec)\n",
      "Step #3111\tEpoch   0 Batch 3110/3125   Loss: 0.740502 mae: 0.687749 (1897.1711853520412 steps/sec)\n",
      "Step #3112\tEpoch   0 Batch 3111/3125   Loss: 0.945437 mae: 0.773369 (1948.79057362958 steps/sec)\n",
      "Step #3113\tEpoch   0 Batch 3112/3125   Loss: 0.881368 mae: 0.739701 (1624.2260895157106 steps/sec)\n",
      "Step #3114\tEpoch   0 Batch 3113/3125   Loss: 0.887952 mae: 0.745958 (1767.6601483479433 steps/sec)\n",
      "Step #3115\tEpoch   0 Batch 3114/3125   Loss: 0.882308 mae: 0.740974 (2052.148385897273 steps/sec)\n",
      "Step #3116\tEpoch   0 Batch 3115/3125   Loss: 0.755093 mae: 0.670347 (1979.9582699987727 steps/sec)\n",
      "Step #3117\tEpoch   0 Batch 3116/3125   Loss: 0.790086 mae: 0.715302 (1820.9186420074673 steps/sec)\n",
      "Step #3118\tEpoch   0 Batch 3117/3125   Loss: 0.759387 mae: 0.699376 (1906.7271586642057 steps/sec)\n",
      "Step #3119\tEpoch   0 Batch 3118/3125   Loss: 0.843044 mae: 0.730554 (1887.2178827256039 steps/sec)\n",
      "Step #3120\tEpoch   0 Batch 3119/3125   Loss: 0.882461 mae: 0.756850 (2088.8176177053556 steps/sec)\n",
      "Step #3121\tEpoch   0 Batch 3120/3125   Loss: 0.786906 mae: 0.696505 (2160.943038496414 steps/sec)\n",
      "Step #3122\tEpoch   0 Batch 3121/3125   Loss: 0.836601 mae: 0.725520 (1903.6636287716494 steps/sec)\n",
      "Step #3123\tEpoch   0 Batch 3122/3125   Loss: 0.965035 mae: 0.790970 (2076.490915391851 steps/sec)\n",
      "Step #3124\tEpoch   0 Batch 3123/3125   Loss: 0.820235 mae: 0.732940 (2152.448399380074 steps/sec)\n",
      "Step #3125\tEpoch   0 Batch 3124/3125   Loss: 0.863796 mae: 0.740853 (2146.3901909811066 steps/sec)\n",
      "\n",
      "Train time for epoch #1 (3125 total steps): 89.78834199905396\n",
      "Model test set loss: 0.846205 mae: 0.728022\n",
      "best loss = 0.8462047576904297\n",
      "Step #3126\tEpoch   1 Batch    0/3125   Loss: 0.934429 mae: 0.728059 (629.4124708876564 steps/sec)\n",
      "Step #3127\tEpoch   1 Batch    1/3125   Loss: 0.977866 mae: 0.765910 (1586.5999894083025 steps/sec)\n",
      "Step #3128\tEpoch   1 Batch    2/3125   Loss: 0.795654 mae: 0.707733 (1929.373666004269 steps/sec)\n",
      "Step #3129\tEpoch   1 Batch    3/3125   Loss: 0.895251 mae: 0.752497 (1958.3262519960033 steps/sec)\n",
      "Step #3130\tEpoch   1 Batch    4/3125   Loss: 0.909411 mae: 0.765816 (1922.7049774004565 steps/sec)\n",
      "Step #3131\tEpoch   1 Batch    5/3125   Loss: 0.794032 mae: 0.724180 (1966.2581921486635 steps/sec)\n",
      "Step #3132\tEpoch   1 Batch    6/3125   Loss: 0.746328 mae: 0.691113 (2154.9919848740187 steps/sec)\n",
      "Step #3133\tEpoch   1 Batch    7/3125   Loss: 0.852522 mae: 0.733919 (2062.5222513990107 steps/sec)\n",
      "Step #3134\tEpoch   1 Batch    8/3125   Loss: 0.819349 mae: 0.716379 (2071.5272084317 steps/sec)\n",
      "Step #3135\tEpoch   1 Batch    9/3125   Loss: 0.717808 mae: 0.661213 (2131.3819948370838 steps/sec)\n",
      "Step #3136\tEpoch   1 Batch   10/3125   Loss: 0.817455 mae: 0.717439 (1638.1824288961623 steps/sec)\n",
      "Step #3137\tEpoch   1 Batch   11/3125   Loss: 0.899047 mae: 0.741631 (2299.9122653097033 steps/sec)\n",
      "Step #3138\tEpoch   1 Batch   12/3125   Loss: 0.887466 mae: 0.746821 (1997.287619047619 steps/sec)\n",
      "Step #3139\tEpoch   1 Batch   13/3125   Loss: 0.725551 mae: 0.686956 (1942.5088689434147 steps/sec)\n",
      "Step #3140\tEpoch   1 Batch   14/3125   Loss: 0.759214 mae: 0.704004 (2070.565933414952 steps/sec)\n",
      "Step #3141\tEpoch   1 Batch   15/3125   Loss: 0.908369 mae: 0.733721 (1986.6542884750195 steps/sec)\n",
      "Step #3142\tEpoch   1 Batch   16/3125   Loss: 0.932107 mae: 0.767290 (2032.5179298313626 steps/sec)\n",
      "Step #3143\tEpoch   1 Batch   17/3125   Loss: 0.825874 mae: 0.715284 (1923.5161932365377 steps/sec)\n",
      "Step #3144\tEpoch   1 Batch   18/3125   Loss: 0.777746 mae: 0.690125 (2129.3693583924783 steps/sec)\n",
      "Step #3145\tEpoch   1 Batch   19/3125   Loss: 0.896166 mae: 0.767795 (2078.116453287883 steps/sec)\n",
      "Step #3146\tEpoch   1 Batch   20/3125   Loss: 0.798486 mae: 0.712752 (2021.858007789904 steps/sec)\n",
      "Step #3147\tEpoch   1 Batch   21/3125   Loss: 0.837594 mae: 0.722715 (2156.0111031150404 steps/sec)\n",
      "Step #3148\tEpoch   1 Batch   22/3125   Loss: 0.843158 mae: 0.737411 (2320.0637224533143 steps/sec)\n",
      "Step #3149\tEpoch   1 Batch   23/3125   Loss: 0.818027 mae: 0.709120 (1989.8776935411943 steps/sec)\n",
      "Step #3150\tEpoch   1 Batch   24/3125   Loss: 0.756212 mae: 0.694621 (2141.764964204377 steps/sec)\n",
      "Step #3151\tEpoch   1 Batch   25/3125   Loss: 0.885274 mae: 0.750246 (1709.4071713277308 steps/sec)\n",
      "Step #3152\tEpoch   1 Batch   26/3125   Loss: 0.764711 mae: 0.691403 (2215.7383145972444 steps/sec)\n",
      "Step #3153\tEpoch   1 Batch   27/3125   Loss: 0.844467 mae: 0.730447 (2316.9622043242407 steps/sec)\n",
      "Step #3154\tEpoch   1 Batch   28/3125   Loss: 0.950630 mae: 0.761626 (2333.721331359959 steps/sec)\n",
      "Step #3155\tEpoch   1 Batch   29/3125   Loss: 0.776564 mae: 0.701407 (2061.0222794413926 steps/sec)\n",
      "Step #3156\tEpoch   1 Batch   30/3125   Loss: 0.880330 mae: 0.748933 (2055.829820605823 steps/sec)\n",
      "Step #3157\tEpoch   1 Batch   31/3125   Loss: 0.763312 mae: 0.707825 (2184.1920533250013 steps/sec)\n",
      "Step #3158\tEpoch   1 Batch   32/3125   Loss: 0.766703 mae: 0.697537 (2064.329166256521 steps/sec)\n",
      "Step #3159\tEpoch   1 Batch   33/3125   Loss: 0.762751 mae: 0.696894 (2193.949031259154 steps/sec)\n",
      "Step #3160\tEpoch   1 Batch   34/3125   Loss: 0.856883 mae: 0.753808 (1926.6440055121727 steps/sec)\n",
      "Step #3161\tEpoch   1 Batch   35/3125   Loss: 0.904805 mae: 0.744990 (1808.0455211656176 steps/sec)\n",
      "Step #3162\tEpoch   1 Batch   36/3125   Loss: 0.967051 mae: 0.791631 (2034.4305074551576 steps/sec)\n",
      "Step #3163\tEpoch   1 Batch   37/3125   Loss: 0.804428 mae: 0.697062 (2078.4871850779996 steps/sec)\n",
      "Step #3164\tEpoch   1 Batch   38/3125   Loss: 0.886090 mae: 0.745761 (2003.278375331945 steps/sec)\n",
      "Step #3165\tEpoch   1 Batch   39/3125   Loss: 0.910112 mae: 0.759864 (2216.347149711484 steps/sec)\n",
      "Step #3166\tEpoch   1 Batch   40/3125   Loss: 0.856046 mae: 0.744117 (2182.7371225762136 steps/sec)\n",
      "Step #3167\tEpoch   1 Batch   41/3125   Loss: 0.804860 mae: 0.726753 (1920.0644552887213 steps/sec)\n",
      "Step #3168\tEpoch   1 Batch   42/3125   Loss: 0.743773 mae: 0.662987 (2290.642578615665 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3169\tEpoch   1 Batch   43/3125   Loss: 0.848150 mae: 0.724169 (1873.9964971226364 steps/sec)\n",
      "Step #3170\tEpoch   1 Batch   44/3125   Loss: 0.703461 mae: 0.676939 (1994.7419483706508 steps/sec)\n",
      "Step #3171\tEpoch   1 Batch   45/3125   Loss: 0.757609 mae: 0.676198 (2223.490744078543 steps/sec)\n",
      "Step #3172\tEpoch   1 Batch   46/3125   Loss: 0.858191 mae: 0.731691 (2194.2244915041433 steps/sec)\n",
      "Step #3173\tEpoch   1 Batch   47/3125   Loss: 0.944208 mae: 0.773454 (1979.1174361103772 steps/sec)\n",
      "Step #3174\tEpoch   1 Batch   48/3125   Loss: 0.996776 mae: 0.815690 (2018.238860552401 steps/sec)\n",
      "Step #3175\tEpoch   1 Batch   49/3125   Loss: 0.945772 mae: 0.761867 (1916.8878651603231 steps/sec)\n",
      "Step #3176\tEpoch   1 Batch   50/3125   Loss: 0.893599 mae: 0.741405 (2042.4152707440592 steps/sec)\n",
      "Step #3177\tEpoch   1 Batch   51/3125   Loss: 0.843029 mae: 0.732504 (2233.721747651407 steps/sec)\n",
      "Step #3178\tEpoch   1 Batch   52/3125   Loss: 0.803598 mae: 0.719921 (1860.678384157432 steps/sec)\n",
      "Step #3179\tEpoch   1 Batch   53/3125   Loss: 0.828757 mae: 0.714415 (1939.6522382537921 steps/sec)\n",
      "Step #3180\tEpoch   1 Batch   54/3125   Loss: 0.785403 mae: 0.694556 (2193.7654295158795 steps/sec)\n",
      "Step #3181\tEpoch   1 Batch   55/3125   Loss: 0.885898 mae: 0.746267 (2131.576968033745 steps/sec)\n",
      "Step #3182\tEpoch   1 Batch   56/3125   Loss: 0.872462 mae: 0.736940 (2106.9493143115487 steps/sec)\n",
      "Step #3183\tEpoch   1 Batch   57/3125   Loss: 0.690669 mae: 0.657696 (2183.3279543585313 steps/sec)\n",
      "Step #3184\tEpoch   1 Batch   58/3125   Loss: 0.745492 mae: 0.705962 (960.17288269067 steps/sec)\n",
      "Step #3185\tEpoch   1 Batch   59/3125   Loss: 0.806727 mae: 0.719371 (626.9981313999551 steps/sec)\n",
      "Step #3186\tEpoch   1 Batch   60/3125   Loss: 0.723742 mae: 0.670207 (1327.4710249966768 steps/sec)\n",
      "Step #3187\tEpoch   1 Batch   61/3125   Loss: 0.772815 mae: 0.702838 (1815.1502561972027 steps/sec)\n",
      "Step #3188\tEpoch   1 Batch   62/3125   Loss: 0.795829 mae: 0.711349 (1873.0591975992284 steps/sec)\n",
      "Step #3189\tEpoch   1 Batch   63/3125   Loss: 0.659576 mae: 0.662381 (2065.610133265043 steps/sec)\n",
      "Step #3190\tEpoch   1 Batch   64/3125   Loss: 0.762741 mae: 0.687350 (2037.058766391452 steps/sec)\n",
      "Step #3191\tEpoch   1 Batch   65/3125   Loss: 0.904133 mae: 0.752049 (1674.306015727915 steps/sec)\n",
      "Step #3192\tEpoch   1 Batch   66/3125   Loss: 0.867932 mae: 0.747712 (2124.946297572245 steps/sec)\n",
      "Step #3193\tEpoch   1 Batch   67/3125   Loss: 0.839498 mae: 0.694915 (2132.183779497138 steps/sec)\n",
      "Step #3194\tEpoch   1 Batch   68/3125   Loss: 0.806069 mae: 0.696460 (2109.4288760586614 steps/sec)\n",
      "Step #3195\tEpoch   1 Batch   69/3125   Loss: 0.766515 mae: 0.693310 (1997.534932896454 steps/sec)\n",
      "Step #3196\tEpoch   1 Batch   70/3125   Loss: 0.789150 mae: 0.710880 (2033.4244768941378 steps/sec)\n",
      "Step #3197\tEpoch   1 Batch   71/3125   Loss: 0.859490 mae: 0.746476 (2200.5099524673933 steps/sec)\n",
      "Step #3198\tEpoch   1 Batch   72/3125   Loss: 0.869506 mae: 0.724259 (2244.0713513744877 steps/sec)\n",
      "Step #3199\tEpoch   1 Batch   73/3125   Loss: 0.849357 mae: 0.720145 (1775.637345796608 steps/sec)\n",
      "Step #3200\tEpoch   1 Batch   74/3125   Loss: 0.892556 mae: 0.745671 (2055.3462571299765 steps/sec)\n",
      "Step #3201\tEpoch   1 Batch   75/3125   Loss: 0.800020 mae: 0.719621 (2119.5557037890503 steps/sec)\n",
      "Step #3202\tEpoch   1 Batch   76/3125   Loss: 0.747304 mae: 0.682711 (2077.354808673343 steps/sec)\n",
      "Step #3203\tEpoch   1 Batch   77/3125   Loss: 0.737137 mae: 0.661973 (2042.7932710571688 steps/sec)\n",
      "Step #3204\tEpoch   1 Batch   78/3125   Loss: 0.653849 mae: 0.626758 (2243.2552119546035 steps/sec)\n",
      "Step #3205\tEpoch   1 Batch   79/3125   Loss: 0.798405 mae: 0.717955 (2304.5626373626374 steps/sec)\n",
      "Step #3206\tEpoch   1 Batch   80/3125   Loss: 0.811158 mae: 0.732467 (2055.6686075006373 steps/sec)\n",
      "Step #3207\tEpoch   1 Batch   81/3125   Loss: 0.882780 mae: 0.736869 (2000.4693178674654 steps/sec)\n",
      "Step #3208\tEpoch   1 Batch   82/3125   Loss: 0.855981 mae: 0.753505 (2057.120436309419 steps/sec)\n",
      "Step #3209\tEpoch   1 Batch   83/3125   Loss: 0.753293 mae: 0.683825 (1885.5041582378062 steps/sec)\n",
      "Step #3210\tEpoch   1 Batch   84/3125   Loss: 0.840884 mae: 0.733258 (1980.7624012996334 steps/sec)\n",
      "Step #3211\tEpoch   1 Batch   85/3125   Loss: 0.781607 mae: 0.690811 (2105.172708017547 steps/sec)\n",
      "Step #3212\tEpoch   1 Batch   86/3125   Loss: 0.979115 mae: 0.790758 (1994.4004869141813 steps/sec)\n",
      "Step #3213\tEpoch   1 Batch   87/3125   Loss: 0.740404 mae: 0.661980 (2084.105499572675 steps/sec)\n",
      "Step #3214\tEpoch   1 Batch   88/3125   Loss: 1.124334 mae: 0.812379 (2155.900282703675 steps/sec)\n",
      "Step #3215\tEpoch   1 Batch   89/3125   Loss: 0.887231 mae: 0.754267 (1993.945386780254 steps/sec)\n",
      "Step #3216\tEpoch   1 Batch   90/3125   Loss: 0.917511 mae: 0.768638 (1812.232764729265 steps/sec)\n",
      "Step #3217\tEpoch   1 Batch   91/3125   Loss: 0.869125 mae: 0.740653 (1936.267530860778 steps/sec)\n",
      "Step #3218\tEpoch   1 Batch   92/3125   Loss: 0.973202 mae: 0.776889 (2168.54034826488 steps/sec)\n",
      "Step #3219\tEpoch   1 Batch   93/3125   Loss: 0.851152 mae: 0.739772 (2335.6446780786064 steps/sec)\n",
      "Step #3220\tEpoch   1 Batch   94/3125   Loss: 0.859203 mae: 0.741391 (2164.1988813440385 steps/sec)\n",
      "Step #3221\tEpoch   1 Batch   95/3125   Loss: 0.720229 mae: 0.656117 (2334.682608598847 steps/sec)\n",
      "Step #3222\tEpoch   1 Batch   96/3125   Loss: 0.637928 mae: 0.627997 (2263.3742013469177 steps/sec)\n",
      "Step #3223\tEpoch   1 Batch   97/3125   Loss: 0.872236 mae: 0.748707 (2229.7792710415515 steps/sec)\n",
      "Step #3224\tEpoch   1 Batch   98/3125   Loss: 0.789418 mae: 0.720211 (2206.622544429129 steps/sec)\n",
      "Step #3225\tEpoch   1 Batch   99/3125   Loss: 1.046290 mae: 0.824585 (1619.5349483747905 steps/sec)\n",
      "Step #3226\tEpoch   1 Batch  100/3125   Loss: 0.941138 mae: 0.779581 (1420.776933186998 steps/sec)\n",
      "Step #3227\tEpoch   1 Batch  101/3125   Loss: 0.804106 mae: 0.700480 (1589.3053639904815 steps/sec)\n",
      "Step #3228\tEpoch   1 Batch  102/3125   Loss: 0.927047 mae: 0.771672 (2076.573159984553 steps/sec)\n",
      "Step #3229\tEpoch   1 Batch  103/3125   Loss: 0.827618 mae: 0.706888 (2060.63750343906 steps/sec)\n",
      "Step #3230\tEpoch   1 Batch  104/3125   Loss: 0.838177 mae: 0.742241 (2025.4705956209737 steps/sec)\n",
      "Step #3231\tEpoch   1 Batch  105/3125   Loss: 0.688182 mae: 0.650312 (2035.003008131659 steps/sec)\n",
      "Step #3232\tEpoch   1 Batch  106/3125   Loss: 0.779097 mae: 0.712553 (2211.882336810353 steps/sec)\n",
      "Step #3233\tEpoch   1 Batch  107/3125   Loss: 0.949073 mae: 0.764629 (1796.6913119093924 steps/sec)\n",
      "Step #3234\tEpoch   1 Batch  108/3125   Loss: 0.794355 mae: 0.713558 (1941.4478800222182 steps/sec)\n",
      "Step #3235\tEpoch   1 Batch  109/3125   Loss: 0.904751 mae: 0.725891 (2065.101622813928 steps/sec)\n",
      "Step #3236\tEpoch   1 Batch  110/3125   Loss: 0.727115 mae: 0.691756 (1994.0212223785798 steps/sec)\n",
      "Step #3237\tEpoch   1 Batch  111/3125   Loss: 0.797564 mae: 0.712850 (2167.0166156898404 steps/sec)\n",
      "Step #3238\tEpoch   1 Batch  112/3125   Loss: 0.810580 mae: 0.711452 (2311.9048406477714 steps/sec)\n",
      "Step #3239\tEpoch   1 Batch  113/3125   Loss: 0.861773 mae: 0.729032 (2274.8890841441853 steps/sec)\n",
      "Step #3240\tEpoch   1 Batch  114/3125   Loss: 0.924396 mae: 0.761866 (2174.5891185101464 steps/sec)\n",
      "Step #3241\tEpoch   1 Batch  115/3125   Loss: 0.834284 mae: 0.729593 (2184.2603007957337 steps/sec)\n",
      "Step #3242\tEpoch   1 Batch  116/3125   Loss: 0.954693 mae: 0.777109 (1942.2570039360962 steps/sec)\n",
      "Step #3243\tEpoch   1 Batch  117/3125   Loss: 0.859472 mae: 0.717546 (2099.50344385712 steps/sec)\n",
      "Step #3244\tEpoch   1 Batch  118/3125   Loss: 0.775464 mae: 0.705079 (2256.2151694459385 steps/sec)\n",
      "Step #3245\tEpoch   1 Batch  119/3125   Loss: 0.814932 mae: 0.728143 (2097.6764191047764 steps/sec)\n",
      "Step #3246\tEpoch   1 Batch  120/3125   Loss: 0.900814 mae: 0.767247 (2092.8407480589985 steps/sec)\n",
      "Step #3247\tEpoch   1 Batch  121/3125   Loss: 1.003961 mae: 0.822321 (2095.4756195043965 steps/sec)\n",
      "Step #3248\tEpoch   1 Batch  122/3125   Loss: 1.061286 mae: 0.835851 (2139.056109178813 steps/sec)\n",
      "Step #3249\tEpoch   1 Batch  123/3125   Loss: 0.820741 mae: 0.720834 (2209.761443142544 steps/sec)\n",
      "Step #3250\tEpoch   1 Batch  124/3125   Loss: 0.720300 mae: 0.693666 (2014.6713547370646 steps/sec)\n",
      "Step #3251\tEpoch   1 Batch  125/3125   Loss: 0.833214 mae: 0.744612 (1822.5486021187655 steps/sec)\n",
      "Step #3252\tEpoch   1 Batch  126/3125   Loss: 0.949834 mae: 0.771989 (1780.7788326002412 steps/sec)\n",
      "Step #3253\tEpoch   1 Batch  127/3125   Loss: 0.931146 mae: 0.770296 (1805.446077291941 steps/sec)\n",
      "Step #3254\tEpoch   1 Batch  128/3125   Loss: 0.879998 mae: 0.748393 (1872.8082944123453 steps/sec)\n",
      "Step #3255\tEpoch   1 Batch  129/3125   Loss: 0.978262 mae: 0.811658 (1861.454616463404 steps/sec)\n",
      "Step #3256\tEpoch   1 Batch  130/3125   Loss: 0.779382 mae: 0.706414 (1795.9220025177053 steps/sec)\n",
      "Step #3257\tEpoch   1 Batch  131/3125   Loss: 0.841421 mae: 0.729555 (1587.8974188126083 steps/sec)\n",
      "Step #3258\tEpoch   1 Batch  132/3125   Loss: 0.858498 mae: 0.739779 (1775.3667724867726 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3259\tEpoch   1 Batch  133/3125   Loss: 0.855840 mae: 0.746214 (1631.745537728949 steps/sec)\n",
      "Step #3260\tEpoch   1 Batch  134/3125   Loss: 0.807296 mae: 0.690775 (1885.4363520304958 steps/sec)\n",
      "Step #3261\tEpoch   1 Batch  135/3125   Loss: 0.797561 mae: 0.750168 (1840.866557820263 steps/sec)\n",
      "Step #3262\tEpoch   1 Batch  136/3125   Loss: 0.737281 mae: 0.694769 (2004.4080399896777 steps/sec)\n",
      "Step #3263\tEpoch   1 Batch  137/3125   Loss: 0.737186 mae: 0.684188 (1841.6101724682987 steps/sec)\n",
      "Step #3264\tEpoch   1 Batch  138/3125   Loss: 0.813374 mae: 0.716702 (2031.9468263426638 steps/sec)\n",
      "Step #3265\tEpoch   1 Batch  139/3125   Loss: 0.899817 mae: 0.762245 (1951.52891254583 steps/sec)\n",
      "Step #3266\tEpoch   1 Batch  140/3125   Loss: 0.866606 mae: 0.741865 (1888.4244459852503 steps/sec)\n",
      "Step #3267\tEpoch   1 Batch  141/3125   Loss: 0.809149 mae: 0.735056 (1914.5428983549089 steps/sec)\n",
      "Step #3268\tEpoch   1 Batch  142/3125   Loss: 0.793342 mae: 0.717720 (1593.8712227153887 steps/sec)\n",
      "Step #3269\tEpoch   1 Batch  143/3125   Loss: 0.803212 mae: 0.704056 (1969.0643631754378 steps/sec)\n",
      "Step #3270\tEpoch   1 Batch  144/3125   Loss: 0.694211 mae: 0.643944 (1875.0520810049622 steps/sec)\n",
      "Step #3271\tEpoch   1 Batch  145/3125   Loss: 0.856332 mae: 0.720526 (2011.5794118211293 steps/sec)\n",
      "Step #3272\tEpoch   1 Batch  146/3125   Loss: 0.729278 mae: 0.684987 (1850.6132966237801 steps/sec)\n",
      "Step #3273\tEpoch   1 Batch  147/3125   Loss: 0.935338 mae: 0.768296 (1952.4918768445848 steps/sec)\n",
      "Step #3274\tEpoch   1 Batch  148/3125   Loss: 0.698731 mae: 0.672419 (1935.0526403203633 steps/sec)\n",
      "Step #3275\tEpoch   1 Batch  149/3125   Loss: 0.779992 mae: 0.696976 (1733.0402446078836 steps/sec)\n",
      "Step #3276\tEpoch   1 Batch  150/3125   Loss: 0.847636 mae: 0.726216 (1665.8342070981475 steps/sec)\n",
      "Step #3277\tEpoch   1 Batch  151/3125   Loss: 0.842145 mae: 0.715480 (1878.6297835746022 steps/sec)\n",
      "Step #3278\tEpoch   1 Batch  152/3125   Loss: 0.739481 mae: 0.684944 (1731.9668001816906 steps/sec)\n",
      "Step #3279\tEpoch   1 Batch  153/3125   Loss: 0.996726 mae: 0.791735 (1711.0947928395424 steps/sec)\n",
      "Step #3280\tEpoch   1 Batch  154/3125   Loss: 0.884049 mae: 0.753564 (1617.9730897420072 steps/sec)\n",
      "Step #3281\tEpoch   1 Batch  155/3125   Loss: 0.788298 mae: 0.706304 (1845.2236192621401 steps/sec)\n",
      "Step #3282\tEpoch   1 Batch  156/3125   Loss: 0.941092 mae: 0.771222 (1663.1655748885753 steps/sec)\n",
      "Step #3283\tEpoch   1 Batch  157/3125   Loss: 0.872099 mae: 0.749787 (1794.2932434397967 steps/sec)\n",
      "Step #3284\tEpoch   1 Batch  158/3125   Loss: 0.819815 mae: 0.719564 (1927.9724201333026 steps/sec)\n",
      "Step #3285\tEpoch   1 Batch  159/3125   Loss: 0.780093 mae: 0.690894 (1855.082309441039 steps/sec)\n",
      "Step #3286\tEpoch   1 Batch  160/3125   Loss: 0.760632 mae: 0.667197 (1933.99977867129 steps/sec)\n",
      "Step #3287\tEpoch   1 Batch  161/3125   Loss: 0.797356 mae: 0.686544 (1778.483352838413 steps/sec)\n",
      "Step #3288\tEpoch   1 Batch  162/3125   Loss: 0.890938 mae: 0.777143 (1851.6590439527452 steps/sec)\n",
      "Step #3289\tEpoch   1 Batch  163/3125   Loss: 0.817794 mae: 0.717168 (2021.3513253012047 steps/sec)\n",
      "Step #3290\tEpoch   1 Batch  164/3125   Loss: 0.833606 mae: 0.720466 (1593.0721198401725 steps/sec)\n",
      "Step #3291\tEpoch   1 Batch  165/3125   Loss: 0.804475 mae: 0.711515 (1718.0335389581132 steps/sec)\n",
      "Step #3292\tEpoch   1 Batch  166/3125   Loss: 0.983842 mae: 0.774134 (1926.785615846824 steps/sec)\n",
      "Step #3293\tEpoch   1 Batch  167/3125   Loss: 0.827389 mae: 0.722804 (1878.3437379645138 steps/sec)\n",
      "Step #3294\tEpoch   1 Batch  168/3125   Loss: 0.973336 mae: 0.798482 (1851.9860824104983 steps/sec)\n",
      "Step #3295\tEpoch   1 Batch  169/3125   Loss: 0.953255 mae: 0.756924 (2083.1945962054237 steps/sec)\n",
      "Step #3296\tEpoch   1 Batch  170/3125   Loss: 0.865853 mae: 0.756541 (1958.3262519960033 steps/sec)\n",
      "Step #3297\tEpoch   1 Batch  171/3125   Loss: 0.904081 mae: 0.768439 (1799.7442608882213 steps/sec)\n",
      "Step #3298\tEpoch   1 Batch  172/3125   Loss: 0.712237 mae: 0.676091 (1481.2592262976855 steps/sec)\n",
      "Step #3299\tEpoch   1 Batch  173/3125   Loss: 0.805672 mae: 0.701889 (1955.1859482943473 steps/sec)\n",
      "Step #3300\tEpoch   1 Batch  174/3125   Loss: 0.753333 mae: 0.681070 (1868.6697497037255 steps/sec)\n",
      "Step #3301\tEpoch   1 Batch  175/3125   Loss: 0.833282 mae: 0.741501 (1842.7752978805665 steps/sec)\n",
      "Step #3302\tEpoch   1 Batch  176/3125   Loss: 0.892268 mae: 0.748557 (1979.8087362051224 steps/sec)\n",
      "Step #3303\tEpoch   1 Batch  177/3125   Loss: 0.653823 mae: 0.635082 (1947.1801823550165 steps/sec)\n",
      "Step #3304\tEpoch   1 Batch  178/3125   Loss: 0.708258 mae: 0.685545 (1799.9141734040545 steps/sec)\n",
      "Step #3305\tEpoch   1 Batch  179/3125   Loss: 0.859210 mae: 0.730402 (1856.8726757570391 steps/sec)\n",
      "Step #3306\tEpoch   1 Batch  180/3125   Loss: 0.800705 mae: 0.701072 (1698.8966478184086 steps/sec)\n",
      "Step #3307\tEpoch   1 Batch  181/3125   Loss: 0.802893 mae: 0.709798 (1550.918503180003 steps/sec)\n",
      "Step #3308\tEpoch   1 Batch  182/3125   Loss: 0.842846 mae: 0.740272 (1982.9536966121086 steps/sec)\n",
      "Step #3309\tEpoch   1 Batch  183/3125   Loss: 0.793955 mae: 0.705895 (1954.8943389542958 steps/sec)\n",
      "Step #3310\tEpoch   1 Batch  184/3125   Loss: 0.961390 mae: 0.783731 (1801.1353974320436 steps/sec)\n",
      "Step #3311\tEpoch   1 Batch  185/3125   Loss: 0.726579 mae: 0.665562 (2018.258283690537 steps/sec)\n",
      "Step #3312\tEpoch   1 Batch  186/3125   Loss: 0.809578 mae: 0.686712 (1849.1610161271835 steps/sec)\n",
      "Step #3313\tEpoch   1 Batch  187/3125   Loss: 0.846524 mae: 0.732578 (1738.8310794564163 steps/sec)\n",
      "Step #3314\tEpoch   1 Batch  188/3125   Loss: 0.801620 mae: 0.701518 (1550.5286350126428 steps/sec)\n",
      "Step #3315\tEpoch   1 Batch  189/3125   Loss: 0.904441 mae: 0.747834 (1751.4360400537837 steps/sec)\n",
      "Step #3316\tEpoch   1 Batch  190/3125   Loss: 0.802118 mae: 0.729013 (1717.0348294552064 steps/sec)\n",
      "Step #3317\tEpoch   1 Batch  191/3125   Loss: 0.867474 mae: 0.747714 (1862.8269926007515 steps/sec)\n",
      "Step #3318\tEpoch   1 Batch  192/3125   Loss: 0.940872 mae: 0.743866 (1788.844628310658 steps/sec)\n",
      "Step #3319\tEpoch   1 Batch  193/3125   Loss: 0.870915 mae: 0.728625 (1895.0796562536711 steps/sec)\n",
      "Step #3320\tEpoch   1 Batch  194/3125   Loss: 0.816670 mae: 0.698230 (1708.5158904087268 steps/sec)\n",
      "Step #3321\tEpoch   1 Batch  195/3125   Loss: 0.870139 mae: 0.739832 (1740.2884503676166 steps/sec)\n",
      "Step #3322\tEpoch   1 Batch  196/3125   Loss: 0.906986 mae: 0.773533 (1532.9946418520333 steps/sec)\n",
      "Step #3323\tEpoch   1 Batch  197/3125   Loss: 0.792136 mae: 0.712715 (1771.2282835449025 steps/sec)\n",
      "Step #3324\tEpoch   1 Batch  198/3125   Loss: 0.956515 mae: 0.806839 (1932.1466740372214 steps/sec)\n",
      "Step #3325\tEpoch   1 Batch  199/3125   Loss: 0.888529 mae: 0.740177 (1661.294717830096 steps/sec)\n",
      "Step #3326\tEpoch   1 Batch  200/3125   Loss: 0.989748 mae: 0.762678 (1897.8922886179967 steps/sec)\n",
      "Step #3327\tEpoch   1 Batch  201/3125   Loss: 0.885684 mae: 0.746226 (1847.3529359947852 steps/sec)\n",
      "Step #3328\tEpoch   1 Batch  202/3125   Loss: 0.915287 mae: 0.770622 (1922.2467666981365 steps/sec)\n",
      "Step #3329\tEpoch   1 Batch  203/3125   Loss: 0.916642 mae: 0.744528 (1878.00732522007 steps/sec)\n",
      "Step #3330\tEpoch   1 Batch  204/3125   Loss: 0.763677 mae: 0.682084 (1453.9928172275609 steps/sec)\n",
      "Step #3331\tEpoch   1 Batch  205/3125   Loss: 0.735734 mae: 0.672328 (1975.8729201605456 steps/sec)\n",
      "Step #3332\tEpoch   1 Batch  206/3125   Loss: 0.883368 mae: 0.738826 (1755.2620566129328 steps/sec)\n",
      "Step #3333\tEpoch   1 Batch  207/3125   Loss: 0.892222 mae: 0.730278 (1590.739940076611 steps/sec)\n",
      "Step #3334\tEpoch   1 Batch  208/3125   Loss: 0.850157 mae: 0.717512 (1922.8636396977922 steps/sec)\n",
      "Step #3335\tEpoch   1 Batch  209/3125   Loss: 0.744418 mae: 0.695180 (1744.4140374809726 steps/sec)\n",
      "Step #3336\tEpoch   1 Batch  210/3125   Loss: 0.788533 mae: 0.698747 (1792.5601750547046 steps/sec)\n",
      "Step #3337\tEpoch   1 Batch  211/3125   Loss: 0.822650 mae: 0.709303 (1808.9812818079877 steps/sec)\n",
      "Step #3338\tEpoch   1 Batch  212/3125   Loss: 0.888115 mae: 0.750640 (1692.8488977502966 steps/sec)\n",
      "Step #3339\tEpoch   1 Batch  213/3125   Loss: 0.713258 mae: 0.667288 (1813.2982862676604 steps/sec)\n",
      "Step #3340\tEpoch   1 Batch  214/3125   Loss: 0.908765 mae: 0.743499 (1751.2605322711293 steps/sec)\n",
      "Step #3341\tEpoch   1 Batch  215/3125   Loss: 0.863341 mae: 0.737440 (1788.844628310658 steps/sec)\n",
      "Step #3342\tEpoch   1 Batch  216/3125   Loss: 0.876912 mae: 0.746935 (1336.463630686091 steps/sec)\n",
      "Step #3343\tEpoch   1 Batch  217/3125   Loss: 0.780318 mae: 0.693238 (1352.4079758557536 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3344\tEpoch   1 Batch  218/3125   Loss: 0.848105 mae: 0.743659 (1047.3139866460915 steps/sec)\n",
      "Step #3345\tEpoch   1 Batch  219/3125   Loss: 1.007406 mae: 0.790276 (1241.5574881743855 steps/sec)\n",
      "Step #3346\tEpoch   1 Batch  220/3125   Loss: 0.825259 mae: 0.707524 (1287.4651605377862 steps/sec)\n",
      "Step #3347\tEpoch   1 Batch  221/3125   Loss: 0.747160 mae: 0.663890 (1821.6304017372422 steps/sec)\n",
      "Step #3348\tEpoch   1 Batch  222/3125   Loss: 0.881651 mae: 0.755115 (1743.9933471933473 steps/sec)\n",
      "Step #3349\tEpoch   1 Batch  223/3125   Loss: 0.893544 mae: 0.742654 (1737.5056959875392 steps/sec)\n",
      "Step #3350\tEpoch   1 Batch  224/3125   Loss: 0.602181 mae: 0.622786 (1745.8807858807859 steps/sec)\n",
      "Step #3351\tEpoch   1 Batch  225/3125   Loss: 0.744725 mae: 0.681626 (1552.7558122316007 steps/sec)\n",
      "Step #3352\tEpoch   1 Batch  226/3125   Loss: 0.753199 mae: 0.681285 (1646.0257285706437 steps/sec)\n",
      "Step #3353\tEpoch   1 Batch  227/3125   Loss: 0.855700 mae: 0.751629 (1778.9812105017602 steps/sec)\n",
      "Step #3354\tEpoch   1 Batch  228/3125   Loss: 0.944304 mae: 0.768021 (1854.7377730609358 steps/sec)\n",
      "Step #3355\tEpoch   1 Batch  229/3125   Loss: 0.805656 mae: 0.693017 (1864.6323464034854 steps/sec)\n",
      "Step #3356\tEpoch   1 Batch  230/3125   Loss: 0.886326 mae: 0.757607 (1696.4229667858472 steps/sec)\n",
      "Step #3357\tEpoch   1 Batch  231/3125   Loss: 0.755802 mae: 0.667475 (1880.3984685322837 steps/sec)\n",
      "Step #3358\tEpoch   1 Batch  232/3125   Loss: 0.789738 mae: 0.720622 (1851.8552531656746 steps/sec)\n",
      "Step #3359\tEpoch   1 Batch  233/3125   Loss: 0.720060 mae: 0.689133 (1253.4079227330321 steps/sec)\n",
      "Step #3360\tEpoch   1 Batch  234/3125   Loss: 0.781668 mae: 0.701520 (1428.3246836391374 steps/sec)\n",
      "Step #3361\tEpoch   1 Batch  235/3125   Loss: 0.790134 mae: 0.712799 (1527.0450143445905 steps/sec)\n",
      "Step #3362\tEpoch   1 Batch  236/3125   Loss: 0.778828 mae: 0.709527 (1634.14866012639 steps/sec)\n",
      "Step #3363\tEpoch   1 Batch  237/3125   Loss: 0.893797 mae: 0.734211 (1531.807723491129 steps/sec)\n",
      "Step #3364\tEpoch   1 Batch  238/3125   Loss: 0.713005 mae: 0.655841 (1490.3542621611057 steps/sec)\n",
      "Step #3365\tEpoch   1 Batch  239/3125   Loss: 0.776650 mae: 0.709892 (1437.9419105345437 steps/sec)\n",
      "Step #3366\tEpoch   1 Batch  240/3125   Loss: 0.921001 mae: 0.752327 (1551.8825479516931 steps/sec)\n",
      "Step #3367\tEpoch   1 Batch  241/3125   Loss: 0.752276 mae: 0.695175 (1853.8196347435603 steps/sec)\n",
      "Step #3368\tEpoch   1 Batch  242/3125   Loss: 0.804036 mae: 0.701421 (1799.7288158865833 steps/sec)\n",
      "Step #3369\tEpoch   1 Batch  243/3125   Loss: 0.837604 mae: 0.728956 (2038.325914118539 steps/sec)\n",
      "Step #3370\tEpoch   1 Batch  244/3125   Loss: 0.807585 mae: 0.701852 (1735.507042486635 steps/sec)\n",
      "Step #3371\tEpoch   1 Batch  245/3125   Loss: 0.972739 mae: 0.791667 (1651.0407809793733 steps/sec)\n",
      "Step #3372\tEpoch   1 Batch  246/3125   Loss: 0.984667 mae: 0.797189 (1712.6179023788718 steps/sec)\n",
      "Step #3373\tEpoch   1 Batch  247/3125   Loss: 0.833748 mae: 0.730312 (1073.9260237916008 steps/sec)\n",
      "Step #3374\tEpoch   1 Batch  248/3125   Loss: 0.758503 mae: 0.691108 (1088.637873754153 steps/sec)\n",
      "Step #3375\tEpoch   1 Batch  249/3125   Loss: 0.812145 mae: 0.691776 (875.3926876406183 steps/sec)\n",
      "Step #3376\tEpoch   1 Batch  250/3125   Loss: 0.792800 mae: 0.705699 (1438.8100661379292 steps/sec)\n",
      "Step #3377\tEpoch   1 Batch  251/3125   Loss: 0.904244 mae: 0.756965 (802.4273869240982 steps/sec)\n",
      "Step #3378\tEpoch   1 Batch  252/3125   Loss: 0.837636 mae: 0.722938 (479.32273429571865 steps/sec)\n",
      "Step #3379\tEpoch   1 Batch  253/3125   Loss: 0.928936 mae: 0.737219 (712.7884765717937 steps/sec)\n",
      "Step #3380\tEpoch   1 Batch  254/3125   Loss: 0.807816 mae: 0.733227 (971.4973965571553 steps/sec)\n",
      "Step #3381\tEpoch   1 Batch  255/3125   Loss: 0.976454 mae: 0.801167 (1120.2974422530397 steps/sec)\n",
      "Step #3382\tEpoch   1 Batch  256/3125   Loss: 0.822359 mae: 0.715050 (1201.8131909065382 steps/sec)\n",
      "Step #3383\tEpoch   1 Batch  257/3125   Loss: 0.870169 mae: 0.744092 (1452.5124496990602 steps/sec)\n",
      "Step #3384\tEpoch   1 Batch  258/3125   Loss: 0.976632 mae: 0.795503 (897.9187093514645 steps/sec)\n",
      "Step #3385\tEpoch   1 Batch  259/3125   Loss: 0.776734 mae: 0.685656 (1327.3533972594068 steps/sec)\n",
      "Step #3386\tEpoch   1 Batch  260/3125   Loss: 0.936013 mae: 0.760352 (1221.390548741424 steps/sec)\n",
      "Step #3387\tEpoch   1 Batch  261/3125   Loss: 1.034707 mae: 0.793129 (1896.8108391671642 steps/sec)\n",
      "Step #3388\tEpoch   1 Batch  262/3125   Loss: 0.733814 mae: 0.665410 (1720.6977469272551 steps/sec)\n",
      "Step #3389\tEpoch   1 Batch  263/3125   Loss: 0.737680 mae: 0.684728 (2007.3818822268167 steps/sec)\n",
      "Step #3390\tEpoch   1 Batch  264/3125   Loss: 0.872833 mae: 0.754252 (2004.2547904620824 steps/sec)\n",
      "Step #3391\tEpoch   1 Batch  265/3125   Loss: 0.792828 mae: 0.695357 (1911.10584590149 steps/sec)\n",
      "Step #3392\tEpoch   1 Batch  266/3125   Loss: 0.744232 mae: 0.687328 (1902.800007258606 steps/sec)\n",
      "Step #3393\tEpoch   1 Batch  267/3125   Loss: 0.869147 mae: 0.746060 (2010.7693487765591 steps/sec)\n",
      "Step #3394\tEpoch   1 Batch  268/3125   Loss: 0.921992 mae: 0.741165 (2160.3642581947793 steps/sec)\n",
      "Step #3395\tEpoch   1 Batch  269/3125   Loss: 0.881812 mae: 0.751199 (2024.1412259789397 steps/sec)\n",
      "Step #3396\tEpoch   1 Batch  270/3125   Loss: 0.696205 mae: 0.666025 (1815.920406625854 steps/sec)\n",
      "Step #3397\tEpoch   1 Batch  271/3125   Loss: 0.836550 mae: 0.724142 (1965.0974512743628 steps/sec)\n",
      "Step #3398\tEpoch   1 Batch  272/3125   Loss: 0.881243 mae: 0.754953 (1861.3554869173147 steps/sec)\n",
      "Step #3399\tEpoch   1 Batch  273/3125   Loss: 0.692470 mae: 0.648024 (1810.3398565299588 steps/sec)\n",
      "Step #3400\tEpoch   1 Batch  274/3125   Loss: 0.848201 mae: 0.738158 (1803.6293582400192 steps/sec)\n",
      "Step #3401\tEpoch   1 Batch  275/3125   Loss: 0.747223 mae: 0.697320 (1895.6620777553806 steps/sec)\n",
      "Step #3402\tEpoch   1 Batch  276/3125   Loss: 0.811798 mae: 0.701709 (1858.4688460959032 steps/sec)\n",
      "Step #3403\tEpoch   1 Batch  277/3125   Loss: 0.791989 mae: 0.702288 (1906.8658562089126 steps/sec)\n",
      "Step #3404\tEpoch   1 Batch  278/3125   Loss: 0.762244 mae: 0.687670 (1988.0667760008341 steps/sec)\n",
      "Step #3405\tEpoch   1 Batch  279/3125   Loss: 0.820822 mae: 0.705797 (1888.7305804476066 steps/sec)\n",
      "Step #3406\tEpoch   1 Batch  280/3125   Loss: 0.916207 mae: 0.758221 (1892.138764830604 steps/sec)\n",
      "Step #3407\tEpoch   1 Batch  281/3125   Loss: 0.846097 mae: 0.722212 (1805.3217406275555 steps/sec)\n",
      "Step #3408\tEpoch   1 Batch  282/3125   Loss: 0.934043 mae: 0.760062 (1797.9081650149171 steps/sec)\n",
      "Step #3409\tEpoch   1 Batch  283/3125   Loss: 0.786033 mae: 0.708989 (1769.2110413713976 steps/sec)\n",
      "Step #3410\tEpoch   1 Batch  284/3125   Loss: 0.892638 mae: 0.742153 (1793.9862616446676 steps/sec)\n",
      "Step #3411\tEpoch   1 Batch  285/3125   Loss: 0.868860 mae: 0.730507 (1470.0659624133411 steps/sec)\n",
      "Step #3412\tEpoch   1 Batch  286/3125   Loss: 0.884589 mae: 0.716340 (1703.865715539234 steps/sec)\n",
      "Step #3413\tEpoch   1 Batch  287/3125   Loss: 0.793858 mae: 0.714630 (1816.6597366597366 steps/sec)\n",
      "Step #3414\tEpoch   1 Batch  288/3125   Loss: 0.908972 mae: 0.746576 (1572.9151197413917 steps/sec)\n",
      "Step #3415\tEpoch   1 Batch  289/3125   Loss: 0.776960 mae: 0.692670 (1466.7449993006014 steps/sec)\n",
      "Step #3416\tEpoch   1 Batch  290/3125   Loss: 0.953230 mae: 0.797962 (1648.3804283749264 steps/sec)\n",
      "Step #3417\tEpoch   1 Batch  291/3125   Loss: 0.763973 mae: 0.690705 (1626.4935588698356 steps/sec)\n",
      "Step #3418\tEpoch   1 Batch  292/3125   Loss: 0.929095 mae: 0.753182 (1745.7499854323269 steps/sec)\n",
      "Step #3419\tEpoch   1 Batch  293/3125   Loss: 0.996948 mae: 0.783338 (1429.434538415399 steps/sec)\n",
      "Step #3420\tEpoch   1 Batch  294/3125   Loss: 0.785226 mae: 0.702287 (1473.9197661015996 steps/sec)\n",
      "Step #3421\tEpoch   1 Batch  295/3125   Loss: 0.750439 mae: 0.669363 (1164.2823832470965 steps/sec)\n",
      "Step #3422\tEpoch   1 Batch  296/3125   Loss: 0.897072 mae: 0.743115 (1628.7167698293738 steps/sec)\n",
      "Step #3423\tEpoch   1 Batch  297/3125   Loss: 0.802607 mae: 0.709054 (1753.823509734395 steps/sec)\n",
      "Step #3424\tEpoch   1 Batch  298/3125   Loss: 0.836922 mae: 0.711103 (1607.456463085602 steps/sec)\n",
      "Step #3425\tEpoch   1 Batch  299/3125   Loss: 0.816360 mae: 0.730124 (1511.6243197462788 steps/sec)\n",
      "Step #3426\tEpoch   1 Batch  300/3125   Loss: 1.019576 mae: 0.808545 (1715.0268643534155 steps/sec)\n",
      "Step #3427\tEpoch   1 Batch  301/3125   Loss: 0.847110 mae: 0.730103 (1950.8572172764398 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3428\tEpoch   1 Batch  302/3125   Loss: 0.767312 mae: 0.709087 (1526.2559586623486 steps/sec)\n",
      "Step #3429\tEpoch   1 Batch  303/3125   Loss: 0.832427 mae: 0.733875 (1762.0460770639736 steps/sec)\n",
      "Step #3430\tEpoch   1 Batch  304/3125   Loss: 0.896385 mae: 0.752357 (1728.2557995797108 steps/sec)\n",
      "Step #3431\tEpoch   1 Batch  305/3125   Loss: 0.755012 mae: 0.676858 (1801.5995876465788 steps/sec)\n",
      "Step #3432\tEpoch   1 Batch  306/3125   Loss: 0.818763 mae: 0.714597 (1631.8598118478287 steps/sec)\n",
      "Step #3433\tEpoch   1 Batch  307/3125   Loss: 0.782535 mae: 0.710398 (2019.4632488179725 steps/sec)\n",
      "Step #3434\tEpoch   1 Batch  308/3125   Loss: 0.730435 mae: 0.687684 (1939.6522382537921 steps/sec)\n",
      "Step #3435\tEpoch   1 Batch  309/3125   Loss: 0.747883 mae: 0.690686 (1846.5884176139616 steps/sec)\n",
      "Step #3436\tEpoch   1 Batch  310/3125   Loss: 0.895298 mae: 0.743681 (1549.3373129035595 steps/sec)\n",
      "Step #3437\tEpoch   1 Batch  311/3125   Loss: 0.916853 mae: 0.747291 (1676.4474999000759 steps/sec)\n",
      "Step #3438\tEpoch   1 Batch  312/3125   Loss: 0.778602 mae: 0.702421 (1674.2659151511282 steps/sec)\n",
      "Step #3439\tEpoch   1 Batch  313/3125   Loss: 0.847046 mae: 0.709673 (1715.6161290586474 steps/sec)\n",
      "Step #3440\tEpoch   1 Batch  314/3125   Loss: 0.965676 mae: 0.784410 (2035.160996060012 steps/sec)\n",
      "Step #3441\tEpoch   1 Batch  315/3125   Loss: 0.809013 mae: 0.704848 (1778.8454034980575 steps/sec)\n",
      "Step #3442\tEpoch   1 Batch  316/3125   Loss: 0.754561 mae: 0.706603 (1830.1033230941077 steps/sec)\n",
      "Step #3443\tEpoch   1 Batch  317/3125   Loss: 0.949980 mae: 0.764148 (1984.2107255042954 steps/sec)\n",
      "Step #3444\tEpoch   1 Batch  318/3125   Loss: 0.809613 mae: 0.709806 (1683.6210080120743 steps/sec)\n",
      "Step #3445\tEpoch   1 Batch  319/3125   Loss: 0.907288 mae: 0.757194 (1793.1426030747132 steps/sec)\n",
      "Step #3446\tEpoch   1 Batch  320/3125   Loss: 0.968977 mae: 0.779938 (1716.2619790004337 steps/sec)\n",
      "Step #3447\tEpoch   1 Batch  321/3125   Loss: 0.879717 mae: 0.737121 (1886.2164179775684 steps/sec)\n",
      "Step #3448\tEpoch   1 Batch  322/3125   Loss: 0.707770 mae: 0.669802 (1708.014953210135 steps/sec)\n",
      "Step #3449\tEpoch   1 Batch  323/3125   Loss: 0.809451 mae: 0.693210 (1683.2156157698728 steps/sec)\n",
      "Step #3450\tEpoch   1 Batch  324/3125   Loss: 0.818517 mae: 0.724355 (1736.2398271337147 steps/sec)\n",
      "Step #3451\tEpoch   1 Batch  325/3125   Loss: 0.828419 mae: 0.722481 (1758.514804162439 steps/sec)\n",
      "Step #3452\tEpoch   1 Batch  326/3125   Loss: 0.730252 mae: 0.644814 (1768.9871869490767 steps/sec)\n",
      "Step #3453\tEpoch   1 Batch  327/3125   Loss: 0.969270 mae: 0.801026 (1710.913318376504 steps/sec)\n",
      "Step #3454\tEpoch   1 Batch  328/3125   Loss: 0.814602 mae: 0.711113 (1712.9116570831154 steps/sec)\n",
      "Step #3455\tEpoch   1 Batch  329/3125   Loss: 0.945483 mae: 0.777841 (1408.619022031166 steps/sec)\n",
      "Step #3456\tEpoch   1 Batch  330/3125   Loss: 0.892986 mae: 0.750955 (1796.8606484337515 steps/sec)\n",
      "Step #3457\tEpoch   1 Batch  331/3125   Loss: 0.820690 mae: 0.714506 (1570.6062535105787 steps/sec)\n",
      "Step #3458\tEpoch   1 Batch  332/3125   Loss: 0.844097 mae: 0.723109 (1466.2322589666503 steps/sec)\n",
      "Step #3459\tEpoch   1 Batch  333/3125   Loss: 0.904021 mae: 0.769286 (1662.5590613603931 steps/sec)\n",
      "Step #3460\tEpoch   1 Batch  334/3125   Loss: 0.856711 mae: 0.727937 (1762.4310878042222 steps/sec)\n",
      "Step #3461\tEpoch   1 Batch  335/3125   Loss: 0.755985 mae: 0.687978 (1664.115789306629 steps/sec)\n",
      "Step #3462\tEpoch   1 Batch  336/3125   Loss: 0.701898 mae: 0.673048 (1665.5166936687951 steps/sec)\n",
      "Step #3463\tEpoch   1 Batch  337/3125   Loss: 0.731328 mae: 0.662466 (1748.5738108141911 steps/sec)\n",
      "Step #3464\tEpoch   1 Batch  338/3125   Loss: 0.920342 mae: 0.771655 (1777.9556263935635 steps/sec)\n",
      "Step #3465\tEpoch   1 Batch  339/3125   Loss: 0.816764 mae: 0.690269 (1785.463616472411 steps/sec)\n",
      "Step #3466\tEpoch   1 Batch  340/3125   Loss: 0.685282 mae: 0.663141 (1801.7388913708376 steps/sec)\n",
      "Step #3467\tEpoch   1 Batch  341/3125   Loss: 0.844085 mae: 0.684810 (1786.2239900517004 steps/sec)\n",
      "Step #3468\tEpoch   1 Batch  342/3125   Loss: 0.909864 mae: 0.750610 (1902.800007258606 steps/sec)\n",
      "Step #3469\tEpoch   1 Batch  343/3125   Loss: 0.909998 mae: 0.748058 (1832.5821194194186 steps/sec)\n",
      "Step #3470\tEpoch   1 Batch  344/3125   Loss: 0.953097 mae: 0.756376 (1577.3752933389494 steps/sec)\n",
      "Step #3471\tEpoch   1 Batch  345/3125   Loss: 0.815452 mae: 0.733269 (1764.982326207709 steps/sec)\n",
      "Step #3472\tEpoch   1 Batch  346/3125   Loss: 0.915008 mae: 0.738571 (1674.092168179387 steps/sec)\n",
      "Step #3473\tEpoch   1 Batch  347/3125   Loss: 0.761517 mae: 0.693051 (1635.6781293628571 steps/sec)\n",
      "Step #3474\tEpoch   1 Batch  348/3125   Loss: 0.947330 mae: 0.770593 (1681.3533231780646 steps/sec)\n",
      "Step #3475\tEpoch   1 Batch  349/3125   Loss: 0.909928 mae: 0.768312 (1865.8765959339828 steps/sec)\n",
      "Step #3476\tEpoch   1 Batch  350/3125   Loss: 0.780594 mae: 0.677004 (1821.962746733389 steps/sec)\n",
      "Step #3477\tEpoch   1 Batch  351/3125   Loss: 1.030897 mae: 0.806706 (1727.2309478903285 steps/sec)\n",
      "Step #3478\tEpoch   1 Batch  352/3125   Loss: 0.754553 mae: 0.702254 (1979.2481855847184 steps/sec)\n",
      "Step #3479\tEpoch   1 Batch  353/3125   Loss: 0.885885 mae: 0.762695 (1798.1085646183262 steps/sec)\n",
      "Step #3480\tEpoch   1 Batch  354/3125   Loss: 0.809224 mae: 0.723865 (1787.8838513870655 steps/sec)\n",
      "Step #3481\tEpoch   1 Batch  355/3125   Loss: 0.799836 mae: 0.726722 (1780.5520415007513 steps/sec)\n",
      "Step #3482\tEpoch   1 Batch  356/3125   Loss: 0.849642 mae: 0.721760 (1739.3069816046577 steps/sec)\n",
      "Step #3483\tEpoch   1 Batch  357/3125   Loss: 0.880698 mae: 0.731657 (1841.3999596097956 steps/sec)\n",
      "Step #3484\tEpoch   1 Batch  358/3125   Loss: 0.906151 mae: 0.755585 (1819.4336479733481 steps/sec)\n",
      "Step #3485\tEpoch   1 Batch  359/3125   Loss: 0.773233 mae: 0.691440 (1973.474361749179 steps/sec)\n",
      "Step #3486\tEpoch   1 Batch  360/3125   Loss: 0.785892 mae: 0.691092 (1677.842404653135 steps/sec)\n",
      "Step #3487\tEpoch   1 Batch  361/3125   Loss: 0.839367 mae: 0.736812 (1700.853203568532 steps/sec)\n",
      "Step #3488\tEpoch   1 Batch  362/3125   Loss: 0.738081 mae: 0.680041 (1774.9911129919594 steps/sec)\n",
      "Step #3489\tEpoch   1 Batch  363/3125   Loss: 0.871982 mae: 0.738232 (1759.3114267258375 steps/sec)\n",
      "Step #3490\tEpoch   1 Batch  364/3125   Loss: 0.717756 mae: 0.686252 (1946.041850322461 steps/sec)\n",
      "Step #3491\tEpoch   1 Batch  365/3125   Loss: 0.814546 mae: 0.730769 (2146.01680259509 steps/sec)\n",
      "Step #3492\tEpoch   1 Batch  366/3125   Loss: 0.913527 mae: 0.755610 (1694.0522638232562 steps/sec)\n",
      "Step #3493\tEpoch   1 Batch  367/3125   Loss: 0.869886 mae: 0.732541 (1973.0101983216045 steps/sec)\n",
      "Step #3494\tEpoch   1 Batch  368/3125   Loss: 0.868816 mae: 0.729570 (1606.7790896344593 steps/sec)\n",
      "Step #3495\tEpoch   1 Batch  369/3125   Loss: 0.920370 mae: 0.757498 (1726.5768176317067 steps/sec)\n",
      "Step #3496\tEpoch   1 Batch  370/3125   Loss: 0.961164 mae: 0.799877 (1681.0568167243812 steps/sec)\n",
      "Step #3497\tEpoch   1 Batch  371/3125   Loss: 0.791137 mae: 0.719266 (1882.2380584824714 steps/sec)\n",
      "Step #3498\tEpoch   1 Batch  372/3125   Loss: 0.788481 mae: 0.720746 (2143.6258075067462 steps/sec)\n",
      "Step #3499\tEpoch   1 Batch  373/3125   Loss: 0.869076 mae: 0.734290 (1878.7812547593237 steps/sec)\n",
      "Step #3500\tEpoch   1 Batch  374/3125   Loss: 0.816976 mae: 0.712977 (1845.8407780662765 steps/sec)\n",
      "Step #3501\tEpoch   1 Batch  375/3125   Loss: 0.835276 mae: 0.748348 (1720.401315843444 steps/sec)\n",
      "Step #3502\tEpoch   1 Batch  376/3125   Loss: 0.794894 mae: 0.702384 (1537.986315334013 steps/sec)\n",
      "Step #3503\tEpoch   1 Batch  377/3125   Loss: 0.802552 mae: 0.722239 (1954.7303469231772 steps/sec)\n",
      "Step #3504\tEpoch   1 Batch  378/3125   Loss: 0.898310 mae: 0.760368 (1702.0679803915202 steps/sec)\n",
      "Step #3505\tEpoch   1 Batch  379/3125   Loss: 0.798524 mae: 0.708416 (1755.2032942200499 steps/sec)\n",
      "Step #3506\tEpoch   1 Batch  380/3125   Loss: 0.804963 mae: 0.719024 (2027.5855401185331 steps/sec)\n",
      "Step #3507\tEpoch   1 Batch  381/3125   Loss: 0.792538 mae: 0.719235 (1757.8220344665729 steps/sec)\n",
      "Step #3508\tEpoch   1 Batch  382/3125   Loss: 0.765174 mae: 0.696826 (1892.8047943969095 steps/sec)\n",
      "Step #3509\tEpoch   1 Batch  383/3125   Loss: 0.866038 mae: 0.716833 (1731.3090786008536 steps/sec)\n",
      "Step #3510\tEpoch   1 Batch  384/3125   Loss: 0.807488 mae: 0.722419 (1826.882938132655 steps/sec)\n",
      "Step #3511\tEpoch   1 Batch  385/3125   Loss: 0.811050 mae: 0.727372 (1682.2029887620622 steps/sec)\n",
      "Step #3512\tEpoch   1 Batch  386/3125   Loss: 0.753441 mae: 0.703109 (1763.4833208600667 steps/sec)\n",
      "Step #3513\tEpoch   1 Batch  387/3125   Loss: 0.828877 mae: 0.740337 (1883.4551757584466 steps/sec)\n",
      "Step #3514\tEpoch   1 Batch  388/3125   Loss: 0.840190 mae: 0.730256 (1823.7531632910402 steps/sec)\n",
      "Step #3515\tEpoch   1 Batch  389/3125   Loss: 0.665519 mae: 0.657749 (2008.0353894176449 steps/sec)\n",
      "Step #3516\tEpoch   1 Batch  390/3125   Loss: 0.930511 mae: 0.761696 (1745.6337348194145 steps/sec)\n",
      "Step #3517\tEpoch   1 Batch  391/3125   Loss: 0.879277 mae: 0.717196 (1837.7048318407262 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3518\tEpoch   1 Batch  392/3125   Loss: 0.707522 mae: 0.665920 (1801.3984091807108 steps/sec)\n",
      "Step #3519\tEpoch   1 Batch  393/3125   Loss: 0.903008 mae: 0.739757 (1836.256654525077 steps/sec)\n",
      "Step #3520\tEpoch   1 Batch  394/3125   Loss: 0.846011 mae: 0.733993 (1528.358208955224 steps/sec)\n",
      "Step #3521\tEpoch   1 Batch  395/3125   Loss: 0.838467 mae: 0.718995 (1720.5706925266845 steps/sec)\n",
      "Step #3522\tEpoch   1 Batch  396/3125   Loss: 0.814275 mae: 0.699959 (1674.2659151511282 steps/sec)\n",
      "Step #3523\tEpoch   1 Batch  397/3125   Loss: 0.795615 mae: 0.704219 (1617.9730897420072 steps/sec)\n",
      "Step #3524\tEpoch   1 Batch  398/3125   Loss: 0.891001 mae: 0.765619 (1714.3258863248072 steps/sec)\n",
      "Step #3525\tEpoch   1 Batch  399/3125   Loss: 0.922920 mae: 0.763850 (1418.9600460096756 steps/sec)\n",
      "Step #3526\tEpoch   1 Batch  400/3125   Loss: 0.840021 mae: 0.719579 (1544.9771622218948 steps/sec)\n",
      "Step #3527\tEpoch   1 Batch  401/3125   Loss: 0.933491 mae: 0.782431 (1683.2021060573227 steps/sec)\n",
      "Step #3528\tEpoch   1 Batch  402/3125   Loss: 0.882683 mae: 0.734328 (1642.1332873955635 steps/sec)\n",
      "Step #3529\tEpoch   1 Batch  403/3125   Loss: 0.964194 mae: 0.790746 (1560.8919586769478 steps/sec)\n",
      "Step #3530\tEpoch   1 Batch  404/3125   Loss: 0.792562 mae: 0.705126 (1677.1178375784718 steps/sec)\n",
      "Step #3531\tEpoch   1 Batch  405/3125   Loss: 0.811849 mae: 0.698771 (1625.5354111601157 steps/sec)\n",
      "Step #3532\tEpoch   1 Batch  406/3125   Loss: 0.987395 mae: 0.783808 (1789.4857201368682 steps/sec)\n",
      "Step #3533\tEpoch   1 Batch  407/3125   Loss: 0.779628 mae: 0.695920 (1560.2996867722663 steps/sec)\n",
      "Step #3534\tEpoch   1 Batch  408/3125   Loss: 0.829878 mae: 0.709074 (1615.9783010726173 steps/sec)\n",
      "Step #3535\tEpoch   1 Batch  409/3125   Loss: 0.780253 mae: 0.688188 (1738.5860193659637 steps/sec)\n",
      "Step #3536\tEpoch   1 Batch  410/3125   Loss: 0.821913 mae: 0.731290 (1764.2844523711374 steps/sec)\n",
      "Step #3537\tEpoch   1 Batch  411/3125   Loss: 0.907310 mae: 0.781554 (1727.1740473229506 steps/sec)\n",
      "Step #3538\tEpoch   1 Batch  412/3125   Loss: 0.687710 mae: 0.660163 (1613.069763864318 steps/sec)\n",
      "Step #3539\tEpoch   1 Batch  413/3125   Loss: 0.778235 mae: 0.696929 (1581.3717801773541 steps/sec)\n",
      "Step #3540\tEpoch   1 Batch  414/3125   Loss: 0.778886 mae: 0.686524 (1779.479346977565 steps/sec)\n",
      "Step #3541\tEpoch   1 Batch  415/3125   Loss: 0.954637 mae: 0.764497 (1642.6220519930132 steps/sec)\n",
      "Step #3542\tEpoch   1 Batch  416/3125   Loss: 0.879175 mae: 0.719728 (1586.8400941290415 steps/sec)\n",
      "Step #3543\tEpoch   1 Batch  417/3125   Loss: 0.953278 mae: 0.761140 (1809.1061230827625 steps/sec)\n",
      "Step #3544\tEpoch   1 Batch  418/3125   Loss: 0.831127 mae: 0.720300 (1811.2623505838458 steps/sec)\n",
      "Step #3545\tEpoch   1 Batch  419/3125   Loss: 0.878290 mae: 0.747979 (1813.1728657640365 steps/sec)\n",
      "Step #3546\tEpoch   1 Batch  420/3125   Loss: 0.746295 mae: 0.692614 (1648.4711282994544 steps/sec)\n",
      "Step #3547\tEpoch   1 Batch  421/3125   Loss: 0.837449 mae: 0.731086 (1497.890819744727 steps/sec)\n",
      "Step #3548\tEpoch   1 Batch  422/3125   Loss: 0.923958 mae: 0.780612 (1978.2959776620633 steps/sec)\n",
      "Step #3549\tEpoch   1 Batch  423/3125   Loss: 0.878657 mae: 0.741570 (1937.842008482642 steps/sec)\n",
      "Step #3550\tEpoch   1 Batch  424/3125   Loss: 0.889398 mae: 0.771667 (1674.1857197598672 steps/sec)\n",
      "Step #3551\tEpoch   1 Batch  425/3125   Loss: 0.869493 mae: 0.744079 (1747.6994874786449 steps/sec)\n",
      "Step #3552\tEpoch   1 Batch  426/3125   Loss: 0.757417 mae: 0.688209 (1569.0081624408017 steps/sec)\n",
      "Step #3553\tEpoch   1 Batch  427/3125   Loss: 0.731183 mae: 0.700269 (1286.2096670326098 steps/sec)\n",
      "Step #3554\tEpoch   1 Batch  428/3125   Loss: 0.760048 mae: 0.692020 (1411.5581880595005 steps/sec)\n",
      "Step #3555\tEpoch   1 Batch  429/3125   Loss: 0.764679 mae: 0.706151 (1670.7312604064593 steps/sec)\n",
      "Step #3556\tEpoch   1 Batch  430/3125   Loss: 0.952223 mae: 0.781492 (1668.4981422695341 steps/sec)\n",
      "Step #3557\tEpoch   1 Batch  431/3125   Loss: 0.789879 mae: 0.720875 (1694.860024568436 steps/sec)\n",
      "Step #3558\tEpoch   1 Batch  432/3125   Loss: 0.883985 mae: 0.731084 (1599.5362672565022 steps/sec)\n",
      "Step #3559\tEpoch   1 Batch  433/3125   Loss: 0.862868 mae: 0.737153 (1951.52891254583 steps/sec)\n",
      "Step #3560\tEpoch   1 Batch  434/3125   Loss: 0.853419 mae: 0.749379 (1692.2751664313093 steps/sec)\n",
      "Step #3561\tEpoch   1 Batch  435/3125   Loss: 0.865846 mae: 0.724401 (1580.2754920577509 steps/sec)\n",
      "Step #3562\tEpoch   1 Batch  436/3125   Loss: 0.789233 mae: 0.713289 (1613.9884250708042 steps/sec)\n",
      "Step #3563\tEpoch   1 Batch  437/3125   Loss: 0.828593 mae: 0.731619 (1593.2657681613055 steps/sec)\n",
      "Step #3564\tEpoch   1 Batch  438/3125   Loss: 0.916169 mae: 0.774870 (1672.8502600427555 steps/sec)\n",
      "Step #3565\tEpoch   1 Batch  439/3125   Loss: 0.953214 mae: 0.740607 (1810.9338975001078 steps/sec)\n",
      "Step #3566\tEpoch   1 Batch  440/3125   Loss: 0.779097 mae: 0.702981 (1596.6622508489029 steps/sec)\n",
      "Step #3567\tEpoch   1 Batch  441/3125   Loss: 0.803545 mae: 0.709465 (1387.923229649239 steps/sec)\n",
      "Step #3568\tEpoch   1 Batch  442/3125   Loss: 0.943876 mae: 0.764701 (1370.9474344810453 steps/sec)\n",
      "Step #3569\tEpoch   1 Batch  443/3125   Loss: 0.754218 mae: 0.700327 (1536.9042820604898 steps/sec)\n",
      "Step #3570\tEpoch   1 Batch  444/3125   Loss: 0.850734 mae: 0.718212 (1739.8120110503655 steps/sec)\n",
      "Step #3571\tEpoch   1 Batch  445/3125   Loss: 0.922694 mae: 0.765620 (1793.6487029703817 steps/sec)\n",
      "Step #3572\tEpoch   1 Batch  446/3125   Loss: 0.871726 mae: 0.732414 (1918.9400386139248 steps/sec)\n",
      "Step #3573\tEpoch   1 Batch  447/3125   Loss: 0.801344 mae: 0.690520 (1943.6250567660497 steps/sec)\n",
      "Step #3574\tEpoch   1 Batch  448/3125   Loss: 0.819843 mae: 0.711059 (1084.0179674456351 steps/sec)\n",
      "Step #3575\tEpoch   1 Batch  449/3125   Loss: 0.822915 mae: 0.725463 (1751.816427622731 steps/sec)\n",
      "Step #3576\tEpoch   1 Batch  450/3125   Loss: 0.789074 mae: 0.695184 (1719.5126350830587 steps/sec)\n",
      "Step #3577\tEpoch   1 Batch  451/3125   Loss: 0.762042 mae: 0.693599 (1976.1522007481885 steps/sec)\n",
      "Step #3578\tEpoch   1 Batch  452/3125   Loss: 0.827780 mae: 0.733443 (1877.3012505482898 steps/sec)\n",
      "Step #3579\tEpoch   1 Batch  453/3125   Loss: 0.753385 mae: 0.695876 (2002.8000878608743 steps/sec)\n",
      "Step #3580\tEpoch   1 Batch  454/3125   Loss: 0.857615 mae: 0.737536 (1626.7080359913125 steps/sec)\n",
      "Step #3581\tEpoch   1 Batch  455/3125   Loss: 0.761307 mae: 0.686982 (1742.8193898496647 steps/sec)\n",
      "Step #3582\tEpoch   1 Batch  456/3125   Loss: 0.914494 mae: 0.769047 (1220.9354587054481 steps/sec)\n",
      "Step #3583\tEpoch   1 Batch  457/3125   Loss: 0.916924 mae: 0.784256 (1780.0533043611115 steps/sec)\n",
      "Step #3584\tEpoch   1 Batch  458/3125   Loss: 0.907434 mae: 0.743379 (1465.709633005081 steps/sec)\n",
      "Step #3585\tEpoch   1 Batch  459/3125   Loss: 0.875102 mae: 0.739314 (1616.177558569667 steps/sec)\n",
      "Step #3586\tEpoch   1 Batch  460/3125   Loss: 0.815846 mae: 0.720852 (1538.6633601619992 steps/sec)\n",
      "Step #3587\tEpoch   1 Batch  461/3125   Loss: 0.796628 mae: 0.714250 (1772.8603794001285 steps/sec)\n",
      "Step #3588\tEpoch   1 Batch  462/3125   Loss: 0.776392 mae: 0.724090 (1919.5372209458778 steps/sec)\n",
      "Step #3589\tEpoch   1 Batch  463/3125   Loss: 0.752147 mae: 0.669976 (1503.7228245281938 steps/sec)\n",
      "Step #3590\tEpoch   1 Batch  464/3125   Loss: 0.737762 mae: 0.675399 (1730.7089863253366 steps/sec)\n",
      "Step #3591\tEpoch   1 Batch  465/3125   Loss: 0.830087 mae: 0.738958 (1969.3417222274393 steps/sec)\n",
      "Step #3592\tEpoch   1 Batch  466/3125   Loss: 0.864040 mae: 0.739891 (2060.49578007251 steps/sec)\n",
      "Step #3593\tEpoch   1 Batch  467/3125   Loss: 0.871562 mae: 0.741955 (1839.3165991334702 steps/sec)\n",
      "Step #3594\tEpoch   1 Batch  468/3125   Loss: 0.697501 mae: 0.689618 (1990.6143215126433 steps/sec)\n",
      "Step #3595\tEpoch   1 Batch  469/3125   Loss: 0.773225 mae: 0.687858 (1983.7414985290918 steps/sec)\n",
      "Step #3596\tEpoch   1 Batch  470/3125   Loss: 0.768529 mae: 0.694518 (2127.2957812198856 steps/sec)\n",
      "Step #3597\tEpoch   1 Batch  471/3125   Loss: 0.873854 mae: 0.744962 (1674.2525487190542 steps/sec)\n",
      "Step #3598\tEpoch   1 Batch  472/3125   Loss: 0.880830 mae: 0.753650 (1945.4461121727675 steps/sec)\n",
      "Step #3599\tEpoch   1 Batch  473/3125   Loss: 0.839501 mae: 0.745341 (2084.6441351888666 steps/sec)\n",
      "Step #3600\tEpoch   1 Batch  474/3125   Loss: 0.790401 mae: 0.693249 (2144.7219324620073 steps/sec)\n",
      "Step #3601\tEpoch   1 Batch  475/3125   Loss: 0.787313 mae: 0.695994 (1991.483866066511 steps/sec)\n",
      "Step #3602\tEpoch   1 Batch  476/3125   Loss: 0.733703 mae: 0.679179 (1952.891877042845 steps/sec)\n",
      "Step #3603\tEpoch   1 Batch  477/3125   Loss: 0.919114 mae: 0.765958 (2044.645503470868 steps/sec)\n",
      "Step #3604\tEpoch   1 Batch  478/3125   Loss: 0.884264 mae: 0.735155 (2048.340056454685 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3605\tEpoch   1 Batch  479/3125   Loss: 0.905422 mae: 0.732287 (1942.580842372426 steps/sec)\n",
      "Step #3606\tEpoch   1 Batch  480/3125   Loss: 0.941948 mae: 0.762882 (1859.0289781843646 steps/sec)\n",
      "Step #3607\tEpoch   1 Batch  481/3125   Loss: 0.817729 mae: 0.690089 (2047.1803281889088 steps/sec)\n",
      "Step #3608\tEpoch   1 Batch  482/3125   Loss: 0.909002 mae: 0.727024 (2071.158955113328 steps/sec)\n",
      "Step #3609\tEpoch   1 Batch  483/3125   Loss: 0.840860 mae: 0.730199 (2009.9599378941516 steps/sec)\n",
      "Step #3610\tEpoch   1 Batch  484/3125   Loss: 0.723717 mae: 0.684481 (1905.79147772194 steps/sec)\n",
      "Step #3611\tEpoch   1 Batch  485/3125   Loss: 0.877108 mae: 0.763032 (2100.2183187453684 steps/sec)\n",
      "Step #3612\tEpoch   1 Batch  486/3125   Loss: 0.863248 mae: 0.750731 (2182.169316573712 steps/sec)\n",
      "Step #3613\tEpoch   1 Batch  487/3125   Loss: 0.791896 mae: 0.710834 (2218.0818208740534 steps/sec)\n",
      "Step #3614\tEpoch   1 Batch  488/3125   Loss: 0.753586 mae: 0.682307 (1919.4669449097082 steps/sec)\n",
      "Step #3615\tEpoch   1 Batch  489/3125   Loss: 0.773713 mae: 0.710808 (2059.7065351902415 steps/sec)\n",
      "Step #3616\tEpoch   1 Batch  490/3125   Loss: 0.830879 mae: 0.720883 (2256.749311294766 steps/sec)\n",
      "Step #3617\tEpoch   1 Batch  491/3125   Loss: 0.785215 mae: 0.715915 (2060.414804043897 steps/sec)\n",
      "Step #3618\tEpoch   1 Batch  492/3125   Loss: 0.749731 mae: 0.687710 (2139.056109178813 steps/sec)\n",
      "Step #3619\tEpoch   1 Batch  493/3125   Loss: 0.726490 mae: 0.684321 (2073.308947108255 steps/sec)\n",
      "Step #3620\tEpoch   1 Batch  494/3125   Loss: 0.783753 mae: 0.723290 (2034.4107717977572 steps/sec)\n",
      "Step #3621\tEpoch   1 Batch  495/3125   Loss: 0.785722 mae: 0.708798 (1844.1364755539923 steps/sec)\n",
      "Step #3622\tEpoch   1 Batch  496/3125   Loss: 0.725374 mae: 0.691325 (1747.5684143861872 steps/sec)\n",
      "Step #3623\tEpoch   1 Batch  497/3125   Loss: 0.972637 mae: 0.780918 (2128.8938066572596 steps/sec)\n",
      "Step #3624\tEpoch   1 Batch  498/3125   Loss: 0.762019 mae: 0.691185 (2150.4619517847436 steps/sec)\n",
      "Step #3625\tEpoch   1 Batch  499/3125   Loss: 0.811065 mae: 0.703966 (1909.1920433337884 steps/sec)\n",
      "Step #3626\tEpoch   1 Batch  500/3125   Loss: 0.666499 mae: 0.666674 (2096.6697659538304 steps/sec)\n",
      "Step #3627\tEpoch   1 Batch  501/3125   Loss: 0.881019 mae: 0.737738 (2038.5834961554538 steps/sec)\n",
      "Step #3628\tEpoch   1 Batch  502/3125   Loss: 0.911167 mae: 0.760998 (1930.1194617777533 steps/sec)\n",
      "Step #3629\tEpoch   1 Batch  503/3125   Loss: 0.771432 mae: 0.668737 (1796.6297429043836 steps/sec)\n",
      "Step #3630\tEpoch   1 Batch  504/3125   Loss: 0.963378 mae: 0.744369 (2044.745181010696 steps/sec)\n",
      "Step #3631\tEpoch   1 Batch  505/3125   Loss: 0.735999 mae: 0.684399 (1875.672581568403 steps/sec)\n",
      "Step #3632\tEpoch   1 Batch  506/3125   Loss: 0.841248 mae: 0.692725 (1888.4924672892146 steps/sec)\n",
      "Step #3633\tEpoch   1 Batch  507/3125   Loss: 0.780462 mae: 0.710083 (1865.1796116936596 steps/sec)\n",
      "Step #3634\tEpoch   1 Batch  508/3125   Loss: 0.866998 mae: 0.732761 (2118.827606413611 steps/sec)\n",
      "Step #3635\tEpoch   1 Batch  509/3125   Loss: 0.734001 mae: 0.669809 (2101.9022991961833 steps/sec)\n",
      "Step #3636\tEpoch   1 Batch  510/3125   Loss: 0.899282 mae: 0.743551 (1981.6233582160069 steps/sec)\n",
      "Step #3637\tEpoch   1 Batch  511/3125   Loss: 0.973291 mae: 0.789710 (1886.8443308801036 steps/sec)\n",
      "Step #3638\tEpoch   1 Batch  512/3125   Loss: 0.765200 mae: 0.690650 (1966.1844535490948 steps/sec)\n",
      "Step #3639\tEpoch   1 Batch  513/3125   Loss: 0.781058 mae: 0.705774 (2033.6808215591393 steps/sec)\n",
      "Step #3640\tEpoch   1 Batch  514/3125   Loss: 0.756154 mae: 0.693988 (2000.4693178674654 steps/sec)\n",
      "Step #3641\tEpoch   1 Batch  515/3125   Loss: 0.902022 mae: 0.761904 (1990.9355864622396 steps/sec)\n",
      "Step #3642\tEpoch   1 Batch  516/3125   Loss: 0.939336 mae: 0.797086 (2045.5629035719162 steps/sec)\n",
      "Step #3643\tEpoch   1 Batch  517/3125   Loss: 0.798311 mae: 0.714072 (2055.7693627282797 steps/sec)\n",
      "Step #3644\tEpoch   1 Batch  518/3125   Loss: 0.810343 mae: 0.706543 (2122.258315876823 steps/sec)\n",
      "Step #3645\tEpoch   1 Batch  519/3125   Loss: 0.822509 mae: 0.743819 (1930.723623642055 steps/sec)\n",
      "Step #3646\tEpoch   1 Batch  520/3125   Loss: 0.895107 mae: 0.739792 (2039.3764647535324 steps/sec)\n",
      "Step #3647\tEpoch   1 Batch  521/3125   Loss: 0.870595 mae: 0.728536 (2067.56514280644 steps/sec)\n",
      "Step #3648\tEpoch   1 Batch  522/3125   Loss: 0.830980 mae: 0.729872 (2115.9415610622327 steps/sec)\n",
      "Step #3649\tEpoch   1 Batch  523/3125   Loss: 0.675086 mae: 0.629114 (2130.039814739579 steps/sec)\n",
      "Step #3650\tEpoch   1 Batch  524/3125   Loss: 0.836231 mae: 0.705516 (2008.362302601967 steps/sec)\n",
      "Step #3651\tEpoch   1 Batch  525/3125   Loss: 0.903366 mae: 0.733971 (2252.7736003093714 steps/sec)\n",
      "Step #3652\tEpoch   1 Batch  526/3125   Loss: 0.751771 mae: 0.703557 (2200.994941332046 steps/sec)\n",
      "Step #3653\tEpoch   1 Batch  527/3125   Loss: 0.819188 mae: 0.697844 (1910.4964926664845 steps/sec)\n",
      "Step #3654\tEpoch   1 Batch  528/3125   Loss: 0.840714 mae: 0.715756 (1753.0904652834668 steps/sec)\n",
      "Step #3655\tEpoch   1 Batch  529/3125   Loss: 0.736961 mae: 0.701120 (1885.385500575374 steps/sec)\n",
      "Step #3656\tEpoch   1 Batch  530/3125   Loss: 0.740552 mae: 0.688873 (2119.74851921483 steps/sec)\n",
      "Step #3657\tEpoch   1 Batch  531/3125   Loss: 0.924269 mae: 0.772875 (2121.1635716308615 steps/sec)\n",
      "Step #3658\tEpoch   1 Batch  532/3125   Loss: 0.806384 mae: 0.710447 (2197.6045017761894 steps/sec)\n",
      "Step #3659\tEpoch   1 Batch  533/3125   Loss: 0.809066 mae: 0.710180 (2108.8985650070895 steps/sec)\n",
      "Step #3660\tEpoch   1 Batch  534/3125   Loss: 0.771428 mae: 0.701806 (2259.1804197009524 steps/sec)\n",
      "Step #3661\tEpoch   1 Batch  535/3125   Loss: 0.916539 mae: 0.749638 (2005.615699475919 steps/sec)\n",
      "Step #3662\tEpoch   1 Batch  536/3125   Loss: 0.934718 mae: 0.754263 (2235.7220528346943 steps/sec)\n",
      "Step #3663\tEpoch   1 Batch  537/3125   Loss: 0.790486 mae: 0.705525 (1976.357056694813 steps/sec)\n",
      "Step #3664\tEpoch   1 Batch  538/3125   Loss: 0.803694 mae: 0.724928 (2018.258283690537 steps/sec)\n",
      "Step #3665\tEpoch   1 Batch  539/3125   Loss: 0.930534 mae: 0.741454 (2017.9087243930835 steps/sec)\n",
      "Step #3666\tEpoch   1 Batch  540/3125   Loss: 0.757407 mae: 0.704289 (2108.5381057711643 steps/sec)\n",
      "Step #3667\tEpoch   1 Batch  541/3125   Loss: 0.641266 mae: 0.634770 (2161.232544958005 steps/sec)\n",
      "Step #3668\tEpoch   1 Batch  542/3125   Loss: 0.732664 mae: 0.671338 (2195.8787066510304 steps/sec)\n",
      "Step #3669\tEpoch   1 Batch  543/3125   Loss: 0.924707 mae: 0.774599 (2374.2776922380217 steps/sec)\n",
      "Step #3670\tEpoch   1 Batch  544/3125   Loss: 0.867261 mae: 0.757890 (2337.5711976815473 steps/sec)\n",
      "Step #3671\tEpoch   1 Batch  545/3125   Loss: 0.722859 mae: 0.673917 (2127.74903106674 steps/sec)\n",
      "Step #3672\tEpoch   1 Batch  546/3125   Loss: 0.990437 mae: 0.801177 (1883.0831118454134 steps/sec)\n",
      "Step #3673\tEpoch   1 Batch  547/3125   Loss: 0.931771 mae: 0.775322 (1918.2730391035902 steps/sec)\n",
      "Step #3674\tEpoch   1 Batch  548/3125   Loss: 0.776808 mae: 0.702384 (1739.3069816046577 steps/sec)\n",
      "Step #3675\tEpoch   1 Batch  549/3125   Loss: 0.799989 mae: 0.690977 (1886.2164179775684 steps/sec)\n",
      "Step #3676\tEpoch   1 Batch  550/3125   Loss: 0.870821 mae: 0.737951 (2258.061459611947 steps/sec)\n",
      "Step #3677\tEpoch   1 Batch  551/3125   Loss: 0.826369 mae: 0.731960 (2240.0205080002565 steps/sec)\n",
      "Step #3678\tEpoch   1 Batch  552/3125   Loss: 0.871481 mae: 0.748424 (2134.6579400059036 steps/sec)\n",
      "Step #3679\tEpoch   1 Batch  553/3125   Loss: 0.828652 mae: 0.703615 (2088.6511896580914 steps/sec)\n",
      "Step #3680\tEpoch   1 Batch  554/3125   Loss: 0.830416 mae: 0.725372 (2113.1059499219105 steps/sec)\n",
      "Step #3681\tEpoch   1 Batch  555/3125   Loss: 0.951565 mae: 0.777006 (1801.0889915663272 steps/sec)\n",
      "Step #3682\tEpoch   1 Batch  556/3125   Loss: 0.770679 mae: 0.682333 (2134.3972316930435 steps/sec)\n",
      "Step #3683\tEpoch   1 Batch  557/3125   Loss: 0.956263 mae: 0.773097 (2151.8300003078216 steps/sec)\n",
      "Step #3684\tEpoch   1 Batch  558/3125   Loss: 0.757950 mae: 0.705395 (2306.057773721424 steps/sec)\n",
      "Step #3685\tEpoch   1 Batch  559/3125   Loss: 0.964317 mae: 0.786353 (1885.4533031251124 steps/sec)\n",
      "Step #3686\tEpoch   1 Batch  560/3125   Loss: 0.989674 mae: 0.788442 (2059.039184691363 steps/sec)\n",
      "Step #3687\tEpoch   1 Batch  561/3125   Loss: 0.897578 mae: 0.765733 (2101.4389354282735 steps/sec)\n",
      "Step #3688\tEpoch   1 Batch  562/3125   Loss: 0.668838 mae: 0.672880 (2252.072035308899 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3689\tEpoch   1 Batch  563/3125   Loss: 0.805471 mae: 0.740936 (1777.3980845834392 steps/sec)\n",
      "Step #3690\tEpoch   1 Batch  564/3125   Loss: 0.938224 mae: 0.748518 (2061.0222794413926 steps/sec)\n",
      "Step #3691\tEpoch   1 Batch  565/3125   Loss: 0.919383 mae: 0.748894 (2039.4756292060529 steps/sec)\n",
      "Step #3692\tEpoch   1 Batch  566/3125   Loss: 0.728538 mae: 0.667719 (1962.8719311874654 steps/sec)\n",
      "Step #3693\tEpoch   1 Batch  567/3125   Loss: 0.818084 mae: 0.730400 (2140.9341023939564 steps/sec)\n",
      "Step #3694\tEpoch   1 Batch  568/3125   Loss: 1.115884 mae: 0.858790 (2338.535649769174 steps/sec)\n",
      "Step #3695\tEpoch   1 Batch  569/3125   Loss: 0.793754 mae: 0.704583 (2124.429677053365 steps/sec)\n",
      "Step #3696\tEpoch   1 Batch  570/3125   Loss: 0.912964 mae: 0.751783 (2185.6944835277072 steps/sec)\n",
      "Step #3697\tEpoch   1 Batch  571/3125   Loss: 0.884992 mae: 0.731166 (2152.006649495644 steps/sec)\n",
      "Step #3698\tEpoch   1 Batch  572/3125   Loss: 0.901945 mae: 0.760933 (1858.024275715425 steps/sec)\n",
      "Step #3699\tEpoch   1 Batch  573/3125   Loss: 0.809176 mae: 0.706996 (1998.5628925123651 steps/sec)\n",
      "Step #3700\tEpoch   1 Batch  574/3125   Loss: 0.909222 mae: 0.733549 (2126.9290060851927 steps/sec)\n",
      "Step #3701\tEpoch   1 Batch  575/3125   Loss: 0.929846 mae: 0.788040 (2041.8187128809268 steps/sec)\n",
      "Step #3702\tEpoch   1 Batch  576/3125   Loss: 0.794146 mae: 0.722966 (2030.3730310100784 steps/sec)\n",
      "Step #3703\tEpoch   1 Batch  577/3125   Loss: 0.826402 mae: 0.751888 (1750.3981303730907 steps/sec)\n",
      "Step #3704\tEpoch   1 Batch  578/3125   Loss: 0.864635 mae: 0.743465 (1832.646177239083 steps/sec)\n",
      "Step #3705\tEpoch   1 Batch  579/3125   Loss: 0.862833 mae: 0.722539 (1857.004214925796 steps/sec)\n",
      "Step #3706\tEpoch   1 Batch  580/3125   Loss: 0.930520 mae: 0.763086 (1925.282069642972 steps/sec)\n",
      "Step #3707\tEpoch   1 Batch  581/3125   Loss: 0.810174 mae: 0.718494 (1970.5259992858887 steps/sec)\n",
      "Step #3708\tEpoch   1 Batch  582/3125   Loss: 0.895016 mae: 0.733288 (1884.08125129145 steps/sec)\n",
      "Step #3709\tEpoch   1 Batch  583/3125   Loss: 0.866216 mae: 0.740814 (1981.3050913110433 steps/sec)\n",
      "Step #3710\tEpoch   1 Batch  584/3125   Loss: 0.725368 mae: 0.676725 (2038.8213219782035 steps/sec)\n",
      "Step #3711\tEpoch   1 Batch  585/3125   Loss: 0.740863 mae: 0.668884 (2057.0397253555666 steps/sec)\n",
      "Step #3712\tEpoch   1 Batch  586/3125   Loss: 0.919640 mae: 0.740945 (2127.74903106674 steps/sec)\n",
      "Step #3713\tEpoch   1 Batch  587/3125   Loss: 0.815204 mae: 0.710197 (2062.725117784182 steps/sec)\n",
      "Step #3714\tEpoch   1 Batch  588/3125   Loss: 0.870762 mae: 0.739409 (1846.7998168304625 steps/sec)\n",
      "Step #3715\tEpoch   1 Batch  589/3125   Loss: 0.775335 mae: 0.694289 (1946.041850322461 steps/sec)\n",
      "Step #3716\tEpoch   1 Batch  590/3125   Loss: 0.946171 mae: 0.729209 (1744.4720796559554 steps/sec)\n",
      "Step #3717\tEpoch   1 Batch  591/3125   Loss: 0.785836 mae: 0.685347 (2080.219017199992 steps/sec)\n",
      "Step #3718\tEpoch   1 Batch  592/3125   Loss: 0.776517 mae: 0.703544 (2074.4369157722936 steps/sec)\n",
      "Step #3719\tEpoch   1 Batch  593/3125   Loss: 0.824482 mae: 0.693809 (2324.6156404145654 steps/sec)\n",
      "Step #3720\tEpoch   1 Batch  594/3125   Loss: 0.841140 mae: 0.698159 (2093.8019169329073 steps/sec)\n",
      "Step #3721\tEpoch   1 Batch  595/3125   Loss: 0.962968 mae: 0.768965 (2191.7249307623974 steps/sec)\n",
      "Step #3722\tEpoch   1 Batch  596/3125   Loss: 0.801330 mae: 0.717506 (2179.130904632266 steps/sec)\n",
      "Step #3723\tEpoch   1 Batch  597/3125   Loss: 0.870206 mae: 0.760654 (1681.016392128572 steps/sec)\n",
      "Step #3724\tEpoch   1 Batch  598/3125   Loss: 0.907951 mae: 0.764489 (2012.9502893946228 steps/sec)\n",
      "Step #3725\tEpoch   1 Batch  599/3125   Loss: 0.751043 mae: 0.697042 (1923.092864806375 steps/sec)\n",
      "Step #3726\tEpoch   1 Batch  600/3125   Loss: 0.796599 mae: 0.711557 (1927.2991278615607 steps/sec)\n",
      "Step #3727\tEpoch   1 Batch  601/3125   Loss: 0.778343 mae: 0.684995 (1850.7439504386043 steps/sec)\n",
      "Step #3728\tEpoch   1 Batch  602/3125   Loss: 0.979805 mae: 0.785252 (1927.812913663774 steps/sec)\n",
      "Step #3729\tEpoch   1 Batch  603/3125   Loss: 0.817985 mae: 0.711703 (2011.5022348404918 steps/sec)\n",
      "Step #3730\tEpoch   1 Batch  604/3125   Loss: 0.894372 mae: 0.743063 (1869.319356793953 steps/sec)\n",
      "Step #3731\tEpoch   1 Batch  605/3125   Loss: 0.861593 mae: 0.738896 (1786.0262306251066 steps/sec)\n",
      "Step #3732\tEpoch   1 Batch  606/3125   Loss: 0.748176 mae: 0.693085 (2098.5370344427324 steps/sec)\n",
      "Step #3733\tEpoch   1 Batch  607/3125   Loss: 0.929137 mae: 0.771985 (2172.2015640374952 steps/sec)\n",
      "Step #3734\tEpoch   1 Batch  608/3125   Loss: 0.842820 mae: 0.741102 (1994.9127229488704 steps/sec)\n",
      "Step #3735\tEpoch   1 Batch  609/3125   Loss: 0.774143 mae: 0.688365 (2146.478065955661 steps/sec)\n",
      "Step #3736\tEpoch   1 Batch  610/3125   Loss: 0.810730 mae: 0.704974 (2180.7397548015433 steps/sec)\n",
      "Step #3737\tEpoch   1 Batch  611/3125   Loss: 0.834062 mae: 0.728063 (2131.2953515315353 steps/sec)\n",
      "Step #3738\tEpoch   1 Batch  612/3125   Loss: 0.900489 mae: 0.745550 (1869.2860326232285 steps/sec)\n",
      "Step #3739\tEpoch   1 Batch  613/3125   Loss: 0.887531 mae: 0.729338 (1957.1748544124234 steps/sec)\n",
      "Step #3740\tEpoch   1 Batch  614/3125   Loss: 0.905326 mae: 0.754328 (1816.990270232804 steps/sec)\n",
      "Step #3741\tEpoch   1 Batch  615/3125   Loss: 0.845600 mae: 0.718038 (1566.991698609461 steps/sec)\n",
      "Step #3742\tEpoch   1 Batch  616/3125   Loss: 0.829278 mae: 0.718182 (1604.418908890606 steps/sec)\n",
      "Step #3743\tEpoch   1 Batch  617/3125   Loss: 0.931961 mae: 0.769523 (1734.8466298269416 steps/sec)\n",
      "Step #3744\tEpoch   1 Batch  618/3125   Loss: 0.747786 mae: 0.724192 (1841.0120003862596 steps/sec)\n",
      "Step #3745\tEpoch   1 Batch  619/3125   Loss: 0.815701 mae: 0.710603 (2197.5123908920395 steps/sec)\n",
      "Step #3746\tEpoch   1 Batch  620/3125   Loss: 0.868797 mae: 0.733909 (2076.6759748876084 steps/sec)\n",
      "Step #3747\tEpoch   1 Batch  621/3125   Loss: 0.638186 mae: 0.631345 (1773.3550934812572 steps/sec)\n",
      "Step #3748\tEpoch   1 Batch  622/3125   Loss: 0.783036 mae: 0.712272 (1984.2294992004995 steps/sec)\n",
      "Step #3749\tEpoch   1 Batch  623/3125   Loss: 0.802201 mae: 0.714521 (2213.002690866881 steps/sec)\n",
      "Step #3750\tEpoch   1 Batch  624/3125   Loss: 0.667186 mae: 0.633806 (1951.1294704328086 steps/sec)\n",
      "Step #3751\tEpoch   1 Batch  625/3125   Loss: 0.816353 mae: 0.714645 (2042.653991506604 steps/sec)\n",
      "Step #3752\tEpoch   1 Batch  626/3125   Loss: 0.761456 mae: 0.690142 (2074.4369157722936 steps/sec)\n",
      "Step #3753\tEpoch   1 Batch  627/3125   Loss: 0.892670 mae: 0.769153 (1920.556802051376 steps/sec)\n",
      "Step #3754\tEpoch   1 Batch  628/3125   Loss: 0.819046 mae: 0.719360 (1791.1057589655554 steps/sec)\n",
      "Step #3755\tEpoch   1 Batch  629/3125   Loss: 0.958217 mae: 0.768772 (1992.8274813512614 steps/sec)\n",
      "Step #3756\tEpoch   1 Batch  630/3125   Loss: 0.878250 mae: 0.763520 (2250.211377926565 steps/sec)\n",
      "Step #3757\tEpoch   1 Batch  631/3125   Loss: 0.830486 mae: 0.720325 (1973.7901176470589 steps/sec)\n",
      "Step #3758\tEpoch   1 Batch  632/3125   Loss: 0.829661 mae: 0.687249 (1833.8480910823903 steps/sec)\n",
      "Step #3759\tEpoch   1 Batch  633/3125   Loss: 0.806835 mae: 0.711531 (1925.7417287260907 steps/sec)\n",
      "Step #3760\tEpoch   1 Batch  634/3125   Loss: 0.750130 mae: 0.691547 (2175.6945741259465 steps/sec)\n",
      "Step #3761\tEpoch   1 Batch  635/3125   Loss: 0.819217 mae: 0.744162 (2028.6250459478806 steps/sec)\n",
      "Step #3762\tEpoch   1 Batch  636/3125   Loss: 0.863311 mae: 0.747733 (1949.2434100457301 steps/sec)\n",
      "Step #3763\tEpoch   1 Batch  637/3125   Loss: 0.832393 mae: 0.722757 (1974.7937775434102 steps/sec)\n",
      "Step #3764\tEpoch   1 Batch  638/3125   Loss: 0.871555 mae: 0.733621 (2048.42008615048 steps/sec)\n",
      "Step #3765\tEpoch   1 Batch  639/3125   Loss: 0.886328 mae: 0.751205 (2143.143867394281 steps/sec)\n",
      "Step #3766\tEpoch   1 Batch  640/3125   Loss: 0.807388 mae: 0.728435 (2112.0418953623043 steps/sec)\n",
      "Step #3767\tEpoch   1 Batch  641/3125   Loss: 0.973827 mae: 0.790446 (2183.2370363430045 steps/sec)\n",
      "Step #3768\tEpoch   1 Batch  642/3125   Loss: 0.812711 mae: 0.704320 (1941.2322274881517 steps/sec)\n",
      "Step #3769\tEpoch   1 Batch  643/3125   Loss: 0.725668 mae: 0.681310 (2168.9440479884165 steps/sec)\n",
      "Step #3770\tEpoch   1 Batch  644/3125   Loss: 0.954895 mae: 0.779001 (2088.567985579269 steps/sec)\n",
      "Step #3771\tEpoch   1 Batch  645/3125   Loss: 0.879686 mae: 0.740799 (1909.6440506651854 steps/sec)\n",
      "Step #3772\tEpoch   1 Batch  646/3125   Loss: 0.916829 mae: 0.767233 (2109.874543497289 steps/sec)\n",
      "Step #3773\tEpoch   1 Batch  647/3125   Loss: 0.799141 mae: 0.710295 (2036.0699029126213 steps/sec)\n",
      "Step #3774\tEpoch   1 Batch  648/3125   Loss: 0.886314 mae: 0.753664 (2115.4079707878996 steps/sec)\n",
      "Step #3775\tEpoch   1 Batch  649/3125   Loss: 0.793398 mae: 0.702127 (1863.7872041663334 steps/sec)\n",
      "Step #3776\tEpoch   1 Batch  650/3125   Loss: 0.830873 mae: 0.740494 (1927.812913663774 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3777\tEpoch   1 Batch  651/3125   Loss: 0.809175 mae: 0.691973 (1394.8095827181185 steps/sec)\n",
      "Step #3778\tEpoch   1 Batch  652/3125   Loss: 0.878270 mae: 0.749667 (1888.7305804476066 steps/sec)\n",
      "Step #3779\tEpoch   1 Batch  653/3125   Loss: 0.815306 mae: 0.720407 (1936.8576600539363 steps/sec)\n",
      "Step #3780\tEpoch   1 Batch  654/3125   Loss: 0.807765 mae: 0.704147 (2068.1768424374513 steps/sec)\n",
      "Step #3781\tEpoch   1 Batch  655/3125   Loss: 0.754005 mae: 0.692154 (1892.2070539830913 steps/sec)\n",
      "Step #3782\tEpoch   1 Batch  656/3125   Loss: 0.883181 mae: 0.732247 (2034.8450447303564 steps/sec)\n",
      "Step #3783\tEpoch   1 Batch  657/3125   Loss: 0.850364 mae: 0.736013 (2043.4103088765469 steps/sec)\n",
      "Step #3784\tEpoch   1 Batch  658/3125   Loss: 0.784082 mae: 0.699880 (1910.9491179472227 steps/sec)\n",
      "Step #3785\tEpoch   1 Batch  659/3125   Loss: 0.906878 mae: 0.754451 (1899.9900341556665 steps/sec)\n",
      "Step #3786\tEpoch   1 Batch  660/3125   Loss: 0.836196 mae: 0.711473 (1993.4715449472915 steps/sec)\n",
      "Step #3787\tEpoch   1 Batch  661/3125   Loss: 0.848286 mae: 0.737347 (1767.7197477999932 steps/sec)\n",
      "Step #3788\tEpoch   1 Batch  662/3125   Loss: 0.856949 mae: 0.729776 (2024.5320358732274 steps/sec)\n",
      "Step #3789\tEpoch   1 Batch  663/3125   Loss: 0.767384 mae: 0.694793 (1933.0371462807632 steps/sec)\n",
      "Step #3790\tEpoch   1 Batch  664/3125   Loss: 0.788619 mae: 0.705564 (1786.2392041292608 steps/sec)\n",
      "Step #3791\tEpoch   1 Batch  665/3125   Loss: 0.913578 mae: 0.738480 (2026.1946629050647 steps/sec)\n",
      "Step #3792\tEpoch   1 Batch  666/3125   Loss: 0.757283 mae: 0.704653 (1952.0556998315228 steps/sec)\n",
      "Step #3793\tEpoch   1 Batch  667/3125   Loss: 0.753182 mae: 0.669172 (1763.4240067269288 steps/sec)\n",
      "Step #3794\tEpoch   1 Batch  668/3125   Loss: 0.795394 mae: 0.725213 (1891.1491257338155 steps/sec)\n",
      "Step #3795\tEpoch   1 Batch  669/3125   Loss: 0.833822 mae: 0.715828 (1669.3880150289754 steps/sec)\n",
      "Step #3796\tEpoch   1 Batch  670/3125   Loss: 0.882652 mae: 0.731399 (2088.4015973072824 steps/sec)\n",
      "Step #3797\tEpoch   1 Batch  671/3125   Loss: 0.777784 mae: 0.691312 (1761.9276460605247 steps/sec)\n",
      "Step #3798\tEpoch   1 Batch  672/3125   Loss: 0.883732 mae: 0.736968 (2094.2410050030458 steps/sec)\n",
      "Step #3799\tEpoch   1 Batch  673/3125   Loss: 0.899459 mae: 0.758471 (1986.823680994382 steps/sec)\n",
      "Step #3800\tEpoch   1 Batch  674/3125   Loss: 0.854937 mae: 0.745162 (2040.9841170974773 steps/sec)\n",
      "Step #3801\tEpoch   1 Batch  675/3125   Loss: 0.703052 mae: 0.648222 (1656.9893492620333 steps/sec)\n",
      "Step #3802\tEpoch   1 Batch  676/3125   Loss: 0.980041 mae: 0.780318 (1920.644747687517 steps/sec)\n",
      "Step #3803\tEpoch   1 Batch  677/3125   Loss: 0.846954 mae: 0.720695 (2122.494585349068 steps/sec)\n",
      "Step #3804\tEpoch   1 Batch  678/3125   Loss: 0.795857 mae: 0.680339 (2136.9418573844996 steps/sec)\n",
      "Step #3805\tEpoch   1 Batch  679/3125   Loss: 0.906190 mae: 0.754749 (1968.5653137085571 steps/sec)\n",
      "Step #3806\tEpoch   1 Batch  680/3125   Loss: 0.898864 mae: 0.766382 (2048.920413466987 steps/sec)\n",
      "Step #3807\tEpoch   1 Batch  681/3125   Loss: 0.775710 mae: 0.682795 (2261.6899433809654 steps/sec)\n",
      "Step #3808\tEpoch   1 Batch  682/3125   Loss: 0.847773 mae: 0.703743 (2045.2437145253466 steps/sec)\n",
      "Step #3809\tEpoch   1 Batch  683/3125   Loss: 0.835636 mae: 0.735229 (1861.9173606548645 steps/sec)\n",
      "Step #3810\tEpoch   1 Batch  684/3125   Loss: 0.885734 mae: 0.756024 (1837.495509546048 steps/sec)\n",
      "Step #3811\tEpoch   1 Batch  685/3125   Loss: 0.866475 mae: 0.747688 (1781.7622620028717 steps/sec)\n",
      "Step #3812\tEpoch   1 Batch  686/3125   Loss: 0.916809 mae: 0.772528 (2002.2264442768353 steps/sec)\n",
      "Step #3813\tEpoch   1 Batch  687/3125   Loss: 0.865086 mae: 0.744226 (1865.4616616260453 steps/sec)\n",
      "Step #3814\tEpoch   1 Batch  688/3125   Loss: 0.881495 mae: 0.756955 (1885.0126737016196 steps/sec)\n",
      "Step #3815\tEpoch   1 Batch  689/3125   Loss: 0.905117 mae: 0.753801 (2047.0004880429478 steps/sec)\n",
      "Step #3816\tEpoch   1 Batch  690/3125   Loss: 0.721161 mae: 0.677945 (1975.1843654344243 steps/sec)\n",
      "Step #3817\tEpoch   1 Batch  691/3125   Loss: 0.850852 mae: 0.767175 (1968.8979852413768 steps/sec)\n",
      "Step #3818\tEpoch   1 Batch  692/3125   Loss: 0.959435 mae: 0.794129 (1671.397033624764 steps/sec)\n",
      "Step #3819\tEpoch   1 Batch  693/3125   Loss: 0.869630 mae: 0.759057 (1896.3991825366684 steps/sec)\n",
      "Step #3820\tEpoch   1 Batch  694/3125   Loss: 0.795986 mae: 0.703858 (2024.5320358732274 steps/sec)\n",
      "Step #3821\tEpoch   1 Batch  695/3125   Loss: 0.766146 mae: 0.709693 (1926.024704963953 steps/sec)\n",
      "Step #3822\tEpoch   1 Batch  696/3125   Loss: 0.736833 mae: 0.704894 (1976.1149587750294 steps/sec)\n",
      "Step #3823\tEpoch   1 Batch  697/3125   Loss: 0.927042 mae: 0.754440 (1989.7455359684245 steps/sec)\n",
      "Step #3824\tEpoch   1 Batch  698/3125   Loss: 0.878985 mae: 0.740501 (1762.2385614049829 steps/sec)\n",
      "Step #3825\tEpoch   1 Batch  699/3125   Loss: 0.762455 mae: 0.696280 (2081.685079856665 steps/sec)\n",
      "Step #3826\tEpoch   1 Batch  700/3125   Loss: 0.867624 mae: 0.718929 (1899.972820670786 steps/sec)\n",
      "Step #3827\tEpoch   1 Batch  701/3125   Loss: 0.806431 mae: 0.696129 (1832.3739624290083 steps/sec)\n",
      "Step #3828\tEpoch   1 Batch  702/3125   Loss: 0.789803 mae: 0.716244 (1893.7277636307815 steps/sec)\n",
      "Step #3829\tEpoch   1 Batch  703/3125   Loss: 0.828843 mae: 0.700436 (2080.1364835644426 steps/sec)\n",
      "Step #3830\tEpoch   1 Batch  704/3125   Loss: 0.782818 mae: 0.691504 (1736.843761646445 steps/sec)\n",
      "Step #3831\tEpoch   1 Batch  705/3125   Loss: 0.953367 mae: 0.796360 (2025.5292845000772 steps/sec)\n",
      "Step #3832\tEpoch   1 Batch  706/3125   Loss: 0.867231 mae: 0.753826 (2038.325914118539 steps/sec)\n",
      "Step #3833\tEpoch   1 Batch  707/3125   Loss: 0.889135 mae: 0.747165 (1865.8765959339828 steps/sec)\n",
      "Step #3834\tEpoch   1 Batch  708/3125   Loss: 0.798394 mae: 0.686722 (1684.6894756713768 steps/sec)\n",
      "Step #3835\tEpoch   1 Batch  709/3125   Loss: 0.780701 mae: 0.697681 (1846.9299327156798 steps/sec)\n",
      "Step #3836\tEpoch   1 Batch  710/3125   Loss: 0.801516 mae: 0.694876 (2179.6971303254236 steps/sec)\n",
      "Step #3837\tEpoch   1 Batch  711/3125   Loss: 0.874988 mae: 0.745706 (2025.607541629641 steps/sec)\n",
      "Step #3838\tEpoch   1 Batch  712/3125   Loss: 0.706987 mae: 0.672125 (1837.495509546048 steps/sec)\n",
      "Step #3839\tEpoch   1 Batch  713/3125   Loss: 0.796721 mae: 0.710109 (2002.4749828126194 steps/sec)\n",
      "Step #3840\tEpoch   1 Batch  714/3125   Loss: 0.782994 mae: 0.720590 (2003.4506147482255 steps/sec)\n",
      "Step #3841\tEpoch   1 Batch  715/3125   Loss: 0.790604 mae: 0.702114 (2162.257575601357 steps/sec)\n",
      "Step #3842\tEpoch   1 Batch  716/3125   Loss: 0.860243 mae: 0.745193 (1869.4193364354353 steps/sec)\n",
      "Step #3843\tEpoch   1 Batch  717/3125   Loss: 0.738420 mae: 0.702865 (1855.4269738471883 steps/sec)\n",
      "Step #3844\tEpoch   1 Batch  718/3125   Loss: 0.830536 mae: 0.701783 (1870.436403528331 steps/sec)\n",
      "Step #3845\tEpoch   1 Batch  719/3125   Loss: 0.847011 mae: 0.723504 (2049.180680274768 steps/sec)\n",
      "Step #3846\tEpoch   1 Batch  720/3125   Loss: 0.757940 mae: 0.685400 (1819.986288173984 steps/sec)\n",
      "Step #3847\tEpoch   1 Batch  721/3125   Loss: 0.717701 mae: 0.662345 (2112.9782067686974 steps/sec)\n",
      "Step #3848\tEpoch   1 Batch  722/3125   Loss: 0.862444 mae: 0.747538 (1976.6923671461157 steps/sec)\n",
      "Step #3849\tEpoch   1 Batch  723/3125   Loss: 1.009796 mae: 0.793243 (1710.3412278984797 steps/sec)\n",
      "Step #3850\tEpoch   1 Batch  724/3125   Loss: 0.671212 mae: 0.651096 (1477.1900907944691 steps/sec)\n",
      "Step #3851\tEpoch   1 Batch  725/3125   Loss: 0.786903 mae: 0.715931 (2001.0992366412213 steps/sec)\n",
      "Step #3852\tEpoch   1 Batch  726/3125   Loss: 0.742367 mae: 0.701390 (1938.038998244155 steps/sec)\n",
      "Step #3853\tEpoch   1 Batch  727/3125   Loss: 0.728685 mae: 0.695320 (1961.7702360127594 steps/sec)\n",
      "Step #3854\tEpoch   1 Batch  728/3125   Loss: 0.837900 mae: 0.705989 (1891.4220262092228 steps/sec)\n",
      "Step #3855\tEpoch   1 Batch  729/3125   Loss: 0.746642 mae: 0.686390 (2119.105937512631 steps/sec)\n",
      "Step #3856\tEpoch   1 Batch  730/3125   Loss: 0.711229 mae: 0.679777 (2097.4875980156826 steps/sec)\n",
      "Step #3857\tEpoch   1 Batch  731/3125   Loss: 0.862974 mae: 0.742025 (2089.44195917066 steps/sec)\n",
      "Step #3858\tEpoch   1 Batch  732/3125   Loss: 0.919731 mae: 0.766867 (1828.9541617246912 steps/sec)\n",
      "Step #3859\tEpoch   1 Batch  733/3125   Loss: 0.842294 mae: 0.723990 (1696.0799695908513 steps/sec)\n",
      "Step #3860\tEpoch   1 Batch  734/3125   Loss: 0.906939 mae: 0.762821 (2044.326600639476 steps/sec)\n",
      "Step #3861\tEpoch   1 Batch  735/3125   Loss: 0.794248 mae: 0.695078 (2129.2180234328994 steps/sec)\n",
      "Step #3862\tEpoch   1 Batch  736/3125   Loss: 0.873276 mae: 0.722497 (2049.2808005003126 steps/sec)\n",
      "Step #3863\tEpoch   1 Batch  737/3125   Loss: 0.676830 mae: 0.658307 (1951.4381158867373 steps/sec)\n",
      "Step #3864\tEpoch   1 Batch  738/3125   Loss: 0.902625 mae: 0.770585 (1839.1875537158191 steps/sec)\n",
      "Step #3865\tEpoch   1 Batch  739/3125   Loss: 0.799647 mae: 0.716912 (1966.6454105555347 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3866\tEpoch   1 Batch  740/3125   Loss: 0.884434 mae: 0.742768 (1602.3104604876112 steps/sec)\n",
      "Step #3867\tEpoch   1 Batch  741/3125   Loss: 0.888520 mae: 0.725898 (1675.5500870871351 steps/sec)\n",
      "Step #3868\tEpoch   1 Batch  742/3125   Loss: 0.825399 mae: 0.723984 (1896.1591320072332 steps/sec)\n",
      "Step #3869\tEpoch   1 Batch  743/3125   Loss: 0.717344 mae: 0.674919 (1951.65604206412 steps/sec)\n",
      "Step #3870\tEpoch   1 Batch  744/3125   Loss: 0.819735 mae: 0.715354 (1953.3284278568967 steps/sec)\n",
      "Step #3871\tEpoch   1 Batch  745/3125   Loss: 0.786131 mae: 0.706990 (2075.6688276339883 steps/sec)\n",
      "Step #3872\tEpoch   1 Batch  746/3125   Loss: 0.775889 mae: 0.698934 (1896.7422173181628 steps/sec)\n",
      "Step #3873\tEpoch   1 Batch  747/3125   Loss: 0.746582 mae: 0.696676 (1970.2853277464087 steps/sec)\n",
      "Step #3874\tEpoch   1 Batch  748/3125   Loss: 0.760534 mae: 0.698905 (1693.2042597511647 steps/sec)\n",
      "Step #3875\tEpoch   1 Batch  749/3125   Loss: 0.786691 mae: 0.698256 (1960.99978493216 steps/sec)\n",
      "Step #3876\tEpoch   1 Batch  750/3125   Loss: 0.801713 mae: 0.711929 (2078.38419075746 steps/sec)\n",
      "Step #3877\tEpoch   1 Batch  751/3125   Loss: 0.772440 mae: 0.689861 (2192.8708108955925 steps/sec)\n",
      "Step #3878\tEpoch   1 Batch  752/3125   Loss: 0.876902 mae: 0.762057 (2218.4807100316298 steps/sec)\n",
      "Step #3879\tEpoch   1 Batch  753/3125   Loss: 0.899260 mae: 0.778248 (2031.6121907271424 steps/sec)\n",
      "Step #3880\tEpoch   1 Batch  754/3125   Loss: 0.831809 mae: 0.720963 (2039.0790292470442 steps/sec)\n",
      "Step #3881\tEpoch   1 Batch  755/3125   Loss: 0.934455 mae: 0.762513 (1966.1107204800076 steps/sec)\n",
      "Step #3882\tEpoch   1 Batch  756/3125   Loss: 0.820270 mae: 0.720196 (2045.0043881033641 steps/sec)\n",
      "Step #3883\tEpoch   1 Batch  757/3125   Loss: 0.827961 mae: 0.709688 (1600.1953363447687 steps/sec)\n",
      "Step #3884\tEpoch   1 Batch  758/3125   Loss: 0.760553 mae: 0.684658 (1843.860836842892 steps/sec)\n",
      "Step #3885\tEpoch   1 Batch  759/3125   Loss: 0.898075 mae: 0.747828 (1962.3760152711757 steps/sec)\n",
      "Step #3886\tEpoch   1 Batch  760/3125   Loss: 0.765187 mae: 0.707874 (1761.9276460605247 steps/sec)\n",
      "Step #3887\tEpoch   1 Batch  761/3125   Loss: 0.799653 mae: 0.703742 (1608.7882414311578 steps/sec)\n",
      "Step #3888\tEpoch   1 Batch  762/3125   Loss: 0.830558 mae: 0.719030 (1852.198719364098 steps/sec)\n",
      "Step #3889\tEpoch   1 Batch  763/3125   Loss: 0.909039 mae: 0.759001 (1995.6720749869153 steps/sec)\n",
      "Step #3890\tEpoch   1 Batch  764/3125   Loss: 0.866829 mae: 0.747113 (1865.3952892620794 steps/sec)\n",
      "Step #3891\tEpoch   1 Batch  765/3125   Loss: 0.793840 mae: 0.727349 (1602.396161251872 steps/sec)\n",
      "Step #3892\tEpoch   1 Batch  766/3125   Loss: 0.829240 mae: 0.718967 (1444.259879068358 steps/sec)\n",
      "Step #3893\tEpoch   1 Batch  767/3125   Loss: 0.881390 mae: 0.732823 (1528.2579704864274 steps/sec)\n",
      "Step #3894\tEpoch   1 Batch  768/3125   Loss: 0.720397 mae: 0.685364 (1389.6982910004174 steps/sec)\n",
      "Step #3895\tEpoch   1 Batch  769/3125   Loss: 0.904278 mae: 0.726119 (1336.8981366380438 steps/sec)\n",
      "Step #3896\tEpoch   1 Batch  770/3125   Loss: 0.749064 mae: 0.664435 (1262.1812425896612 steps/sec)\n",
      "Step #3897\tEpoch   1 Batch  771/3125   Loss: 0.769715 mae: 0.705141 (920.1546204033996 steps/sec)\n",
      "Step #3898\tEpoch   1 Batch  772/3125   Loss: 0.826584 mae: 0.720209 (1039.32599861235 steps/sec)\n",
      "Step #3899\tEpoch   1 Batch  773/3125   Loss: 0.718690 mae: 0.687284 (1831.55780298862 steps/sec)\n",
      "Step #3900\tEpoch   1 Batch  774/3125   Loss: 0.778324 mae: 0.707834 (1656.897709585924 steps/sec)\n",
      "Step #3901\tEpoch   1 Batch  775/3125   Loss: 0.798454 mae: 0.705828 (1293.0616271541758 steps/sec)\n",
      "Step #3902\tEpoch   1 Batch  776/3125   Loss: 0.814649 mae: 0.694806 (1513.7847650808087 steps/sec)\n",
      "Step #3903\tEpoch   1 Batch  777/3125   Loss: 0.744908 mae: 0.699614 (1692.739585603474 steps/sec)\n",
      "Step #3904\tEpoch   1 Batch  778/3125   Loss: 0.739484 mae: 0.668732 (1806.285797955264 steps/sec)\n",
      "Step #3905\tEpoch   1 Batch  779/3125   Loss: 0.796877 mae: 0.677037 (1586.2879618773875 steps/sec)\n",
      "Step #3906\tEpoch   1 Batch  780/3125   Loss: 0.848177 mae: 0.749713 (1479.9839097818647 steps/sec)\n",
      "Step #3907\tEpoch   1 Batch  781/3125   Loss: 0.799252 mae: 0.701567 (1528.302519293694 steps/sec)\n",
      "Step #3908\tEpoch   1 Batch  782/3125   Loss: 0.938345 mae: 0.789951 (1711.4578572827577 steps/sec)\n",
      "Step #3909\tEpoch   1 Batch  783/3125   Loss: 0.777857 mae: 0.695671 (1859.3586253978667 steps/sec)\n",
      "Step #3910\tEpoch   1 Batch  784/3125   Loss: 0.910345 mae: 0.748477 (1703.0493499321915 steps/sec)\n",
      "Step #3911\tEpoch   1 Batch  785/3125   Loss: 0.993641 mae: 0.789143 (1287.8604765413904 steps/sec)\n",
      "Step #3912\tEpoch   1 Batch  786/3125   Loss: 0.905194 mae: 0.743957 (1412.4328183299883 steps/sec)\n",
      "Step #3913\tEpoch   1 Batch  787/3125   Loss: 0.864613 mae: 0.736834 (1783.2925170068027 steps/sec)\n",
      "Step #3914\tEpoch   1 Batch  788/3125   Loss: 0.789252 mae: 0.711214 (1782.9741287695224 steps/sec)\n",
      "Step #3915\tEpoch   1 Batch  789/3125   Loss: 0.882296 mae: 0.739577 (2075.6277403327495 steps/sec)\n",
      "Step #3916\tEpoch   1 Batch  790/3125   Loss: 0.781520 mae: 0.711497 (1378.6984504736672 steps/sec)\n",
      "Step #3917\tEpoch   1 Batch  791/3125   Loss: 0.890484 mae: 0.754348 (1330.6717596969563 steps/sec)\n",
      "Step #3918\tEpoch   1 Batch  792/3125   Loss: 0.915038 mae: 0.754733 (1469.3758582999355 steps/sec)\n",
      "Step #3919\tEpoch   1 Batch  793/3125   Loss: 0.833119 mae: 0.751080 (1928.1851364894312 steps/sec)\n",
      "Step #3920\tEpoch   1 Batch  794/3125   Loss: 0.907027 mae: 0.750389 (1843.2612020320987 steps/sec)\n",
      "Step #3921\tEpoch   1 Batch  795/3125   Loss: 0.815643 mae: 0.723780 (1571.4300700610693 steps/sec)\n",
      "Step #3922\tEpoch   1 Batch  796/3125   Loss: 0.953448 mae: 0.761341 (1703.5750550352145 steps/sec)\n",
      "Step #3923\tEpoch   1 Batch  797/3125   Loss: 0.827438 mae: 0.721591 (1538.889174255377 steps/sec)\n",
      "Step #3924\tEpoch   1 Batch  798/3125   Loss: 0.730658 mae: 0.661055 (1356.8881182225214 steps/sec)\n",
      "Step #3925\tEpoch   1 Batch  799/3125   Loss: 0.904741 mae: 0.761875 (1259.2557899350902 steps/sec)\n",
      "Step #3926\tEpoch   1 Batch  800/3125   Loss: 0.713555 mae: 0.675928 (1646.9047188999443 steps/sec)\n",
      "Step #3927\tEpoch   1 Batch  801/3125   Loss: 0.986547 mae: 0.776877 (1892.7022977924587 steps/sec)\n",
      "Step #3928\tEpoch   1 Batch  802/3125   Loss: 0.727367 mae: 0.685371 (2019.210475640285 steps/sec)\n",
      "Step #3929\tEpoch   1 Batch  803/3125   Loss: 0.836367 mae: 0.723537 (2040.725928088357 steps/sec)\n",
      "Step #3930\tEpoch   1 Batch  804/3125   Loss: 0.807802 mae: 0.720784 (2050.3627226687004 steps/sec)\n",
      "Step #3931\tEpoch   1 Batch  805/3125   Loss: 0.743352 mae: 0.674234 (1972.546253186226 steps/sec)\n",
      "Step #3932\tEpoch   1 Batch  806/3125   Loss: 0.860315 mae: 0.731940 (1794.7539131699887 steps/sec)\n",
      "Step #3933\tEpoch   1 Batch  807/3125   Loss: 0.761014 mae: 0.720595 (1738.5283682063866 steps/sec)\n",
      "Step #3934\tEpoch   1 Batch  808/3125   Loss: 0.838400 mae: 0.731371 (1896.79368323942 steps/sec)\n",
      "Step #3935\tEpoch   1 Batch  809/3125   Loss: 0.870129 mae: 0.728572 (2035.062250730221 steps/sec)\n",
      "Step #3936\tEpoch   1 Batch  810/3125   Loss: 0.776524 mae: 0.686092 (2221.841759545705 steps/sec)\n",
      "Step #3937\tEpoch   1 Batch  811/3125   Loss: 0.835033 mae: 0.737104 (2005.5389794200903 steps/sec)\n",
      "Step #3938\tEpoch   1 Batch  812/3125   Loss: 0.888584 mae: 0.738167 (2077.704684156297 steps/sec)\n",
      "Step #3939\tEpoch   1 Batch  813/3125   Loss: 0.942688 mae: 0.771866 (1917.4662387653034 steps/sec)\n",
      "Step #3940\tEpoch   1 Batch  814/3125   Loss: 0.905441 mae: 0.740931 (1449.3503621385526 steps/sec)\n",
      "Step #3941\tEpoch   1 Batch  815/3125   Loss: 0.766498 mae: 0.682109 (1937.3944533747206 steps/sec)\n",
      "Step #3942\tEpoch   1 Batch  816/3125   Loss: 0.940332 mae: 0.764490 (1953.2738483318742 steps/sec)\n",
      "Step #3943\tEpoch   1 Batch  817/3125   Loss: 0.728808 mae: 0.662304 (1981.2863729121004 steps/sec)\n",
      "Step #3944\tEpoch   1 Batch  818/3125   Loss: 0.671363 mae: 0.658863 (1989.651148449285 steps/sec)\n",
      "Step #3945\tEpoch   1 Batch  819/3125   Loss: 0.728778 mae: 0.674667 (1935.7313617441548 steps/sec)\n",
      "Step #3946\tEpoch   1 Batch  820/3125   Loss: 0.802229 mae: 0.708600 (1696.2445909329883 steps/sec)\n",
      "Step #3947\tEpoch   1 Batch  821/3125   Loss: 0.997037 mae: 0.801263 (1617.6361238169434 steps/sec)\n",
      "Step #3948\tEpoch   1 Batch  822/3125   Loss: 0.775367 mae: 0.703151 (1937.4660482991815 steps/sec)\n",
      "Step #3949\tEpoch   1 Batch  823/3125   Loss: 0.786733 mae: 0.697966 (1969.9706921169309 steps/sec)\n",
      "Step #3950\tEpoch   1 Batch  824/3125   Loss: 0.857624 mae: 0.725170 (1695.4355102106813 steps/sec)\n",
      "Step #3951\tEpoch   1 Batch  825/3125   Loss: 0.833302 mae: 0.732812 (1773.8050732053896 steps/sec)\n",
      "Step #3952\tEpoch   1 Batch  826/3125   Loss: 0.876932 mae: 0.744318 (2128.6777169886013 steps/sec)\n",
      "Step #3953\tEpoch   1 Batch  827/3125   Loss: 0.715720 mae: 0.673147 (2032.124031007752 steps/sec)\n",
      "Step #3954\tEpoch   1 Batch  828/3125   Loss: 0.947376 mae: 0.779304 (1996.3369823893383 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3955\tEpoch   1 Batch  829/3125   Loss: 0.965459 mae: 0.771743 (1704.6135838996001 steps/sec)\n",
      "Step #3956\tEpoch   1 Batch  830/3125   Loss: 0.735099 mae: 0.668688 (1570.9945165253348 steps/sec)\n",
      "Step #3957\tEpoch   1 Batch  831/3125   Loss: 0.936965 mae: 0.786048 (1909.922315419433 steps/sec)\n",
      "Step #3958\tEpoch   1 Batch  832/3125   Loss: 0.893751 mae: 0.753412 (2034.4305074551576 steps/sec)\n",
      "Step #3959\tEpoch   1 Batch  833/3125   Loss: 0.990730 mae: 0.791197 (1914.6128142859752 steps/sec)\n",
      "Step #3960\tEpoch   1 Batch  834/3125   Loss: 0.853200 mae: 0.732271 (1962.174047287119 steps/sec)\n",
      "Step #3961\tEpoch   1 Batch  835/3125   Loss: 0.758147 mae: 0.701635 (2026.8408895417951 steps/sec)\n",
      "Step #3962\tEpoch   1 Batch  836/3125   Loss: 0.790470 mae: 0.681776 (2014.5939403254627 steps/sec)\n",
      "Step #3963\tEpoch   1 Batch  837/3125   Loss: 0.854512 mae: 0.752082 (1642.9566375494535 steps/sec)\n",
      "Step #3964\tEpoch   1 Batch  838/3125   Loss: 0.913288 mae: 0.745816 (1708.5854882599274 steps/sec)\n",
      "Step #3965\tEpoch   1 Batch  839/3125   Loss: 0.827069 mae: 0.723265 (1599.5484673058295 steps/sec)\n",
      "Step #3966\tEpoch   1 Batch  840/3125   Loss: 0.775792 mae: 0.713092 (1933.8571059717458 steps/sec)\n",
      "Step #3967\tEpoch   1 Batch  841/3125   Loss: 0.917490 mae: 0.765202 (1967.8079813836525 steps/sec)\n",
      "Step #3968\tEpoch   1 Batch  842/3125   Loss: 0.747721 mae: 0.680788 (1893.4883888908953 steps/sec)\n",
      "Step #3969\tEpoch   1 Batch  843/3125   Loss: 0.775863 mae: 0.718460 (1999.3822099342167 steps/sec)\n",
      "Step #3970\tEpoch   1 Batch  844/3125   Loss: 0.944044 mae: 0.734285 (1980.0330453665674 steps/sec)\n",
      "Step #3971\tEpoch   1 Batch  845/3125   Loss: 0.774480 mae: 0.704157 (1829.241316748949 steps/sec)\n",
      "Step #3972\tEpoch   1 Batch  846/3125   Loss: 0.673817 mae: 0.658771 (1913.7916244604448 steps/sec)\n",
      "Step #3973\tEpoch   1 Batch  847/3125   Loss: 0.874695 mae: 0.729058 (1895.2337918195456 steps/sec)\n",
      "Step #3974\tEpoch   1 Batch  848/3125   Loss: 0.975318 mae: 0.767551 (1930.1372258474225 steps/sec)\n",
      "Step #3975\tEpoch   1 Batch  849/3125   Loss: 0.968555 mae: 0.798057 (1842.2896498409966 steps/sec)\n",
      "Step #3976\tEpoch   1 Batch  850/3125   Loss: 0.901833 mae: 0.754628 (1791.0139803404131 steps/sec)\n",
      "Step #3977\tEpoch   1 Batch  851/3125   Loss: 0.833465 mae: 0.721077 (1928.0433203703194 steps/sec)\n",
      "Step #3978\tEpoch   1 Batch  852/3125   Loss: 0.828312 mae: 0.737608 (1905.1508930031432 steps/sec)\n",
      "Step #3979\tEpoch   1 Batch  853/3125   Loss: 0.872800 mae: 0.741401 (1691.9884465815758 steps/sec)\n",
      "Step #3980\tEpoch   1 Batch  854/3125   Loss: 0.770359 mae: 0.676971 (1401.8208312723093 steps/sec)\n",
      "Step #3981\tEpoch   1 Batch  855/3125   Loss: 1.011828 mae: 0.806159 (1894.2238038893354 steps/sec)\n",
      "Step #3982\tEpoch   1 Batch  856/3125   Loss: 0.726378 mae: 0.671117 (1890.859255252006 steps/sec)\n",
      "Step #3983\tEpoch   1 Batch  857/3125   Loss: 0.744625 mae: 0.695664 (2029.8620723031506 steps/sec)\n",
      "Step #3984\tEpoch   1 Batch  858/3125   Loss: 0.885558 mae: 0.745915 (1928.5575030806865 steps/sec)\n",
      "Step #3985\tEpoch   1 Batch  859/3125   Loss: 0.973793 mae: 0.740592 (2205.647816072612 steps/sec)\n",
      "Step #3986\tEpoch   1 Batch  860/3125   Loss: 0.811214 mae: 0.697455 (1776.901112495022 steps/sec)\n",
      "Step #3987\tEpoch   1 Batch  861/3125   Loss: 0.667782 mae: 0.666381 (1576.403024790655 steps/sec)\n",
      "Step #3988\tEpoch   1 Batch  862/3125   Loss: 0.818049 mae: 0.724274 (1864.3671212417546 steps/sec)\n",
      "Step #3989\tEpoch   1 Batch  863/3125   Loss: 0.721991 mae: 0.670072 (1926.785615846824 steps/sec)\n",
      "Step #3990\tEpoch   1 Batch  864/3125   Loss: 0.797480 mae: 0.709654 (2027.0172047167987 steps/sec)\n",
      "Step #3991\tEpoch   1 Batch  865/3125   Loss: 0.928716 mae: 0.735090 (2059.90884802766 steps/sec)\n",
      "Step #3992\tEpoch   1 Batch  866/3125   Loss: 0.850447 mae: 0.727113 (1964.8765131357043 steps/sec)\n",
      "Step #3993\tEpoch   1 Batch  867/3125   Loss: 0.701760 mae: 0.674975 (2021.3513253012047 steps/sec)\n",
      "Step #3994\tEpoch   1 Batch  868/3125   Loss: 0.881752 mae: 0.738774 (1980.3509036997866 steps/sec)\n",
      "Step #3995\tEpoch   1 Batch  869/3125   Loss: 0.931020 mae: 0.770997 (2034.9832613652904 steps/sec)\n",
      "Step #3996\tEpoch   1 Batch  870/3125   Loss: 0.745825 mae: 0.669580 (1815.605980589921 steps/sec)\n",
      "Step #3997\tEpoch   1 Batch  871/3125   Loss: 0.739411 mae: 0.681440 (1805.4927079566783 steps/sec)\n",
      "Step #3998\tEpoch   1 Batch  872/3125   Loss: 0.710575 mae: 0.688543 (2069.8915285687494 steps/sec)\n",
      "Step #3999\tEpoch   1 Batch  873/3125   Loss: 0.852336 mae: 0.746005 (2148.6992961137694 steps/sec)\n",
      "Step #4000\tEpoch   1 Batch  874/3125   Loss: 0.788550 mae: 0.713487 (2195.304043798218 steps/sec)\n",
      "Step #4001\tEpoch   1 Batch  875/3125   Loss: 0.898635 mae: 0.770962 (2139.9510204081635 steps/sec)\n",
      "Step #4002\tEpoch   1 Batch  876/3125   Loss: 0.922725 mae: 0.779250 (2131.5553025837007 steps/sec)\n",
      "Step #4003\tEpoch   1 Batch  877/3125   Loss: 0.777922 mae: 0.696013 (1978.0907195879984 steps/sec)\n",
      "Step #4004\tEpoch   1 Batch  878/3125   Loss: 0.891267 mae: 0.733487 (1798.24732897738 steps/sec)\n",
      "Step #4005\tEpoch   1 Batch  879/3125   Loss: 0.725658 mae: 0.703575 (2003.2209687741788 steps/sec)\n",
      "Step #4006\tEpoch   1 Batch  880/3125   Loss: 0.771822 mae: 0.713710 (1976.6551048107374 steps/sec)\n",
      "Step #4007\tEpoch   1 Batch  881/3125   Loss: 0.764500 mae: 0.708019 (2216.8157121413924 steps/sec)\n",
      "Step #4008\tEpoch   1 Batch  882/3125   Loss: 0.782770 mae: 0.703929 (2039.8128604915817 steps/sec)\n",
      "Step #4009\tEpoch   1 Batch  883/3125   Loss: 0.868152 mae: 0.714347 (2154.549190433139 steps/sec)\n",
      "Step #4010\tEpoch   1 Batch  884/3125   Loss: 1.002094 mae: 0.807231 (2276.98855616599 steps/sec)\n",
      "Step #4011\tEpoch   1 Batch  885/3125   Loss: 0.859585 mae: 0.740104 (2168.069555148921 steps/sec)\n",
      "Step #4012\tEpoch   1 Batch  886/3125   Loss: 0.846869 mae: 0.738549 (1569.0316402187657 steps/sec)\n",
      "Step #4013\tEpoch   1 Batch  887/3125   Loss: 0.839811 mae: 0.721864 (2136.0059481977164 steps/sec)\n",
      "Step #4014\tEpoch   1 Batch  888/3125   Loss: 0.859440 mae: 0.745236 (2101.270490160715 steps/sec)\n",
      "Step #4015\tEpoch   1 Batch  889/3125   Loss: 0.973042 mae: 0.772923 (1940.0111008325625 steps/sec)\n",
      "Step #4016\tEpoch   1 Batch  890/3125   Loss: 0.772733 mae: 0.670234 (1958.307965262863 steps/sec)\n",
      "Step #4017\tEpoch   1 Batch  891/3125   Loss: 0.785894 mae: 0.708658 (2084.2919188606297 steps/sec)\n",
      "Step #4018\tEpoch   1 Batch  892/3125   Loss: 0.794657 mae: 0.709314 (2049.4209852534473 steps/sec)\n",
      "Step #4019\tEpoch   1 Batch  893/3125   Loss: 0.796920 mae: 0.712103 (2303.625998220504 steps/sec)\n",
      "Step #4020\tEpoch   1 Batch  894/3125   Loss: 1.012036 mae: 0.792732 (2334.60463770052 steps/sec)\n",
      "Step #4021\tEpoch   1 Batch  895/3125   Loss: 0.757877 mae: 0.694281 (1926.4847187646405 steps/sec)\n",
      "Step #4022\tEpoch   1 Batch  896/3125   Loss: 0.839925 mae: 0.727085 (2019.8717084352668 steps/sec)\n",
      "Step #4023\tEpoch   1 Batch  897/3125   Loss: 0.856278 mae: 0.714700 (2110.0656014810643 steps/sec)\n",
      "Step #4024\tEpoch   1 Batch  898/3125   Loss: 0.711314 mae: 0.667122 (2109.4288760586614 steps/sec)\n",
      "Step #4025\tEpoch   1 Batch  899/3125   Loss: 0.911169 mae: 0.759528 (2018.0640691307653 steps/sec)\n",
      "Step #4026\tEpoch   1 Batch  900/3125   Loss: 0.835131 mae: 0.727851 (1892.3094969546582 steps/sec)\n",
      "Step #4027\tEpoch   1 Batch  901/3125   Loss: 0.732319 mae: 0.686195 (2022.09194692996 steps/sec)\n",
      "Step #4028\tEpoch   1 Batch  902/3125   Loss: 0.780706 mae: 0.718039 (2139.6671870058053 steps/sec)\n",
      "Step #4029\tEpoch   1 Batch  903/3125   Loss: 0.870117 mae: 0.737782 (2281.0254625349417 steps/sec)\n",
      "Step #4030\tEpoch   1 Batch  904/3125   Loss: 0.926489 mae: 0.755453 (1599.3776835490341 steps/sec)\n",
      "Step #4031\tEpoch   1 Batch  905/3125   Loss: 0.866483 mae: 0.722317 (2035.9117738428083 steps/sec)\n",
      "Step #4032\tEpoch   1 Batch  906/3125   Loss: 0.902077 mae: 0.759719 (1895.387952460572 steps/sec)\n",
      "Step #4033\tEpoch   1 Batch  907/3125   Loss: 0.805630 mae: 0.716702 (1856.741155221872 steps/sec)\n",
      "Step #4034\tEpoch   1 Batch  908/3125   Loss: 0.837830 mae: 0.753075 (2049.4209852534473 steps/sec)\n",
      "Step #4035\tEpoch   1 Batch  909/3125   Loss: 0.824954 mae: 0.714540 (2037.256654361764 steps/sec)\n",
      "Step #4036\tEpoch   1 Batch  910/3125   Loss: 0.905123 mae: 0.755314 (1996.6600973027525 steps/sec)\n",
      "Step #4037\tEpoch   1 Batch  911/3125   Loss: 0.793874 mae: 0.698810 (1986.6354688669326 steps/sec)\n",
      "Step #4038\tEpoch   1 Batch  912/3125   Loss: 0.962655 mae: 0.768465 (1953.3648159014913 steps/sec)\n",
      "Step #4039\tEpoch   1 Batch  913/3125   Loss: 0.802591 mae: 0.718881 (2026.8996578586202 steps/sec)\n",
      "Step #4040\tEpoch   1 Batch  914/3125   Loss: 0.896758 mae: 0.761499 (2038.2268614358884 steps/sec)\n",
      "Step #4041\tEpoch   1 Batch  915/3125   Loss: 0.886064 mae: 0.737309 (1829.8957288076435 steps/sec)\n",
      "Step #4042\tEpoch   1 Batch  916/3125   Loss: 0.896574 mae: 0.752388 (2122.516067000658 steps/sec)\n",
      "Step #4043\tEpoch   1 Batch  917/3125   Loss: 0.660887 mae: 0.657143 (2186.3780898466416 steps/sec)\n",
      "Step #4044\tEpoch   1 Batch  918/3125   Loss: 0.853233 mae: 0.724037 (2199.3560768932284 steps/sec)\n",
      "Step #4045\tEpoch   1 Batch  919/3125   Loss: 0.820062 mae: 0.712565 (2068.1768424374513 steps/sec)\n",
      "Step #4046\tEpoch   1 Batch  920/3125   Loss: 0.824018 mae: 0.696769 (1961.1648306432005 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4047\tEpoch   1 Batch  921/3125   Loss: 0.867770 mae: 0.733390 (1876.377431419215 steps/sec)\n",
      "Step #4048\tEpoch   1 Batch  922/3125   Loss: 0.792056 mae: 0.710353 (2136.027704216745 steps/sec)\n",
      "Step #4049\tEpoch   1 Batch  923/3125   Loss: 0.802555 mae: 0.695233 (2133.3116321652 steps/sec)\n",
      "Step #4050\tEpoch   1 Batch  924/3125   Loss: 0.933895 mae: 0.734555 (1734.4879206676096 steps/sec)\n",
      "Step #4051\tEpoch   1 Batch  925/3125   Loss: 0.848445 mae: 0.753371 (1676.3268960225093 steps/sec)\n",
      "Step #4052\tEpoch   1 Batch  926/3125   Loss: 0.801140 mae: 0.725339 (2092.402246899538 steps/sec)\n",
      "Step #4053\tEpoch   1 Batch  927/3125   Loss: 0.717473 mae: 0.660624 (2269.129310438104 steps/sec)\n",
      "Step #4054\tEpoch   1 Batch  928/3125   Loss: 0.860719 mae: 0.729067 (2063.4557673196696 steps/sec)\n",
      "Step #4055\tEpoch   1 Batch  929/3125   Loss: 0.817845 mae: 0.721437 (2195.487903183593 steps/sec)\n",
      "Step #4056\tEpoch   1 Batch  930/3125   Loss: 0.827833 mae: 0.708446 (2025.0794233239023 steps/sec)\n",
      "Step #4057\tEpoch   1 Batch  931/3125   Loss: 0.937261 mae: 0.771047 (2304.8919076351567 steps/sec)\n",
      "Step #4058\tEpoch   1 Batch  932/3125   Loss: 0.861317 mae: 0.745568 (2221.2299023449914 steps/sec)\n",
      "Step #4059\tEpoch   1 Batch  933/3125   Loss: 0.778097 mae: 0.714081 (2101.2915443423544 steps/sec)\n",
      "Step #4060\tEpoch   1 Batch  934/3125   Loss: 0.823704 mae: 0.731694 (2205.4390577347776 steps/sec)\n",
      "Step #4061\tEpoch   1 Batch  935/3125   Loss: 0.917041 mae: 0.770208 (2204.3726875210227 steps/sec)\n",
      "Step #4062\tEpoch   1 Batch  936/3125   Loss: 0.823476 mae: 0.736982 (2238.5380641304814 steps/sec)\n",
      "Step #4063\tEpoch   1 Batch  937/3125   Loss: 0.778783 mae: 0.682981 (2166.1884251081983 steps/sec)\n",
      "Step #4064\tEpoch   1 Batch  938/3125   Loss: 0.861820 mae: 0.761404 (1755.673503557974 steps/sec)\n",
      "Step #4065\tEpoch   1 Batch  939/3125   Loss: 0.914247 mae: 0.736885 (1838.9133923169331 steps/sec)\n",
      "Step #4066\tEpoch   1 Batch  940/3125   Loss: 0.824002 mae: 0.718256 (2070.8317287278687 steps/sec)\n",
      "Step #4067\tEpoch   1 Batch  941/3125   Loss: 0.830401 mae: 0.730099 (1929.2316750073594 steps/sec)\n",
      "Step #4068\tEpoch   1 Batch  942/3125   Loss: 0.800007 mae: 0.717220 (1891.6608787422315 steps/sec)\n",
      "Step #4069\tEpoch   1 Batch  943/3125   Loss: 0.815409 mae: 0.709959 (2120.0913888271093 steps/sec)\n",
      "Step #4070\tEpoch   1 Batch  944/3125   Loss: 0.809009 mae: 0.706058 (2160.097233380714 steps/sec)\n",
      "Step #4071\tEpoch   1 Batch  945/3125   Loss: 0.849274 mae: 0.735891 (2016.0464512655856 steps/sec)\n",
      "Step #4072\tEpoch   1 Batch  946/3125   Loss: 0.921338 mae: 0.747394 (1881.2587462772256 steps/sec)\n",
      "Step #4073\tEpoch   1 Batch  947/3125   Loss: 0.717693 mae: 0.663763 (2150.506055230263 steps/sec)\n",
      "Step #4074\tEpoch   1 Batch  948/3125   Loss: 0.647630 mae: 0.643876 (1988.1421650882132 steps/sec)\n",
      "Step #4075\tEpoch   1 Batch  949/3125   Loss: 0.708963 mae: 0.669951 (2094.5965921575676 steps/sec)\n",
      "Step #4076\tEpoch   1 Batch  950/3125   Loss: 0.850248 mae: 0.723640 (2272.104008667389 steps/sec)\n",
      "Step #4077\tEpoch   1 Batch  951/3125   Loss: 0.921733 mae: 0.747695 (2110.5964997031087 steps/sec)\n",
      "Step #4078\tEpoch   1 Batch  952/3125   Loss: 0.915121 mae: 0.741747 (2189.231058312629 steps/sec)\n",
      "Step #4079\tEpoch   1 Batch  953/3125   Loss: 0.868951 mae: 0.747207 (2150.351701084839 steps/sec)\n",
      "Step #4080\tEpoch   1 Batch  954/3125   Loss: 0.907693 mae: 0.763067 (2255.220397672893 steps/sec)\n",
      "Step #4081\tEpoch   1 Batch  955/3125   Loss: 0.908777 mae: 0.753301 (1866.2252834285512 steps/sec)\n",
      "Step #4082\tEpoch   1 Batch  956/3125   Loss: 0.787889 mae: 0.693130 (2132.921086622662 steps/sec)\n",
      "Step #4083\tEpoch   1 Batch  957/3125   Loss: 0.905542 mae: 0.756275 (2387.631213425325 steps/sec)\n",
      "Step #4084\tEpoch   1 Batch  958/3125   Loss: 0.937563 mae: 0.739827 (2086.718407960199 steps/sec)\n",
      "Step #4085\tEpoch   1 Batch  959/3125   Loss: 0.975466 mae: 0.771691 (2071.69091861028 steps/sec)\n",
      "Step #4086\tEpoch   1 Batch  960/3125   Loss: 0.916342 mae: 0.728277 (2103.947751236494 steps/sec)\n",
      "Step #4087\tEpoch   1 Batch  961/3125   Loss: 0.881160 mae: 0.744201 (2118.634958478977 steps/sec)\n",
      "Step #4088\tEpoch   1 Batch  962/3125   Loss: 0.738964 mae: 0.691855 (2205.647816072612 steps/sec)\n",
      "Step #4089\tEpoch   1 Batch  963/3125   Loss: 0.797516 mae: 0.701214 (1962.706598034628 steps/sec)\n",
      "Step #4090\tEpoch   1 Batch  964/3125   Loss: 0.772166 mae: 0.696792 (1853.8524097450586 steps/sec)\n",
      "Step #4091\tEpoch   1 Batch  965/3125   Loss: 0.715294 mae: 0.687411 (1935.8743111390093 steps/sec)\n",
      "Step #4092\tEpoch   1 Batch  966/3125   Loss: 0.833448 mae: 0.727009 (2167.4197482378718 steps/sec)\n",
      "Step #4093\tEpoch   1 Batch  967/3125   Loss: 0.790288 mae: 0.704430 (2111.680360882874 steps/sec)\n",
      "Step #4094\tEpoch   1 Batch  968/3125   Loss: 0.986157 mae: 0.761588 (2086.9883666543933 steps/sec)\n",
      "Step #4095\tEpoch   1 Batch  969/3125   Loss: 0.841936 mae: 0.729878 (2296.512226371292 steps/sec)\n",
      "Step #4096\tEpoch   1 Batch  970/3125   Loss: 0.915780 mae: 0.769325 (2198.6873833637374 steps/sec)\n",
      "Step #4097\tEpoch   1 Batch  971/3125   Loss: 0.822122 mae: 0.711013 (2078.9198727162784 steps/sec)\n",
      "Step #4098\tEpoch   1 Batch  972/3125   Loss: 0.805345 mae: 0.730497 (2093.258539117242 steps/sec)\n",
      "Step #4099\tEpoch   1 Batch  973/3125   Loss: 0.910993 mae: 0.753188 (1780.5671591102055 steps/sec)\n",
      "Step #4100\tEpoch   1 Batch  974/3125   Loss: 0.750905 mae: 0.672680 (1969.8781713491326 steps/sec)\n",
      "Step #4101\tEpoch   1 Batch  975/3125   Loss: 0.766690 mae: 0.685363 (2228.855044584498 steps/sec)\n",
      "Step #4102\tEpoch   1 Batch  976/3125   Loss: 0.848739 mae: 0.719827 (2266.5297696888474 steps/sec)\n",
      "Step #4103\tEpoch   1 Batch  977/3125   Loss: 0.714267 mae: 0.687832 (2039.059203298039 steps/sec)\n",
      "Step #4104\tEpoch   1 Batch  978/3125   Loss: 0.665994 mae: 0.647682 (2028.8998103788554 steps/sec)\n",
      "Step #4105\tEpoch   1 Batch  979/3125   Loss: 0.929267 mae: 0.752854 (2046.4212180056402 steps/sec)\n",
      "Step #4106\tEpoch   1 Batch  980/3125   Loss: 0.887358 mae: 0.737299 (1995.9379847912364 steps/sec)\n",
      "Step #4107\tEpoch   1 Batch  981/3125   Loss: 0.853203 mae: 0.763779 (2140.890390681626 steps/sec)\n",
      "Step #4108\tEpoch   1 Batch  982/3125   Loss: 0.938574 mae: 0.741063 (1952.5100551169373 steps/sec)\n",
      "Step #4109\tEpoch   1 Batch  983/3125   Loss: 0.842093 mae: 0.717433 (1984.360925021763 steps/sec)\n",
      "Step #4110\tEpoch   1 Batch  984/3125   Loss: 0.863790 mae: 0.746745 (2045.582856195316 steps/sec)\n",
      "Step #4111\tEpoch   1 Batch  985/3125   Loss: 0.890225 mae: 0.750116 (2207.2031490096197 steps/sec)\n",
      "Step #4112\tEpoch   1 Batch  986/3125   Loss: 0.940660 mae: 0.778155 (1980.6875708349075 steps/sec)\n",
      "Step #4113\tEpoch   1 Batch  987/3125   Loss: 0.740174 mae: 0.683258 (2205.1839623137507 steps/sec)\n",
      "Step #4114\tEpoch   1 Batch  988/3125   Loss: 0.757372 mae: 0.678341 (2166.568866482086 steps/sec)\n",
      "Step #4115\tEpoch   1 Batch  989/3125   Loss: 0.845006 mae: 0.740463 (2319.935396086153 steps/sec)\n",
      "Step #4116\tEpoch   1 Batch  990/3125   Loss: 0.703335 mae: 0.650828 (2218.292979616878 steps/sec)\n",
      "Step #4117\tEpoch   1 Batch  991/3125   Loss: 0.839794 mae: 0.709828 (1656.5705077569592 steps/sec)\n",
      "Step #4118\tEpoch   1 Batch  992/3125   Loss: 0.867809 mae: 0.701047 (2017.2875845285162 steps/sec)\n",
      "Step #4119\tEpoch   1 Batch  993/3125   Loss: 0.890790 mae: 0.738296 (2219.560776842885 steps/sec)\n",
      "Step #4120\tEpoch   1 Batch  994/3125   Loss: 0.753712 mae: 0.683501 (2150.704543123782 steps/sec)\n",
      "Step #4121\tEpoch   1 Batch  995/3125   Loss: 0.717683 mae: 0.675118 (2147.203309135959 steps/sec)\n",
      "Step #4122\tEpoch   1 Batch  996/3125   Loss: 0.846191 mae: 0.738291 (2236.341921173861 steps/sec)\n",
      "Step #4123\tEpoch   1 Batch  997/3125   Loss: 0.791753 mae: 0.713471 (2165.2267283389774 steps/sec)\n",
      "Step #4124\tEpoch   1 Batch  998/3125   Loss: 0.751200 mae: 0.711989 (2128.8289751502357 steps/sec)\n",
      "Step #4125\tEpoch   1 Batch  999/3125   Loss: 0.881633 mae: 0.734886 (2034.7463300571474 steps/sec)\n",
      "Step #4126\tEpoch   1 Batch 1000/3125   Loss: 0.919867 mae: 0.762755 (1765.2943206592647 steps/sec)\n",
      "Step #4127\tEpoch   1 Batch 1001/3125   Loss: 0.801343 mae: 0.723436 (1895.5078725211952 steps/sec)\n",
      "Step #4128\tEpoch   1 Batch 1002/3125   Loss: 0.792340 mae: 0.698752 (2244.1674068208326 steps/sec)\n",
      "Step #4129\tEpoch   1 Batch 1003/3125   Loss: 1.045473 mae: 0.826371 (2066.200319218113 steps/sec)\n",
      "Step #4130\tEpoch   1 Batch 1004/3125   Loss: 0.778375 mae: 0.697243 (1882.8633249836148 steps/sec)\n",
      "Step #4131\tEpoch   1 Batch 1005/3125   Loss: 0.688898 mae: 0.674245 (1943.8412412987664 steps/sec)\n",
      "Step #4132\tEpoch   1 Batch 1006/3125   Loss: 0.856972 mae: 0.704440 (2060.3135929579125 steps/sec)\n",
      "Step #4133\tEpoch   1 Batch 1007/3125   Loss: 0.855233 mae: 0.752962 (2017.7534035695387 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4134\tEpoch   1 Batch 1008/3125   Loss: 0.798417 mae: 0.689458 (1940.4598658339116 steps/sec)\n",
      "Step #4135\tEpoch   1 Batch 1009/3125   Loss: 0.868057 mae: 0.743194 (2089.0048809642394 steps/sec)\n",
      "Step #4136\tEpoch   1 Batch 1010/3125   Loss: 0.979710 mae: 0.758379 (1983.3287623298877 steps/sec)\n",
      "Step #4137\tEpoch   1 Batch 1011/3125   Loss: 0.861423 mae: 0.737057 (2150.1753234769412 steps/sec)\n",
      "Step #4138\tEpoch   1 Batch 1012/3125   Loss: 0.806122 mae: 0.721135 (2129.758604230773 steps/sec)\n",
      "Step #4139\tEpoch   1 Batch 1013/3125   Loss: 0.823913 mae: 0.712320 (1986.3344036219323 steps/sec)\n",
      "Step #4140\tEpoch   1 Batch 1014/3125   Loss: 0.872878 mae: 0.739964 (2206.1582806467563 steps/sec)\n",
      "Step #4141\tEpoch   1 Batch 1015/3125   Loss: 0.900400 mae: 0.743474 (2052.5701757820148 steps/sec)\n",
      "Step #4142\tEpoch   1 Batch 1016/3125   Loss: 0.934345 mae: 0.750290 (2084.4783715012722 steps/sec)\n",
      "Step #4143\tEpoch   1 Batch 1017/3125   Loss: 0.862406 mae: 0.761552 (1645.2251135570218 steps/sec)\n",
      "Step #4144\tEpoch   1 Batch 1018/3125   Loss: 0.764734 mae: 0.685217 (1939.0245481022607 steps/sec)\n",
      "Step #4145\tEpoch   1 Batch 1019/3125   Loss: 0.889649 mae: 0.748897 (2329.2371940112844 steps/sec)\n",
      "Step #4146\tEpoch   1 Batch 1020/3125   Loss: 0.859737 mae: 0.744923 (2082.1397722421343 steps/sec)\n",
      "Step #4147\tEpoch   1 Batch 1021/3125   Loss: 0.892811 mae: 0.724204 (2125.937189546459 steps/sec)\n",
      "Step #4148\tEpoch   1 Batch 1022/3125   Loss: 0.835525 mae: 0.720212 (2277.4831128776527 steps/sec)\n",
      "Step #4149\tEpoch   1 Batch 1023/3125   Loss: 0.788423 mae: 0.708477 (2251.4434173940117 steps/sec)\n",
      "Step #4150\tEpoch   1 Batch 1024/3125   Loss: 0.833868 mae: 0.720232 (2302.235102972819 steps/sec)\n",
      "Step #4151\tEpoch   1 Batch 1025/3125   Loss: 0.809576 mae: 0.719288 (2174.9499600717672 steps/sec)\n",
      "Step #4152\tEpoch   1 Batch 1026/3125   Loss: 0.850691 mae: 0.747700 (1925.2997447808605 steps/sec)\n",
      "Step #4153\tEpoch   1 Batch 1027/3125   Loss: 0.870353 mae: 0.740819 (2021.7410585173045 steps/sec)\n",
      "Step #4154\tEpoch   1 Batch 1028/3125   Loss: 0.963964 mae: 0.762856 (2066.200319218113 steps/sec)\n",
      "Step #4155\tEpoch   1 Batch 1029/3125   Loss: 0.816167 mae: 0.716530 (2174.837184220351 steps/sec)\n",
      "Step #4156\tEpoch   1 Batch 1030/3125   Loss: 0.751455 mae: 0.681743 (2121.3352215253894 steps/sec)\n",
      "Step #4157\tEpoch   1 Batch 1031/3125   Loss: 0.801459 mae: 0.689502 (2051.285261551704 steps/sec)\n",
      "Step #4158\tEpoch   1 Batch 1032/3125   Loss: 0.943672 mae: 0.757686 (1990.9922910416587 steps/sec)\n",
      "Step #4159\tEpoch   1 Batch 1033/3125   Loss: 0.859587 mae: 0.739692 (2109.9806825499036 steps/sec)\n",
      "Step #4160\tEpoch   1 Batch 1034/3125   Loss: 0.831173 mae: 0.710066 (2074.518997734714 steps/sec)\n",
      "Step #4161\tEpoch   1 Batch 1035/3125   Loss: 0.833594 mae: 0.729306 (1859.441055468861 steps/sec)\n",
      "Step #4162\tEpoch   1 Batch 1036/3125   Loss: 0.926167 mae: 0.747175 (1935.82070264183 steps/sec)\n",
      "Step #4163\tEpoch   1 Batch 1037/3125   Loss: 0.702751 mae: 0.690916 (2092.7363263513985 steps/sec)\n",
      "Step #4164\tEpoch   1 Batch 1038/3125   Loss: 0.919971 mae: 0.754368 (2265.942021155903 steps/sec)\n",
      "Step #4165\tEpoch   1 Batch 1039/3125   Loss: 0.848580 mae: 0.710568 (2198.1112496986593 steps/sec)\n",
      "Step #4166\tEpoch   1 Batch 1040/3125   Loss: 0.705040 mae: 0.682466 (2280.603767019009 steps/sec)\n",
      "Step #4167\tEpoch   1 Batch 1041/3125   Loss: 0.998321 mae: 0.775902 (2097.7603505016455 steps/sec)\n",
      "Step #4168\tEpoch   1 Batch 1042/3125   Loss: 0.825252 mae: 0.695384 (2202.9370364923634 steps/sec)\n",
      "Step #4169\tEpoch   1 Batch 1043/3125   Loss: 0.848251 mae: 0.725778 (1927.7420303709969 steps/sec)\n",
      "Step #4170\tEpoch   1 Batch 1044/3125   Loss: 0.860202 mae: 0.744681 (1723.2994231432938 steps/sec)\n",
      "Step #4171\tEpoch   1 Batch 1045/3125   Loss: 1.018716 mae: 0.796946 (2091.171250224358 steps/sec)\n",
      "Step #4172\tEpoch   1 Batch 1046/3125   Loss: 0.785725 mae: 0.708118 (2293.3729933073796 steps/sec)\n",
      "Step #4173\tEpoch   1 Batch 1047/3125   Loss: 0.818280 mae: 0.723177 (2109.153081031067 steps/sec)\n",
      "Step #4174\tEpoch   1 Batch 1048/3125   Loss: 0.803031 mae: 0.709309 (2077.4371217149255 steps/sec)\n",
      "Step #4175\tEpoch   1 Batch 1049/3125   Loss: 0.909720 mae: 0.760286 (2116.7317688619733 steps/sec)\n",
      "Step #4176\tEpoch   1 Batch 1050/3125   Loss: 0.805264 mae: 0.727532 (2151.675456056471 steps/sec)\n",
      "Step #4177\tEpoch   1 Batch 1051/3125   Loss: 0.869850 mae: 0.752191 (2205.346288935159 steps/sec)\n",
      "Step #4178\tEpoch   1 Batch 1052/3125   Loss: 0.754781 mae: 0.672169 (2042.7932710571688 steps/sec)\n",
      "Step #4179\tEpoch   1 Batch 1053/3125   Loss: 0.897016 mae: 0.755751 (1932.360312546071 steps/sec)\n",
      "Step #4180\tEpoch   1 Batch 1054/3125   Loss: 0.771015 mae: 0.716406 (2149.249815528409 steps/sec)\n",
      "Step #4181\tEpoch   1 Batch 1055/3125   Loss: 0.826125 mae: 0.734632 (2158.229906349696 steps/sec)\n",
      "Step #4182\tEpoch   1 Batch 1056/3125   Loss: 0.998001 mae: 0.805887 (2173.800194871157 steps/sec)\n",
      "Step #4183\tEpoch   1 Batch 1057/3125   Loss: 0.871555 mae: 0.749408 (1892.1558366566217 steps/sec)\n",
      "Step #4184\tEpoch   1 Batch 1058/3125   Loss: 0.862503 mae: 0.741076 (2227.0798377333645 steps/sec)\n",
      "Step #4185\tEpoch   1 Batch 1059/3125   Loss: 0.750162 mae: 0.699481 (2261.1047019374873 steps/sec)\n",
      "Step #4186\tEpoch   1 Batch 1060/3125   Loss: 0.911530 mae: 0.754287 (2026.8408895417951 steps/sec)\n",
      "Step #4187\tEpoch   1 Batch 1061/3125   Loss: 0.806498 mae: 0.689055 (1805.7725425363367 steps/sec)\n",
      "Step #4188\tEpoch   1 Batch 1062/3125   Loss: 0.758274 mae: 0.692766 (1952.1283824665593 steps/sec)\n",
      "Step #4189\tEpoch   1 Batch 1063/3125   Loss: 0.656849 mae: 0.641102 (2231.036500388302 steps/sec)\n",
      "Step #4190\tEpoch   1 Batch 1064/3125   Loss: 0.690826 mae: 0.664641 (2296.738582849633 steps/sec)\n",
      "Step #4191\tEpoch   1 Batch 1065/3125   Loss: 0.854709 mae: 0.728170 (2060.6779994104354 steps/sec)\n",
      "Step #4192\tEpoch   1 Batch 1066/3125   Loss: 0.762806 mae: 0.697520 (1964.7844702399354 steps/sec)\n",
      "Step #4193\tEpoch   1 Batch 1067/3125   Loss: 0.850028 mae: 0.732006 (2083.422247389702 steps/sec)\n",
      "Step #4194\tEpoch   1 Batch 1068/3125   Loss: 0.853646 mae: 0.753218 (2302.058200419324 steps/sec)\n",
      "Step #4195\tEpoch   1 Batch 1069/3125   Loss: 0.853981 mae: 0.719522 (2295.808291460037 steps/sec)\n",
      "Step #4196\tEpoch   1 Batch 1070/3125   Loss: 0.824280 mae: 0.724838 (1677.1849008317338 steps/sec)\n",
      "Step #4197\tEpoch   1 Batch 1071/3125   Loss: 0.771506 mae: 0.692152 (2115.4933271463588 steps/sec)\n",
      "Step #4198\tEpoch   1 Batch 1072/3125   Loss: 0.798869 mae: 0.697770 (2078.4665853972783 steps/sec)\n",
      "Step #4199\tEpoch   1 Batch 1073/3125   Loss: 0.670668 mae: 0.650536 (2237.3439733714554 steps/sec)\n",
      "Step #4200\tEpoch   1 Batch 1074/3125   Loss: 0.848465 mae: 0.727553 (2195.487903183593 steps/sec)\n",
      "Step #4201\tEpoch   1 Batch 1075/3125   Loss: 0.802019 mae: 0.722389 (2173.349638319481 steps/sec)\n",
      "Step #4202\tEpoch   1 Batch 1076/3125   Loss: 0.776029 mae: 0.703050 (1993.3957511525118 steps/sec)\n",
      "Step #4203\tEpoch   1 Batch 1077/3125   Loss: 0.766168 mae: 0.677237 (2078.8374421347926 steps/sec)\n",
      "Step #4204\tEpoch   1 Batch 1078/3125   Loss: 0.884483 mae: 0.741631 (2234.745266029432 steps/sec)\n",
      "Step #4205\tEpoch   1 Batch 1079/3125   Loss: 0.846195 mae: 0.722259 (1858.880675069581 steps/sec)\n",
      "Step #4206\tEpoch   1 Batch 1080/3125   Loss: 0.859725 mae: 0.734723 (1995.0645471236812 steps/sec)\n",
      "Step #4207\tEpoch   1 Batch 1081/3125   Loss: 0.988898 mae: 0.786517 (2115.6213746002604 steps/sec)\n",
      "Step #4208\tEpoch   1 Batch 1082/3125   Loss: 0.948383 mae: 0.785512 (2176.778559714351 steps/sec)\n",
      "Step #4209\tEpoch   1 Batch 1083/3125   Loss: 0.765413 mae: 0.683093 (2103.842218254048 steps/sec)\n",
      "Step #4210\tEpoch   1 Batch 1084/3125   Loss: 0.859515 mae: 0.705212 (2211.299268225817 steps/sec)\n",
      "Step #4211\tEpoch   1 Batch 1085/3125   Loss: 0.755208 mae: 0.687669 (2125.764794128976 steps/sec)\n",
      "Step #4212\tEpoch   1 Batch 1086/3125   Loss: 0.905185 mae: 0.738453 (2447.399316131592 steps/sec)\n",
      "Step #4213\tEpoch   1 Batch 1087/3125   Loss: 0.801009 mae: 0.704276 (2200.8101584636374 steps/sec)\n",
      "Step #4214\tEpoch   1 Batch 1088/3125   Loss: 0.830445 mae: 0.750424 (1856.1659718718745 steps/sec)\n",
      "Step #4215\tEpoch   1 Batch 1089/3125   Loss: 0.837158 mae: 0.732819 (1887.01410883962 steps/sec)\n",
      "Step #4216\tEpoch   1 Batch 1090/3125   Loss: 0.913353 mae: 0.740774 (2011.9846881505857 steps/sec)\n",
      "Step #4217\tEpoch   1 Batch 1091/3125   Loss: 0.925539 mae: 0.769175 (2189.436759409093 steps/sec)\n",
      "Step #4218\tEpoch   1 Batch 1092/3125   Loss: 0.814637 mae: 0.715696 (2115.066614222464 steps/sec)\n",
      "Step #4219\tEpoch   1 Batch 1093/3125   Loss: 0.853820 mae: 0.725848 (2036.5839920756696 steps/sec)\n",
      "Step #4220\tEpoch   1 Batch 1094/3125   Loss: 0.845134 mae: 0.725678 (1982.2976728358885 steps/sec)\n",
      "Step #4221\tEpoch   1 Batch 1095/3125   Loss: 0.775049 mae: 0.699106 (1998.4105354437256 steps/sec)\n",
      "Step #4222\tEpoch   1 Batch 1096/3125   Loss: 0.724573 mae: 0.671740 (2079.1465905260443 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4223\tEpoch   1 Batch 1097/3125   Loss: 0.960034 mae: 0.752272 (1832.4540172135087 steps/sec)\n",
      "Step #4224\tEpoch   1 Batch 1098/3125   Loss: 0.968355 mae: 0.785462 (1967.6418156912048 steps/sec)\n",
      "Step #4225\tEpoch   1 Batch 1099/3125   Loss: 0.804115 mae: 0.731766 (2148.8754316395643 steps/sec)\n",
      "Step #4226\tEpoch   1 Batch 1100/3125   Loss: 0.791872 mae: 0.698448 (2212.8625845459055 steps/sec)\n",
      "Step #4227\tEpoch   1 Batch 1101/3125   Loss: 0.893590 mae: 0.733673 (2148.8093774335016 steps/sec)\n",
      "Step #4228\tEpoch   1 Batch 1102/3125   Loss: 0.989886 mae: 0.789598 (2056.9993722536096 steps/sec)\n",
      "Step #4229\tEpoch   1 Batch 1103/3125   Loss: 0.947723 mae: 0.767563 (2122.258315876823 steps/sec)\n",
      "Step #4230\tEpoch   1 Batch 1104/3125   Loss: 0.913207 mae: 0.754775 (2078.9198727162784 steps/sec)\n",
      "Step #4231\tEpoch   1 Batch 1105/3125   Loss: 0.709107 mae: 0.660371 (1984.2107255042954 steps/sec)\n",
      "Step #4232\tEpoch   1 Batch 1106/3125   Loss: 0.909498 mae: 0.733632 (1743.0221830663995 steps/sec)\n",
      "Step #4233\tEpoch   1 Batch 1107/3125   Loss: 0.790976 mae: 0.709644 (2114.1923906687903 steps/sec)\n",
      "Step #4234\tEpoch   1 Batch 1108/3125   Loss: 0.965369 mae: 0.791735 (2166.0094401008046 steps/sec)\n",
      "Step #4235\tEpoch   1 Batch 1109/3125   Loss: 0.783966 mae: 0.700576 (2162.1461121306475 steps/sec)\n",
      "Step #4236\tEpoch   1 Batch 1110/3125   Loss: 0.943482 mae: 0.791967 (2168.54034826488 steps/sec)\n",
      "Step #4237\tEpoch   1 Batch 1111/3125   Loss: 0.788187 mae: 0.705681 (2068.5032302608865 steps/sec)\n",
      "Step #4238\tEpoch   1 Batch 1112/3125   Loss: 0.794634 mae: 0.711951 (2023.8677488153946 steps/sec)\n",
      "Step #4239\tEpoch   1 Batch 1113/3125   Loss: 0.883900 mae: 0.759612 (2124.946297572245 steps/sec)\n",
      "Step #4240\tEpoch   1 Batch 1114/3125   Loss: 0.781797 mae: 0.702536 (2094.4083250941267 steps/sec)\n",
      "Step #4241\tEpoch   1 Batch 1115/3125   Loss: 0.858672 mae: 0.741152 (1694.860024568436 steps/sec)\n",
      "Step #4242\tEpoch   1 Batch 1116/3125   Loss: 0.884785 mae: 0.729530 (1798.6946043072912 steps/sec)\n",
      "Step #4243\tEpoch   1 Batch 1117/3125   Loss: 0.769320 mae: 0.701905 (2094.0528018532573 steps/sec)\n",
      "Step #4244\tEpoch   1 Batch 1118/3125   Loss: 0.776113 mae: 0.695211 (2042.8131696863434 steps/sec)\n",
      "Step #4245\tEpoch   1 Batch 1119/3125   Loss: 0.802455 mae: 0.677726 (1879.0674336505206 steps/sec)\n",
      "Step #4246\tEpoch   1 Batch 1120/3125   Loss: 0.822103 mae: 0.734743 (2138.2273473424484 steps/sec)\n",
      "Step #4247\tEpoch   1 Batch 1121/3125   Loss: 0.846543 mae: 0.745364 (1999.9923706345726 steps/sec)\n",
      "Step #4248\tEpoch   1 Batch 1122/3125   Loss: 0.829184 mae: 0.717050 (1940.7651446445427 steps/sec)\n",
      "Step #4249\tEpoch   1 Batch 1123/3125   Loss: 0.643858 mae: 0.632895 (1997.6871564789149 steps/sec)\n",
      "Step #4250\tEpoch   1 Batch 1124/3125   Loss: 0.853333 mae: 0.714844 (1932.8233580947817 steps/sec)\n",
      "Step #4251\tEpoch   1 Batch 1125/3125   Loss: 0.785824 mae: 0.693596 (1935.3562200073827 steps/sec)\n",
      "Step #4252\tEpoch   1 Batch 1126/3125   Loss: 0.910953 mae: 0.781788 (2239.4464259018005 steps/sec)\n",
      "Step #4253\tEpoch   1 Batch 1127/3125   Loss: 0.791834 mae: 0.678801 (2074.683181148165 steps/sec)\n",
      "Step #4254\tEpoch   1 Batch 1128/3125   Loss: 0.899930 mae: 0.738596 (2196.5456925896833 steps/sec)\n",
      "Step #4255\tEpoch   1 Batch 1129/3125   Loss: 0.891962 mae: 0.763976 (2226.276008492569 steps/sec)\n",
      "Step #4256\tEpoch   1 Batch 1130/3125   Loss: 0.772279 mae: 0.711330 (2193.191872078309 steps/sec)\n",
      "Step #4257\tEpoch   1 Batch 1131/3125   Loss: 0.762835 mae: 0.680825 (2022.5599876552735 steps/sec)\n",
      "Step #4258\tEpoch   1 Batch 1132/3125   Loss: 0.878825 mae: 0.737694 (1877.0156093369612 steps/sec)\n",
      "Step #4259\tEpoch   1 Batch 1133/3125   Loss: 0.947919 mae: 0.779582 (1995.0455678380488 steps/sec)\n",
      "Step #4260\tEpoch   1 Batch 1134/3125   Loss: 0.740777 mae: 0.691894 (2266.5542658279833 steps/sec)\n",
      "Step #4261\tEpoch   1 Batch 1135/3125   Loss: 0.777564 mae: 0.721691 (2009.5747331301866 steps/sec)\n",
      "Step #4262\tEpoch   1 Batch 1136/3125   Loss: 0.826745 mae: 0.713180 (2072.796639486039 steps/sec)\n",
      "Step #4263\tEpoch   1 Batch 1137/3125   Loss: 0.884809 mae: 0.752382 (1788.08202242401 steps/sec)\n",
      "Step #4264\tEpoch   1 Batch 1138/3125   Loss: 0.815216 mae: 0.717042 (2248.9324511265295 steps/sec)\n",
      "Step #4265\tEpoch   1 Batch 1139/3125   Loss: 0.852128 mae: 0.753585 (2041.4013296862681 steps/sec)\n",
      "Step #4266\tEpoch   1 Batch 1140/3125   Loss: 0.822046 mae: 0.720962 (2031.6121907271424 steps/sec)\n",
      "Step #4267\tEpoch   1 Batch 1141/3125   Loss: 0.928300 mae: 0.754034 (1972.8617121354657 steps/sec)\n",
      "Step #4268\tEpoch   1 Batch 1142/3125   Loss: 0.656827 mae: 0.615797 (1942.1310959233947 steps/sec)\n",
      "Step #4269\tEpoch   1 Batch 1143/3125   Loss: 0.993251 mae: 0.802063 (2038.246671202255 steps/sec)\n",
      "Step #4270\tEpoch   1 Batch 1144/3125   Loss: 0.840604 mae: 0.722941 (2090.2749952655763 steps/sec)\n",
      "Step #4271\tEpoch   1 Batch 1145/3125   Loss: 0.809673 mae: 0.704237 (2057.180973681371 steps/sec)\n",
      "Step #4272\tEpoch   1 Batch 1146/3125   Loss: 0.916626 mae: 0.754934 (2069.2992323328003 steps/sec)\n",
      "Step #4273\tEpoch   1 Batch 1147/3125   Loss: 0.933499 mae: 0.742392 (2131.468645187519 steps/sec)\n",
      "Step #4274\tEpoch   1 Batch 1148/3125   Loss: 1.000668 mae: 0.808417 (2001.366594774111 steps/sec)\n",
      "Step #4275\tEpoch   1 Batch 1149/3125   Loss: 0.760826 mae: 0.703195 (1969.0643631754378 steps/sec)\n",
      "Step #4276\tEpoch   1 Batch 1150/3125   Loss: 0.797479 mae: 0.736235 (1868.8695807155907 steps/sec)\n",
      "Step #4277\tEpoch   1 Batch 1151/3125   Loss: 0.774157 mae: 0.691036 (1807.5935838095484 steps/sec)\n",
      "Step #4278\tEpoch   1 Batch 1152/3125   Loss: 0.873441 mae: 0.768594 (1898.9921673382532 steps/sec)\n",
      "Step #4279\tEpoch   1 Batch 1153/3125   Loss: 0.842006 mae: 0.727292 (2120.6273447058943 steps/sec)\n",
      "Step #4280\tEpoch   1 Batch 1154/3125   Loss: 0.766547 mae: 0.700188 (2116.8385989704248 steps/sec)\n",
      "Step #4281\tEpoch   1 Batch 1155/3125   Loss: 0.875113 mae: 0.734221 (2049.7414795773752 steps/sec)\n",
      "Step #4282\tEpoch   1 Batch 1156/3125   Loss: 0.863908 mae: 0.750854 (2070.2184578631995 steps/sec)\n",
      "Step #4283\tEpoch   1 Batch 1157/3125   Loss: 0.847193 mae: 0.754955 (1894.0869392436846 steps/sec)\n",
      "Step #4284\tEpoch   1 Batch 1158/3125   Loss: 0.807284 mae: 0.722425 (1986.4849248373132 steps/sec)\n",
      "Step #4285\tEpoch   1 Batch 1159/3125   Loss: 0.873457 mae: 0.740312 (2034.4305074551576 steps/sec)\n",
      "Step #4286\tEpoch   1 Batch 1160/3125   Loss: 0.750704 mae: 0.699773 (1958.3262519960033 steps/sec)\n",
      "Step #4287\tEpoch   1 Batch 1161/3125   Loss: 0.842907 mae: 0.731803 (2014.4971806768297 steps/sec)\n",
      "Step #4288\tEpoch   1 Batch 1162/3125   Loss: 0.904923 mae: 0.761157 (2095.6221957970683 steps/sec)\n",
      "Step #4289\tEpoch   1 Batch 1163/3125   Loss: 0.787501 mae: 0.722118 (2011.5987069916453 steps/sec)\n",
      "Step #4290\tEpoch   1 Batch 1164/3125   Loss: 0.829409 mae: 0.696239 (2000.8701293744991 steps/sec)\n",
      "Step #4291\tEpoch   1 Batch 1165/3125   Loss: 0.922307 mae: 0.755507 (2072.3460181625937 steps/sec)\n",
      "Step #4292\tEpoch   1 Batch 1166/3125   Loss: 0.834639 mae: 0.735428 (2101.1020718951627 steps/sec)\n",
      "Step #4293\tEpoch   1 Batch 1167/3125   Loss: 0.833960 mae: 0.719530 (1959.9917755472065 steps/sec)\n",
      "Step #4294\tEpoch   1 Batch 1168/3125   Loss: 0.827377 mae: 0.735569 (2002.9722450382992 steps/sec)\n",
      "Step #4295\tEpoch   1 Batch 1169/3125   Loss: 0.733938 mae: 0.667577 (2065.427040655531 steps/sec)\n",
      "Step #4296\tEpoch   1 Batch 1170/3125   Loss: 0.843093 mae: 0.694766 (1976.915970664203 steps/sec)\n",
      "Step #4297\tEpoch   1 Batch 1171/3125   Loss: 0.827200 mae: 0.724862 (2088.2144421874377 steps/sec)\n",
      "Step #4298\tEpoch   1 Batch 1172/3125   Loss: 0.827554 mae: 0.707734 (1975.9660049183572 steps/sec)\n",
      "Step #4299\tEpoch   1 Batch 1173/3125   Loss: 0.801664 mae: 0.716527 (2126.842724433086 steps/sec)\n",
      "Step #4300\tEpoch   1 Batch 1174/3125   Loss: 0.763606 mae: 0.669813 (2172.1340680283383 steps/sec)\n",
      "Step #4301\tEpoch   1 Batch 1175/3125   Loss: 0.773197 mae: 0.694259 (2229.542216835704 steps/sec)\n",
      "Step #4302\tEpoch   1 Batch 1176/3125   Loss: 0.852720 mae: 0.703935 (1740.8229503025675 steps/sec)\n",
      "Step #4303\tEpoch   1 Batch 1177/3125   Loss: 0.888383 mae: 0.744019 (1718.2728390004097 steps/sec)\n",
      "Step #4304\tEpoch   1 Batch 1178/3125   Loss: 0.817868 mae: 0.707952 (2146.1924985928467 steps/sec)\n",
      "Step #4305\tEpoch   1 Batch 1179/3125   Loss: 0.767591 mae: 0.710975 (2249.7285931901565 steps/sec)\n",
      "Step #4306\tEpoch   1 Batch 1180/3125   Loss: 0.806982 mae: 0.709568 (2050.6033049770217 steps/sec)\n",
      "Step #4307\tEpoch   1 Batch 1181/3125   Loss: 0.837707 mae: 0.737517 (2063.7197402086204 steps/sec)\n",
      "Step #4308\tEpoch   1 Batch 1182/3125   Loss: 0.843156 mae: 0.728764 (2006.901633539719 steps/sec)\n",
      "Step #4309\tEpoch   1 Batch 1183/3125   Loss: 0.765539 mae: 0.680674 (2032.1831060980455 steps/sec)\n",
      "Step #4310\tEpoch   1 Batch 1184/3125   Loss: 0.826685 mae: 0.736765 (2089.775093918468 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4311\tEpoch   1 Batch 1185/3125   Loss: 0.795414 mae: 0.707763 (1824.2132182808232 steps/sec)\n",
      "Step #4312\tEpoch   1 Batch 1186/3125   Loss: 0.872399 mae: 0.759780 (1932.431535881464 steps/sec)\n",
      "Step #4313\tEpoch   1 Batch 1187/3125   Loss: 0.711665 mae: 0.668768 (2104.8135212172306 steps/sec)\n",
      "Step #4314\tEpoch   1 Batch 1188/3125   Loss: 0.814121 mae: 0.726146 (2083.9398215314905 steps/sec)\n",
      "Step #4315\tEpoch   1 Batch 1189/3125   Loss: 0.697212 mae: 0.661839 (2115.0452830949835 steps/sec)\n",
      "Step #4316\tEpoch   1 Batch 1190/3125   Loss: 0.887096 mae: 0.732415 (2061.346412809499 steps/sec)\n",
      "Step #4317\tEpoch   1 Batch 1191/3125   Loss: 0.834177 mae: 0.710974 (2205.949425674254 steps/sec)\n",
      "Step #4318\tEpoch   1 Batch 1192/3125   Loss: 0.890037 mae: 0.720808 (2170.627749314289 steps/sec)\n",
      "Step #4319\tEpoch   1 Batch 1193/3125   Loss: 0.749150 mae: 0.662974 (2133.6371960524975 steps/sec)\n",
      "Step #4320\tEpoch   1 Batch 1194/3125   Loss: 0.910556 mae: 0.747641 (1968.8055652043297 steps/sec)\n",
      "Step #4321\tEpoch   1 Batch 1195/3125   Loss: 0.973660 mae: 0.797451 (1846.0357560979903 steps/sec)\n",
      "Step #4322\tEpoch   1 Batch 1196/3125   Loss: 0.765924 mae: 0.683533 (2096.8165093585026 steps/sec)\n",
      "Step #4323\tEpoch   1 Batch 1197/3125   Loss: 0.918673 mae: 0.775494 (1816.0147556740935 steps/sec)\n",
      "Step #4324\tEpoch   1 Batch 1198/3125   Loss: 0.801721 mae: 0.686719 (2133.463549614438 steps/sec)\n",
      "Step #4325\tEpoch   1 Batch 1199/3125   Loss: 0.820648 mae: 0.708053 (2094.9313727448903 steps/sec)\n",
      "Step #4326\tEpoch   1 Batch 1200/3125   Loss: 0.936691 mae: 0.764427 (2046.3213769954334 steps/sec)\n",
      "Step #4327\tEpoch   1 Batch 1201/3125   Loss: 0.875977 mae: 0.723315 (2141.3275881433983 steps/sec)\n",
      "Step #4328\tEpoch   1 Batch 1202/3125   Loss: 0.930607 mae: 0.793894 (1935.4276643656096 steps/sec)\n",
      "Step #4329\tEpoch   1 Batch 1203/3125   Loss: 0.809656 mae: 0.734039 (1994.5712030282568 steps/sec)\n",
      "Step #4330\tEpoch   1 Batch 1204/3125   Loss: 0.910377 mae: 0.773919 (1808.4508985547238 steps/sec)\n",
      "Step #4331\tEpoch   1 Batch 1205/3125   Loss: 0.815893 mae: 0.720845 (1766.602926434787 steps/sec)\n",
      "Step #4332\tEpoch   1 Batch 1206/3125   Loss: 0.842184 mae: 0.735488 (2094.5965921575676 steps/sec)\n",
      "Step #4333\tEpoch   1 Batch 1207/3125   Loss: 0.881786 mae: 0.716065 (2090.0458441299584 steps/sec)\n",
      "Step #4334\tEpoch   1 Batch 1208/3125   Loss: 0.855633 mae: 0.718508 (2168.360974399272 steps/sec)\n",
      "Step #4335\tEpoch   1 Batch 1209/3125   Loss: 0.860642 mae: 0.722167 (2033.0302266514143 steps/sec)\n",
      "Step #4336\tEpoch   1 Batch 1210/3125   Loss: 0.770222 mae: 0.688078 (2150.0651021642625 steps/sec)\n",
      "Step #4337\tEpoch   1 Batch 1211/3125   Loss: 0.709618 mae: 0.666350 (1913.809089249863 steps/sec)\n",
      "Step #4338\tEpoch   1 Batch 1212/3125   Loss: 0.869396 mae: 0.731377 (2067.137168316051 steps/sec)\n",
      "Step #4339\tEpoch   1 Batch 1213/3125   Loss: 0.895853 mae: 0.748542 (2261.958280302867 steps/sec)\n",
      "Step #4340\tEpoch   1 Batch 1214/3125   Loss: 0.889074 mae: 0.771772 (2126.0880584758565 steps/sec)\n",
      "Step #4341\tEpoch   1 Batch 1215/3125   Loss: 0.668009 mae: 0.652991 (1946.8186628543845 steps/sec)\n",
      "Step #4342\tEpoch   1 Batch 1216/3125   Loss: 0.843776 mae: 0.708075 (2170.403104786546 steps/sec)\n",
      "Step #4343\tEpoch   1 Batch 1217/3125   Loss: 0.794066 mae: 0.698506 (2164.5116010238626 steps/sec)\n",
      "Step #4344\tEpoch   1 Batch 1218/3125   Loss: 0.814917 mae: 0.684994 (2196.338653596414 steps/sec)\n",
      "Step #4345\tEpoch   1 Batch 1219/3125   Loss: 1.000862 mae: 0.805073 (1726.505746369414 steps/sec)\n",
      "Step #4346\tEpoch   1 Batch 1220/3125   Loss: 0.884296 mae: 0.732749 (1974.5706538113889 steps/sec)\n",
      "Step #4347\tEpoch   1 Batch 1221/3125   Loss: 0.852955 mae: 0.737999 (2213.680121601081 steps/sec)\n",
      "Step #4348\tEpoch   1 Batch 1222/3125   Loss: 0.726629 mae: 0.653523 (2069.789384339038 steps/sec)\n",
      "Step #4349\tEpoch   1 Batch 1223/3125   Loss: 0.595538 mae: 0.640072 (2050.2825411102203 steps/sec)\n",
      "Step #4350\tEpoch   1 Batch 1224/3125   Loss: 0.768457 mae: 0.697299 (2306.6918914162525 steps/sec)\n",
      "Step #4351\tEpoch   1 Batch 1225/3125   Loss: 0.851989 mae: 0.736384 (2310.0967152078606 steps/sec)\n",
      "Step #4352\tEpoch   1 Batch 1226/3125   Loss: 0.839070 mae: 0.725426 (2297.141104563279 steps/sec)\n",
      "Step #4353\tEpoch   1 Batch 1227/3125   Loss: 0.898843 mae: 0.761778 (2083.856992388561 steps/sec)\n",
      "Step #4354\tEpoch   1 Batch 1228/3125   Loss: 0.895975 mae: 0.760506 (1953.5649743828599 steps/sec)\n",
      "Step #4355\tEpoch   1 Batch 1229/3125   Loss: 0.748592 mae: 0.693422 (2014.8262014103723 steps/sec)\n",
      "Step #4356\tEpoch   1 Batch 1230/3125   Loss: 0.731509 mae: 0.671621 (2172.989327530826 steps/sec)\n",
      "Step #4357\tEpoch   1 Batch 1231/3125   Loss: 0.790956 mae: 0.704538 (2020.53337444119 steps/sec)\n",
      "Step #4358\tEpoch   1 Batch 1232/3125   Loss: 0.833363 mae: 0.699110 (2117.2019020120542 steps/sec)\n",
      "Step #4359\tEpoch   1 Batch 1233/3125   Loss: 0.796030 mae: 0.695709 (2128.591293403571 steps/sec)\n",
      "Step #4360\tEpoch   1 Batch 1234/3125   Loss: 0.866460 mae: 0.721751 (1961.4950054248195 steps/sec)\n",
      "Step #4361\tEpoch   1 Batch 1235/3125   Loss: 0.944861 mae: 0.770799 (2058.129857894324 steps/sec)\n",
      "Step #4362\tEpoch   1 Batch 1236/3125   Loss: 0.783122 mae: 0.710378 (2181.8968746098462 steps/sec)\n",
      "Step #4363\tEpoch   1 Batch 1237/3125   Loss: 0.959174 mae: 0.793041 (2127.4900075070504 steps/sec)\n",
      "Step #4364\tEpoch   1 Batch 1238/3125   Loss: 0.958738 mae: 0.757466 (1972.0083501024956 steps/sec)\n",
      "Step #4365\tEpoch   1 Batch 1239/3125   Loss: 0.883697 mae: 0.758273 (2278.547137626441 steps/sec)\n",
      "Step #4366\tEpoch   1 Batch 1240/3125   Loss: 0.778652 mae: 0.695271 (2320.294745693327 steps/sec)\n",
      "Step #4367\tEpoch   1 Batch 1241/3125   Loss: 0.836854 mae: 0.726513 (2249.101282656257 steps/sec)\n",
      "Step #4368\tEpoch   1 Batch 1242/3125   Loss: 0.810645 mae: 0.735972 (2199.748258789964 steps/sec)\n",
      "Step #4369\tEpoch   1 Batch 1243/3125   Loss: 0.876512 mae: 0.732111 (2053.534918334574 steps/sec)\n",
      "Step #4370\tEpoch   1 Batch 1244/3125   Loss: 0.914253 mae: 0.725529 (2029.5281229435218 steps/sec)\n",
      "Step #4371\tEpoch   1 Batch 1245/3125   Loss: 0.987127 mae: 0.789704 (2173.146948799519 steps/sec)\n",
      "Step #4372\tEpoch   1 Batch 1246/3125   Loss: 0.816134 mae: 0.709999 (2045.2437145253466 steps/sec)\n",
      "Step #4373\tEpoch   1 Batch 1247/3125   Loss: 0.974340 mae: 0.806047 (2047.5001220405175 steps/sec)\n",
      "Step #4374\tEpoch   1 Batch 1248/3125   Loss: 0.829046 mae: 0.725703 (2268.4175229853977 steps/sec)\n",
      "Step #4375\tEpoch   1 Batch 1249/3125   Loss: 0.945096 mae: 0.771657 (2326.627245193428 steps/sec)\n",
      "Step #4376\tEpoch   1 Batch 1250/3125   Loss: 0.838162 mae: 0.713161 (2218.950175111892 steps/sec)\n",
      "Step #4377\tEpoch   1 Batch 1251/3125   Loss: 0.881308 mae: 0.753790 (2131.1870573051633 steps/sec)\n",
      "Step #4378\tEpoch   1 Batch 1252/3125   Loss: 0.805319 mae: 0.713723 (2179.4253052740974 steps/sec)\n",
      "Step #4379\tEpoch   1 Batch 1253/3125   Loss: 1.007320 mae: 0.795580 (2134.3103430728993 steps/sec)\n",
      "Step #4380\tEpoch   1 Batch 1254/3125   Loss: 0.928991 mae: 0.754305 (2024.8643429564545 steps/sec)\n",
      "Step #4381\tEpoch   1 Batch 1255/3125   Loss: 0.859526 mae: 0.724282 (1695.20252847362 steps/sec)\n",
      "Step #4382\tEpoch   1 Batch 1256/3125   Loss: 0.857388 mae: 0.741654 (1634.0977270779276 steps/sec)\n",
      "Step #4383\tEpoch   1 Batch 1257/3125   Loss: 1.098388 mae: 0.788619 (2040.9841170974773 steps/sec)\n",
      "Step #4384\tEpoch   1 Batch 1258/3125   Loss: 0.827355 mae: 0.726838 (1864.5660330387468 steps/sec)\n",
      "Step #4385\tEpoch   1 Batch 1259/3125   Loss: 0.846779 mae: 0.732230 (2058.352063601119 steps/sec)\n",
      "Step #4386\tEpoch   1 Batch 1260/3125   Loss: 0.816532 mae: 0.737492 (1945.9154511375868 steps/sec)\n",
      "Step #4387\tEpoch   1 Batch 1261/3125   Loss: 0.791527 mae: 0.704636 (1956.0430540787584 steps/sec)\n",
      "Step #4388\tEpoch   1 Batch 1262/3125   Loss: 0.873346 mae: 0.740609 (2081.0034135111537 steps/sec)\n",
      "Step #4389\tEpoch   1 Batch 1263/3125   Loss: 0.740032 mae: 0.683742 (1688.500990322217 steps/sec)\n",
      "Step #4390\tEpoch   1 Batch 1264/3125   Loss: 0.848480 mae: 0.717964 (1849.0468885009434 steps/sec)\n",
      "Step #4391\tEpoch   1 Batch 1265/3125   Loss: 0.797067 mae: 0.712102 (2048.500122100122 steps/sec)\n",
      "Step #4392\tEpoch   1 Batch 1266/3125   Loss: 0.811697 mae: 0.719248 (2064.8372963127063 steps/sec)\n",
      "Step #4393\tEpoch   1 Batch 1267/3125   Loss: 0.809901 mae: 0.716202 (2017.2681800692574 steps/sec)\n",
      "Step #4394\tEpoch   1 Batch 1268/3125   Loss: 0.942008 mae: 0.749391 (2014.8068442745011 steps/sec)\n",
      "Step #4395\tEpoch   1 Batch 1269/3125   Loss: 0.841631 mae: 0.703108 (2003.1444318149256 steps/sec)\n",
      "Step #4396\tEpoch   1 Batch 1270/3125   Loss: 0.852266 mae: 0.735732 (2070.8317287278687 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4397\tEpoch   1 Batch 1271/3125   Loss: 0.967517 mae: 0.771000 (1922.1939102857875 steps/sec)\n",
      "Step #4398\tEpoch   1 Batch 1272/3125   Loss: 0.904185 mae: 0.758555 (1900.4893609308733 steps/sec)\n",
      "Step #4399\tEpoch   1 Batch 1273/3125   Loss: 0.765151 mae: 0.705594 (1738.5860193659637 steps/sec)\n",
      "Step #4400\tEpoch   1 Batch 1274/3125   Loss: 0.953151 mae: 0.756699 (1749.171768395416 steps/sec)\n",
      "Step #4401\tEpoch   1 Batch 1275/3125   Loss: 0.844182 mae: 0.732842 (2010.460924917555 steps/sec)\n",
      "Step #4402\tEpoch   1 Batch 1276/3125   Loss: 0.885942 mae: 0.742713 (1927.1397327746229 steps/sec)\n",
      "Step #4403\tEpoch   1 Batch 1277/3125   Loss: 0.935168 mae: 0.787486 (2100.3024536805206 steps/sec)\n",
      "Step #4404\tEpoch   1 Batch 1278/3125   Loss: 0.881565 mae: 0.741342 (2076.922772198784 steps/sec)\n",
      "Step #4405\tEpoch   1 Batch 1279/3125   Loss: 0.968310 mae: 0.773163 (1773.6850562852576 steps/sec)\n",
      "Step #4406\tEpoch   1 Batch 1280/3125   Loss: 0.874010 mae: 0.762928 (1761.1878128254225 steps/sec)\n",
      "Step #4407\tEpoch   1 Batch 1281/3125   Loss: 0.911479 mae: 0.737601 (1801.47578019637 steps/sec)\n",
      "Step #4408\tEpoch   1 Batch 1282/3125   Loss: 0.767605 mae: 0.720296 (2010.7693487765591 steps/sec)\n",
      "Step #4409\tEpoch   1 Batch 1283/3125   Loss: 0.727506 mae: 0.683158 (1959.6254835635127 steps/sec)\n",
      "Step #4410\tEpoch   1 Batch 1284/3125   Loss: 0.882576 mae: 0.749709 (2076.26477634992 steps/sec)\n",
      "Step #4411\tEpoch   1 Batch 1285/3125   Loss: 0.893560 mae: 0.759715 (1921.1549912514542 steps/sec)\n",
      "Step #4412\tEpoch   1 Batch 1286/3125   Loss: 0.755184 mae: 0.702779 (2054.0578659719095 steps/sec)\n",
      "Step #4413\tEpoch   1 Batch 1287/3125   Loss: 0.773355 mae: 0.698836 (1747.8160134014518 steps/sec)\n",
      "Step #4414\tEpoch   1 Batch 1288/3125   Loss: 0.947795 mae: 0.757551 (1686.5049176109176 steps/sec)\n",
      "Step #4415\tEpoch   1 Batch 1289/3125   Loss: 0.857417 mae: 0.721539 (1892.0875512008517 steps/sec)\n",
      "Step #4416\tEpoch   1 Batch 1290/3125   Loss: 0.760171 mae: 0.715155 (1938.8094335610676 steps/sec)\n",
      "Step #4417\tEpoch   1 Batch 1291/3125   Loss: 0.915055 mae: 0.759389 (1848.7534821397087 steps/sec)\n",
      "Step #4418\tEpoch   1 Batch 1292/3125   Loss: 0.821940 mae: 0.692501 (1782.5952433571902 steps/sec)\n",
      "Step #4419\tEpoch   1 Batch 1293/3125   Loss: 0.867034 mae: 0.733356 (1925.8124649898527 steps/sec)\n",
      "Step #4420\tEpoch   1 Batch 1294/3125   Loss: 0.791588 mae: 0.707948 (1875.2532794434558 steps/sec)\n",
      "Step #4421\tEpoch   1 Batch 1295/3125   Loss: 0.820274 mae: 0.729784 (1770.0323258581545 steps/sec)\n",
      "Step #4422\tEpoch   1 Batch 1296/3125   Loss: 0.736703 mae: 0.693907 (1637.9521224665132 steps/sec)\n",
      "Step #4423\tEpoch   1 Batch 1297/3125   Loss: 0.882025 mae: 0.729413 (1949.9321245932124 steps/sec)\n",
      "Step #4424\tEpoch   1 Batch 1298/3125   Loss: 0.755712 mae: 0.679625 (2185.9906604402936 steps/sec)\n",
      "Step #4425\tEpoch   1 Batch 1299/3125   Loss: 0.815383 mae: 0.714794 (1929.44466934089 steps/sec)\n",
      "Step #4426\tEpoch   1 Batch 1300/3125   Loss: 0.759265 mae: 0.677831 (1946.5114768096976 steps/sec)\n",
      "Step #4427\tEpoch   1 Batch 1301/3125   Loss: 0.779927 mae: 0.700234 (2182.441826582858 steps/sec)\n",
      "Step #4428\tEpoch   1 Batch 1302/3125   Loss: 0.775968 mae: 0.697640 (2077.293078172669 steps/sec)\n",
      "Step #4429\tEpoch   1 Batch 1303/3125   Loss: 0.733854 mae: 0.669762 (1629.8813233956896 steps/sec)\n",
      "Step #4430\tEpoch   1 Batch 1304/3125   Loss: 0.755505 mae: 0.703594 (1126.0299715962478 steps/sec)\n",
      "Step #4431\tEpoch   1 Batch 1305/3125   Loss: 0.728771 mae: 0.681620 (1431.5519301000033 steps/sec)\n",
      "Step #4432\tEpoch   1 Batch 1306/3125   Loss: 0.711702 mae: 0.658134 (1398.0920127199151 steps/sec)\n",
      "Step #4433\tEpoch   1 Batch 1307/3125   Loss: 1.020629 mae: 0.791175 (1033.2324974134108 steps/sec)\n",
      "Step #4434\tEpoch   1 Batch 1308/3125   Loss: 0.852287 mae: 0.705997 (1801.7388913708376 steps/sec)\n",
      "Step #4435\tEpoch   1 Batch 1309/3125   Loss: 0.878466 mae: 0.756763 (1551.216769974999 steps/sec)\n",
      "Step #4436\tEpoch   1 Batch 1310/3125   Loss: 0.799660 mae: 0.700208 (1579.5851347483542 steps/sec)\n",
      "Step #4437\tEpoch   1 Batch 1311/3125   Loss: 0.839303 mae: 0.736292 (1843.2450010986597 steps/sec)\n",
      "Step #4438\tEpoch   1 Batch 1312/3125   Loss: 0.822802 mae: 0.726460 (1767.272849847471 steps/sec)\n",
      "Step #4439\tEpoch   1 Batch 1313/3125   Loss: 0.794993 mae: 0.725750 (2018.0058120513463 steps/sec)\n",
      "Step #4440\tEpoch   1 Batch 1314/3125   Loss: 0.906162 mae: 0.780522 (1864.6157676200976 steps/sec)\n",
      "Step #4441\tEpoch   1 Batch 1315/3125   Loss: 0.928139 mae: 0.768905 (1937.179700345471 steps/sec)\n",
      "Step #4442\tEpoch   1 Batch 1316/3125   Loss: 0.810989 mae: 0.698264 (2002.2264442768353 steps/sec)\n",
      "Step #4443\tEpoch   1 Batch 1317/3125   Loss: 0.718445 mae: 0.675644 (1710.5644371941273 steps/sec)\n",
      "Step #4444\tEpoch   1 Batch 1318/3125   Loss: 0.861497 mae: 0.719127 (1758.9572831657258 steps/sec)\n",
      "Step #4445\tEpoch   1 Batch 1319/3125   Loss: 0.856324 mae: 0.729897 (1914.4729875298972 steps/sec)\n",
      "Step #4446\tEpoch   1 Batch 1320/3125   Loss: 0.796864 mae: 0.677600 (2029.8227784391727 steps/sec)\n",
      "Step #4447\tEpoch   1 Batch 1321/3125   Loss: 0.858311 mae: 0.754904 (1701.0739430907497 steps/sec)\n",
      "Step #4448\tEpoch   1 Batch 1322/3125   Loss: 0.691343 mae: 0.660396 (1936.8576600539363 steps/sec)\n",
      "Step #4449\tEpoch   1 Batch 1323/3125   Loss: 0.839964 mae: 0.711682 (1954.9490090795532 steps/sec)\n",
      "Step #4450\tEpoch   1 Batch 1324/3125   Loss: 0.796447 mae: 0.717600 (2067.3205642577605 steps/sec)\n",
      "Step #4451\tEpoch   1 Batch 1325/3125   Loss: 0.774201 mae: 0.678847 (1779.554846538308 steps/sec)\n",
      "Step #4452\tEpoch   1 Batch 1326/3125   Loss: 0.795075 mae: 0.698086 (1691.7427640282663 steps/sec)\n",
      "Step #4453\tEpoch   1 Batch 1327/3125   Loss: 0.719505 mae: 0.669779 (1813.7060227626525 steps/sec)\n",
      "Step #4454\tEpoch   1 Batch 1328/3125   Loss: 0.878501 mae: 0.742294 (1875.4880655344798 steps/sec)\n",
      "Step #4455\tEpoch   1 Batch 1329/3125   Loss: 0.762146 mae: 0.709435 (1972.063981644302 steps/sec)\n",
      "Step #4456\tEpoch   1 Batch 1330/3125   Loss: 0.949165 mae: 0.773076 (2059.746994578455 steps/sec)\n",
      "Step #4457\tEpoch   1 Batch 1331/3125   Loss: 0.654723 mae: 0.624629 (719.3828231928403 steps/sec)\n",
      "Step #4458\tEpoch   1 Batch 1332/3125   Loss: 0.857108 mae: 0.744002 (1016.9784786677917 steps/sec)\n",
      "Step #4459\tEpoch   1 Batch 1333/3125   Loss: 0.726906 mae: 0.662091 (1654.1532248523044 steps/sec)\n",
      "Step #4460\tEpoch   1 Batch 1334/3125   Loss: 0.697921 mae: 0.655772 (2008.9202237719364 steps/sec)\n",
      "Step #4461\tEpoch   1 Batch 1335/3125   Loss: 0.844085 mae: 0.736008 (1707.764594750857 steps/sec)\n",
      "Step #4462\tEpoch   1 Batch 1336/3125   Loss: 0.796015 mae: 0.722490 (1601.2949925934975 steps/sec)\n",
      "Step #4463\tEpoch   1 Batch 1337/3125   Loss: 0.712734 mae: 0.662062 (2019.152152354544 steps/sec)\n",
      "Step #4464\tEpoch   1 Batch 1338/3125   Loss: 0.802103 mae: 0.734764 (1600.9771589104678 steps/sec)\n",
      "Step #4465\tEpoch   1 Batch 1339/3125   Loss: 0.908107 mae: 0.771775 (1707.0000651168848 steps/sec)\n",
      "Step #4466\tEpoch   1 Batch 1340/3125   Loss: 0.740112 mae: 0.699823 (1749.6679459369263 steps/sec)\n",
      "Step #4467\tEpoch   1 Batch 1341/3125   Loss: 0.879185 mae: 0.730922 (1888.084412953643 steps/sec)\n",
      "Step #4468\tEpoch   1 Batch 1342/3125   Loss: 0.844535 mae: 0.730695 (2070.484163968091 steps/sec)\n",
      "Step #4469\tEpoch   1 Batch 1343/3125   Loss: 0.725158 mae: 0.670007 (1926.785615846824 steps/sec)\n",
      "Step #4470\tEpoch   1 Batch 1344/3125   Loss: 0.752565 mae: 0.685266 (1815.999030151885 steps/sec)\n",
      "Step #4471\tEpoch   1 Batch 1345/3125   Loss: 0.862434 mae: 0.745603 (1902.3684902802095 steps/sec)\n",
      "Step #4472\tEpoch   1 Batch 1346/3125   Loss: 0.772219 mae: 0.710078 (1673.4643068034918 steps/sec)\n",
      "Step #4473\tEpoch   1 Batch 1347/3125   Loss: 0.813725 mae: 0.724026 (1947.7950737452168 steps/sec)\n",
      "Step #4474\tEpoch   1 Batch 1348/3125   Loss: 0.780901 mae: 0.727328 (1949.098479497379 steps/sec)\n",
      "Step #4475\tEpoch   1 Batch 1349/3125   Loss: 0.858087 mae: 0.751210 (1848.6394048112268 steps/sec)\n",
      "Step #4476\tEpoch   1 Batch 1350/3125   Loss: 0.791496 mae: 0.677631 (2095.2243935579268 steps/sec)\n",
      "Step #4477\tEpoch   1 Batch 1351/3125   Loss: 0.869121 mae: 0.741453 (1971.2854255769141 steps/sec)\n",
      "Step #4478\tEpoch   1 Batch 1352/3125   Loss: 0.840654 mae: 0.738432 (2040.3089914968964 steps/sec)\n",
      "Step #4479\tEpoch   1 Batch 1353/3125   Loss: 0.697182 mae: 0.654554 (2182.260145681582 steps/sec)\n",
      "Step #4480\tEpoch   1 Batch 1354/3125   Loss: 0.729440 mae: 0.675276 (2106.3990920139413 steps/sec)\n",
      "Step #4481\tEpoch   1 Batch 1355/3125   Loss: 0.855016 mae: 0.725201 (1913.4423955985803 steps/sec)\n",
      "Step #4482\tEpoch   1 Batch 1356/3125   Loss: 0.903863 mae: 0.753210 (1982.8599523467342 steps/sec)\n",
      "Step #4483\tEpoch   1 Batch 1357/3125   Loss: 0.784345 mae: 0.712652 (1907.5075949136817 steps/sec)\n",
      "Step #4484\tEpoch   1 Batch 1358/3125   Loss: 1.040271 mae: 0.795792 (2118.378149053516 steps/sec)\n",
      "Step #4485\tEpoch   1 Batch 1359/3125   Loss: 1.002564 mae: 0.800609 (2050.021994349896 steps/sec)\n",
      "Step #4486\tEpoch   1 Batch 1360/3125   Loss: 0.742259 mae: 0.689564 (1949.0079088484308 steps/sec)\n",
      "Step #4487\tEpoch   1 Batch 1361/3125   Loss: 0.795960 mae: 0.719258 (2091.3380801372186 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4488\tEpoch   1 Batch 1362/3125   Loss: 0.844566 mae: 0.728755 (1994.8178445733854 steps/sec)\n",
      "Step #4489\tEpoch   1 Batch 1363/3125   Loss: 0.993289 mae: 0.779472 (1765.9037706933427 steps/sec)\n",
      "Step #4490\tEpoch   1 Batch 1364/3125   Loss: 0.845934 mae: 0.728147 (1563.5219563110415 steps/sec)\n",
      "Step #4491\tEpoch   1 Batch 1365/3125   Loss: 0.745145 mae: 0.697209 (1552.5144172755606 steps/sec)\n",
      "Step #4492\tEpoch   1 Batch 1366/3125   Loss: 0.922895 mae: 0.759695 (1561.8684461391802 steps/sec)\n",
      "Step #4493\tEpoch   1 Batch 1367/3125   Loss: 0.816989 mae: 0.718356 (1470.4679633706826 steps/sec)\n",
      "Step #4494\tEpoch   1 Batch 1368/3125   Loss: 0.905500 mae: 0.745694 (1890.859255252006 steps/sec)\n",
      "Step #4495\tEpoch   1 Batch 1369/3125   Loss: 0.727298 mae: 0.683785 (1821.56711167473 steps/sec)\n",
      "Step #4496\tEpoch   1 Batch 1370/3125   Loss: 0.748827 mae: 0.699621 (1953.201080376269 steps/sec)\n",
      "Step #4497\tEpoch   1 Batch 1371/3125   Loss: 0.829101 mae: 0.726136 (2005.0596120199248 steps/sec)\n",
      "Step #4498\tEpoch   1 Batch 1372/3125   Loss: 0.936033 mae: 0.757848 (1559.2903772659004 steps/sec)\n",
      "Step #4499\tEpoch   1 Batch 1373/3125   Loss: 0.993098 mae: 0.783813 (1588.9561534440043 steps/sec)\n",
      "Step #4500\tEpoch   1 Batch 1374/3125   Loss: 0.800339 mae: 0.680121 (1881.309375364438 steps/sec)\n",
      "Step #4501\tEpoch   1 Batch 1375/3125   Loss: 0.871706 mae: 0.730732 (1689.4667730059373 steps/sec)\n",
      "Step #4502\tEpoch   1 Batch 1376/3125   Loss: 0.831287 mae: 0.703523 (2018.977202711029 steps/sec)\n",
      "Step #4503\tEpoch   1 Batch 1377/3125   Loss: 0.894073 mae: 0.750272 (1807.858485198531 steps/sec)\n",
      "Step #4504\tEpoch   1 Batch 1378/3125   Loss: 0.783015 mae: 0.694596 (2008.766283524904 steps/sec)\n",
      "Step #4505\tEpoch   1 Batch 1379/3125   Loss: 0.870658 mae: 0.743809 (1905.999327450036 steps/sec)\n",
      "Step #4506\tEpoch   1 Batch 1380/3125   Loss: 0.793244 mae: 0.700578 (1941.2322274881517 steps/sec)\n",
      "Step #4507\tEpoch   1 Batch 1381/3125   Loss: 0.873921 mae: 0.770453 (2018.724551186408 steps/sec)\n",
      "Step #4508\tEpoch   1 Batch 1382/3125   Loss: 0.654994 mae: 0.652796 (1751.9335031953553 steps/sec)\n",
      "Step #4509\tEpoch   1 Batch 1383/3125   Loss: 0.870068 mae: 0.737842 (1962.5412927315435 steps/sec)\n",
      "Step #4510\tEpoch   1 Batch 1384/3125   Loss: 0.901894 mae: 0.750180 (1903.6636287716494 steps/sec)\n",
      "Step #4511\tEpoch   1 Batch 1385/3125   Loss: 0.720043 mae: 0.658936 (1864.847897418569 steps/sec)\n",
      "Step #4512\tEpoch   1 Batch 1386/3125   Loss: 0.888301 mae: 0.758252 (2074.765282600738 steps/sec)\n",
      "Step #4513\tEpoch   1 Batch 1387/3125   Loss: 0.774198 mae: 0.693680 (1958.1799676928392 steps/sec)\n",
      "Step #4514\tEpoch   1 Batch 1388/3125   Loss: 0.804797 mae: 0.726461 (2039.2178216859036 steps/sec)\n",
      "Step #4515\tEpoch   1 Batch 1389/3125   Loss: 0.791871 mae: 0.705622 (2266.3338196358136 steps/sec)\n",
      "Step #4516\tEpoch   1 Batch 1390/3125   Loss: 0.858203 mae: 0.722216 (1963.0189173757171 steps/sec)\n",
      "Step #4517\tEpoch   1 Batch 1391/3125   Loss: 0.809645 mae: 0.698010 (2089.5460523693755 steps/sec)\n",
      "Step #4518\tEpoch   1 Batch 1392/3125   Loss: 0.788511 mae: 0.730810 (2230.8466390800686 steps/sec)\n",
      "Step #4519\tEpoch   1 Batch 1393/3125   Loss: 0.776787 mae: 0.693459 (1910.635739144695 steps/sec)\n",
      "Step #4520\tEpoch   1 Batch 1394/3125   Loss: 0.873371 mae: 0.734731 (1927.0688989763476 steps/sec)\n",
      "Step #4521\tEpoch   1 Batch 1395/3125   Loss: 0.822918 mae: 0.715315 (2142.1368743615935 steps/sec)\n",
      "Step #4522\tEpoch   1 Batch 1396/3125   Loss: 0.805203 mae: 0.723907 (1894.3093543375367 steps/sec)\n",
      "Step #4523\tEpoch   1 Batch 1397/3125   Loss: 0.964486 mae: 0.781911 (1985.7701521650617 steps/sec)\n",
      "Step #4524\tEpoch   1 Batch 1398/3125   Loss: 0.867002 mae: 0.746844 (2068.095261574873 steps/sec)\n",
      "Step #4525\tEpoch   1 Batch 1399/3125   Loss: 0.835238 mae: 0.730503 (2049.581219886436 steps/sec)\n",
      "Step #4526\tEpoch   1 Batch 1400/3125   Loss: 0.887582 mae: 0.747462 (2111.7654166834495 steps/sec)\n",
      "Step #4527\tEpoch   1 Batch 1401/3125   Loss: 0.795701 mae: 0.712620 (1812.0448614927334 steps/sec)\n",
      "Step #4528\tEpoch   1 Batch 1402/3125   Loss: 0.733774 mae: 0.698488 (1628.4006025499666 steps/sec)\n",
      "Step #4529\tEpoch   1 Batch 1403/3125   Loss: 0.756148 mae: 0.708383 (2045.582856195316 steps/sec)\n",
      "Step #4530\tEpoch   1 Batch 1404/3125   Loss: 0.871064 mae: 0.749738 (2024.2779922779923 steps/sec)\n",
      "Step #4531\tEpoch   1 Batch 1405/3125   Loss: 0.840349 mae: 0.739735 (2015.3877201918178 steps/sec)\n",
      "Step #4532\tEpoch   1 Batch 1406/3125   Loss: 0.944851 mae: 0.772759 (1986.5037415932557 steps/sec)\n",
      "Step #4533\tEpoch   1 Batch 1407/3125   Loss: 0.931220 mae: 0.782073 (1920.0468761444372 steps/sec)\n",
      "Step #4534\tEpoch   1 Batch 1408/3125   Loss: 0.892433 mae: 0.750737 (2160.097233380714 steps/sec)\n",
      "Step #4535\tEpoch   1 Batch 1409/3125   Loss: 0.818601 mae: 0.694771 (1917.6065031134845 steps/sec)\n",
      "Step #4536\tEpoch   1 Batch 1410/3125   Loss: 0.875812 mae: 0.722369 (1471.2416604112443 steps/sec)\n",
      "Step #4537\tEpoch   1 Batch 1411/3125   Loss: 0.667469 mae: 0.654144 (1133.4608857325074 steps/sec)\n",
      "Step #4538\tEpoch   1 Batch 1412/3125   Loss: 0.966324 mae: 0.771891 (1481.7092471173412 steps/sec)\n",
      "Step #4539\tEpoch   1 Batch 1413/3125   Loss: 0.763201 mae: 0.691081 (1295.9061725648678 steps/sec)\n",
      "Step #4540\tEpoch   1 Batch 1414/3125   Loss: 0.735100 mae: 0.677598 (1446.8405693115415 steps/sec)\n",
      "Step #4541\tEpoch   1 Batch 1415/3125   Loss: 0.813289 mae: 0.724708 (2025.6858048064291 steps/sec)\n",
      "Step #4542\tEpoch   1 Batch 1416/3125   Loss: 0.812976 mae: 0.708224 (1793.7100678258935 steps/sec)\n",
      "Step #4543\tEpoch   1 Batch 1417/3125   Loss: 0.853732 mae: 0.724236 (2002.1499832927586 steps/sec)\n",
      "Step #4544\tEpoch   1 Batch 1418/3125   Loss: 0.902499 mae: 0.726777 (1996.9643010179304 steps/sec)\n",
      "Step #4545\tEpoch   1 Batch 1419/3125   Loss: 0.868693 mae: 0.703825 (2060.49578007251 steps/sec)\n",
      "Step #4546\tEpoch   1 Batch 1420/3125   Loss: 0.826776 mae: 0.725783 (2072.5303395659566 steps/sec)\n",
      "Step #4547\tEpoch   1 Batch 1421/3125   Loss: 0.784837 mae: 0.725865 (1938.3614315291334 steps/sec)\n",
      "Step #4548\tEpoch   1 Batch 1422/3125   Loss: 0.834801 mae: 0.706825 (1889.4963510226146 steps/sec)\n",
      "Step #4549\tEpoch   1 Batch 1423/3125   Loss: 0.785474 mae: 0.717673 (1712.3242484119078 steps/sec)\n",
      "Step #4550\tEpoch   1 Batch 1424/3125   Loss: 0.824585 mae: 0.714592 (1907.0739403639272 steps/sec)\n",
      "Step #4551\tEpoch   1 Batch 1425/3125   Loss: 1.030112 mae: 0.795040 (2106.758820220204 steps/sec)\n",
      "Step #4552\tEpoch   1 Batch 1426/3125   Loss: 0.860834 mae: 0.732305 (2120.8846997906576 steps/sec)\n",
      "Step #4553\tEpoch   1 Batch 1427/3125   Loss: 0.687941 mae: 0.636996 (2026.2533937525966 steps/sec)\n",
      "Step #4554\tEpoch   1 Batch 1428/3125   Loss: 0.799355 mae: 0.700635 (2122.6879358684982 steps/sec)\n",
      "Step #4555\tEpoch   1 Batch 1429/3125   Loss: 0.917151 mae: 0.761138 (1945.6988050174423 steps/sec)\n",
      "Step #4556\tEpoch   1 Batch 1430/3125   Loss: 0.809582 mae: 0.713660 (1796.1065766822826 steps/sec)\n",
      "Step #4557\tEpoch   1 Batch 1431/3125   Loss: 0.855513 mae: 0.741506 (1808.1858235400625 steps/sec)\n",
      "Step #4558\tEpoch   1 Batch 1432/3125   Loss: 0.897539 mae: 0.736299 (1619.3848791147695 steps/sec)\n",
      "Step #4559\tEpoch   1 Batch 1433/3125   Loss: 0.823682 mae: 0.721789 (1705.6117635576954 steps/sec)\n",
      "Step #4560\tEpoch   1 Batch 1434/3125   Loss: 0.783017 mae: 0.711343 (1869.1027709200453 steps/sec)\n",
      "Step #4561\tEpoch   1 Batch 1435/3125   Loss: 0.839565 mae: 0.735909 (1865.3786969090504 steps/sec)\n",
      "Step #4562\tEpoch   1 Batch 1436/3125   Loss: 0.852990 mae: 0.721639 (1828.1569817110378 steps/sec)\n",
      "Step #4563\tEpoch   1 Batch 1437/3125   Loss: 0.840447 mae: 0.728280 (2142.881082296201 steps/sec)\n",
      "Step #4564\tEpoch   1 Batch 1438/3125   Loss: 0.938713 mae: 0.771183 (2110.2354598510765 steps/sec)\n",
      "Step #4565\tEpoch   1 Batch 1439/3125   Loss: 0.909529 mae: 0.747746 (1685.3122463575946 steps/sec)\n",
      "Step #4566\tEpoch   1 Batch 1440/3125   Loss: 0.722625 mae: 0.669845 (1902.5755940014697 steps/sec)\n",
      "Step #4567\tEpoch   1 Batch 1441/3125   Loss: 0.775906 mae: 0.697171 (2121.700069807675 steps/sec)\n",
      "Step #4568\tEpoch   1 Batch 1442/3125   Loss: 0.871525 mae: 0.760003 (2157.696977179661 steps/sec)\n",
      "Step #4569\tEpoch   1 Batch 1443/3125   Loss: 0.785960 mae: 0.712028 (2153.951706500416 steps/sec)\n",
      "Step #4570\tEpoch   1 Batch 1444/3125   Loss: 0.794157 mae: 0.711779 (2118.1000090898992 steps/sec)\n",
      "Step #4571\tEpoch   1 Batch 1445/3125   Loss: 0.881312 mae: 0.755132 (1952.3464628503868 steps/sec)\n",
      "Step #4572\tEpoch   1 Batch 1446/3125   Loss: 0.711867 mae: 0.663589 (1918.7293570846944 steps/sec)\n",
      "Step #4573\tEpoch   1 Batch 1447/3125   Loss: 0.870507 mae: 0.753306 (2031.257990779125 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4574\tEpoch   1 Batch 1448/3125   Loss: 0.867413 mae: 0.751339 (1391.9858753874644 steps/sec)\n",
      "Step #4575\tEpoch   1 Batch 1449/3125   Loss: 0.823734 mae: 0.713502 (1222.9931710958322 steps/sec)\n",
      "Step #4576\tEpoch   1 Batch 1450/3125   Loss: 0.865573 mae: 0.746965 (906.4688745385859 steps/sec)\n",
      "Step #4577\tEpoch   1 Batch 1451/3125   Loss: 0.858463 mae: 0.725257 (1185.0324913827203 steps/sec)\n",
      "Step #4578\tEpoch   1 Batch 1452/3125   Loss: 0.802774 mae: 0.741934 (1259.4448514839594 steps/sec)\n",
      "Step #4579\tEpoch   1 Batch 1453/3125   Loss: 0.866617 mae: 0.732572 (1045.127080633908 steps/sec)\n",
      "Step #4580\tEpoch   1 Batch 1454/3125   Loss: 0.861957 mae: 0.730538 (1106.6881972369101 steps/sec)\n",
      "Step #4581\tEpoch   1 Batch 1455/3125   Loss: 0.802037 mae: 0.720185 (1151.8066730742826 steps/sec)\n",
      "Step #4582\tEpoch   1 Batch 1456/3125   Loss: 0.844685 mae: 0.712190 (1277.4970912701556 steps/sec)\n",
      "Step #4583\tEpoch   1 Batch 1457/3125   Loss: 0.821511 mae: 0.740525 (1432.745110095441 steps/sec)\n",
      "Step #4584\tEpoch   1 Batch 1458/3125   Loss: 0.852917 mae: 0.708496 (1259.452415126747 steps/sec)\n",
      "Step #4585\tEpoch   1 Batch 1459/3125   Loss: 0.822382 mae: 0.684784 (1141.7110814709965 steps/sec)\n",
      "Step #4586\tEpoch   1 Batch 1460/3125   Loss: 0.813386 mae: 0.729197 (1324.1267836848087 steps/sec)\n",
      "Step #4587\tEpoch   1 Batch 1461/3125   Loss: 0.799096 mae: 0.717207 (1389.9009179176194 steps/sec)\n",
      "Step #4588\tEpoch   1 Batch 1462/3125   Loss: 0.918267 mae: 0.732519 (1403.4625603137317 steps/sec)\n",
      "Step #4589\tEpoch   1 Batch 1463/3125   Loss: 0.812861 mae: 0.736329 (1315.8600784313726 steps/sec)\n",
      "Step #4590\tEpoch   1 Batch 1464/3125   Loss: 0.839855 mae: 0.732519 (1249.0036091622694 steps/sec)\n",
      "Step #4591\tEpoch   1 Batch 1465/3125   Loss: 0.833705 mae: 0.720770 (1395.644998136613 steps/sec)\n",
      "Step #4592\tEpoch   1 Batch 1466/3125   Loss: 0.704144 mae: 0.645916 (1676.2197071424005 steps/sec)\n",
      "Step #4593\tEpoch   1 Batch 1467/3125   Loss: 0.769797 mae: 0.678859 (2170.4255671468786 steps/sec)\n",
      "Step #4594\tEpoch   1 Batch 1468/3125   Loss: 0.965271 mae: 0.774893 (2098.36905405135 steps/sec)\n",
      "Step #4595\tEpoch   1 Batch 1469/3125   Loss: 0.880277 mae: 0.743507 (2053.0524337236166 steps/sec)\n",
      "Step #4596\tEpoch   1 Batch 1470/3125   Loss: 0.898589 mae: 0.742653 (2002.819214974692 steps/sec)\n",
      "Step #4597\tEpoch   1 Batch 1471/3125   Loss: 0.783105 mae: 0.702072 (1951.746859004188 steps/sec)\n",
      "Step #4598\tEpoch   1 Batch 1472/3125   Loss: 0.790816 mae: 0.711333 (1758.5737885000797 steps/sec)\n",
      "Step #4599\tEpoch   1 Batch 1473/3125   Loss: 0.892192 mae: 0.742771 (1850.8746227031224 steps/sec)\n",
      "Step #4600\tEpoch   1 Batch 1474/3125   Loss: 0.732972 mae: 0.666801 (1928.433364904505 steps/sec)\n",
      "Step #4601\tEpoch   1 Batch 1475/3125   Loss: 0.913199 mae: 0.753240 (2040.0708184984144 steps/sec)\n",
      "Step #4602\tEpoch   1 Batch 1476/3125   Loss: 0.732500 mae: 0.681360 (1895.8163080817212 steps/sec)\n",
      "Step #4603\tEpoch   1 Batch 1477/3125   Loss: 0.854874 mae: 0.756865 (2076.5114759292633 steps/sec)\n",
      "Step #4604\tEpoch   1 Batch 1478/3125   Loss: 0.783245 mae: 0.689768 (2093.091402678803 steps/sec)\n",
      "Step #4605\tEpoch   1 Batch 1479/3125   Loss: 0.711522 mae: 0.685892 (2152.5588651899902 steps/sec)\n",
      "Step #4606\tEpoch   1 Batch 1480/3125   Loss: 0.775277 mae: 0.673926 (2088.713597067846 steps/sec)\n",
      "Step #4607\tEpoch   1 Batch 1481/3125   Loss: 0.781025 mae: 0.720128 (1831.55780298862 steps/sec)\n",
      "Step #4608\tEpoch   1 Batch 1482/3125   Loss: 0.886381 mae: 0.736876 (1829.4806814910453 steps/sec)\n",
      "Step #4609\tEpoch   1 Batch 1483/3125   Loss: 0.902842 mae: 0.741099 (1809.3714680125966 steps/sec)\n",
      "Step #4610\tEpoch   1 Batch 1484/3125   Loss: 0.826038 mae: 0.741829 (2087.757093081135 steps/sec)\n",
      "Step #4611\tEpoch   1 Batch 1485/3125   Loss: 0.891367 mae: 0.727177 (2140.125724548943 steps/sec)\n",
      "Step #4612\tEpoch   1 Batch 1486/3125   Loss: 0.869580 mae: 0.728955 (2189.894011382029 steps/sec)\n",
      "Step #4613\tEpoch   1 Batch 1487/3125   Loss: 0.830465 mae: 0.713983 (1970.211286791992 steps/sec)\n",
      "Step #4614\tEpoch   1 Batch 1488/3125   Loss: 0.892006 mae: 0.732512 (1954.0564464280724 steps/sec)\n",
      "Step #4615\tEpoch   1 Batch 1489/3125   Loss: 0.747455 mae: 0.670179 (1819.2284670836334 steps/sec)\n",
      "Step #4616\tEpoch   1 Batch 1490/3125   Loss: 0.912143 mae: 0.760321 (1895.7306214689265 steps/sec)\n",
      "Step #4617\tEpoch   1 Batch 1491/3125   Loss: 0.831192 mae: 0.706864 (2079.1465905260443 steps/sec)\n",
      "Step #4618\tEpoch   1 Batch 1492/3125   Loss: 0.793089 mae: 0.720490 (2071.424902708362 steps/sec)\n",
      "Step #4619\tEpoch   1 Batch 1493/3125   Loss: 0.860764 mae: 0.759318 (2241.04980818346 steps/sec)\n",
      "Step #4620\tEpoch   1 Batch 1494/3125   Loss: 0.765676 mae: 0.712721 (2217.9879854471615 steps/sec)\n",
      "Step #4621\tEpoch   1 Batch 1495/3125   Loss: 0.854863 mae: 0.725338 (2148.0390449754686 steps/sec)\n",
      "Step #4622\tEpoch   1 Batch 1496/3125   Loss: 0.780673 mae: 0.723014 (2061.913891592681 steps/sec)\n",
      "Step #4623\tEpoch   1 Batch 1497/3125   Loss: 0.926130 mae: 0.752994 (1738.398667075607 steps/sec)\n",
      "Step #4624\tEpoch   1 Batch 1498/3125   Loss: 0.836649 mae: 0.733974 (1629.8306560039791 steps/sec)\n",
      "Step #4625\tEpoch   1 Batch 1499/3125   Loss: 0.857172 mae: 0.728370 (2093.6346937145595 steps/sec)\n",
      "Step #4626\tEpoch   1 Batch 1500/3125   Loss: 0.867775 mae: 0.735780 (1791.3352466858003 steps/sec)\n",
      "Step #4627\tEpoch   1 Batch 1501/3125   Loss: 0.726500 mae: 0.680985 (1613.3179475344257 steps/sec)\n",
      "Step #4628\tEpoch   1 Batch 1502/3125   Loss: 0.777528 mae: 0.692836 (1282.1520496438725 steps/sec)\n",
      "Step #4629\tEpoch   1 Batch 1503/3125   Loss: 0.939589 mae: 0.762724 (1313.914454517546 steps/sec)\n",
      "Step #4630\tEpoch   1 Batch 1504/3125   Loss: 0.911650 mae: 0.752951 (1147.1255565644522 steps/sec)\n",
      "Step #4631\tEpoch   1 Batch 1505/3125   Loss: 0.650603 mae: 0.625801 (1161.2201617949158 steps/sec)\n",
      "Step #4632\tEpoch   1 Batch 1506/3125   Loss: 0.765412 mae: 0.682276 (1325.0219557347116 steps/sec)\n",
      "Step #4633\tEpoch   1 Batch 1507/3125   Loss: 0.875000 mae: 0.745375 (1496.6507996545893 steps/sec)\n",
      "Step #4634\tEpoch   1 Batch 1508/3125   Loss: 0.827310 mae: 0.719180 (1440.549526033796 steps/sec)\n",
      "Step #4635\tEpoch   1 Batch 1509/3125   Loss: 0.767556 mae: 0.684053 (1443.7527967671094 steps/sec)\n",
      "Step #4636\tEpoch   1 Batch 1510/3125   Loss: 0.909712 mae: 0.750200 (1011.0021067043334 steps/sec)\n",
      "Step #4637\tEpoch   1 Batch 1511/3125   Loss: 0.882052 mae: 0.757752 (1675.6571903414992 steps/sec)\n",
      "Step #4638\tEpoch   1 Batch 1512/3125   Loss: 0.867580 mae: 0.719350 (1998.391492443445 steps/sec)\n",
      "Step #4639\tEpoch   1 Batch 1513/3125   Loss: 0.819550 mae: 0.706188 (1827.7427226773575 steps/sec)\n",
      "Step #4640\tEpoch   1 Batch 1514/3125   Loss: 0.661122 mae: 0.670694 (1590.872678723146 steps/sec)\n",
      "Step #4641\tEpoch   1 Batch 1515/3125   Loss: 0.753734 mae: 0.703488 (1075.264053815706 steps/sec)\n",
      "Step #4642\tEpoch   1 Batch 1516/3125   Loss: 0.805412 mae: 0.734603 (1509.8503938141657 steps/sec)\n",
      "Step #4643\tEpoch   1 Batch 1517/3125   Loss: 0.821126 mae: 0.706568 (1177.2493544403278 steps/sec)\n",
      "Step #4644\tEpoch   1 Batch 1518/3125   Loss: 0.918598 mae: 0.780984 (1479.9421333051057 steps/sec)\n",
      "Step #4645\tEpoch   1 Batch 1519/3125   Loss: 0.767974 mae: 0.711371 (1224.3855162830887 steps/sec)\n",
      "Step #4646\tEpoch   1 Batch 1520/3125   Loss: 0.774001 mae: 0.703199 (1666.0062440915483 steps/sec)\n",
      "Step #4647\tEpoch   1 Batch 1521/3125   Loss: 0.846180 mae: 0.724904 (1345.9675245491303 steps/sec)\n",
      "Step #4648\tEpoch   1 Batch 1522/3125   Loss: 0.899082 mae: 0.755341 (1281.259049725377 steps/sec)\n",
      "Step #4649\tEpoch   1 Batch 1523/3125   Loss: 0.772310 mae: 0.709600 (1523.1521225986853 steps/sec)\n",
      "Step #4650\tEpoch   1 Batch 1524/3125   Loss: 0.767856 mae: 0.701166 (1632.4822518370906 steps/sec)\n",
      "Step #4651\tEpoch   1 Batch 1525/3125   Loss: 0.764954 mae: 0.688375 (1272.4973605329903 steps/sec)\n",
      "Step #4652\tEpoch   1 Batch 1526/3125   Loss: 0.921761 mae: 0.767292 (932.4032207150129 steps/sec)\n",
      "Step #4653\tEpoch   1 Batch 1527/3125   Loss: 0.693135 mae: 0.644098 (1149.143273588059 steps/sec)\n",
      "Step #4654\tEpoch   1 Batch 1528/3125   Loss: 0.880253 mae: 0.738180 (918.5264347376674 steps/sec)\n",
      "Step #4655\tEpoch   1 Batch 1529/3125   Loss: 0.905590 mae: 0.723724 (862.8089245086625 steps/sec)\n",
      "Step #4656\tEpoch   1 Batch 1530/3125   Loss: 0.901502 mae: 0.753472 (1004.0753791941167 steps/sec)\n",
      "Step #4657\tEpoch   1 Batch 1531/3125   Loss: 0.761899 mae: 0.701096 (1077.5845768076622 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4658\tEpoch   1 Batch 1532/3125   Loss: 0.795999 mae: 0.693529 (1115.144554160618 steps/sec)\n",
      "Step #4659\tEpoch   1 Batch 1533/3125   Loss: 0.941934 mae: 0.753187 (1246.0129880160894 steps/sec)\n",
      "Step #4660\tEpoch   1 Batch 1534/3125   Loss: 0.937046 mae: 0.762916 (1183.3274086613062 steps/sec)\n",
      "Step #4661\tEpoch   1 Batch 1535/3125   Loss: 0.692931 mae: 0.663136 (812.8748178238085 steps/sec)\n",
      "Step #4662\tEpoch   1 Batch 1536/3125   Loss: 0.843638 mae: 0.734686 (1104.3105536423284 steps/sec)\n",
      "Step #4663\tEpoch   1 Batch 1537/3125   Loss: 0.996846 mae: 0.782447 (680.6150730545296 steps/sec)\n",
      "Step #4664\tEpoch   1 Batch 1538/3125   Loss: 0.796549 mae: 0.704458 (829.1563540324047 steps/sec)\n",
      "Step #4665\tEpoch   1 Batch 1539/3125   Loss: 0.801275 mae: 0.689160 (689.4534743043502 steps/sec)\n",
      "Step #4666\tEpoch   1 Batch 1540/3125   Loss: 0.912120 mae: 0.754646 (1213.5594004976565 steps/sec)\n",
      "Step #4667\tEpoch   1 Batch 1541/3125   Loss: 0.802246 mae: 0.724333 (1078.4878608198383 steps/sec)\n",
      "Step #4668\tEpoch   1 Batch 1542/3125   Loss: 0.817699 mae: 0.718424 (1430.6632284119903 steps/sec)\n",
      "Step #4669\tEpoch   1 Batch 1543/3125   Loss: 0.873268 mae: 0.743641 (1408.1271989901431 steps/sec)\n",
      "Step #4670\tEpoch   1 Batch 1544/3125   Loss: 0.951378 mae: 0.791079 (1608.6524964139699 steps/sec)\n",
      "Step #4671\tEpoch   1 Batch 1545/3125   Loss: 0.817127 mae: 0.727123 (1487.4367867452531 steps/sec)\n",
      "Step #4672\tEpoch   1 Batch 1546/3125   Loss: 0.876930 mae: 0.755727 (1551.342994311415 steps/sec)\n",
      "Step #4673\tEpoch   1 Batch 1547/3125   Loss: 0.737960 mae: 0.677098 (853.444458913921 steps/sec)\n",
      "Step #4674\tEpoch   1 Batch 1548/3125   Loss: 0.944348 mae: 0.797724 (954.1399940853978 steps/sec)\n",
      "Step #4675\tEpoch   1 Batch 1549/3125   Loss: 0.911937 mae: 0.737529 (1183.3741493527746 steps/sec)\n",
      "Step #4676\tEpoch   1 Batch 1550/3125   Loss: 0.820604 mae: 0.733801 (1022.492223381537 steps/sec)\n",
      "Step #4677\tEpoch   1 Batch 1551/3125   Loss: 0.946224 mae: 0.774032 (1054.5606315842406 steps/sec)\n",
      "Step #4678\tEpoch   1 Batch 1552/3125   Loss: 0.696099 mae: 0.663208 (1157.2983980001213 steps/sec)\n",
      "Step #4679\tEpoch   1 Batch 1553/3125   Loss: 0.966839 mae: 0.780411 (1148.186959687707 steps/sec)\n",
      "Step #4680\tEpoch   1 Batch 1554/3125   Loss: 0.755879 mae: 0.695120 (1220.3812760410606 steps/sec)\n",
      "Step #4681\tEpoch   1 Batch 1555/3125   Loss: 0.808202 mae: 0.725433 (1305.660565309426 steps/sec)\n",
      "Step #4682\tEpoch   1 Batch 1556/3125   Loss: 0.898978 mae: 0.760152 (1163.4943355192347 steps/sec)\n",
      "Step #4683\tEpoch   1 Batch 1557/3125   Loss: 1.050892 mae: 0.799767 (1383.8568336588714 steps/sec)\n",
      "Step #4684\tEpoch   1 Batch 1558/3125   Loss: 0.772550 mae: 0.696858 (1887.0820285786272 steps/sec)\n",
      "Step #4685\tEpoch   1 Batch 1559/3125   Loss: 0.774903 mae: 0.714613 (1931.6305759470936 steps/sec)\n",
      "Step #4686\tEpoch   1 Batch 1560/3125   Loss: 0.739025 mae: 0.702761 (1831.5737991266376 steps/sec)\n",
      "Step #4687\tEpoch   1 Batch 1561/3125   Loss: 0.991900 mae: 0.754888 (2161.054377956164 steps/sec)\n",
      "Step #4688\tEpoch   1 Batch 1562/3125   Loss: 0.749936 mae: 0.685434 (1829.959598956379 steps/sec)\n",
      "Step #4689\tEpoch   1 Batch 1563/3125   Loss: 0.903879 mae: 0.753586 (1965.7052874295837 steps/sec)\n",
      "Step #4690\tEpoch   1 Batch 1564/3125   Loss: 0.768907 mae: 0.696312 (1503.3563205207242 steps/sec)\n",
      "Step #4691\tEpoch   1 Batch 1565/3125   Loss: 0.868778 mae: 0.745744 (1849.0468885009434 steps/sec)\n",
      "Step #4692\tEpoch   1 Batch 1566/3125   Loss: 0.884525 mae: 0.752124 (1873.9295160483236 steps/sec)\n",
      "Step #4693\tEpoch   1 Batch 1567/3125   Loss: 0.790088 mae: 0.710209 (1943.5530059405207 steps/sec)\n",
      "Step #4694\tEpoch   1 Batch 1568/3125   Loss: 0.805129 mae: 0.687233 (1930.9547266750762 steps/sec)\n",
      "Step #4695\tEpoch   1 Batch 1569/3125   Loss: 0.801627 mae: 0.714016 (1870.6032414303681 steps/sec)\n",
      "Step #4696\tEpoch   1 Batch 1570/3125   Loss: 0.823874 mae: 0.690538 (1776.148653798921 steps/sec)\n",
      "Step #4697\tEpoch   1 Batch 1571/3125   Loss: 0.984683 mae: 0.781200 (1811.0120898100172 steps/sec)\n",
      "Step #4698\tEpoch   1 Batch 1572/3125   Loss: 0.917162 mae: 0.751654 (793.522120143482 steps/sec)\n",
      "Step #4699\tEpoch   1 Batch 1573/3125   Loss: 0.699842 mae: 0.661746 (953.9837420563979 steps/sec)\n",
      "Step #4700\tEpoch   1 Batch 1574/3125   Loss: 0.800497 mae: 0.722072 (1024.3601266070102 steps/sec)\n",
      "Step #4701\tEpoch   1 Batch 1575/3125   Loss: 0.754330 mae: 0.678658 (1074.3496480568847 steps/sec)\n",
      "Step #4702\tEpoch   1 Batch 1576/3125   Loss: 0.842530 mae: 0.705379 (1176.3908677848208 steps/sec)\n",
      "Step #4703\tEpoch   1 Batch 1577/3125   Loss: 0.689230 mae: 0.654553 (1211.9393669708336 steps/sec)\n",
      "Step #4704\tEpoch   1 Batch 1578/3125   Loss: 0.723319 mae: 0.669056 (1257.2929094299127 steps/sec)\n",
      "Step #4705\tEpoch   1 Batch 1579/3125   Loss: 0.781966 mae: 0.712856 (1565.1439275772254 steps/sec)\n",
      "Step #4706\tEpoch   1 Batch 1580/3125   Loss: 0.910589 mae: 0.775421 (1765.6658864734707 steps/sec)\n",
      "Step #4707\tEpoch   1 Batch 1581/3125   Loss: 0.970402 mae: 0.775619 (2089.0881198573506 steps/sec)\n",
      "Step #4708\tEpoch   1 Batch 1582/3125   Loss: 0.801876 mae: 0.703803 (1583.1266183031503 steps/sec)\n",
      "Step #4709\tEpoch   1 Batch 1583/3125   Loss: 0.741706 mae: 0.693795 (832.5038009757492 steps/sec)\n",
      "Step #4710\tEpoch   1 Batch 1584/3125   Loss: 0.717873 mae: 0.702243 (920.7606152008887 steps/sec)\n",
      "Step #4711\tEpoch   1 Batch 1585/3125   Loss: 0.739373 mae: 0.711474 (798.0971880244625 steps/sec)\n",
      "Step #4712\tEpoch   1 Batch 1586/3125   Loss: 0.833092 mae: 0.707734 (814.4817327745909 steps/sec)\n",
      "Step #4713\tEpoch   1 Batch 1587/3125   Loss: 0.750450 mae: 0.716224 (871.5509323713859 steps/sec)\n",
      "Step #4714\tEpoch   1 Batch 1588/3125   Loss: 0.764969 mae: 0.710568 (1328.269764260289 steps/sec)\n",
      "Step #4715\tEpoch   1 Batch 1589/3125   Loss: 0.759704 mae: 0.702223 (1480.872217828494 steps/sec)\n",
      "Step #4716\tEpoch   1 Batch 1590/3125   Loss: 0.902143 mae: 0.766252 (802.3874749391653 steps/sec)\n",
      "Step #4717\tEpoch   1 Batch 1591/3125   Loss: 0.778072 mae: 0.714070 (674.2451862640136 steps/sec)\n",
      "Step #4718\tEpoch   1 Batch 1592/3125   Loss: 0.755111 mae: 0.682221 (775.5538872884208 steps/sec)\n",
      "Step #4719\tEpoch   1 Batch 1593/3125   Loss: 0.905006 mae: 0.751896 (1505.9364206263149 steps/sec)\n",
      "Step #4720\tEpoch   1 Batch 1594/3125   Loss: 0.691622 mae: 0.669146 (1190.9884430814664 steps/sec)\n",
      "Step #4721\tEpoch   1 Batch 1595/3125   Loss: 0.948282 mae: 0.769920 (1106.7057178289665 steps/sec)\n",
      "Step #4722\tEpoch   1 Batch 1596/3125   Loss: 0.927908 mae: 0.741159 (706.1022747168388 steps/sec)\n",
      "Step #4723\tEpoch   1 Batch 1597/3125   Loss: 0.698895 mae: 0.676922 (1027.6378782310426 steps/sec)\n",
      "Step #4724\tEpoch   1 Batch 1598/3125   Loss: 0.870497 mae: 0.750975 (1034.4658853241783 steps/sec)\n",
      "Step #4725\tEpoch   1 Batch 1599/3125   Loss: 0.825101 mae: 0.712884 (1131.2231643903597 steps/sec)\n",
      "Step #4726\tEpoch   1 Batch 1600/3125   Loss: 0.757853 mae: 0.687440 (1386.4276128332772 steps/sec)\n",
      "Step #4727\tEpoch   1 Batch 1601/3125   Loss: 0.730133 mae: 0.666410 (1039.5423767460766 steps/sec)\n",
      "Step #4728\tEpoch   1 Batch 1602/3125   Loss: 0.782869 mae: 0.711352 (1062.8123717191784 steps/sec)\n",
      "Step #4729\tEpoch   1 Batch 1603/3125   Loss: 0.736989 mae: 0.706369 (907.4219747221032 steps/sec)\n",
      "Step #4730\tEpoch   1 Batch 1604/3125   Loss: 0.823589 mae: 0.717372 (1032.1595031031445 steps/sec)\n",
      "Step #4731\tEpoch   1 Batch 1605/3125   Loss: 0.801472 mae: 0.701014 (1243.8771515676342 steps/sec)\n",
      "Step #4732\tEpoch   1 Batch 1606/3125   Loss: 0.784793 mae: 0.695782 (1305.8556875638249 steps/sec)\n",
      "Step #4733\tEpoch   1 Batch 1607/3125   Loss: 0.987664 mae: 0.771741 (1041.9906192861117 steps/sec)\n",
      "Step #4734\tEpoch   1 Batch 1608/3125   Loss: 0.809736 mae: 0.725012 (1242.2046497852807 steps/sec)\n",
      "Step #4735\tEpoch   1 Batch 1609/3125   Loss: 0.879611 mae: 0.766804 (1426.0422545746324 steps/sec)\n",
      "Step #4736\tEpoch   1 Batch 1610/3125   Loss: 0.758612 mae: 0.670632 (1443.8323155408987 steps/sec)\n",
      "Step #4737\tEpoch   1 Batch 1611/3125   Loss: 0.739145 mae: 0.693896 (1520.6891550889 steps/sec)\n",
      "Step #4738\tEpoch   1 Batch 1612/3125   Loss: 0.872437 mae: 0.749026 (1065.2092423188083 steps/sec)\n",
      "Step #4739\tEpoch   1 Batch 1613/3125   Loss: 0.737656 mae: 0.686036 (1346.3909451017905 steps/sec)\n",
      "Step #4740\tEpoch   1 Batch 1614/3125   Loss: 0.834850 mae: 0.710533 (1255.02061627399 steps/sec)\n",
      "Step #4741\tEpoch   1 Batch 1615/3125   Loss: 0.860741 mae: 0.736174 (1236.8783618005095 steps/sec)\n",
      "Step #4742\tEpoch   1 Batch 1616/3125   Loss: 0.921710 mae: 0.755831 (1684.1888853196274 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4743\tEpoch   1 Batch 1617/3125   Loss: 0.880366 mae: 0.735734 (1611.9167121434555 steps/sec)\n",
      "Step #4744\tEpoch   1 Batch 1618/3125   Loss: 0.708514 mae: 0.655401 (1117.2952439810547 steps/sec)\n",
      "Step #4745\tEpoch   1 Batch 1619/3125   Loss: 0.914045 mae: 0.744434 (1106.4663177443863 steps/sec)\n",
      "Step #4746\tEpoch   1 Batch 1620/3125   Loss: 0.785118 mae: 0.708523 (1396.0630812347306 steps/sec)\n",
      "Step #4747\tEpoch   1 Batch 1621/3125   Loss: 0.876850 mae: 0.733176 (1362.592181094023 steps/sec)\n",
      "Step #4748\tEpoch   1 Batch 1622/3125   Loss: 0.807001 mae: 0.697592 (1564.396702845847 steps/sec)\n",
      "Step #4749\tEpoch   1 Batch 1623/3125   Loss: 0.885638 mae: 0.764023 (1527.96846653212 steps/sec)\n",
      "Step #4750\tEpoch   1 Batch 1624/3125   Loss: 0.906598 mae: 0.757660 (1273.5560427281396 steps/sec)\n",
      "Step #4751\tEpoch   1 Batch 1625/3125   Loss: 0.860425 mae: 0.753799 (1411.5106848393068 steps/sec)\n",
      "Step #4752\tEpoch   1 Batch 1626/3125   Loss: 0.756296 mae: 0.691458 (1757.601052640401 steps/sec)\n",
      "Step #4753\tEpoch   1 Batch 1627/3125   Loss: 0.866711 mae: 0.749194 (1996.812187574387 steps/sec)\n",
      "Step #4754\tEpoch   1 Batch 1628/3125   Loss: 0.874444 mae: 0.766025 (1799.6670385308505 steps/sec)\n",
      "Step #4755\tEpoch   1 Batch 1629/3125   Loss: 0.783993 mae: 0.702666 (1657.4346004900024 steps/sec)\n",
      "Step #4756\tEpoch   1 Batch 1630/3125   Loss: 0.939930 mae: 0.769208 (1574.4029789118863 steps/sec)\n",
      "Step #4757\tEpoch   1 Batch 1631/3125   Loss: 0.922738 mae: 0.754400 (1425.5963346407716 steps/sec)\n",
      "Step #4758\tEpoch   1 Batch 1632/3125   Loss: 0.824400 mae: 0.701906 (1372.311034622658 steps/sec)\n",
      "Step #4759\tEpoch   1 Batch 1633/3125   Loss: 0.963992 mae: 0.754069 (1100.2722937204555 steps/sec)\n",
      "Step #4760\tEpoch   1 Batch 1634/3125   Loss: 0.997339 mae: 0.798807 (1739.725413745904 steps/sec)\n",
      "Step #4761\tEpoch   1 Batch 1635/3125   Loss: 0.875871 mae: 0.751237 (1700.4946240046706 steps/sec)\n",
      "Step #4762\tEpoch   1 Batch 1636/3125   Loss: 0.857737 mae: 0.733731 (1384.0029565492846 steps/sec)\n",
      "Step #4763\tEpoch   1 Batch 1637/3125   Loss: 0.831017 mae: 0.714596 (1680.4910492491626 steps/sec)\n",
      "Step #4764\tEpoch   1 Batch 1638/3125   Loss: 0.868448 mae: 0.720168 (1466.027263194687 steps/sec)\n",
      "Step #4765\tEpoch   1 Batch 1639/3125   Loss: 0.771066 mae: 0.674966 (1975.5006688144088 steps/sec)\n",
      "Step #4766\tEpoch   1 Batch 1640/3125   Loss: 0.897238 mae: 0.771766 (1966.2581921486635 steps/sec)\n",
      "Step #4767\tEpoch   1 Batch 1641/3125   Loss: 0.660607 mae: 0.636322 (1696.0662525879918 steps/sec)\n",
      "Step #4768\tEpoch   1 Batch 1642/3125   Loss: 0.835152 mae: 0.738698 (1911.9595937494303 steps/sec)\n",
      "Step #4769\tEpoch   1 Batch 1643/3125   Loss: 0.848375 mae: 0.708261 (1852.395042972097 steps/sec)\n",
      "Step #4770\tEpoch   1 Batch 1644/3125   Loss: 0.771480 mae: 0.690957 (1513.1731039807205 steps/sec)\n",
      "Step #4771\tEpoch   1 Batch 1645/3125   Loss: 0.893174 mae: 0.741741 (1381.9782537067545 steps/sec)\n",
      "Step #4772\tEpoch   1 Batch 1646/3125   Loss: 0.744819 mae: 0.681821 (1398.987358660485 steps/sec)\n",
      "Step #4773\tEpoch   1 Batch 1647/3125   Loss: 0.861764 mae: 0.737647 (1415.4357025708173 steps/sec)\n",
      "Step #4774\tEpoch   1 Batch 1648/3125   Loss: 0.904512 mae: 0.728751 (1758.6475244867838 steps/sec)\n",
      "Step #4775\tEpoch   1 Batch 1649/3125   Loss: 0.870417 mae: 0.749378 (1948.0302819190933 steps/sec)\n",
      "Step #4776\tEpoch   1 Batch 1650/3125   Loss: 0.748139 mae: 0.680725 (1430.6144306267097 steps/sec)\n",
      "Step #4777\tEpoch   1 Batch 1651/3125   Loss: 0.832021 mae: 0.696765 (1226.8351468351468 steps/sec)\n",
      "Step #4778\tEpoch   1 Batch 1652/3125   Loss: 0.719211 mae: 0.672728 (1530.3657433083276 steps/sec)\n",
      "Step #4779\tEpoch   1 Batch 1653/3125   Loss: 0.789606 mae: 0.715834 (1613.9014798796395 steps/sec)\n",
      "Step #4780\tEpoch   1 Batch 1654/3125   Loss: 0.757223 mae: 0.699064 (1918.7995791207284 steps/sec)\n",
      "Step #4781\tEpoch   1 Batch 1655/3125   Loss: 0.901911 mae: 0.724754 (1881.3262523324242 steps/sec)\n",
      "Step #4782\tEpoch   1 Batch 1656/3125   Loss: 0.866101 mae: 0.718460 (1276.0046972066223 steps/sec)\n",
      "Step #4783\tEpoch   1 Batch 1657/3125   Loss: 0.805606 mae: 0.710962 (1589.7149787750152 steps/sec)\n",
      "Step #4784\tEpoch   1 Batch 1658/3125   Loss: 0.663061 mae: 0.654791 (1468.820126349998 steps/sec)\n",
      "Step #4785\tEpoch   1 Batch 1659/3125   Loss: 0.726634 mae: 0.683540 (1773.2501310604907 steps/sec)\n",
      "Step #4786\tEpoch   1 Batch 1660/3125   Loss: 0.942090 mae: 0.761772 (1942.7248052321004 steps/sec)\n",
      "Step #4787\tEpoch   1 Batch 1661/3125   Loss: 0.977594 mae: 0.778756 (1628.982445238465 steps/sec)\n",
      "Step #4788\tEpoch   1 Batch 1662/3125   Loss: 0.869089 mae: 0.737395 (1695.1477185466597 steps/sec)\n",
      "Step #4789\tEpoch   1 Batch 1663/3125   Loss: 0.857939 mae: 0.752389 (1398.1665811071184 steps/sec)\n",
      "Step #4790\tEpoch   1 Batch 1664/3125   Loss: 0.856785 mae: 0.738357 (1520.3473999376536 steps/sec)\n",
      "Step #4791\tEpoch   1 Batch 1665/3125   Loss: 0.851048 mae: 0.728750 (1464.379132887837 steps/sec)\n",
      "Step #4792\tEpoch   1 Batch 1666/3125   Loss: 0.818775 mae: 0.726666 (2071.8546546665216 steps/sec)\n",
      "Step #4793\tEpoch   1 Batch 1667/3125   Loss: 0.785812 mae: 0.716418 (1974.1988929472454 steps/sec)\n",
      "Step #4794\tEpoch   1 Batch 1668/3125   Loss: 0.757583 mae: 0.674816 (1960.2482614222688 steps/sec)\n",
      "Step #4795\tEpoch   1 Batch 1669/3125   Loss: 0.867842 mae: 0.739701 (1615.567487616421 steps/sec)\n",
      "Step #4796\tEpoch   1 Batch 1670/3125   Loss: 0.733245 mae: 0.646402 (1438.3561268021015 steps/sec)\n",
      "Step #4797\tEpoch   1 Batch 1671/3125   Loss: 0.845397 mae: 0.745786 (1257.3682916739112 steps/sec)\n",
      "Step #4798\tEpoch   1 Batch 1672/3125   Loss: 0.836221 mae: 0.723783 (1384.3227079799067 steps/sec)\n",
      "Step #4799\tEpoch   1 Batch 1673/3125   Loss: 0.741687 mae: 0.709682 (1502.1287568403861 steps/sec)\n",
      "Step #4800\tEpoch   1 Batch 1674/3125   Loss: 0.803170 mae: 0.704795 (1806.3169137216735 steps/sec)\n",
      "Step #4801\tEpoch   1 Batch 1675/3125   Loss: 0.816996 mae: 0.722922 (1870.2028804565925 steps/sec)\n",
      "Step #4802\tEpoch   1 Batch 1676/3125   Loss: 0.762458 mae: 0.683834 (1948.6276040214825 steps/sec)\n",
      "Step #4803\tEpoch   1 Batch 1677/3125   Loss: 0.717790 mae: 0.672149 (1289.6661992964848 steps/sec)\n",
      "Step #4804\tEpoch   1 Batch 1678/3125   Loss: 0.896173 mae: 0.735546 (1533.936525816102 steps/sec)\n",
      "Step #4805\tEpoch   1 Batch 1679/3125   Loss: 0.825549 mae: 0.689566 (1600.0488296151616 steps/sec)\n",
      "Step #4806\tEpoch   1 Batch 1680/3125   Loss: 0.813684 mae: 0.711065 (1497.259863207345 steps/sec)\n",
      "Step #4807\tEpoch   1 Batch 1681/3125   Loss: 0.882589 mae: 0.735080 (1494.8585440263453 steps/sec)\n",
      "Step #4808\tEpoch   1 Batch 1682/3125   Loss: 0.894873 mae: 0.762257 (1862.54573874738 steps/sec)\n",
      "Step #4809\tEpoch   1 Batch 1683/3125   Loss: 0.752423 mae: 0.692647 (1937.9136364896458 steps/sec)\n",
      "Step #4810\tEpoch   1 Batch 1684/3125   Loss: 0.662963 mae: 0.652321 (1693.655511047939 steps/sec)\n",
      "Step #4811\tEpoch   1 Batch 1685/3125   Loss: 0.882351 mae: 0.743459 (1295.505902556848 steps/sec)\n",
      "Step #4812\tEpoch   1 Batch 1686/3125   Loss: 0.677943 mae: 0.649263 (1576.2016069026163 steps/sec)\n",
      "Step #4813\tEpoch   1 Batch 1687/3125   Loss: 0.958559 mae: 0.776950 (1551.0102653610627 steps/sec)\n",
      "Step #4814\tEpoch   1 Batch 1688/3125   Loss: 0.856158 mae: 0.742269 (1722.124867585833 steps/sec)\n",
      "Step #4815\tEpoch   1 Batch 1689/3125   Loss: 0.824481 mae: 0.739852 (1655.4067174487902 steps/sec)\n",
      "Step #4816\tEpoch   1 Batch 1690/3125   Loss: 0.812366 mae: 0.708101 (1293.524212500077 steps/sec)\n",
      "Step #4817\tEpoch   1 Batch 1691/3125   Loss: 0.933104 mae: 0.767840 (1091.3685612881068 steps/sec)\n",
      "Step #4818\tEpoch   1 Batch 1692/3125   Loss: 0.838322 mae: 0.737095 (1454.123879324093 steps/sec)\n",
      "Step #4819\tEpoch   1 Batch 1693/3125   Loss: 1.039961 mae: 0.794674 (1461.9393516904845 steps/sec)\n",
      "Step #4820\tEpoch   1 Batch 1694/3125   Loss: 0.879543 mae: 0.727636 (1396.1467279142535 steps/sec)\n",
      "Step #4821\tEpoch   1 Batch 1695/3125   Loss: 0.880334 mae: 0.706301 (1563.4753306395098 steps/sec)\n",
      "Step #4822\tEpoch   1 Batch 1696/3125   Loss: 0.834566 mae: 0.716436 (1160.0382779354254 steps/sec)\n",
      "Step #4823\tEpoch   1 Batch 1697/3125   Loss: 0.795528 mae: 0.724037 (1670.8909976018037 steps/sec)\n",
      "Step #4824\tEpoch   1 Batch 1698/3125   Loss: 0.780988 mae: 0.702576 (1925.158352764059 steps/sec)\n",
      "Step #4825\tEpoch   1 Batch 1699/3125   Loss: 0.912595 mae: 0.735913 (2092.9451801878226 steps/sec)\n",
      "Step #4826\tEpoch   1 Batch 1700/3125   Loss: 0.784147 mae: 0.707391 (1909.991894279547 steps/sec)\n",
      "Step #4827\tEpoch   1 Batch 1701/3125   Loss: 0.846084 mae: 0.735586 (2002.7809611123844 steps/sec)\n",
      "Step #4828\tEpoch   1 Batch 1702/3125   Loss: 0.811967 mae: 0.714047 (1938.9707649919562 steps/sec)\n",
      "Step #4829\tEpoch   1 Batch 1703/3125   Loss: 0.925084 mae: 0.754158 (2026.9192480548977 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4830\tEpoch   1 Batch 1704/3125   Loss: 0.853323 mae: 0.744230 (1637.1976829515825 steps/sec)\n",
      "Step #4831\tEpoch   1 Batch 1705/3125   Loss: 0.853032 mae: 0.740134 (1501.9028453159353 steps/sec)\n",
      "Step #4832\tEpoch   1 Batch 1706/3125   Loss: 0.796311 mae: 0.715758 (1117.1226455297026 steps/sec)\n",
      "Step #4833\tEpoch   1 Batch 1707/3125   Loss: 0.900158 mae: 0.761273 (1813.502131596925 steps/sec)\n",
      "Step #4834\tEpoch   1 Batch 1708/3125   Loss: 0.843965 mae: 0.713464 (1770.8543731950754 steps/sec)\n",
      "Step #4835\tEpoch   1 Batch 1709/3125   Loss: 0.715951 mae: 0.678535 (1744.0948745457115 steps/sec)\n",
      "Step #4836\tEpoch   1 Batch 1710/3125   Loss: 0.775788 mae: 0.674259 (1669.574078496935 steps/sec)\n",
      "Step #4837\tEpoch   1 Batch 1711/3125   Loss: 0.779454 mae: 0.693909 (1325.030327537404 steps/sec)\n",
      "Step #4838\tEpoch   1 Batch 1712/3125   Loss: 0.702252 mae: 0.663288 (1580.43031010965 steps/sec)\n",
      "Step #4839\tEpoch   1 Batch 1713/3125   Loss: 0.766828 mae: 0.709068 (1640.2067902924316 steps/sec)\n",
      "Step #4840\tEpoch   1 Batch 1714/3125   Loss: 0.851466 mae: 0.751258 (1921.8239967742823 steps/sec)\n",
      "Step #4841\tEpoch   1 Batch 1715/3125   Loss: 0.727693 mae: 0.651351 (1931.9864761536264 steps/sec)\n",
      "Step #4842\tEpoch   1 Batch 1716/3125   Loss: 1.016747 mae: 0.770758 (2094.8685932333756 steps/sec)\n",
      "Step #4843\tEpoch   1 Batch 1717/3125   Loss: 0.827280 mae: 0.702143 (1951.1294704328086 steps/sec)\n",
      "Step #4844\tEpoch   1 Batch 1718/3125   Loss: 0.818556 mae: 0.700799 (2001.1374262867612 steps/sec)\n",
      "Step #4845\tEpoch   1 Batch 1719/3125   Loss: 0.844732 mae: 0.741203 (1823.4835837506955 steps/sec)\n",
      "Step #4846\tEpoch   1 Batch 1720/3125   Loss: 0.852861 mae: 0.727917 (1725.199078644291 steps/sec)\n",
      "Step #4847\tEpoch   1 Batch 1721/3125   Loss: 0.883118 mae: 0.726135 (2145.3581987253588 steps/sec)\n",
      "Step #4848\tEpoch   1 Batch 1722/3125   Loss: 0.734920 mae: 0.698018 (1795.9220025177053 steps/sec)\n",
      "Step #4849\tEpoch   1 Batch 1723/3125   Loss: 0.895179 mae: 0.726178 (1765.3686213108406 steps/sec)\n",
      "Step #4850\tEpoch   1 Batch 1724/3125   Loss: 0.955645 mae: 0.792375 (1816.0776604865039 steps/sec)\n",
      "Step #4851\tEpoch   1 Batch 1725/3125   Loss: 0.781963 mae: 0.691484 (1919.4142412593812 steps/sec)\n",
      "Step #4852\tEpoch   1 Batch 1726/3125   Loss: 0.862700 mae: 0.753254 (1799.9914169720794 steps/sec)\n",
      "Step #4853\tEpoch   1 Batch 1727/3125   Loss: 0.751590 mae: 0.689000 (1638.2208195978565 steps/sec)\n",
      "Step #4854\tEpoch   1 Batch 1728/3125   Loss: 0.944959 mae: 0.765428 (1792.0393758651924 steps/sec)\n",
      "Step #4855\tEpoch   1 Batch 1729/3125   Loss: 0.776890 mae: 0.701159 (1859.1443414123862 steps/sec)\n",
      "Step #4856\tEpoch   1 Batch 1730/3125   Loss: 0.841084 mae: 0.738371 (1906.0686207680073 steps/sec)\n",
      "Step #4857\tEpoch   1 Batch 1731/3125   Loss: 0.841862 mae: 0.720749 (1970.9889944643378 steps/sec)\n",
      "Step #4858\tEpoch   1 Batch 1732/3125   Loss: 0.886214 mae: 0.715622 (1703.0770105328124 steps/sec)\n",
      "Step #4859\tEpoch   1 Batch 1733/3125   Loss: 0.818175 mae: 0.724356 (1941.0705195249952 steps/sec)\n",
      "Step #4860\tEpoch   1 Batch 1734/3125   Loss: 0.866017 mae: 0.750959 (1942.670816659256 steps/sec)\n",
      "Step #4861\tEpoch   1 Batch 1735/3125   Loss: 0.880397 mae: 0.758192 (1820.776356801153 steps/sec)\n",
      "Step #4862\tEpoch   1 Batch 1736/3125   Loss: 0.704353 mae: 0.664296 (1493.6022619632645 steps/sec)\n",
      "Step #4863\tEpoch   1 Batch 1737/3125   Loss: 0.750486 mae: 0.694892 (2035.003008131659 steps/sec)\n",
      "Step #4864\tEpoch   1 Batch 1738/3125   Loss: 0.852057 mae: 0.717654 (2014.6520005763966 steps/sec)\n",
      "Step #4865\tEpoch   1 Batch 1739/3125   Loss: 0.831227 mae: 0.715508 (2225.49637600416 steps/sec)\n",
      "Step #4866\tEpoch   1 Batch 1740/3125   Loss: 0.923829 mae: 0.757256 (2063.4760705289673 steps/sec)\n",
      "Step #4867\tEpoch   1 Batch 1741/3125   Loss: 0.846558 mae: 0.712988 (2008.7470426528482 steps/sec)\n",
      "Step #4868\tEpoch   1 Batch 1742/3125   Loss: 0.892274 mae: 0.744007 (2024.5515803293881 steps/sec)\n",
      "Step #4869\tEpoch   1 Batch 1743/3125   Loss: 0.840127 mae: 0.747299 (1876.4613774035665 steps/sec)\n",
      "Step #4870\tEpoch   1 Batch 1744/3125   Loss: 0.843319 mae: 0.736233 (1741.733800641164 steps/sec)\n",
      "Step #4871\tEpoch   1 Batch 1745/3125   Loss: 0.678202 mae: 0.663222 (1726.6478947455087 steps/sec)\n",
      "Step #4872\tEpoch   1 Batch 1746/3125   Loss: 0.811582 mae: 0.719836 (2130.2129042743377 steps/sec)\n",
      "Step #4873\tEpoch   1 Batch 1747/3125   Loss: 0.808066 mae: 0.733217 (1972.3792863457668 steps/sec)\n",
      "Step #4874\tEpoch   1 Batch 1748/3125   Loss: 0.709530 mae: 0.663575 (1802.5906602143698 steps/sec)\n",
      "Step #4875\tEpoch   1 Batch 1749/3125   Loss: 0.842202 mae: 0.735404 (1776.2088270418146 steps/sec)\n",
      "Step #4876\tEpoch   1 Batch 1750/3125   Loss: 0.728921 mae: 0.686310 (2121.2493931056806 steps/sec)\n",
      "Step #4877\tEpoch   1 Batch 1751/3125   Loss: 0.878872 mae: 0.746970 (2017.3458001462157 steps/sec)\n",
      "Step #4878\tEpoch   1 Batch 1752/3125   Loss: 0.800993 mae: 0.702768 (1837.495509546048 steps/sec)\n",
      "Step #4879\tEpoch   1 Batch 1753/3125   Loss: 0.651933 mae: 0.641032 (1988.0667760008341 steps/sec)\n",
      "Step #4880\tEpoch   1 Batch 1754/3125   Loss: 0.869284 mae: 0.736320 (2031.1989694615825 steps/sec)\n",
      "Step #4881\tEpoch   1 Batch 1755/3125   Loss: 0.759828 mae: 0.701360 (2208.876998588612 steps/sec)\n",
      "Step #4882\tEpoch   1 Batch 1756/3125   Loss: 0.913266 mae: 0.773398 (1980.2013105961892 steps/sec)\n",
      "Step #4883\tEpoch   1 Batch 1757/3125   Loss: 0.767218 mae: 0.715204 (2104.1166260321666 steps/sec)\n",
      "Step #4884\tEpoch   1 Batch 1758/3125   Loss: 0.916794 mae: 0.782110 (1886.0128602904806 steps/sec)\n",
      "Step #4885\tEpoch   1 Batch 1759/3125   Loss: 1.001226 mae: 0.787957 (1900.9717186366934 steps/sec)\n",
      "Step #4886\tEpoch   1 Batch 1760/3125   Loss: 0.840078 mae: 0.747713 (1603.1801363789255 steps/sec)\n",
      "Step #4887\tEpoch   1 Batch 1761/3125   Loss: 0.763128 mae: 0.711438 (2092.3396188765837 steps/sec)\n",
      "Step #4888\tEpoch   1 Batch 1762/3125   Loss: 0.706981 mae: 0.677621 (2195.0283124522457 steps/sec)\n",
      "Step #4889\tEpoch   1 Batch 1763/3125   Loss: 0.748282 mae: 0.705905 (2306.463568875447 steps/sec)\n",
      "Step #4890\tEpoch   1 Batch 1764/3125   Loss: 0.969982 mae: 0.792198 (1995.7100577638628 steps/sec)\n",
      "Step #4891\tEpoch   1 Batch 1765/3125   Loss: 0.806681 mae: 0.730771 (1998.4105354437256 steps/sec)\n",
      "Step #4892\tEpoch   1 Batch 1766/3125   Loss: 0.797160 mae: 0.736855 (2171.4368548028037 steps/sec)\n",
      "Step #4893\tEpoch   1 Batch 1767/3125   Loss: 0.719744 mae: 0.667624 (2184.3740560584124 steps/sec)\n",
      "Step #4894\tEpoch   1 Batch 1768/3125   Loss: 0.812702 mae: 0.690061 (1710.913318376504 steps/sec)\n",
      "Step #4895\tEpoch   1 Batch 1769/3125   Loss: 0.899628 mae: 0.739959 (2058.715776453611 steps/sec)\n",
      "Step #4896\tEpoch   1 Batch 1770/3125   Loss: 0.960288 mae: 0.751808 (2021.0201701889812 steps/sec)\n",
      "Step #4897\tEpoch   1 Batch 1771/3125   Loss: 0.870037 mae: 0.740010 (2009.6517622707322 steps/sec)\n",
      "Step #4898\tEpoch   1 Batch 1772/3125   Loss: 0.942325 mae: 0.757932 (2347.094044834417 steps/sec)\n",
      "Step #4899\tEpoch   1 Batch 1773/3125   Loss: 0.746777 mae: 0.677131 (2071.7727834033094 steps/sec)\n",
      "Step #4900\tEpoch   1 Batch 1774/3125   Loss: 0.767859 mae: 0.680331 (2173.4397346875326 steps/sec)\n",
      "Step #4901\tEpoch   1 Batch 1775/3125   Loss: 0.877713 mae: 0.739852 (2061.7111847344154 steps/sec)\n",
      "Step #4902\tEpoch   1 Batch 1776/3125   Loss: 0.873089 mae: 0.737465 (1669.2817116658177 steps/sec)\n",
      "Step #4903\tEpoch   1 Batch 1777/3125   Loss: 0.831382 mae: 0.745720 (1885.1651759629647 steps/sec)\n",
      "Step #4904\tEpoch   1 Batch 1778/3125   Loss: 0.855324 mae: 0.731135 (1980.9120791929572 steps/sec)\n",
      "Step #4905\tEpoch   1 Batch 1779/3125   Loss: 0.764468 mae: 0.677324 (2020.53337444119 steps/sec)\n",
      "Step #4906\tEpoch   1 Batch 1780/3125   Loss: 0.872607 mae: 0.748276 (2031.0415960486175 steps/sec)\n",
      "Step #4907\tEpoch   1 Batch 1781/3125   Loss: 0.765415 mae: 0.689902 (1978.874661483152 steps/sec)\n",
      "Step #4908\tEpoch   1 Batch 1782/3125   Loss: 0.823767 mae: 0.749327 (1829.2732284288754 steps/sec)\n",
      "Step #4909\tEpoch   1 Batch 1783/3125   Loss: 0.935635 mae: 0.748531 (1876.7972364664715 steps/sec)\n",
      "Step #4910\tEpoch   1 Batch 1784/3125   Loss: 0.950030 mae: 0.763668 (1623.496806657635 steps/sec)\n",
      "Step #4911\tEpoch   1 Batch 1785/3125   Loss: 0.921663 mae: 0.743540 (1529.5509412219476 steps/sec)\n",
      "Step #4912\tEpoch   1 Batch 1786/3125   Loss: 0.768233 mae: 0.691691 (2048.240028128296 steps/sec)\n",
      "Step #4913\tEpoch   1 Batch 1787/3125   Loss: 0.752011 mae: 0.674824 (2067.8301682147153 steps/sec)\n",
      "Step #4914\tEpoch   1 Batch 1788/3125   Loss: 0.840654 mae: 0.727941 (1756.3057442193506 steps/sec)\n",
      "Step #4915\tEpoch   1 Batch 1789/3125   Loss: 0.745753 mae: 0.694098 (1922.2643861482336 steps/sec)\n",
      "Step #4916\tEpoch   1 Batch 1790/3125   Loss: 0.868061 mae: 0.739238 (1847.8249759897087 steps/sec)\n",
      "Step #4917\tEpoch   1 Batch 1791/3125   Loss: 0.930351 mae: 0.755314 (1888.8666720707577 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #4918\tEpoch   1 Batch 1792/3125   Loss: 0.820793 mae: 0.724822 (1488.3551921875887 steps/sec)\n",
      "Step #4919\tEpoch   1 Batch 1793/3125   Loss: 0.734337 mae: 0.663352 (1702.9525449053172 steps/sec)\n",
      "Step #4920\tEpoch   1 Batch 1794/3125   Loss: 0.784084 mae: 0.679026 (1926.4847187646405 steps/sec)\n",
      "Step #4921\tEpoch   1 Batch 1795/3125   Loss: 0.888986 mae: 0.735433 (1862.54573874738 steps/sec)\n",
      "Step #4922\tEpoch   1 Batch 1796/3125   Loss: 0.830035 mae: 0.725684 (1633.2959501557632 steps/sec)\n",
      "Step #4923\tEpoch   1 Batch 1797/3125   Loss: 0.820663 mae: 0.714913 (2052.8715604412814 steps/sec)\n",
      "Step #4924\tEpoch   1 Batch 1798/3125   Loss: 1.035733 mae: 0.796033 (1898.613940266348 steps/sec)\n",
      "Step #4925\tEpoch   1 Batch 1799/3125   Loss: 0.946518 mae: 0.766591 (1952.600951556288 steps/sec)\n",
      "Step #4926\tEpoch   1 Batch 1800/3125   Loss: 0.785538 mae: 0.706937 (1786.6500822123207 steps/sec)\n",
      "Step #4927\tEpoch   1 Batch 1801/3125   Loss: 0.785481 mae: 0.706360 (1575.5976619434718 steps/sec)\n",
      "Step #4928\tEpoch   1 Batch 1802/3125   Loss: 0.843959 mae: 0.736826 (1888.8666720707577 steps/sec)\n",
      "Step #4929\tEpoch   1 Batch 1803/3125   Loss: 0.765345 mae: 0.699319 (2087.0714449210313 steps/sec)\n",
      "Step #4930\tEpoch   1 Batch 1804/3125   Loss: 0.866254 mae: 0.747196 (1964.6924359670982 steps/sec)\n",
      "Step #4931\tEpoch   1 Batch 1805/3125   Loss: 0.874017 mae: 0.740415 (1893.5225815771892 steps/sec)\n",
      "Step #4932\tEpoch   1 Batch 1806/3125   Loss: 0.793132 mae: 0.704837 (1892.5144161786072 steps/sec)\n",
      "Step #4933\tEpoch   1 Batch 1807/3125   Loss: 0.891026 mae: 0.756220 (1928.4865650230813 steps/sec)\n",
      "Step #4934\tEpoch   1 Batch 1808/3125   Loss: 0.797091 mae: 0.690921 (1696.5190308619503 steps/sec)\n",
      "Step #4935\tEpoch   1 Batch 1809/3125   Loss: 0.816173 mae: 0.715675 (1588.0537339653788 steps/sec)\n",
      "Step #4936\tEpoch   1 Batch 1810/3125   Loss: 0.908312 mae: 0.754760 (1647.5128052037835 steps/sec)\n",
      "Step #4937\tEpoch   1 Batch 1811/3125   Loss: 0.878029 mae: 0.753545 (1795.6759626334672 steps/sec)\n",
      "Step #4938\tEpoch   1 Batch 1812/3125   Loss: 0.783557 mae: 0.715542 (1890.859255252006 steps/sec)\n",
      "Step #4939\tEpoch   1 Batch 1813/3125   Loss: 0.800262 mae: 0.709597 (1646.5168133533277 steps/sec)\n",
      "Step #4940\tEpoch   1 Batch 1814/3125   Loss: 0.806662 mae: 0.708116 (1648.5747975788067 steps/sec)\n",
      "Step #4941\tEpoch   1 Batch 1815/3125   Loss: 0.799758 mae: 0.699473 (1779.4944463772051 steps/sec)\n",
      "Step #4942\tEpoch   1 Batch 1816/3125   Loss: 0.871253 mae: 0.729038 (1835.9994396974366 steps/sec)\n",
      "Step #4943\tEpoch   1 Batch 1817/3125   Loss: 0.795653 mae: 0.706946 (1673.477660652585 steps/sec)\n",
      "Step #4944\tEpoch   1 Batch 1818/3125   Loss: 0.906117 mae: 0.782893 (1888.7816125081058 steps/sec)\n",
      "Step #4945\tEpoch   1 Batch 1819/3125   Loss: 0.794266 mae: 0.722409 (1635.8439937597504 steps/sec)\n",
      "Step #4946\tEpoch   1 Batch 1820/3125   Loss: 0.787859 mae: 0.695511 (2069.033830246944 steps/sec)\n",
      "Step #4947\tEpoch   1 Batch 1821/3125   Loss: 0.828677 mae: 0.723393 (2059.9897842913833 steps/sec)\n",
      "Step #4948\tEpoch   1 Batch 1822/3125   Loss: 0.817748 mae: 0.713820 (1926.555509622893 steps/sec)\n",
      "Step #4949\tEpoch   1 Batch 1823/3125   Loss: 0.853327 mae: 0.746170 (1621.5385329116762 steps/sec)\n",
      "Step #4950\tEpoch   1 Batch 1824/3125   Loss: 0.752560 mae: 0.700022 (1880.1287395892168 steps/sec)\n",
      "Step #4951\tEpoch   1 Batch 1825/3125   Loss: 0.940966 mae: 0.782714 (1826.7397193453132 steps/sec)\n",
      "Step #4952\tEpoch   1 Batch 1826/3125   Loss: 0.881042 mae: 0.748976 (1961.4032790564997 steps/sec)\n",
      "Step #4953\tEpoch   1 Batch 1827/3125   Loss: 0.858121 mae: 0.744895 (2017.7534035695387 steps/sec)\n",
      "Step #4954\tEpoch   1 Batch 1828/3125   Loss: 0.744599 mae: 0.676242 (2016.7831898831562 steps/sec)\n",
      "Step #4955\tEpoch   1 Batch 1829/3125   Loss: 0.842658 mae: 0.749070 (1665.5696041679903 steps/sec)\n",
      "Step #4956\tEpoch   1 Batch 1830/3125   Loss: 0.852577 mae: 0.739824 (1996.3369823893383 steps/sec)\n",
      "Step #4957\tEpoch   1 Batch 1831/3125   Loss: 0.738335 mae: 0.680681 (1610.827169312779 steps/sec)\n",
      "Step #4958\tEpoch   1 Batch 1832/3125   Loss: 0.726966 mae: 0.688459 (1968.8055652043297 steps/sec)\n",
      "Step #4959\tEpoch   1 Batch 1833/3125   Loss: 0.778591 mae: 0.703454 (2273.138372824037 steps/sec)\n",
      "Step #4960\tEpoch   1 Batch 1834/3125   Loss: 0.836304 mae: 0.735365 (2269.0311063024074 steps/sec)\n",
      "Step #4961\tEpoch   1 Batch 1835/3125   Loss: 0.928789 mae: 0.755836 (2215.6212692677464 steps/sec)\n",
      "Step #4962\tEpoch   1 Batch 1836/3125   Loss: 0.881433 mae: 0.756727 (2173.2595494207135 steps/sec)\n",
      "Step #4963\tEpoch   1 Batch 1837/3125   Loss: 0.799623 mae: 0.737265 (2165.919958688355 steps/sec)\n",
      "Step #4964\tEpoch   1 Batch 1838/3125   Loss: 0.839974 mae: 0.732489 (2013.7814480507009 steps/sec)\n",
      "Step #4965\tEpoch   1 Batch 1839/3125   Loss: 0.754554 mae: 0.679865 (1734.8896848967165 steps/sec)\n",
      "Step #4966\tEpoch   1 Batch 1840/3125   Loss: 0.864943 mae: 0.713909 (1776.7054119082313 steps/sec)\n",
      "Step #4967\tEpoch   1 Batch 1841/3125   Loss: 0.840655 mae: 0.702937 (2001.5194029280956 steps/sec)\n",
      "Step #4968\tEpoch   1 Batch 1842/3125   Loss: 0.769107 mae: 0.678822 (2116.8385989704248 steps/sec)\n",
      "Step #4969\tEpoch   1 Batch 1843/3125   Loss: 0.610783 mae: 0.627472 (2066.200319218113 steps/sec)\n",
      "Step #4970\tEpoch   1 Batch 1844/3125   Loss: 0.843223 mae: 0.729410 (2133.9194318100876 steps/sec)\n",
      "Step #4971\tEpoch   1 Batch 1845/3125   Loss: 0.847881 mae: 0.752341 (2156.2106085686964 steps/sec)\n",
      "Step #4972\tEpoch   1 Batch 1846/3125   Loss: 1.087566 mae: 0.816104 (2227.2454040505954 steps/sec)\n",
      "Step #4973\tEpoch   1 Batch 1847/3125   Loss: 0.841639 mae: 0.729198 (1945.8432304038004 steps/sec)\n",
      "Step #4974\tEpoch   1 Batch 1848/3125   Loss: 0.966827 mae: 0.788802 (2100.4076318293355 steps/sec)\n",
      "Step #4975\tEpoch   1 Batch 1849/3125   Loss: 0.792524 mae: 0.695624 (1724.5606677357016 steps/sec)\n",
      "Step #4976\tEpoch   1 Batch 1850/3125   Loss: 0.789514 mae: 0.711439 (1983.2724934273988 steps/sec)\n",
      "Step #4977\tEpoch   1 Batch 1851/3125   Loss: 0.846897 mae: 0.733409 (2197.719651240778 steps/sec)\n",
      "Step #4978\tEpoch   1 Batch 1852/3125   Loss: 0.925421 mae: 0.775814 (2151.0795646866954 steps/sec)\n",
      "Step #4979\tEpoch   1 Batch 1853/3125   Loss: 0.814994 mae: 0.727697 (2159.830274567962 steps/sec)\n",
      "Step #4980\tEpoch   1 Batch 1854/3125   Loss: 0.936916 mae: 0.774666 (2106.9493143115487 steps/sec)\n",
      "Step #4981\tEpoch   1 Batch 1855/3125   Loss: 0.799359 mae: 0.690247 (2300.114065104852 steps/sec)\n",
      "Step #4982\tEpoch   1 Batch 1856/3125   Loss: 0.909088 mae: 0.738388 (2117.650860327975 steps/sec)\n",
      "Step #4983\tEpoch   1 Batch 1857/3125   Loss: 0.882372 mae: 0.727278 (2200.2098283604014 steps/sec)\n",
      "Step #4984\tEpoch   1 Batch 1858/3125   Loss: 0.763173 mae: 0.668611 (1996.0899650685778 steps/sec)\n",
      "Step #4985\tEpoch   1 Batch 1859/3125   Loss: 0.926136 mae: 0.740088 (1753.588868820657 steps/sec)\n",
      "Step #4986\tEpoch   1 Batch 1860/3125   Loss: 0.896494 mae: 0.754924 (2116.2191344009525 steps/sec)\n",
      "Step #4987\tEpoch   1 Batch 1861/3125   Loss: 0.863904 mae: 0.724688 (2245.777558844317 steps/sec)\n",
      "Step #4988\tEpoch   1 Batch 1862/3125   Loss: 0.727351 mae: 0.665630 (2141.1526877329115 steps/sec)\n",
      "Step #4989\tEpoch   1 Batch 1863/3125   Loss: 0.770030 mae: 0.679261 (2182.055790820839 steps/sec)\n",
      "Step #4990\tEpoch   1 Batch 1864/3125   Loss: 0.706004 mae: 0.666326 (2334.1369216557036 steps/sec)\n",
      "Step #4991\tEpoch   1 Batch 1865/3125   Loss: 0.699065 mae: 0.683458 (2140.868535494804 steps/sec)\n",
      "Step #4992\tEpoch   1 Batch 1866/3125   Loss: 0.841821 mae: 0.743622 (2161.7895062364705 steps/sec)\n",
      "Step #4993\tEpoch   1 Batch 1867/3125   Loss: 0.703007 mae: 0.675340 (1769.0468759226299 steps/sec)\n",
      "Step #4994\tEpoch   1 Batch 1868/3125   Loss: 0.922347 mae: 0.773054 (1939.2576427290044 steps/sec)\n",
      "Step #4995\tEpoch   1 Batch 1869/3125   Loss: 0.883881 mae: 0.748184 (2158.6964353724693 steps/sec)\n",
      "Step #4996\tEpoch   1 Batch 1870/3125   Loss: 0.891198 mae: 0.749867 (2142.093113521685 steps/sec)\n",
      "Step #4997\tEpoch   1 Batch 1871/3125   Loss: 0.725201 mae: 0.685380 (2329.366551521143 steps/sec)\n",
      "Step #4998\tEpoch   1 Batch 1872/3125   Loss: 0.858296 mae: 0.738911 (2020.53337444119 steps/sec)\n",
      "Step #4999\tEpoch   1 Batch 1873/3125   Loss: 0.708488 mae: 0.677027 (1948.247447581357 steps/sec)\n",
      "Step #5000\tEpoch   1 Batch 1874/3125   Loss: 0.902100 mae: 0.749188 (2143.7134562701885 steps/sec)\n",
      "Step #5001\tEpoch   1 Batch 1875/3125   Loss: 0.845425 mae: 0.726637 (2249.825133563628 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5002\tEpoch   1 Batch 1876/3125   Loss: 0.899103 mae: 0.757499 (1675.817870898659 steps/sec)\n",
      "Step #5003\tEpoch   1 Batch 1877/3125   Loss: 0.837905 mae: 0.710432 (1954.639252127392 steps/sec)\n",
      "Step #5004\tEpoch   1 Batch 1878/3125   Loss: 0.821165 mae: 0.734672 (1969.2122782801393 steps/sec)\n",
      "Step #5005\tEpoch   1 Batch 1879/3125   Loss: 0.751462 mae: 0.678234 (2037.7317423918535 steps/sec)\n",
      "Step #5006\tEpoch   1 Batch 1880/3125   Loss: 0.845612 mae: 0.732759 (2188.9340027346643 steps/sec)\n",
      "Step #5007\tEpoch   1 Batch 1881/3125   Loss: 0.768390 mae: 0.696075 (2055.6686075006373 steps/sec)\n",
      "Step #5008\tEpoch   1 Batch 1882/3125   Loss: 0.773970 mae: 0.712867 (2189.6196372824375 steps/sec)\n",
      "Step #5009\tEpoch   1 Batch 1883/3125   Loss: 0.855961 mae: 0.732397 (2017.9281411773761 steps/sec)\n",
      "Step #5010\tEpoch   1 Batch 1884/3125   Loss: 0.926468 mae: 0.771887 (1842.8400702987697 steps/sec)\n",
      "Step #5011\tEpoch   1 Batch 1885/3125   Loss: 0.853609 mae: 0.716992 (1670.3852679033685 steps/sec)\n",
      "Step #5012\tEpoch   1 Batch 1886/3125   Loss: 0.815719 mae: 0.728045 (1937.842008482642 steps/sec)\n",
      "Step #5013\tEpoch   1 Batch 1887/3125   Loss: 0.760608 mae: 0.679549 (2111.4890103804837 steps/sec)\n",
      "Step #5014\tEpoch   1 Batch 1888/3125   Loss: 0.816756 mae: 0.705696 (1959.1861138618485 steps/sec)\n",
      "Step #5015\tEpoch   1 Batch 1889/3125   Loss: 0.918620 mae: 0.750598 (2076.8610673717776 steps/sec)\n",
      "Step #5016\tEpoch   1 Batch 1890/3125   Loss: 0.723084 mae: 0.651956 (2094.178266861057 steps/sec)\n",
      "Step #5017\tEpoch   1 Batch 1891/3125   Loss: 0.835642 mae: 0.727480 (1985.713744650229 steps/sec)\n",
      "Step #5018\tEpoch   1 Batch 1892/3125   Loss: 0.798606 mae: 0.693710 (2077.087336331042 steps/sec)\n",
      "Step #5019\tEpoch   1 Batch 1893/3125   Loss: 0.814952 mae: 0.740424 (2273.5573118244597 steps/sec)\n",
      "Step #5020\tEpoch   1 Batch 1894/3125   Loss: 0.728961 mae: 0.674347 (1798.3090089008558 steps/sec)\n",
      "Step #5021\tEpoch   1 Batch 1895/3125   Loss: 0.754239 mae: 0.694090 (1936.786110084965 steps/sec)\n",
      "Step #5022\tEpoch   1 Batch 1896/3125   Loss: 0.945031 mae: 0.777674 (1984.135634271874 steps/sec)\n",
      "Step #5023\tEpoch   1 Batch 1897/3125   Loss: 0.827747 mae: 0.740714 (1774.5705630489601 steps/sec)\n",
      "Step #5024\tEpoch   1 Batch 1898/3125   Loss: 0.787476 mae: 0.733123 (1901.0578797081087 steps/sec)\n",
      "Step #5025\tEpoch   1 Batch 1899/3125   Loss: 0.885423 mae: 0.733824 (1992.1649092808968 steps/sec)\n",
      "Step #5026\tEpoch   1 Batch 1900/3125   Loss: 0.714035 mae: 0.665749 (2057.927894333994 steps/sec)\n",
      "Step #5027\tEpoch   1 Batch 1901/3125   Loss: 0.889890 mae: 0.750751 (2033.5033452923494 steps/sec)\n",
      "Step #5028\tEpoch   1 Batch 1902/3125   Loss: 0.831093 mae: 0.724038 (1833.2389244379174 steps/sec)\n",
      "Step #5029\tEpoch   1 Batch 1903/3125   Loss: 0.847964 mae: 0.743896 (1965.8158434960303 steps/sec)\n",
      "Step #5030\tEpoch   1 Batch 1904/3125   Loss: 0.865447 mae: 0.741131 (1988.0667760008341 steps/sec)\n",
      "Step #5031\tEpoch   1 Batch 1905/3125   Loss: 0.759019 mae: 0.695245 (2090.7542918668873 steps/sec)\n",
      "Step #5032\tEpoch   1 Batch 1906/3125   Loss: 0.929304 mae: 0.798089 (2101.628468638199 steps/sec)\n",
      "Step #5033\tEpoch   1 Batch 1907/3125   Loss: 0.910049 mae: 0.756248 (2279.983909720486 steps/sec)\n",
      "Step #5034\tEpoch   1 Batch 1908/3125   Loss: 0.850741 mae: 0.746792 (2130.5591677503253 steps/sec)\n",
      "Step #5035\tEpoch   1 Batch 1909/3125   Loss: 0.747774 mae: 0.664299 (1977.531141264887 steps/sec)\n",
      "Step #5036\tEpoch   1 Batch 1910/3125   Loss: 0.746220 mae: 0.711266 (2124.8601767042232 steps/sec)\n",
      "Step #5037\tEpoch   1 Batch 1911/3125   Loss: 0.819534 mae: 0.725158 (1719.6959384660802 steps/sec)\n",
      "Step #5038\tEpoch   1 Batch 1912/3125   Loss: 0.882532 mae: 0.756326 (1936.3390425188127 steps/sec)\n",
      "Step #5039\tEpoch   1 Batch 1913/3125   Loss: 0.838235 mae: 0.716906 (2282.887747104415 steps/sec)\n",
      "Step #5040\tEpoch   1 Batch 1914/3125   Loss: 0.865832 mae: 0.720830 (1983.3475193402562 steps/sec)\n",
      "Step #5041\tEpoch   1 Batch 1915/3125   Loss: 0.724201 mae: 0.677839 (2069.4625905385938 steps/sec)\n",
      "Step #5042\tEpoch   1 Batch 1916/3125   Loss: 0.792830 mae: 0.695068 (2194.9134448328555 steps/sec)\n",
      "Step #5043\tEpoch   1 Batch 1917/3125   Loss: 0.803663 mae: 0.717957 (2137.0071839812504 steps/sec)\n",
      "Step #5044\tEpoch   1 Batch 1918/3125   Loss: 0.743180 mae: 0.679354 (2013.704102012598 steps/sec)\n",
      "Step #5045\tEpoch   1 Batch 1919/3125   Loss: 0.745997 mae: 0.684977 (2353.838038049273 steps/sec)\n",
      "Step #5046\tEpoch   1 Batch 1920/3125   Loss: 0.811924 mae: 0.678666 (1809.1061230827625 steps/sec)\n",
      "Step #5047\tEpoch   1 Batch 1921/3125   Loss: 0.705829 mae: 0.660631 (1782.0347883721522 steps/sec)\n",
      "Step #5048\tEpoch   1 Batch 1922/3125   Loss: 0.954695 mae: 0.771554 (2013.5300951484835 steps/sec)\n",
      "Step #5049\tEpoch   1 Batch 1923/3125   Loss: 0.838096 mae: 0.724298 (2217.565824257164 steps/sec)\n",
      "Step #5050\tEpoch   1 Batch 1924/3125   Loss: 1.004593 mae: 0.796461 (2029.7049059744684 steps/sec)\n",
      "Step #5051\tEpoch   1 Batch 1925/3125   Loss: 0.786842 mae: 0.697012 (2082.6360268925587 steps/sec)\n",
      "Step #5052\tEpoch   1 Batch 1926/3125   Loss: 0.803526 mae: 0.706331 (1989.2548186371225 steps/sec)\n",
      "Step #5053\tEpoch   1 Batch 1927/3125   Loss: 0.884417 mae: 0.745416 (2128.0081177067477 steps/sec)\n",
      "Step #5054\tEpoch   1 Batch 1928/3125   Loss: 0.838513 mae: 0.731944 (1931.9330827621784 steps/sec)\n",
      "Step #5055\tEpoch   1 Batch 1929/3125   Loss: 0.798202 mae: 0.719866 (1867.7710387331783 steps/sec)\n",
      "Step #5056\tEpoch   1 Batch 1930/3125   Loss: 0.937144 mae: 0.768684 (1919.3088426408947 steps/sec)\n",
      "Step #5057\tEpoch   1 Batch 1931/3125   Loss: 0.663489 mae: 0.666982 (2070.2184578631995 steps/sec)\n",
      "Step #5058\tEpoch   1 Batch 1932/3125   Loss: 0.909011 mae: 0.762064 (2055.003870613713 steps/sec)\n",
      "Step #5059\tEpoch   1 Batch 1933/3125   Loss: 0.659421 mae: 0.636084 (2088.2144421874377 steps/sec)\n",
      "Step #5060\tEpoch   1 Batch 1934/3125   Loss: 0.741099 mae: 0.689423 (2329.4700478745267 steps/sec)\n",
      "Step #5061\tEpoch   1 Batch 1935/3125   Loss: 0.751412 mae: 0.685327 (2133.2682311534277 steps/sec)\n",
      "Step #5062\tEpoch   1 Batch 1936/3125   Loss: 0.787554 mae: 0.703416 (2124.3435980551053 steps/sec)\n",
      "Step #5063\tEpoch   1 Batch 1937/3125   Loss: 1.001120 mae: 0.769340 (1773.8800920286913 steps/sec)\n",
      "Step #5064\tEpoch   1 Batch 1938/3125   Loss: 0.873405 mae: 0.737248 (2017.675752123842 steps/sec)\n",
      "Step #5065\tEpoch   1 Batch 1939/3125   Loss: 0.817385 mae: 0.702917 (1918.0625040013902 steps/sec)\n",
      "Step #5066\tEpoch   1 Batch 1940/3125   Loss: 0.758277 mae: 0.695180 (1998.3343656200866 steps/sec)\n",
      "Step #5067\tEpoch   1 Batch 1941/3125   Loss: 0.732292 mae: 0.695528 (1944.7064605569415 steps/sec)\n",
      "Step #5068\tEpoch   1 Batch 1942/3125   Loss: 0.685209 mae: 0.661206 (2037.9891742709153 steps/sec)\n",
      "Step #5069\tEpoch   1 Batch 1943/3125   Loss: 0.790163 mae: 0.705796 (2022.8136001929106 steps/sec)\n",
      "Step #5070\tEpoch   1 Batch 1944/3125   Loss: 0.846979 mae: 0.740723 (2233.4600679467926 steps/sec)\n",
      "Step #5071\tEpoch   1 Batch 1945/3125   Loss: 0.850185 mae: 0.723221 (2191.6562160354483 steps/sec)\n",
      "Step #5072\tEpoch   1 Batch 1946/3125   Loss: 0.872661 mae: 0.766309 (1784.886165368739 steps/sec)\n",
      "Step #5073\tEpoch   1 Batch 1947/3125   Loss: 0.795475 mae: 0.728901 (1976.2080663399925 steps/sec)\n",
      "Step #5074\tEpoch   1 Batch 1948/3125   Loss: 0.783888 mae: 0.717084 (2004.312256288707 steps/sec)\n",
      "Step #5075\tEpoch   1 Batch 1949/3125   Loss: 0.826474 mae: 0.707741 (2135.8319159987373 steps/sec)\n",
      "Step #5076\tEpoch   1 Batch 1950/3125   Loss: 0.776056 mae: 0.692892 (2059.90884802766 steps/sec)\n",
      "Step #5077\tEpoch   1 Batch 1951/3125   Loss: 0.793266 mae: 0.701949 (2173.6199498352025 steps/sec)\n",
      "Step #5078\tEpoch   1 Batch 1952/3125   Loss: 0.823187 mae: 0.709115 (2071.752316598502 steps/sec)\n",
      "Step #5079\tEpoch   1 Batch 1953/3125   Loss: 1.091330 mae: 0.823359 (2263.17879651213 steps/sec)\n",
      "Step #5080\tEpoch   1 Batch 1954/3125   Loss: 0.704349 mae: 0.674439 (2133.3116321652 steps/sec)\n",
      "Step #5081\tEpoch   1 Batch 1955/3125   Loss: 0.754461 mae: 0.690855 (2051.285261551704 steps/sec)\n",
      "Step #5082\tEpoch   1 Batch 1956/3125   Loss: 0.834584 mae: 0.748886 (1871.6048941999625 steps/sec)\n",
      "Step #5083\tEpoch   1 Batch 1957/3125   Loss: 0.895088 mae: 0.738973 (2116.4967805744504 steps/sec)\n",
      "Step #5084\tEpoch   1 Batch 1958/3125   Loss: 0.875548 mae: 0.730953 (2113.191120605395 steps/sec)\n",
      "Step #5085\tEpoch   1 Batch 1959/3125   Loss: 0.909510 mae: 0.735733 (2141.0652482414316 steps/sec)\n",
      "Step #5086\tEpoch   1 Batch 1960/3125   Loss: 0.720894 mae: 0.679703 (2152.2054145029865 steps/sec)\n",
      "Step #5087\tEpoch   1 Batch 1961/3125   Loss: 0.974103 mae: 0.783294 (2123.5261953462 steps/sec)\n",
      "Step #5088\tEpoch   1 Batch 1962/3125   Loss: 0.800713 mae: 0.719594 (2228.357701462088 steps/sec)\n",
      "Step #5089\tEpoch   1 Batch 1963/3125   Loss: 0.773386 mae: 0.708092 (2214.451495728752 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5090\tEpoch   1 Batch 1964/3125   Loss: 0.978652 mae: 0.766247 (1823.4994391645725 steps/sec)\n",
      "Step #5091\tEpoch   1 Batch 1965/3125   Loss: 0.759820 mae: 0.691077 (1957.7046946033997 steps/sec)\n",
      "Step #5092\tEpoch   1 Batch 1966/3125   Loss: 0.839659 mae: 0.744820 (2049.340877331848 steps/sec)\n",
      "Step #5093\tEpoch   1 Batch 1967/3125   Loss: 0.846370 mae: 0.738374 (1949.098479497379 steps/sec)\n",
      "Step #5094\tEpoch   1 Batch 1968/3125   Loss: 0.843369 mae: 0.719590 (2045.5629035719162 steps/sec)\n",
      "Step #5095\tEpoch   1 Batch 1969/3125   Loss: 0.717738 mae: 0.686356 (1984.8304451111594 steps/sec)\n",
      "Step #5096\tEpoch   1 Batch 1970/3125   Loss: 0.876726 mae: 0.740908 (2211.9056659494577 steps/sec)\n",
      "Step #5097\tEpoch   1 Batch 1971/3125   Loss: 0.761971 mae: 0.687203 (1988.0667760008341 steps/sec)\n",
      "Step #5098\tEpoch   1 Batch 1972/3125   Loss: 0.775599 mae: 0.695072 (2054.1584633618368 steps/sec)\n",
      "Step #5099\tEpoch   1 Batch 1973/3125   Loss: 0.794680 mae: 0.735630 (1891.6438158464425 steps/sec)\n",
      "Step #5100\tEpoch   1 Batch 1974/3125   Loss: 0.821748 mae: 0.712760 (2012.5639376985307 steps/sec)\n",
      "Step #5101\tEpoch   1 Batch 1975/3125   Loss: 0.901128 mae: 0.726346 (2103.4201921726744 steps/sec)\n",
      "Step #5102\tEpoch   1 Batch 1976/3125   Loss: 0.823698 mae: 0.718431 (2174.2734803479416 steps/sec)\n",
      "Step #5103\tEpoch   1 Batch 1977/3125   Loss: 0.900654 mae: 0.762739 (2121.077757100089 steps/sec)\n",
      "Step #5104\tEpoch   1 Batch 1978/3125   Loss: 0.965856 mae: 0.776695 (2020.1830266833638 steps/sec)\n",
      "Step #5105\tEpoch   1 Batch 1979/3125   Loss: 0.779465 mae: 0.708847 (2058.8572550559593 steps/sec)\n",
      "Step #5106\tEpoch   1 Batch 1980/3125   Loss: 0.783439 mae: 0.712443 (2161.143457785014 steps/sec)\n",
      "Step #5107\tEpoch   1 Batch 1981/3125   Loss: 0.901111 mae: 0.746260 (1969.8041609918753 steps/sec)\n",
      "Step #5108\tEpoch   1 Batch 1982/3125   Loss: 0.942619 mae: 0.782987 (1837.9142018316463 steps/sec)\n",
      "Step #5109\tEpoch   1 Batch 1983/3125   Loss: 0.794095 mae: 0.696629 (1910.740187324611 steps/sec)\n",
      "Step #5110\tEpoch   1 Batch 1984/3125   Loss: 0.730198 mae: 0.702516 (1876.0249402882266 steps/sec)\n",
      "Step #5111\tEpoch   1 Batch 1985/3125   Loss: 0.829083 mae: 0.732376 (2058.6349402675933 steps/sec)\n",
      "Step #5112\tEpoch   1 Batch 1986/3125   Loss: 0.855444 mae: 0.728466 (1855.9031495853947 steps/sec)\n",
      "Step #5113\tEpoch   1 Batch 1987/3125   Loss: 0.818614 mae: 0.748342 (1791.1516518055414 steps/sec)\n",
      "Step #5114\tEpoch   1 Batch 1988/3125   Loss: 0.738736 mae: 0.692567 (2115.066614222464 steps/sec)\n",
      "Step #5115\tEpoch   1 Batch 1989/3125   Loss: 0.780744 mae: 0.684004 (1860.1172578341893 steps/sec)\n",
      "Step #5116\tEpoch   1 Batch 1990/3125   Loss: 0.773540 mae: 0.685287 (1826.405629485125 steps/sec)\n",
      "Step #5117\tEpoch   1 Batch 1991/3125   Loss: 0.733257 mae: 0.681145 (2295.4564858090434 steps/sec)\n",
      "Step #5118\tEpoch   1 Batch 1992/3125   Loss: 0.908766 mae: 0.767016 (2039.1385094073605 steps/sec)\n",
      "Step #5119\tEpoch   1 Batch 1993/3125   Loss: 0.822883 mae: 0.714363 (1958.3445390706709 steps/sec)\n",
      "Step #5120\tEpoch   1 Batch 1994/3125   Loss: 0.853217 mae: 0.728770 (2047.7600281217044 steps/sec)\n",
      "Step #5121\tEpoch   1 Batch 1995/3125   Loss: 0.911687 mae: 0.760943 (2017.2875845285162 steps/sec)\n",
      "Step #5122\tEpoch   1 Batch 1996/3125   Loss: 0.829845 mae: 0.713742 (2092.5692732914918 steps/sec)\n",
      "Step #5123\tEpoch   1 Batch 1997/3125   Loss: 0.927492 mae: 0.737018 (2084.7270269195596 steps/sec)\n",
      "Step #5124\tEpoch   1 Batch 1998/3125   Loss: 0.890045 mae: 0.725130 (1904.4588532301714 steps/sec)\n",
      "Step #5125\tEpoch   1 Batch 1999/3125   Loss: 0.916478 mae: 0.763723 (1984.28582242071 steps/sec)\n",
      "Step #5126\tEpoch   1 Batch 2000/3125   Loss: 0.962591 mae: 0.785888 (2127.66268287239 steps/sec)\n",
      "Step #5127\tEpoch   1 Batch 2001/3125   Loss: 0.762679 mae: 0.682856 (2120.0913888271093 steps/sec)\n",
      "Step #5128\tEpoch   1 Batch 2002/3125   Loss: 0.799121 mae: 0.728461 (2233.935894841122 steps/sec)\n",
      "Step #5129\tEpoch   1 Batch 2003/3125   Loss: 0.718576 mae: 0.675304 (2118.1000090898992 steps/sec)\n",
      "Step #5130\tEpoch   1 Batch 2004/3125   Loss: 0.897546 mae: 0.766621 (2056.434595018631 steps/sec)\n",
      "Step #5131\tEpoch   1 Batch 2005/3125   Loss: 0.827605 mae: 0.725805 (1863.9362912400454 steps/sec)\n",
      "Step #5132\tEpoch   1 Batch 2006/3125   Loss: 0.902897 mae: 0.762976 (2025.5879767803501 steps/sec)\n",
      "Step #5133\tEpoch   1 Batch 2007/3125   Loss: 0.762085 mae: 0.689862 (1940.5496437494216 steps/sec)\n",
      "Step #5134\tEpoch   1 Batch 2008/3125   Loss: 0.855311 mae: 0.749520 (2115.962910272321 steps/sec)\n",
      "Step #5135\tEpoch   1 Batch 2009/3125   Loss: 0.802436 mae: 0.719034 (2039.416129377328 steps/sec)\n",
      "Step #5136\tEpoch   1 Batch 2010/3125   Loss: 0.736722 mae: 0.642760 (1979.6592249964601 steps/sec)\n",
      "Step #5137\tEpoch   1 Batch 2011/3125   Loss: 0.740459 mae: 0.678928 (1994.969654306425 steps/sec)\n",
      "Step #5138\tEpoch   1 Batch 2012/3125   Loss: 0.842639 mae: 0.741680 (2027.8992409224968 steps/sec)\n",
      "Step #5139\tEpoch   1 Batch 2013/3125   Loss: 0.744724 mae: 0.696272 (1951.8921836898048 steps/sec)\n",
      "Step #5140\tEpoch   1 Batch 2014/3125   Loss: 0.880453 mae: 0.715160 (1723.3135841831495 steps/sec)\n",
      "Step #5141\tEpoch   1 Batch 2015/3125   Loss: 0.951814 mae: 0.774480 (1851.7734942737814 steps/sec)\n",
      "Step #5142\tEpoch   1 Batch 2016/3125   Loss: 0.770477 mae: 0.712168 (1980.0330453665674 steps/sec)\n",
      "Step #5143\tEpoch   1 Batch 2017/3125   Loss: 0.804604 mae: 0.728287 (2065.3660169983946 steps/sec)\n",
      "Step #5144\tEpoch   1 Batch 2018/3125   Loss: 0.740237 mae: 0.689319 (2110.086832281888 steps/sec)\n",
      "Step #5145\tEpoch   1 Batch 2019/3125   Loss: 0.885010 mae: 0.718388 (2198.5029877345632 steps/sec)\n",
      "Step #5146\tEpoch   1 Batch 2020/3125   Loss: 0.837069 mae: 0.732158 (2116.753134021035 steps/sec)\n",
      "Step #5147\tEpoch   1 Batch 2021/3125   Loss: 0.733815 mae: 0.683499 (1897.0338944721345 steps/sec)\n",
      "Step #5148\tEpoch   1 Batch 2022/3125   Loss: 0.917244 mae: 0.786978 (1654.4925249497062 steps/sec)\n",
      "Step #5149\tEpoch   1 Batch 2023/3125   Loss: 1.061019 mae: 0.812871 (2034.4305074551576 steps/sec)\n",
      "Step #5150\tEpoch   1 Batch 2024/3125   Loss: 0.879101 mae: 0.721268 (1976.6737358028183 steps/sec)\n",
      "Step #5151\tEpoch   1 Batch 2025/3125   Loss: 0.856302 mae: 0.748286 (2000.011444157281 steps/sec)\n",
      "Step #5152\tEpoch   1 Batch 2026/3125   Loss: 0.876342 mae: 0.738376 (2037.8307469561078 steps/sec)\n",
      "Step #5153\tEpoch   1 Batch 2027/3125   Loss: 0.819486 mae: 0.705949 (2162.3690505650416 steps/sec)\n",
      "Step #5154\tEpoch   1 Batch 2028/3125   Loss: 0.804688 mae: 0.728323 (1992.505605594193 steps/sec)\n",
      "Step #5155\tEpoch   1 Batch 2029/3125   Loss: 1.008239 mae: 0.804924 (1894.3606883157943 steps/sec)\n",
      "Step #5156\tEpoch   1 Batch 2030/3125   Loss: 0.796495 mae: 0.707841 (1823.8959141430832 steps/sec)\n",
      "Step #5157\tEpoch   1 Batch 2031/3125   Loss: 0.915878 mae: 0.750755 (1897.257002243613 steps/sec)\n",
      "Step #5158\tEpoch   1 Batch 2032/3125   Loss: 0.801585 mae: 0.732052 (2096.0829976711875 steps/sec)\n",
      "Step #5159\tEpoch   1 Batch 2033/3125   Loss: 0.903185 mae: 0.760778 (2036.4257831465693 steps/sec)\n",
      "Step #5160\tEpoch   1 Batch 2034/3125   Loss: 0.825695 mae: 0.708075 (1898.167138835839 steps/sec)\n",
      "Step #5161\tEpoch   1 Batch 2035/3125   Loss: 1.002523 mae: 0.796402 (2016.9383613683794 steps/sec)\n",
      "Step #5162\tEpoch   1 Batch 2036/3125   Loss: 0.726333 mae: 0.675370 (2074.929505001435 steps/sec)\n",
      "Step #5163\tEpoch   1 Batch 2037/3125   Loss: 0.852719 mae: 0.724524 (1802.1879055058564 steps/sec)\n",
      "Step #5164\tEpoch   1 Batch 2038/3125   Loss: 0.910794 mae: 0.759085 (1907.8893740902474 steps/sec)\n",
      "Step #5165\tEpoch   1 Batch 2039/3125   Loss: 0.957688 mae: 0.779590 (2191.450097704212 steps/sec)\n",
      "Step #5166\tEpoch   1 Batch 2040/3125   Loss: 0.790492 mae: 0.727560 (2203.3304966327314 steps/sec)\n",
      "Step #5167\tEpoch   1 Batch 2041/3125   Loss: 0.791014 mae: 0.700817 (1876.898017631002 steps/sec)\n",
      "Step #5168\tEpoch   1 Batch 2042/3125   Loss: 0.793335 mae: 0.703533 (1988.538051620489 steps/sec)\n",
      "Step #5169\tEpoch   1 Batch 2043/3125   Loss: 0.792793 mae: 0.698851 (2054.1584633618368 steps/sec)\n",
      "Step #5170\tEpoch   1 Batch 2044/3125   Loss: 0.987412 mae: 0.789623 (2226.677850567512 steps/sec)\n",
      "Step #5171\tEpoch   1 Batch 2045/3125   Loss: 0.681532 mae: 0.662796 (1869.9360683364393 steps/sec)\n",
      "Step #5172\tEpoch   1 Batch 2046/3125   Loss: 0.755313 mae: 0.689383 (1779.6152507997929 steps/sec)\n",
      "Step #5173\tEpoch   1 Batch 2047/3125   Loss: 0.926597 mae: 0.758178 (2083.5050419750632 steps/sec)\n",
      "Step #5174\tEpoch   1 Batch 2048/3125   Loss: 0.910509 mae: 0.763689 (2090.921055255339 steps/sec)\n",
      "Step #5175\tEpoch   1 Batch 2049/3125   Loss: 0.806408 mae: 0.693938 (2200.0251772900842 steps/sec)\n",
      "Step #5176\tEpoch   1 Batch 2050/3125   Loss: 0.865992 mae: 0.738435 (1985.2438066207862 steps/sec)\n",
      "Step #5177\tEpoch   1 Batch 2051/3125   Loss: 0.813976 mae: 0.739645 (2010.5572972091998 steps/sec)\n",
      "Step #5178\tEpoch   1 Batch 2052/3125   Loss: 0.828279 mae: 0.720195 (2209.4587903115353 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5179\tEpoch   1 Batch 2053/3125   Loss: 0.922469 mae: 0.738050 (1980.7624012996334 steps/sec)\n",
      "Step #5180\tEpoch   1 Batch 2054/3125   Loss: 0.678219 mae: 0.671931 (1939.4728567465088 steps/sec)\n",
      "Step #5181\tEpoch   1 Batch 2055/3125   Loss: 0.773729 mae: 0.691301 (2099.2512512512512 steps/sec)\n",
      "Step #5182\tEpoch   1 Batch 2056/3125   Loss: 0.823775 mae: 0.695261 (2116.5608631147625 steps/sec)\n",
      "Step #5183\tEpoch   1 Batch 2057/3125   Loss: 0.755035 mae: 0.701228 (2040.9841170974773 steps/sec)\n",
      "Step #5184\tEpoch   1 Batch 2058/3125   Loss: 0.748670 mae: 0.678132 (2139.5798687982697 steps/sec)\n",
      "Step #5185\tEpoch   1 Batch 2059/3125   Loss: 0.923383 mae: 0.751215 (2042.4948381315985 steps/sec)\n",
      "Step #5186\tEpoch   1 Batch 2060/3125   Loss: 0.777809 mae: 0.685919 (2062.5425362418614 steps/sec)\n",
      "Step #5187\tEpoch   1 Batch 2061/3125   Loss: 0.724243 mae: 0.678761 (2169.303018391708 steps/sec)\n",
      "Step #5188\tEpoch   1 Batch 2062/3125   Loss: 0.783821 mae: 0.710269 (1979.3602703136355 steps/sec)\n",
      "Step #5189\tEpoch   1 Batch 2063/3125   Loss: 0.793278 mae: 0.710054 (1857.2180058271858 steps/sec)\n",
      "Step #5190\tEpoch   1 Batch 2064/3125   Loss: 0.702530 mae: 0.652787 (2180.5583571614243 steps/sec)\n",
      "Step #5191\tEpoch   1 Batch 2065/3125   Loss: 0.655048 mae: 0.652742 (2259.6916180891526 steps/sec)\n",
      "Step #5192\tEpoch   1 Batch 2066/3125   Loss: 0.732854 mae: 0.672576 (2069.033830246944 steps/sec)\n",
      "Step #5193\tEpoch   1 Batch 2067/3125   Loss: 0.855454 mae: 0.719533 (2099.860820458391 steps/sec)\n",
      "Step #5194\tEpoch   1 Batch 2068/3125   Loss: 0.752826 mae: 0.680924 (2049.761513800923 steps/sec)\n",
      "Step #5195\tEpoch   1 Batch 2069/3125   Loss: 0.906770 mae: 0.736257 (2073.554944728984 steps/sec)\n",
      "Step #5196\tEpoch   1 Batch 2070/3125   Loss: 0.817268 mae: 0.713146 (2034.4502434954695 steps/sec)\n",
      "Step #5197\tEpoch   1 Batch 2071/3125   Loss: 0.957675 mae: 0.774100 (1890.7058303807282 steps/sec)\n",
      "Step #5198\tEpoch   1 Batch 2072/3125   Loss: 0.751551 mae: 0.687461 (1758.7065177283553 steps/sec)\n",
      "Step #5199\tEpoch   1 Batch 2073/3125   Loss: 0.731303 mae: 0.662760 (1865.8765959339828 steps/sec)\n",
      "Step #5200\tEpoch   1 Batch 2074/3125   Loss: 0.898280 mae: 0.724869 (1987.9160149770132 steps/sec)\n",
      "Step #5201\tEpoch   1 Batch 2075/3125   Loss: 0.910285 mae: 0.765761 (1986.014621765976 steps/sec)\n",
      "Step #5202\tEpoch   1 Batch 2076/3125   Loss: 0.943388 mae: 0.766320 (2099.944926752581 steps/sec)\n",
      "Step #5203\tEpoch   1 Batch 2077/3125   Loss: 0.832586 mae: 0.725817 (2047.3402126267902 steps/sec)\n",
      "Step #5204\tEpoch   1 Batch 2078/3125   Loss: 0.875797 mae: 0.733186 (2073.5754471657256 steps/sec)\n",
      "Step #5205\tEpoch   1 Batch 2079/3125   Loss: 0.835958 mae: 0.712244 (1779.0566677977604 steps/sec)\n",
      "Step #5206\tEpoch   1 Batch 2080/3125   Loss: 0.961188 mae: 0.772660 (1865.7935943060497 steps/sec)\n",
      "Step #5207\tEpoch   1 Batch 2081/3125   Loss: 0.826266 mae: 0.732881 (1806.3635893813846 steps/sec)\n",
      "Step #5208\tEpoch   1 Batch 2082/3125   Loss: 0.862564 mae: 0.736642 (1947.270583210303 steps/sec)\n",
      "Step #5209\tEpoch   1 Batch 2083/3125   Loss: 0.779864 mae: 0.718485 (2131.468645187519 steps/sec)\n",
      "Step #5210\tEpoch   1 Batch 2084/3125   Loss: 0.891504 mae: 0.759770 (1983.1037058751217 steps/sec)\n",
      "Step #5211\tEpoch   1 Batch 2085/3125   Loss: 0.913525 mae: 0.769367 (1929.0897049083817 steps/sec)\n",
      "Step #5212\tEpoch   1 Batch 2086/3125   Loss: 0.941692 mae: 0.753528 (1996.0139720368907 steps/sec)\n",
      "Step #5213\tEpoch   1 Batch 2087/3125   Loss: 0.851026 mae: 0.736907 (1604.1611847137656 steps/sec)\n",
      "Step #5214\tEpoch   1 Batch 2088/3125   Loss: 0.826283 mae: 0.723183 (1399.1460290350128 steps/sec)\n",
      "Step #5215\tEpoch   1 Batch 2089/3125   Loss: 0.823985 mae: 0.704289 (1829.0817750488418 steps/sec)\n",
      "Step #5216\tEpoch   1 Batch 2090/3125   Loss: 0.728727 mae: 0.662717 (1803.3036673975666 steps/sec)\n",
      "Step #5217\tEpoch   1 Batch 2091/3125   Loss: 0.715834 mae: 0.678789 (1889.2069869467691 steps/sec)\n",
      "Step #5218\tEpoch   1 Batch 2092/3125   Loss: 0.968793 mae: 0.780843 (1751.4945504656116 steps/sec)\n",
      "Step #5219\tEpoch   1 Batch 2093/3125   Loss: 0.844857 mae: 0.729220 (1805.0575820695117 steps/sec)\n",
      "Step #5220\tEpoch   1 Batch 2094/3125   Loss: 0.835344 mae: 0.742610 (1711.9887671635456 steps/sec)\n",
      "Step #5221\tEpoch   1 Batch 2095/3125   Loss: 0.831730 mae: 0.701900 (1751.0704385291072 steps/sec)\n",
      "Step #5222\tEpoch   1 Batch 2096/3125   Loss: 0.638501 mae: 0.614628 (1700.50841273059 steps/sec)\n",
      "Step #5223\tEpoch   1 Batch 2097/3125   Loss: 0.725274 mae: 0.665985 (1840.188129584781 steps/sec)\n",
      "Step #5224\tEpoch   1 Batch 2098/3125   Loss: 0.802788 mae: 0.718026 (1941.8973100606509 steps/sec)\n",
      "Step #5225\tEpoch   1 Batch 2099/3125   Loss: 0.858990 mae: 0.714257 (2047.0004880429478 steps/sec)\n",
      "Step #5226\tEpoch   1 Batch 2100/3125   Loss: 0.803476 mae: 0.719174 (2150.1753234769412 steps/sec)\n",
      "Step #5227\tEpoch   1 Batch 2101/3125   Loss: 0.768761 mae: 0.687850 (1632.6983113657773 steps/sec)\n",
      "Step #5228\tEpoch   1 Batch 2102/3125   Loss: 0.872979 mae: 0.730635 (1484.4781697717879 steps/sec)\n",
      "Step #5229\tEpoch   1 Batch 2103/3125   Loss: 0.976014 mae: 0.794943 (1855.771766350757 steps/sec)\n",
      "Step #5230\tEpoch   1 Batch 2104/3125   Loss: 0.796977 mae: 0.674705 (1723.469370983383 steps/sec)\n",
      "Step #5231\tEpoch   1 Batch 2105/3125   Loss: 0.788824 mae: 0.712885 (2207.9932617393138 steps/sec)\n",
      "Step #5232\tEpoch   1 Batch 2106/3125   Loss: 0.775029 mae: 0.686286 (1898.5280005793848 steps/sec)\n",
      "Step #5233\tEpoch   1 Batch 2107/3125   Loss: 0.905076 mae: 0.758789 (1786.1479235512554 steps/sec)\n",
      "Step #5234\tEpoch   1 Batch 2108/3125   Loss: 0.886922 mae: 0.735299 (1837.495509546048 steps/sec)\n",
      "Step #5235\tEpoch   1 Batch 2109/3125   Loss: 0.795503 mae: 0.726591 (1836.2888114459836 steps/sec)\n",
      "Step #5236\tEpoch   1 Batch 2110/3125   Loss: 0.856488 mae: 0.725768 (1599.2313264955962 steps/sec)\n",
      "Step #5237\tEpoch   1 Batch 2111/3125   Loss: 0.857084 mae: 0.714592 (1958.6371787208607 steps/sec)\n",
      "Step #5238\tEpoch   1 Batch 2112/3125   Loss: 0.889598 mae: 0.734910 (1748.4863391167323 steps/sec)\n",
      "Step #5239\tEpoch   1 Batch 2113/3125   Loss: 0.905215 mae: 0.738582 (1924.999311566599 steps/sec)\n",
      "Step #5240\tEpoch   1 Batch 2114/3125   Loss: 0.895602 mae: 0.739636 (1940.7112649336948 steps/sec)\n",
      "Step #5241\tEpoch   1 Batch 2115/3125   Loss: 0.989176 mae: 0.778238 (2031.7696525799763 steps/sec)\n",
      "Step #5242\tEpoch   1 Batch 2116/3125   Loss: 0.841859 mae: 0.718427 (1739.3646844156922 steps/sec)\n",
      "Step #5243\tEpoch   1 Batch 2117/3125   Loss: 0.731791 mae: 0.673611 (1934.8384061113213 steps/sec)\n",
      "Step #5244\tEpoch   1 Batch 2118/3125   Loss: 0.821383 mae: 0.710567 (1565.6346818565275 steps/sec)\n",
      "Step #5245\tEpoch   1 Batch 2119/3125   Loss: 0.871796 mae: 0.722973 (1804.2034808193605 steps/sec)\n",
      "Step #5246\tEpoch   1 Batch 2120/3125   Loss: 0.775523 mae: 0.700876 (2033.2864719170843 steps/sec)\n",
      "Step #5247\tEpoch   1 Batch 2121/3125   Loss: 0.825364 mae: 0.703045 (1669.1089975725258 steps/sec)\n",
      "Step #5248\tEpoch   1 Batch 2122/3125   Loss: 0.875688 mae: 0.734076 (1795.9066230496514 steps/sec)\n",
      "Step #5249\tEpoch   1 Batch 2123/3125   Loss: 0.717586 mae: 0.692561 (1799.4045320771877 steps/sec)\n",
      "Step #5250\tEpoch   1 Batch 2124/3125   Loss: 0.666109 mae: 0.621095 (1736.7143117412259 steps/sec)\n",
      "Step #5251\tEpoch   1 Batch 2125/3125   Loss: 0.926880 mae: 0.765855 (1893.2319831002699 steps/sec)\n",
      "Step #5252\tEpoch   1 Batch 2126/3125   Loss: 0.901880 mae: 0.753419 (1558.4676550365994 steps/sec)\n",
      "Step #5253\tEpoch   1 Batch 2127/3125   Loss: 0.825855 mae: 0.712852 (1787.822884520298 steps/sec)\n",
      "Step #5254\tEpoch   1 Batch 2128/3125   Loss: 0.744392 mae: 0.675234 (2092.903406085646 steps/sec)\n",
      "Step #5255\tEpoch   1 Batch 2129/3125   Loss: 0.737441 mae: 0.705222 (1864.5660330387468 steps/sec)\n",
      "Step #5256\tEpoch   1 Batch 2130/3125   Loss: 0.811464 mae: 0.719796 (2070.4637226154864 steps/sec)\n",
      "Step #5257\tEpoch   1 Batch 2131/3125   Loss: 0.867719 mae: 0.724826 (1832.085823112136 steps/sec)\n",
      "Step #5258\tEpoch   1 Batch 2132/3125   Loss: 0.761307 mae: 0.692469 (1958.4725581569091 steps/sec)\n",
      "Step #5259\tEpoch   1 Batch 2133/3125   Loss: 0.780962 mae: 0.695018 (1600.671668562095 steps/sec)\n",
      "Step #5260\tEpoch   1 Batch 2134/3125   Loss: 0.745726 mae: 0.682150 (1929.2494227390227 steps/sec)\n",
      "Step #5261\tEpoch   1 Batch 2135/3125   Loss: 0.793191 mae: 0.692822 (1771.1834059660146 steps/sec)\n",
      "Step #5262\tEpoch   1 Batch 2136/3125   Loss: 0.932306 mae: 0.753423 (1910.9491179472227 steps/sec)\n",
      "Step #5263\tEpoch   1 Batch 2137/3125   Loss: 0.814853 mae: 0.708819 (1779.3736583544744 steps/sec)\n",
      "Step #5264\tEpoch   1 Batch 2138/3125   Loss: 0.737032 mae: 0.690897 (1878.1418758563868 steps/sec)\n",
      "Step #5265\tEpoch   1 Batch 2139/3125   Loss: 0.892758 mae: 0.743918 (1918.5187218120775 steps/sec)\n",
      "Step #5266\tEpoch   1 Batch 2140/3125   Loss: 0.803739 mae: 0.707823 (1793.2806019923896 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5267\tEpoch   1 Batch 2141/3125   Loss: 0.873760 mae: 0.750738 (1552.9397825893782 steps/sec)\n",
      "Step #5268\tEpoch   1 Batch 2142/3125   Loss: 0.680855 mae: 0.665516 (1687.9845460399226 steps/sec)\n",
      "Step #5269\tEpoch   1 Batch 2143/3125   Loss: 0.749761 mae: 0.681196 (1887.234865869352 steps/sec)\n",
      "Step #5270\tEpoch   1 Batch 2144/3125   Loss: 0.684070 mae: 0.653196 (1990.0476362187092 steps/sec)\n",
      "Step #5271\tEpoch   1 Batch 2145/3125   Loss: 0.924412 mae: 0.759246 (1993.6989609179668 steps/sec)\n",
      "Step #5272\tEpoch   1 Batch 2146/3125   Loss: 0.654232 mae: 0.660489 (2031.631872124001 steps/sec)\n",
      "Step #5273\tEpoch   1 Batch 2147/3125   Loss: 0.812938 mae: 0.726393 (1914.4729875298972 steps/sec)\n",
      "Step #5274\tEpoch   1 Batch 2148/3125   Loss: 0.761075 mae: 0.684875 (1992.4109560409283 steps/sec)\n",
      "Step #5275\tEpoch   1 Batch 2149/3125   Loss: 0.768387 mae: 0.690246 (1573.7177418749673 steps/sec)\n",
      "Step #5276\tEpoch   1 Batch 2150/3125   Loss: 0.832195 mae: 0.715421 (1589.8354938973544 steps/sec)\n",
      "Step #5277\tEpoch   1 Batch 2151/3125   Loss: 0.859803 mae: 0.740480 (1809.1061230827625 steps/sec)\n",
      "Step #5278\tEpoch   1 Batch 2152/3125   Loss: 0.952017 mae: 0.759968 (1869.1027709200453 steps/sec)\n",
      "Step #5279\tEpoch   1 Batch 2153/3125   Loss: 0.914465 mae: 0.784999 (1944.3278323753013 steps/sec)\n",
      "Step #5280\tEpoch   1 Batch 2154/3125   Loss: 0.766843 mae: 0.701073 (1878.2259797950849 steps/sec)\n",
      "Step #5281\tEpoch   1 Batch 2155/3125   Loss: 0.827647 mae: 0.722679 (1899.336140922882 steps/sec)\n",
      "Step #5282\tEpoch   1 Batch 2156/3125   Loss: 0.872665 mae: 0.729246 (1865.3289216209484 steps/sec)\n",
      "Step #5283\tEpoch   1 Batch 2157/3125   Loss: 0.769779 mae: 0.703575 (1406.861390256665 steps/sec)\n",
      "Step #5284\tEpoch   1 Batch 2158/3125   Loss: 0.733381 mae: 0.666538 (1514.2219687069035 steps/sec)\n",
      "Step #5285\tEpoch   1 Batch 2159/3125   Loss: 0.799924 mae: 0.720343 (1685.54251728018 steps/sec)\n",
      "Step #5286\tEpoch   1 Batch 2160/3125   Loss: 0.786895 mae: 0.703595 (1485.2666841363484 steps/sec)\n",
      "Step #5287\tEpoch   1 Batch 2161/3125   Loss: 0.875689 mae: 0.757386 (1796.121959575197 steps/sec)\n",
      "Step #5288\tEpoch   1 Batch 2162/3125   Loss: 0.900929 mae: 0.736800 (1760.9364110401116 steps/sec)\n",
      "Step #5289\tEpoch   1 Batch 2163/3125   Loss: 0.942647 mae: 0.768696 (1766.543402265931 steps/sec)\n",
      "Step #5290\tEpoch   1 Batch 2164/3125   Loss: 0.737727 mae: 0.686015 (1795.1379853454769 steps/sec)\n",
      "Step #5291\tEpoch   1 Batch 2165/3125   Loss: 0.766524 mae: 0.703914 (1574.2729743120092 steps/sec)\n",
      "Step #5292\tEpoch   1 Batch 2166/3125   Loss: 0.736316 mae: 0.690490 (1984.8304451111594 steps/sec)\n",
      "Step #5293\tEpoch   1 Batch 2167/3125   Loss: 0.832513 mae: 0.751442 (2059.3020287122686 steps/sec)\n",
      "Step #5294\tEpoch   1 Batch 2168/3125   Loss: 0.883939 mae: 0.756569 (1733.3410475332469 steps/sec)\n",
      "Step #5295\tEpoch   1 Batch 2169/3125   Loss: 0.707961 mae: 0.664797 (1855.3777282338474 steps/sec)\n",
      "Step #5296\tEpoch   1 Batch 2170/3125   Loss: 0.785687 mae: 0.699559 (1834.4416161510135 steps/sec)\n",
      "Step #5297\tEpoch   1 Batch 2171/3125   Loss: 0.806585 mae: 0.714714 (1934.856257150238 steps/sec)\n",
      "Step #5298\tEpoch   1 Batch 2172/3125   Loss: 0.825177 mae: 0.709526 (1534.2507443905508 steps/sec)\n",
      "Step #5299\tEpoch   1 Batch 2173/3125   Loss: 0.797500 mae: 0.694213 (1598.0005486299492 steps/sec)\n",
      "Step #5300\tEpoch   1 Batch 2174/3125   Loss: 0.737504 mae: 0.658812 (1941.4478800222182 steps/sec)\n",
      "Step #5301\tEpoch   1 Batch 2175/3125   Loss: 0.807704 mae: 0.704819 (1772.9203300419315 steps/sec)\n",
      "Step #5302\tEpoch   1 Batch 2176/3125   Loss: 0.758927 mae: 0.714693 (1918.659140188284 steps/sec)\n",
      "Step #5303\tEpoch   1 Batch 2177/3125   Loss: 0.868489 mae: 0.753380 (1781.8985147673588 steps/sec)\n",
      "Step #5304\tEpoch   1 Batch 2178/3125   Loss: 0.774489 mae: 0.701538 (1852.673239337079 steps/sec)\n",
      "Step #5305\tEpoch   1 Batch 2179/3125   Loss: 0.859866 mae: 0.734454 (1844.8827348382215 steps/sec)\n",
      "Step #5306\tEpoch   1 Batch 2180/3125   Loss: 0.852741 mae: 0.715456 (1416.076058772688 steps/sec)\n",
      "Step #5307\tEpoch   1 Batch 2181/3125   Loss: 0.944355 mae: 0.760987 (1787.822884520298 steps/sec)\n",
      "Step #5308\tEpoch   1 Batch 2182/3125   Loss: 0.700021 mae: 0.663685 (1748.1219678909024 steps/sec)\n",
      "Step #5309\tEpoch   1 Batch 2183/3125   Loss: 0.734622 mae: 0.659868 (1903.0244734621283 steps/sec)\n",
      "Step #5310\tEpoch   1 Batch 2184/3125   Loss: 0.850466 mae: 0.726699 (1864.9639839928857 steps/sec)\n",
      "Step #5311\tEpoch   1 Batch 2185/3125   Loss: 0.889334 mae: 0.747789 (1846.4420926587895 steps/sec)\n",
      "Step #5312\tEpoch   1 Batch 2186/3125   Loss: 0.818150 mae: 0.710929 (1696.807288379695 steps/sec)\n",
      "Step #5313\tEpoch   1 Batch 2187/3125   Loss: 0.848717 mae: 0.725205 (2004.1398686939153 steps/sec)\n",
      "Step #5314\tEpoch   1 Batch 2188/3125   Loss: 0.796101 mae: 0.702186 (1645.8190436577383 steps/sec)\n",
      "Step #5315\tEpoch   1 Batch 2189/3125   Loss: 0.916856 mae: 0.724357 (2088.9008416753823 steps/sec)\n",
      "Step #5316\tEpoch   1 Batch 2190/3125   Loss: 0.860484 mae: 0.717505 (1855.8374556427705 steps/sec)\n",
      "Step #5317\tEpoch   1 Batch 2191/3125   Loss: 0.859929 mae: 0.721955 (2175.9880469406603 steps/sec)\n",
      "Step #5318\tEpoch   1 Batch 2192/3125   Loss: 0.853453 mae: 0.721969 (2209.1562203729063 steps/sec)\n",
      "Step #5319\tEpoch   1 Batch 2193/3125   Loss: 0.891472 mae: 0.758103 (2136.4846830142933 steps/sec)\n",
      "Step #5320\tEpoch   1 Batch 2194/3125   Loss: 0.822278 mae: 0.717442 (2040.9642540850389 steps/sec)\n",
      "Step #5321\tEpoch   1 Batch 2195/3125   Loss: 0.869875 mae: 0.728425 (2236.4373160431687 steps/sec)\n",
      "Step #5322\tEpoch   1 Batch 2196/3125   Loss: 0.745408 mae: 0.677040 (1829.2891846863743 steps/sec)\n",
      "Step #5323\tEpoch   1 Batch 2197/3125   Loss: 0.803293 mae: 0.684815 (1603.7440925011088 steps/sec)\n",
      "Step #5324\tEpoch   1 Batch 2198/3125   Loss: 0.836093 mae: 0.719436 (1922.5815914924826 steps/sec)\n",
      "Step #5325\tEpoch   1 Batch 2199/3125   Loss: 0.756547 mae: 0.679745 (1806.503630834963 steps/sec)\n",
      "Step #5326\tEpoch   1 Batch 2200/3125   Loss: 0.774540 mae: 0.686350 (1974.013065004989 steps/sec)\n",
      "Step #5327\tEpoch   1 Batch 2201/3125   Loss: 0.925707 mae: 0.778028 (2188.8654628953136 steps/sec)\n",
      "Step #5328\tEpoch   1 Batch 2202/3125   Loss: 0.895449 mae: 0.746152 (2288.4429458430177 steps/sec)\n",
      "Step #5329\tEpoch   1 Batch 2203/3125   Loss: 0.826771 mae: 0.732626 (2022.501470715877 steps/sec)\n",
      "Step #5330\tEpoch   1 Batch 2204/3125   Loss: 0.809812 mae: 0.725188 (1623.7482095156981 steps/sec)\n",
      "Step #5331\tEpoch   1 Batch 2205/3125   Loss: 0.895184 mae: 0.725277 (1996.622078354834 steps/sec)\n",
      "Step #5332\tEpoch   1 Batch 2206/3125   Loss: 0.696409 mae: 0.660894 (1895.456476351443 steps/sec)\n",
      "Step #5333\tEpoch   1 Batch 2207/3125   Loss: 0.837896 mae: 0.706234 (1931.9152856208489 steps/sec)\n",
      "Step #5334\tEpoch   1 Batch 2208/3125   Loss: 0.852783 mae: 0.754654 (1982.0728503109465 steps/sec)\n",
      "Step #5335\tEpoch   1 Batch 2209/3125   Loss: 0.730164 mae: 0.668669 (2141.699346405229 steps/sec)\n",
      "Step #5336\tEpoch   1 Batch 2210/3125   Loss: 0.869169 mae: 0.729275 (2079.332123699892 steps/sec)\n",
      "Step #5337\tEpoch   1 Batch 2211/3125   Loss: 0.777386 mae: 0.706567 (2106.9704824482087 steps/sec)\n",
      "Step #5338\tEpoch   1 Batch 2212/3125   Loss: 0.788289 mae: 0.707911 (1891.6438158464425 steps/sec)\n",
      "Step #5339\tEpoch   1 Batch 2213/3125   Loss: 0.760564 mae: 0.690778 (1583.2819955305913 steps/sec)\n",
      "Step #5340\tEpoch   1 Batch 2214/3125   Loss: 0.851148 mae: 0.724794 (1647.5516344695927 steps/sec)\n",
      "Step #5341\tEpoch   1 Batch 2215/3125   Loss: 0.860115 mae: 0.735477 (1639.0786810163584 steps/sec)\n",
      "Step #5342\tEpoch   1 Batch 2216/3125   Loss: 0.847035 mae: 0.732531 (1972.6204697449982 steps/sec)\n",
      "Step #5343\tEpoch   1 Batch 2217/3125   Loss: 0.829596 mae: 0.748891 (2073.657460967241 steps/sec)\n",
      "Step #5344\tEpoch   1 Batch 2218/3125   Loss: 0.884377 mae: 0.765508 (1993.490494296578 steps/sec)\n",
      "Step #5345\tEpoch   1 Batch 2219/3125   Loss: 0.978917 mae: 0.780555 (2170.9872773010075 steps/sec)\n",
      "Step #5346\tEpoch   1 Batch 2220/3125   Loss: 0.814191 mae: 0.693678 (1688.8545290555342 steps/sec)\n",
      "Step #5347\tEpoch   1 Batch 2221/3125   Loss: 0.836036 mae: 0.726942 (1761.2321852980945 steps/sec)\n",
      "Step #5348\tEpoch   1 Batch 2222/3125   Loss: 0.954732 mae: 0.780876 (1936.4284395198522 steps/sec)\n",
      "Step #5349\tEpoch   1 Batch 2223/3125   Loss: 0.797644 mae: 0.699570 (1758.2641648640945 steps/sec)\n",
      "Step #5350\tEpoch   1 Batch 2224/3125   Loss: 0.722402 mae: 0.651399 (1954.5117336763035 steps/sec)\n",
      "Step #5351\tEpoch   1 Batch 2225/3125   Loss: 0.757410 mae: 0.691051 (1689.5892750680782 steps/sec)\n",
      "Step #5352\tEpoch   1 Batch 2226/3125   Loss: 0.799472 mae: 0.736588 (1983.8728597105287 steps/sec)\n",
      "Step #5353\tEpoch   1 Batch 2227/3125   Loss: 0.941785 mae: 0.733552 (1962.559658612364 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5354\tEpoch   1 Batch 2228/3125   Loss: 0.814995 mae: 0.708368 (1824.6893815473497 steps/sec)\n",
      "Step #5355\tEpoch   1 Batch 2229/3125   Loss: 0.841127 mae: 0.734708 (1741.849532384259 steps/sec)\n",
      "Step #5356\tEpoch   1 Batch 2230/3125   Loss: 0.732197 mae: 0.666453 (2163.2835790103463 steps/sec)\n",
      "Step #5357\tEpoch   1 Batch 2231/3125   Loss: 0.899954 mae: 0.741714 (2270.2346929938512 steps/sec)\n",
      "Step #5358\tEpoch   1 Batch 2232/3125   Loss: 0.739376 mae: 0.682815 (2204.4885473715194 steps/sec)\n",
      "Step #5359\tEpoch   1 Batch 2233/3125   Loss: 0.758466 mae: 0.695415 (1939.7957673523754 steps/sec)\n",
      "Step #5360\tEpoch   1 Batch 2234/3125   Loss: 0.819889 mae: 0.726878 (1763.0533837746952 steps/sec)\n",
      "Step #5361\tEpoch   1 Batch 2235/3125   Loss: 0.970308 mae: 0.791363 (2109.810865191147 steps/sec)\n",
      "Step #5362\tEpoch   1 Batch 2236/3125   Loss: 0.796567 mae: 0.713655 (1688.405832105564 steps/sec)\n",
      "Step #5363\tEpoch   1 Batch 2237/3125   Loss: 0.739211 mae: 0.699014 (1827.6790071812034 steps/sec)\n",
      "Step #5364\tEpoch   1 Batch 2238/3125   Loss: 0.868435 mae: 0.739335 (1764.3586680352005 steps/sec)\n",
      "Step #5365\tEpoch   1 Batch 2239/3125   Loss: 0.610955 mae: 0.616112 (2023.3991027063535 steps/sec)\n",
      "Step #5366\tEpoch   1 Batch 2240/3125   Loss: 0.855294 mae: 0.735452 (1903.3008122702727 steps/sec)\n",
      "Step #5367\tEpoch   1 Batch 2241/3125   Loss: 0.910698 mae: 0.755212 (1773.1151976326357 steps/sec)\n",
      "Step #5368\tEpoch   1 Batch 2242/3125   Loss: 0.744389 mae: 0.713476 (1712.7297970517375 steps/sec)\n",
      "Step #5369\tEpoch   1 Batch 2243/3125   Loss: 0.698516 mae: 0.658655 (1914.2982328026874 steps/sec)\n",
      "Step #5370\tEpoch   1 Batch 2244/3125   Loss: 0.887249 mae: 0.753447 (1484.2050135175302 steps/sec)\n",
      "Step #5371\tEpoch   1 Batch 2245/3125   Loss: 0.749473 mae: 0.679738 (1706.5416758212696 steps/sec)\n",
      "Step #5372\tEpoch   1 Batch 2246/3125   Loss: 0.782682 mae: 0.696203 (1631.1744071184673 steps/sec)\n",
      "Step #5373\tEpoch   1 Batch 2247/3125   Loss: 0.854172 mae: 0.723611 (1708.9335625870906 steps/sec)\n",
      "Step #5374\tEpoch   1 Batch 2248/3125   Loss: 0.851089 mae: 0.720197 (1701.2119343900579 steps/sec)\n",
      "Step #5375\tEpoch   1 Batch 2249/3125   Loss: 0.783665 mae: 0.713347 (1736.2254528595556 steps/sec)\n",
      "Step #5376\tEpoch   1 Batch 2250/3125   Loss: 0.925175 mae: 0.765405 (1696.1897136016953 steps/sec)\n",
      "Step #5377\tEpoch   1 Batch 2251/3125   Loss: 0.884849 mae: 0.731244 (1574.8995576782993 steps/sec)\n",
      "Step #5378\tEpoch   1 Batch 2252/3125   Loss: 0.901024 mae: 0.747775 (1571.9247749469691 steps/sec)\n",
      "Step #5379\tEpoch   1 Batch 2253/3125   Loss: 0.750331 mae: 0.679391 (1867.0061516821422 steps/sec)\n",
      "Step #5380\tEpoch   1 Batch 2254/3125   Loss: 0.922376 mae: 0.763952 (1797.6615806617522 steps/sec)\n",
      "Step #5381\tEpoch   1 Batch 2255/3125   Loss: 0.932791 mae: 0.756951 (2200.0482569789033 steps/sec)\n",
      "Step #5382\tEpoch   1 Batch 2256/3125   Loss: 0.714309 mae: 0.684136 (1894.5318216721623 steps/sec)\n",
      "Step #5383\tEpoch   1 Batch 2257/3125   Loss: 0.833155 mae: 0.733370 (1712.925647915969 steps/sec)\n",
      "Step #5384\tEpoch   1 Batch 2258/3125   Loss: 0.868098 mae: 0.757507 (1762.1201045263963 steps/sec)\n",
      "Step #5385\tEpoch   1 Batch 2259/3125   Loss: 0.823827 mae: 0.710171 (1724.3337910393764 steps/sec)\n",
      "Step #5386\tEpoch   1 Batch 2260/3125   Loss: 0.862863 mae: 0.700785 (1699.3371687869703 steps/sec)\n",
      "Step #5387\tEpoch   1 Batch 2261/3125   Loss: 0.862764 mae: 0.709871 (1878.4951630240057 steps/sec)\n",
      "Step #5388\tEpoch   1 Batch 2262/3125   Loss: 0.911195 mae: 0.750454 (2058.533903961679 steps/sec)\n",
      "Step #5389\tEpoch   1 Batch 2263/3125   Loss: 0.791779 mae: 0.715242 (1927.0866069377441 steps/sec)\n",
      "Step #5390\tEpoch   1 Batch 2264/3125   Loss: 0.761448 mae: 0.693597 (1954.7303469231772 steps/sec)\n",
      "Step #5391\tEpoch   1 Batch 2265/3125   Loss: 0.862360 mae: 0.733364 (1952.8009535160907 steps/sec)\n",
      "Step #5392\tEpoch   1 Batch 2266/3125   Loss: 0.891419 mae: 0.760060 (1901.5750102008433 steps/sec)\n",
      "Step #5393\tEpoch   1 Batch 2267/3125   Loss: 0.750708 mae: 0.689535 (1722.0683029372399 steps/sec)\n",
      "Step #5394\tEpoch   1 Batch 2268/3125   Loss: 0.875668 mae: 0.727923 (1902.7482148851811 steps/sec)\n",
      "Step #5395\tEpoch   1 Batch 2269/3125   Loss: 0.841480 mae: 0.717654 (2147.1153747709195 steps/sec)\n",
      "Step #5396\tEpoch   1 Batch 2270/3125   Loss: 0.845200 mae: 0.692981 (1928.3447045625908 steps/sec)\n",
      "Step #5397\tEpoch   1 Batch 2271/3125   Loss: 0.926308 mae: 0.753875 (1986.4284767082804 steps/sec)\n",
      "Step #5398\tEpoch   1 Batch 2272/3125   Loss: 0.875333 mae: 0.749477 (1781.7774001699236 steps/sec)\n",
      "Step #5399\tEpoch   1 Batch 2273/3125   Loss: 0.829307 mae: 0.718647 (1926.9449523582002 steps/sec)\n",
      "Step #5400\tEpoch   1 Batch 2274/3125   Loss: 0.868781 mae: 0.730828 (1609.109184378117 steps/sec)\n",
      "Step #5401\tEpoch   1 Batch 2275/3125   Loss: 0.725981 mae: 0.672641 (1794.354652406417 steps/sec)\n",
      "Step #5402\tEpoch   1 Batch 2276/3125   Loss: 0.820308 mae: 0.707461 (1690.3927843111967 steps/sec)\n",
      "Step #5403\tEpoch   1 Batch 2277/3125   Loss: 0.834580 mae: 0.734988 (2046.0817983140805 steps/sec)\n",
      "Step #5404\tEpoch   1 Batch 2278/3125   Loss: 0.890602 mae: 0.753789 (1898.3389606510189 steps/sec)\n",
      "Step #5405\tEpoch   1 Batch 2279/3125   Loss: 0.799201 mae: 0.733766 (1908.5491709288146 steps/sec)\n",
      "Step #5406\tEpoch   1 Batch 2280/3125   Loss: 0.822401 mae: 0.725142 (1748.617549944969 steps/sec)\n",
      "Step #5407\tEpoch   1 Batch 2281/3125   Loss: 0.937754 mae: 0.769135 (1921.8944455136136 steps/sec)\n",
      "Step #5408\tEpoch   1 Batch 2282/3125   Loss: 0.853833 mae: 0.756883 (1611.9167121434555 steps/sec)\n",
      "Step #5409\tEpoch   1 Batch 2283/3125   Loss: 0.763057 mae: 0.710517 (1918.2905857817132 steps/sec)\n",
      "Step #5410\tEpoch   1 Batch 2284/3125   Loss: 0.857463 mae: 0.750627 (2097.3197855828466 steps/sec)\n",
      "Step #5411\tEpoch   1 Batch 2285/3125   Loss: 0.985809 mae: 0.779562 (1807.7338160503405 steps/sec)\n",
      "Step #5412\tEpoch   1 Batch 2286/3125   Loss: 0.838912 mae: 0.746875 (1973.771540973732 steps/sec)\n",
      "Step #5413\tEpoch   1 Batch 2287/3125   Loss: 0.780349 mae: 0.704174 (1873.5109927905876 steps/sec)\n",
      "Step #5414\tEpoch   1 Batch 2288/3125   Loss: 0.807268 mae: 0.698842 (2038.6627652645598 steps/sec)\n",
      "Step #5415\tEpoch   1 Batch 2289/3125   Loss: 1.012719 mae: 0.781201 (2015.9107949629915 steps/sec)\n",
      "Step #5416\tEpoch   1 Batch 2290/3125   Loss: 0.741090 mae: 0.689760 (1781.8985147673588 steps/sec)\n",
      "Step #5417\tEpoch   1 Batch 2291/3125   Loss: 0.659837 mae: 0.651436 (1656.6751982810376 steps/sec)\n",
      "Step #5418\tEpoch   1 Batch 2292/3125   Loss: 0.802634 mae: 0.733664 (1786.4065760892713 steps/sec)\n",
      "Step #5419\tEpoch   1 Batch 2293/3125   Loss: 0.797034 mae: 0.720944 (1791.5494882878572 steps/sec)\n",
      "Step #5420\tEpoch   1 Batch 2294/3125   Loss: 0.750442 mae: 0.667649 (1801.7234121154324 steps/sec)\n",
      "Step #5421\tEpoch   1 Batch 2295/3125   Loss: 1.035422 mae: 0.793099 (2129.455845171248 steps/sec)\n",
      "Step #5422\tEpoch   1 Batch 2296/3125   Loss: 0.869249 mae: 0.751350 (1851.5119143263262 steps/sec)\n",
      "Step #5423\tEpoch   1 Batch 2297/3125   Loss: 0.929130 mae: 0.796618 (2163.8415980519612 steps/sec)\n",
      "Step #5424\tEpoch   1 Batch 2298/3125   Loss: 0.867540 mae: 0.727147 (1610.307679313231 steps/sec)\n",
      "Step #5425\tEpoch   1 Batch 2299/3125   Loss: 0.865147 mae: 0.710882 (1581.4791074377672 steps/sec)\n",
      "Step #5426\tEpoch   1 Batch 2300/3125   Loss: 0.817988 mae: 0.717184 (1719.7523473697158 steps/sec)\n",
      "Step #5427\tEpoch   1 Batch 2301/3125   Loss: 0.867802 mae: 0.726426 (1810.8088038475819 steps/sec)\n",
      "Step #5428\tEpoch   1 Batch 2302/3125   Loss: 0.842260 mae: 0.726976 (2045.7424911962385 steps/sec)\n",
      "Step #5429\tEpoch   1 Batch 2303/3125   Loss: 0.856901 mae: 0.728032 (2024.7861432405816 steps/sec)\n",
      "Step #5430\tEpoch   1 Batch 2304/3125   Loss: 0.777680 mae: 0.688745 (1914.1759234750225 steps/sec)\n",
      "Step #5431\tEpoch   1 Batch 2305/3125   Loss: 0.800005 mae: 0.706749 (1627.326551357559 steps/sec)\n",
      "Step #5432\tEpoch   1 Batch 2306/3125   Loss: 0.779255 mae: 0.704126 (1199.270310918459 steps/sec)\n",
      "Step #5433\tEpoch   1 Batch 2307/3125   Loss: 0.981125 mae: 0.791970 (1389.0815637129572 steps/sec)\n",
      "Step #5434\tEpoch   1 Batch 2308/3125   Loss: 0.925377 mae: 0.763427 (1464.7677983977426 steps/sec)\n",
      "Step #5435\tEpoch   1 Batch 2309/3125   Loss: 0.902580 mae: 0.750514 (1501.3652341372965 steps/sec)\n",
      "Step #5436\tEpoch   1 Batch 2310/3125   Loss: 0.828299 mae: 0.738629 (1889.3601686516874 steps/sec)\n",
      "Step #5437\tEpoch   1 Batch 2311/3125   Loss: 0.856954 mae: 0.732052 (2053.293647685439 steps/sec)\n",
      "Step #5438\tEpoch   1 Batch 2312/3125   Loss: 0.912311 mae: 0.724472 (1945.5183034306176 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5439\tEpoch   1 Batch 2313/3125   Loss: 0.763323 mae: 0.697631 (1559.5686770283335 steps/sec)\n",
      "Step #5440\tEpoch   1 Batch 2314/3125   Loss: 0.823213 mae: 0.706342 (1818.8340184905726 steps/sec)\n",
      "Step #5441\tEpoch   1 Batch 2315/3125   Loss: 0.920294 mae: 0.756945 (1940.9088385006942 steps/sec)\n",
      "Step #5442\tEpoch   1 Batch 2316/3125   Loss: 0.964656 mae: 0.792607 (1892.9927336733313 steps/sec)\n",
      "Step #5443\tEpoch   1 Batch 2317/3125   Loss: 0.886112 mae: 0.759319 (1952.3646384151336 steps/sec)\n",
      "Step #5444\tEpoch   1 Batch 2318/3125   Loss: 0.852537 mae: 0.723096 (1782.6710075568892 steps/sec)\n",
      "Step #5445\tEpoch   1 Batch 2319/3125   Loss: 0.868792 mae: 0.742932 (1797.8465125848707 steps/sec)\n",
      "Step #5446\tEpoch   1 Batch 2320/3125   Loss: 0.855148 mae: 0.749710 (1778.7247035673695 steps/sec)\n",
      "Step #5447\tEpoch   1 Batch 2321/3125   Loss: 0.777932 mae: 0.687979 (1527.1784564745635 steps/sec)\n",
      "Step #5448\tEpoch   1 Batch 2322/3125   Loss: 0.796020 mae: 0.691504 (1948.1750536475704 steps/sec)\n",
      "Step #5449\tEpoch   1 Batch 2323/3125   Loss: 0.807380 mae: 0.711481 (1948.86301331673 steps/sec)\n",
      "Step #5450\tEpoch   1 Batch 2324/3125   Loss: 0.690089 mae: 0.641407 (1950.0953124854707 steps/sec)\n",
      "Step #5451\tEpoch   1 Batch 2325/3125   Loss: 0.764950 mae: 0.698053 (1847.743572573966 steps/sec)\n",
      "Step #5452\tEpoch   1 Batch 2326/3125   Loss: 0.752749 mae: 0.680044 (2009.0741876149602 steps/sec)\n",
      "Step #5453\tEpoch   1 Batch 2327/3125   Loss: 0.828659 mae: 0.705226 (1897.6862031833934 steps/sec)\n",
      "Step #5454\tEpoch   1 Batch 2328/3125   Loss: 0.871824 mae: 0.747223 (1889.7858037540664 steps/sec)\n",
      "Step #5455\tEpoch   1 Batch 2329/3125   Loss: 0.825134 mae: 0.716922 (1731.6092808190901 steps/sec)\n",
      "Step #5456\tEpoch   1 Batch 2330/3125   Loss: 0.897250 mae: 0.768812 (1756.6146784380078 steps/sec)\n",
      "Step #5457\tEpoch   1 Batch 2331/3125   Loss: 0.767523 mae: 0.712713 (1574.3557020276712 steps/sec)\n",
      "Step #5458\tEpoch   1 Batch 2332/3125   Loss: 0.852535 mae: 0.733850 (1978.072061875118 steps/sec)\n",
      "Step #5459\tEpoch   1 Batch 2333/3125   Loss: 0.772553 mae: 0.704964 (1974.1803085786366 steps/sec)\n",
      "Step #5460\tEpoch   1 Batch 2334/3125   Loss: 0.746792 mae: 0.684911 (1905.0643605279652 steps/sec)\n",
      "Step #5461\tEpoch   1 Batch 2335/3125   Loss: 0.883451 mae: 0.762740 (1912.1164875042168 steps/sec)\n",
      "Step #5462\tEpoch   1 Batch 2336/3125   Loss: 0.708536 mae: 0.663818 (1772.5457050366401 steps/sec)\n",
      "Step #5463\tEpoch   1 Batch 2337/3125   Loss: 0.896040 mae: 0.746398 (1497.5806221257392 steps/sec)\n",
      "Step #5464\tEpoch   1 Batch 2338/3125   Loss: 0.810720 mae: 0.709494 (1973.2515360513366 steps/sec)\n",
      "Step #5465\tEpoch   1 Batch 2339/3125   Loss: 0.727399 mae: 0.681636 (1909.278951201748 steps/sec)\n",
      "Step #5466\tEpoch   1 Batch 2340/3125   Loss: 0.818802 mae: 0.707138 (2008.3430694681197 steps/sec)\n",
      "Step #5467\tEpoch   1 Batch 2341/3125   Loss: 0.797415 mae: 0.723794 (2042.1567195427147 steps/sec)\n",
      "Step #5468\tEpoch   1 Batch 2342/3125   Loss: 0.823770 mae: 0.731408 (1911.680735082314 steps/sec)\n",
      "Step #5469\tEpoch   1 Batch 2343/3125   Loss: 0.750099 mae: 0.688394 (2053.534918334574 steps/sec)\n",
      "Step #5470\tEpoch   1 Batch 2344/3125   Loss: 0.902316 mae: 0.758660 (2252.652609643705 steps/sec)\n",
      "Step #5471\tEpoch   1 Batch 2345/3125   Loss: 0.848895 mae: 0.699286 (1858.6006115123853 steps/sec)\n",
      "Step #5472\tEpoch   1 Batch 2346/3125   Loss: 0.737596 mae: 0.668092 (1632.4568368284215 steps/sec)\n",
      "Step #5473\tEpoch   1 Batch 2347/3125   Loss: 0.895903 mae: 0.748642 (1911.0013577423206 steps/sec)\n",
      "Step #5474\tEpoch   1 Batch 2348/3125   Loss: 1.007510 mae: 0.780228 (1893.7106634279367 steps/sec)\n",
      "Step #5475\tEpoch   1 Batch 2349/3125   Loss: 0.821568 mae: 0.700271 (2135.1360706977125 steps/sec)\n",
      "Step #5476\tEpoch   1 Batch 2350/3125   Loss: 0.917418 mae: 0.767739 (2059.46381223608 steps/sec)\n",
      "Step #5477\tEpoch   1 Batch 2351/3125   Loss: 0.814810 mae: 0.719433 (1909.0703856097296 steps/sec)\n",
      "Step #5478\tEpoch   1 Batch 2352/3125   Loss: 0.816344 mae: 0.721431 (2077.2519265437113 steps/sec)\n",
      "Step #5479\tEpoch   1 Batch 2353/3125   Loss: 0.851946 mae: 0.716663 (1975.8915364104882 steps/sec)\n",
      "Step #5480\tEpoch   1 Batch 2354/3125   Loss: 0.824814 mae: 0.717502 (1933.1975184594537 steps/sec)\n",
      "Step #5481\tEpoch   1 Batch 2355/3125   Loss: 0.837790 mae: 0.745019 (2185.967874751139 steps/sec)\n",
      "Step #5482\tEpoch   1 Batch 2356/3125   Loss: 0.912721 mae: 0.782923 (2003.6994573110144 steps/sec)\n",
      "Step #5483\tEpoch   1 Batch 2357/3125   Loss: 0.805491 mae: 0.716405 (2002.9531150014805 steps/sec)\n",
      "Step #5484\tEpoch   1 Batch 2358/3125   Loss: 0.780059 mae: 0.705860 (1976.6923671461157 steps/sec)\n",
      "Step #5485\tEpoch   1 Batch 2359/3125   Loss: 0.869338 mae: 0.732418 (2013.5300951484835 steps/sec)\n",
      "Step #5486\tEpoch   1 Batch 2360/3125   Loss: 0.830453 mae: 0.721754 (2161.8786466816487 steps/sec)\n",
      "Step #5487\tEpoch   1 Batch 2361/3125   Loss: 0.726602 mae: 0.677357 (2195.395969641455 steps/sec)\n",
      "Step #5488\tEpoch   1 Batch 2362/3125   Loss: 0.890579 mae: 0.746337 (1848.0203734545871 steps/sec)\n",
      "Step #5489\tEpoch   1 Batch 2363/3125   Loss: 0.767868 mae: 0.707158 (1891.6438158464425 steps/sec)\n",
      "Step #5490\tEpoch   1 Batch 2364/3125   Loss: 0.866157 mae: 0.729769 (1799.0031997117685 steps/sec)\n",
      "Step #5491\tEpoch   1 Batch 2365/3125   Loss: 0.687563 mae: 0.634459 (1767.5260642736137 steps/sec)\n",
      "Step #5492\tEpoch   1 Batch 2366/3125   Loss: 0.814094 mae: 0.724027 (1927.1397327746229 steps/sec)\n",
      "Step #5493\tEpoch   1 Batch 2367/3125   Loss: 0.890081 mae: 0.729788 (1803.8775826179703 steps/sec)\n",
      "Step #5494\tEpoch   1 Batch 2368/3125   Loss: 0.908788 mae: 0.756303 (2089.6918002730254 steps/sec)\n",
      "Step #5495\tEpoch   1 Batch 2369/3125   Loss: 0.813799 mae: 0.723080 (2010.3838337359562 steps/sec)\n",
      "Step #5496\tEpoch   1 Batch 2370/3125   Loss: 0.829259 mae: 0.725139 (1598.3172014328177 steps/sec)\n",
      "Step #5497\tEpoch   1 Batch 2371/3125   Loss: 0.859163 mae: 0.726308 (1710.8016609154613 steps/sec)\n",
      "Step #5498\tEpoch   1 Batch 2372/3125   Loss: 0.817379 mae: 0.731338 (2081.0860159568133 steps/sec)\n",
      "Step #5499\tEpoch   1 Batch 2373/3125   Loss: 0.759255 mae: 0.684134 (2148.611239178321 steps/sec)\n",
      "Step #5500\tEpoch   1 Batch 2374/3125   Loss: 0.800970 mae: 0.709905 (1948.9354583894801 steps/sec)\n",
      "Step #5501\tEpoch   1 Batch 2375/3125   Loss: 0.910571 mae: 0.771108 (1972.230895104106 steps/sec)\n",
      "Step #5502\tEpoch   1 Batch 2376/3125   Loss: 0.806404 mae: 0.705291 (1970.211286791992 steps/sec)\n",
      "Step #5503\tEpoch   1 Batch 2377/3125   Loss: 0.756977 mae: 0.683990 (1857.4976528316593 steps/sec)\n",
      "Step #5504\tEpoch   1 Batch 2378/3125   Loss: 0.955779 mae: 0.779890 (1717.0910639134065 steps/sec)\n",
      "Step #5505\tEpoch   1 Batch 2379/3125   Loss: 0.911923 mae: 0.778105 (1598.4146583131355 steps/sec)\n",
      "Step #5506\tEpoch   1 Batch 2380/3125   Loss: 0.819584 mae: 0.711757 (1723.3135841831495 steps/sec)\n",
      "Step #5507\tEpoch   1 Batch 2381/3125   Loss: 0.798923 mae: 0.712702 (1934.4636103680473 steps/sec)\n",
      "Step #5508\tEpoch   1 Batch 2382/3125   Loss: 0.824817 mae: 0.721854 (1910.740187324611 steps/sec)\n",
      "Step #5509\tEpoch   1 Batch 2383/3125   Loss: 0.803568 mae: 0.715522 (1489.4228105935242 steps/sec)\n",
      "Step #5510\tEpoch   1 Batch 2384/3125   Loss: 0.918998 mae: 0.782215 (2118.3567510782937 steps/sec)\n",
      "Step #5511\tEpoch   1 Batch 2385/3125   Loss: 0.884871 mae: 0.750412 (2235.0310664918843 steps/sec)\n",
      "Step #5512\tEpoch   1 Batch 2386/3125   Loss: 0.788938 mae: 0.690302 (1647.176361551391 steps/sec)\n",
      "Step #5513\tEpoch   1 Batch 2387/3125   Loss: 0.820186 mae: 0.732631 (1856.8726757570391 steps/sec)\n",
      "Step #5514\tEpoch   1 Batch 2388/3125   Loss: 0.787094 mae: 0.689098 (1850.3520443275866 steps/sec)\n",
      "Step #5515\tEpoch   1 Batch 2389/3125   Loss: 0.854105 mae: 0.763034 (1911.401957746222 steps/sec)\n",
      "Step #5516\tEpoch   1 Batch 2390/3125   Loss: 0.858341 mae: 0.739798 (1617.162113185432 steps/sec)\n",
      "Step #5517\tEpoch   1 Batch 2391/3125   Loss: 0.853899 mae: 0.730732 (1999.1153816823 steps/sec)\n",
      "Step #5518\tEpoch   1 Batch 2392/3125   Loss: 0.816399 mae: 0.711207 (1891.4220262092228 steps/sec)\n",
      "Step #5519\tEpoch   1 Batch 2393/3125   Loss: 0.834718 mae: 0.725647 (1890.2797807903087 steps/sec)\n",
      "Step #5520\tEpoch   1 Batch 2394/3125   Loss: 0.787039 mae: 0.691246 (1529.2386445671118 steps/sec)\n",
      "Step #5521\tEpoch   1 Batch 2395/3125   Loss: 0.793927 mae: 0.728112 (1563.6735089511396 steps/sec)\n",
      "Step #5522\tEpoch   1 Batch 2396/3125   Loss: 0.933648 mae: 0.781779 (1780.491573629919 steps/sec)\n",
      "Step #5523\tEpoch   1 Batch 2397/3125   Loss: 0.726969 mae: 0.667892 (2018.335979981714 steps/sec)\n",
      "Step #5524\tEpoch   1 Batch 2398/3125   Loss: 0.731182 mae: 0.681528 (1947.957904123204 steps/sec)\n",
      "Step #5525\tEpoch   1 Batch 2399/3125   Loss: 0.752101 mae: 0.685593 (1852.395042972097 steps/sec)\n",
      "Step #5526\tEpoch   1 Batch 2400/3125   Loss: 0.880411 mae: 0.733713 (1730.5947301969782 steps/sec)\n",
      "Step #5527\tEpoch   1 Batch 2401/3125   Loss: 0.827054 mae: 0.695494 (2088.713597067846 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5528\tEpoch   1 Batch 2402/3125   Loss: 0.768765 mae: 0.696200 (1459.568633728416 steps/sec)\n",
      "Step #5529\tEpoch   1 Batch 2403/3125   Loss: 0.806070 mae: 0.719446 (1581.2883134901185 steps/sec)\n",
      "Step #5530\tEpoch   1 Batch 2404/3125   Loss: 0.854783 mae: 0.737848 (1970.2853277464087 steps/sec)\n",
      "Step #5531\tEpoch   1 Batch 2405/3125   Loss: 0.839623 mae: 0.725943 (1992.032447733123 steps/sec)\n",
      "Step #5532\tEpoch   1 Batch 2406/3125   Loss: 0.859698 mae: 0.730034 (1907.8893740902474 steps/sec)\n",
      "Step #5533\tEpoch   1 Batch 2407/3125   Loss: 0.749072 mae: 0.709952 (1910.2876610008927 steps/sec)\n",
      "Step #5534\tEpoch   1 Batch 2408/3125   Loss: 0.928407 mae: 0.770816 (2017.6563401962671 steps/sec)\n",
      "Step #5535\tEpoch   1 Batch 2409/3125   Loss: 0.697189 mae: 0.669175 (2004.0058099533676 steps/sec)\n",
      "Step #5536\tEpoch   1 Batch 2410/3125   Loss: 0.903017 mae: 0.743761 (1590.8002730789653 steps/sec)\n",
      "Step #5537\tEpoch   1 Batch 2411/3125   Loss: 0.906052 mae: 0.770983 (1810.8713485135006 steps/sec)\n",
      "Step #5538\tEpoch   1 Batch 2412/3125   Loss: 0.892333 mae: 0.784090 (2076.408677313637 steps/sec)\n",
      "Step #5539\tEpoch   1 Batch 2413/3125   Loss: 0.862921 mae: 0.734996 (1891.2002885742627 steps/sec)\n",
      "Step #5540\tEpoch   1 Batch 2414/3125   Loss: 0.871209 mae: 0.743365 (1756.8501298483707 steps/sec)\n",
      "Step #5541\tEpoch   1 Batch 2415/3125   Loss: 0.866311 mae: 0.749673 (1788.7225676586236 steps/sec)\n",
      "Step #5542\tEpoch   1 Batch 2416/3125   Loss: 0.797369 mae: 0.711260 (1883.438260572804 steps/sec)\n",
      "Step #5543\tEpoch   1 Batch 2417/3125   Loss: 0.783865 mae: 0.696834 (2059.1604889783493 steps/sec)\n",
      "Step #5544\tEpoch   1 Batch 2418/3125   Loss: 0.726569 mae: 0.668809 (1716.4445899492553 steps/sec)\n",
      "Step #5545\tEpoch   1 Batch 2419/3125   Loss: 0.778669 mae: 0.710656 (1637.900952053671 steps/sec)\n",
      "Step #5546\tEpoch   1 Batch 2420/3125   Loss: 0.773805 mae: 0.690000 (1801.2127458558791 steps/sec)\n",
      "Step #5547\tEpoch   1 Batch 2421/3125   Loss: 0.747308 mae: 0.690726 (2081.251240522409 steps/sec)\n",
      "Step #5548\tEpoch   1 Batch 2422/3125   Loss: 0.790249 mae: 0.696712 (1944.7064605569415 steps/sec)\n",
      "Step #5549\tEpoch   1 Batch 2423/3125   Loss: 0.675674 mae: 0.629208 (1990.5198515523412 steps/sec)\n",
      "Step #5550\tEpoch   1 Batch 2424/3125   Loss: 0.784379 mae: 0.708207 (1965.7237125770955 steps/sec)\n",
      "Step #5551\tEpoch   1 Batch 2425/3125   Loss: 0.782507 mae: 0.700740 (2093.1122932739813 steps/sec)\n",
      "Step #5552\tEpoch   1 Batch 2426/3125   Loss: 0.957834 mae: 0.778675 (1685.718649272147 steps/sec)\n",
      "Step #5553\tEpoch   1 Batch 2427/3125   Loss: 0.892203 mae: 0.729554 (1813.9570286821438 steps/sec)\n",
      "Step #5554\tEpoch   1 Batch 2428/3125   Loss: 0.777453 mae: 0.682056 (2065.264318915938 steps/sec)\n",
      "Step #5555\tEpoch   1 Batch 2429/3125   Loss: 0.886590 mae: 0.718632 (2157.031185715461 steps/sec)\n",
      "Step #5556\tEpoch   1 Batch 2430/3125   Loss: 0.802015 mae: 0.723045 (1945.6988050174423 steps/sec)\n",
      "Step #5557\tEpoch   1 Batch 2431/3125   Loss: 0.778581 mae: 0.717497 (2087.798661994266 steps/sec)\n",
      "Step #5558\tEpoch   1 Batch 2432/3125   Loss: 0.808483 mae: 0.708231 (2015.484565409603 steps/sec)\n",
      "Step #5559\tEpoch   1 Batch 2433/3125   Loss: 0.712445 mae: 0.676223 (2038.6429474093516 steps/sec)\n",
      "Step #5560\tEpoch   1 Batch 2434/3125   Loss: 0.942015 mae: 0.749503 (1916.2222912592972 steps/sec)\n",
      "Step #5561\tEpoch   1 Batch 2435/3125   Loss: 0.773107 mae: 0.674524 (1837.4794098061893 steps/sec)\n",
      "Step #5562\tEpoch   1 Batch 2436/3125   Loss: 0.773790 mae: 0.697634 (2061.6909162406605 steps/sec)\n",
      "Step #5563\tEpoch   1 Batch 2437/3125   Loss: 0.778504 mae: 0.690204 (1892.8560468621663 steps/sec)\n",
      "Step #5564\tEpoch   1 Batch 2438/3125   Loss: 0.822650 mae: 0.736894 (2181.3295056219513 steps/sec)\n",
      "Step #5565\tEpoch   1 Batch 2439/3125   Loss: 0.777053 mae: 0.692184 (1928.7171327931724 steps/sec)\n",
      "Step #5566\tEpoch   1 Batch 2440/3125   Loss: 0.782205 mae: 0.720682 (2026.3512860648927 steps/sec)\n",
      "Step #5567\tEpoch   1 Batch 2441/3125   Loss: 0.752337 mae: 0.681301 (2019.0549543651557 steps/sec)\n",
      "Step #5568\tEpoch   1 Batch 2442/3125   Loss: 0.751814 mae: 0.666267 (1962.4678326455369 steps/sec)\n",
      "Step #5569\tEpoch   1 Batch 2443/3125   Loss: 0.784315 mae: 0.706645 (1691.5926598104456 steps/sec)\n",
      "Step #5570\tEpoch   1 Batch 2444/3125   Loss: 0.785571 mae: 0.712291 (2154.261471612446 steps/sec)\n",
      "Step #5571\tEpoch   1 Batch 2445/3125   Loss: 0.891898 mae: 0.753376 (2089.5876925529583 steps/sec)\n",
      "Step #5572\tEpoch   1 Batch 2446/3125   Loss: 0.759161 mae: 0.688543 (2160.2974958023015 steps/sec)\n",
      "Step #5573\tEpoch   1 Batch 2447/3125   Loss: 0.765051 mae: 0.698678 (2008.2661406163215 steps/sec)\n",
      "Step #5574\tEpoch   1 Batch 2448/3125   Loss: 0.784157 mae: 0.704405 (2134.6579400059036 steps/sec)\n",
      "Step #5575\tEpoch   1 Batch 2449/3125   Loss: 0.826276 mae: 0.734223 (2224.009501993722 steps/sec)\n",
      "Step #5576\tEpoch   1 Batch 2450/3125   Loss: 0.899610 mae: 0.761305 (2056.79763048979 steps/sec)\n",
      "Step #5577\tEpoch   1 Batch 2451/3125   Loss: 0.844883 mae: 0.736514 (1778.1516025097508 steps/sec)\n",
      "Step #5578\tEpoch   1 Batch 2452/3125   Loss: 0.873185 mae: 0.743219 (1939.1141932501155 steps/sec)\n",
      "Step #5579\tEpoch   1 Batch 2453/3125   Loss: 0.877343 mae: 0.731723 (2072.202679735979 steps/sec)\n",
      "Step #5580\tEpoch   1 Batch 2454/3125   Loss: 0.755129 mae: 0.677415 (2122.9672821509557 steps/sec)\n",
      "Step #5581\tEpoch   1 Batch 2455/3125   Loss: 0.890342 mae: 0.740981 (2144.0641230114916 steps/sec)\n",
      "Step #5582\tEpoch   1 Batch 2456/3125   Loss: 0.761636 mae: 0.719018 (1888.9177114858048 steps/sec)\n",
      "Step #5583\tEpoch   1 Batch 2457/3125   Loss: 0.774293 mae: 0.687011 (2139.7763447881807 steps/sec)\n",
      "Step #5584\tEpoch   1 Batch 2458/3125   Loss: 0.820551 mae: 0.729293 (2110.4902986877064 steps/sec)\n",
      "Step #5585\tEpoch   1 Batch 2459/3125   Loss: 0.778795 mae: 0.706279 (1947.8855317054142 steps/sec)\n",
      "Step #5586\tEpoch   1 Batch 2460/3125   Loss: 0.848536 mae: 0.731640 (2004.6379582277875 steps/sec)\n",
      "Step #5587\tEpoch   1 Batch 2461/3125   Loss: 0.725029 mae: 0.673672 (2039.1385094073605 steps/sec)\n",
      "Step #5588\tEpoch   1 Batch 2462/3125   Loss: 0.856639 mae: 0.733057 (2390.5161407988326 steps/sec)\n",
      "Step #5589\tEpoch   1 Batch 2463/3125   Loss: 0.825930 mae: 0.706236 (2027.4091260634184 steps/sec)\n",
      "Step #5590\tEpoch   1 Batch 2464/3125   Loss: 0.863812 mae: 0.728063 (2323.3279787292972 steps/sec)\n",
      "Step #5591\tEpoch   1 Batch 2465/3125   Loss: 0.722274 mae: 0.692358 (2008.5161809353242 steps/sec)\n",
      "Step #5592\tEpoch   1 Batch 2466/3125   Loss: 0.751321 mae: 0.663321 (2117.2874032044747 steps/sec)\n",
      "Step #5593\tEpoch   1 Batch 2467/3125   Loss: 0.759742 mae: 0.697974 (1852.5423129924739 steps/sec)\n",
      "Step #5594\tEpoch   1 Batch 2468/3125   Loss: 1.022856 mae: 0.813901 (2091.5257956098094 steps/sec)\n",
      "Step #5595\tEpoch   1 Batch 2469/3125   Loss: 0.859114 mae: 0.706677 (2194.2244915041433 steps/sec)\n",
      "Step #5596\tEpoch   1 Batch 2470/3125   Loss: 0.882730 mae: 0.760754 (2037.9099575344728 steps/sec)\n",
      "Step #5597\tEpoch   1 Batch 2471/3125   Loss: 0.830742 mae: 0.726907 (2214.521647307286 steps/sec)\n",
      "Step #5598\tEpoch   1 Batch 2472/3125   Loss: 0.753031 mae: 0.662977 (2234.435731333106 steps/sec)\n",
      "Step #5599\tEpoch   1 Batch 2473/3125   Loss: 0.848107 mae: 0.731616 (2111.255184633351 steps/sec)\n",
      "Step #5600\tEpoch   1 Batch 2474/3125   Loss: 0.903127 mae: 0.743322 (2099.7136506537977 steps/sec)\n",
      "Step #5601\tEpoch   1 Batch 2475/3125   Loss: 0.874237 mae: 0.750280 (1860.1832551291036 steps/sec)\n",
      "Step #5602\tEpoch   1 Batch 2476/3125   Loss: 0.824898 mae: 0.715861 (1728.1418671149456 steps/sec)\n",
      "Step #5603\tEpoch   1 Batch 2477/3125   Loss: 0.852051 mae: 0.736042 (1958.381114244626 steps/sec)\n",
      "Step #5604\tEpoch   1 Batch 2478/3125   Loss: 0.804811 mae: 0.725171 (1983.9854688564292 steps/sec)\n",
      "Step #5605\tEpoch   1 Batch 2479/3125   Loss: 0.828662 mae: 0.732304 (2158.3409663973653 steps/sec)\n",
      "Step #5606\tEpoch   1 Batch 2480/3125   Loss: 0.915814 mae: 0.741466 (2197.2130838379817 steps/sec)\n",
      "Step #5607\tEpoch   1 Batch 2481/3125   Loss: 0.805148 mae: 0.714426 (2166.1213022640886 steps/sec)\n",
      "Step #5608\tEpoch   1 Batch 2482/3125   Loss: 0.900700 mae: 0.744259 (2009.3244291996818 steps/sec)\n",
      "Step #5609\tEpoch   1 Batch 2483/3125   Loss: 0.910036 mae: 0.775838 (2321.5790464171455 steps/sec)\n",
      "Step #5610\tEpoch   1 Batch 2484/3125   Loss: 0.748070 mae: 0.701986 (2214.615189659542 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5611\tEpoch   1 Batch 2485/3125   Loss: 0.775208 mae: 0.699539 (1779.7511753823176 steps/sec)\n",
      "Step #5612\tEpoch   1 Batch 2486/3125   Loss: 0.853571 mae: 0.699396 (2110.256694070176 steps/sec)\n",
      "Step #5613\tEpoch   1 Batch 2487/3125   Loss: 0.734663 mae: 0.657607 (1966.1660197633646 steps/sec)\n",
      "Step #5614\tEpoch   1 Batch 2488/3125   Loss: 0.831636 mae: 0.731137 (1987.8406430392706 steps/sec)\n",
      "Step #5615\tEpoch   1 Batch 2489/3125   Loss: 0.909370 mae: 0.769015 (2108.813739982101 steps/sec)\n",
      "Step #5616\tEpoch   1 Batch 2490/3125   Loss: 0.926438 mae: 0.756104 (2072.202679735979 steps/sec)\n",
      "Step #5617\tEpoch   1 Batch 2491/3125   Loss: 0.844340 mae: 0.720554 (2037.5733550969649 steps/sec)\n",
      "Step #5618\tEpoch   1 Batch 2492/3125   Loss: 0.945286 mae: 0.759883 (2077.107908681226 steps/sec)\n",
      "Step #5619\tEpoch   1 Batch 2493/3125   Loss: 0.882750 mae: 0.734507 (2107.478645362275 steps/sec)\n",
      "Step #5620\tEpoch   1 Batch 2494/3125   Loss: 0.798018 mae: 0.697834 (2012.7764127764128 steps/sec)\n",
      "Step #5621\tEpoch   1 Batch 2495/3125   Loss: 0.854230 mae: 0.713331 (1955.787667400306 steps/sec)\n",
      "Step #5622\tEpoch   1 Batch 2496/3125   Loss: 0.845169 mae: 0.744804 (1868.4033748207014 steps/sec)\n",
      "Step #5623\tEpoch   1 Batch 2497/3125   Loss: 0.718910 mae: 0.688369 (2169.303018391708 steps/sec)\n",
      "Step #5624\tEpoch   1 Batch 2498/3125   Loss: 0.873306 mae: 0.728028 (2268.7120015578007 steps/sec)\n",
      "Step #5625\tEpoch   1 Batch 2499/3125   Loss: 0.905264 mae: 0.765535 (2079.0229201364104 steps/sec)\n",
      "Step #5626\tEpoch   1 Batch 2500/3125   Loss: 0.784402 mae: 0.711052 (1998.7914716786916 steps/sec)\n",
      "Step #5627\tEpoch   1 Batch 2501/3125   Loss: 0.893352 mae: 0.735307 (2076.8404999108716 steps/sec)\n",
      "Step #5628\tEpoch   1 Batch 2502/3125   Loss: 0.759810 mae: 0.695402 (1996.4510110049123 steps/sec)\n",
      "Step #5629\tEpoch   1 Batch 2503/3125   Loss: 0.835480 mae: 0.726875 (1816.5338507379945 steps/sec)\n",
      "Step #5630\tEpoch   1 Batch 2504/3125   Loss: 0.725708 mae: 0.680889 (2086.5315543881643 steps/sec)\n",
      "Step #5631\tEpoch   1 Batch 2505/3125   Loss: 0.800473 mae: 0.694545 (2066.0171219718836 steps/sec)\n",
      "Step #5632\tEpoch   1 Batch 2506/3125   Loss: 0.978345 mae: 0.776491 (2173.822727602542 steps/sec)\n",
      "Step #5633\tEpoch   1 Batch 2507/3125   Loss: 0.667392 mae: 0.645493 (1953.9654144305307 steps/sec)\n",
      "Step #5634\tEpoch   1 Batch 2508/3125   Loss: 0.870178 mae: 0.737098 (2152.6693423389206 steps/sec)\n",
      "Step #5635\tEpoch   1 Batch 2509/3125   Loss: 0.781047 mae: 0.709689 (1856.741155221872 steps/sec)\n",
      "Step #5636\tEpoch   1 Batch 2510/3125   Loss: 0.913532 mae: 0.762515 (1744.2689489399572 steps/sec)\n",
      "Step #5637\tEpoch   1 Batch 2511/3125   Loss: 0.797351 mae: 0.703235 (2076.490915391851 steps/sec)\n",
      "Step #5638\tEpoch   1 Batch 2512/3125   Loss: 0.872913 mae: 0.742217 (2061.0830466830466 steps/sec)\n",
      "Step #5639\tEpoch   1 Batch 2513/3125   Loss: 0.905617 mae: 0.746605 (2105.067052115956 steps/sec)\n",
      "Step #5640\tEpoch   1 Batch 2514/3125   Loss: 0.833508 mae: 0.727585 (1971.674626753413 steps/sec)\n",
      "Step #5641\tEpoch   1 Batch 2515/3125   Loss: 0.895337 mae: 0.746845 (2054.7421225897474 steps/sec)\n",
      "Step #5642\tEpoch   1 Batch 2516/3125   Loss: 0.716152 mae: 0.675286 (2129.3044979185706 steps/sec)\n",
      "Step #5643\tEpoch   1 Batch 2517/3125   Loss: 0.754106 mae: 0.697547 (1956.0248099612927 steps/sec)\n",
      "Step #5644\tEpoch   1 Batch 2518/3125   Loss: 0.885171 mae: 0.754747 (2038.9799033572185 steps/sec)\n",
      "Step #5645\tEpoch   1 Batch 2519/3125   Loss: 0.796742 mae: 0.713533 (2071.9365324006835 steps/sec)\n",
      "Step #5646\tEpoch   1 Batch 2520/3125   Loss: 0.893379 mae: 0.760161 (2063.7197402086204 steps/sec)\n",
      "Step #5647\tEpoch   1 Batch 2521/3125   Loss: 0.958867 mae: 0.783946 (2174.5665698880134 steps/sec)\n",
      "Step #5648\tEpoch   1 Batch 2522/3125   Loss: 0.747233 mae: 0.682306 (1968.9719275185428 steps/sec)\n",
      "Step #5649\tEpoch   1 Batch 2523/3125   Loss: 0.883201 mae: 0.752659 (2057.5240861016814 steps/sec)\n",
      "Step #5650\tEpoch   1 Batch 2524/3125   Loss: 0.786854 mae: 0.709152 (1992.0513697328927 steps/sec)\n",
      "Step #5651\tEpoch   1 Batch 2525/3125   Loss: 0.801474 mae: 0.708533 (2152.4042162306405 steps/sec)\n",
      "Step #5652\tEpoch   1 Batch 2526/3125   Loss: 0.950450 mae: 0.744777 (1907.9414467279857 steps/sec)\n",
      "Step #5653\tEpoch   1 Batch 2527/3125   Loss: 0.753552 mae: 0.686375 (1868.103793837575 steps/sec)\n",
      "Step #5654\tEpoch   1 Batch 2528/3125   Loss: 0.781002 mae: 0.700791 (1992.694931681268 steps/sec)\n",
      "Step #5655\tEpoch   1 Batch 2529/3125   Loss: 0.858871 mae: 0.741203 (2012.641196172708 steps/sec)\n",
      "Step #5656\tEpoch   1 Batch 2530/3125   Loss: 0.721103 mae: 0.671889 (2231.2501329928714 steps/sec)\n",
      "Step #5657\tEpoch   1 Batch 2531/3125   Loss: 0.918493 mae: 0.763077 (2145.182639293788 steps/sec)\n",
      "Step #5658\tEpoch   1 Batch 2532/3125   Loss: 0.816068 mae: 0.721350 (2099.0831565039834 steps/sec)\n",
      "Step #5659\tEpoch   1 Batch 2533/3125   Loss: 0.880594 mae: 0.744967 (1448.5194676023457 steps/sec)\n",
      "Step #5660\tEpoch   1 Batch 2534/3125   Loss: 0.940685 mae: 0.764673 (1640.7457536947354 steps/sec)\n",
      "Step #5661\tEpoch   1 Batch 2535/3125   Loss: 0.969845 mae: 0.772744 (1969.5081751674009 steps/sec)\n",
      "Step #5662\tEpoch   1 Batch 2536/3125   Loss: 0.768875 mae: 0.702846 (1897.3943254198032 steps/sec)\n",
      "Step #5663\tEpoch   1 Batch 2537/3125   Loss: 0.837437 mae: 0.728264 (1917.5363683743726 steps/sec)\n",
      "Step #5664\tEpoch   1 Batch 2538/3125   Loss: 0.815763 mae: 0.739500 (1937.0007758525142 steps/sec)\n",
      "Step #5665\tEpoch   1 Batch 2539/3125   Loss: 0.879798 mae: 0.754622 (1964.0300436419486 steps/sec)\n",
      "Step #5666\tEpoch   1 Batch 2540/3125   Loss: 0.798949 mae: 0.728258 (1923.5161932365377 steps/sec)\n",
      "Step #5667\tEpoch   1 Batch 2541/3125   Loss: 0.781222 mae: 0.704848 (1846.7185037116615 steps/sec)\n",
      "Step #5668\tEpoch   1 Batch 2542/3125   Loss: 0.987991 mae: 0.781238 (1915.207305936073 steps/sec)\n",
      "Step #5669\tEpoch   1 Batch 2543/3125   Loss: 0.800938 mae: 0.709848 (1748.5009171252293 steps/sec)\n",
      "Step #5670\tEpoch   1 Batch 2544/3125   Loss: 0.917259 mae: 0.762457 (1995.2923267208982 steps/sec)\n",
      "Step #5671\tEpoch   1 Batch 2545/3125   Loss: 0.900533 mae: 0.767000 (1928.5397680770257 steps/sec)\n",
      "Step #5672\tEpoch   1 Batch 2546/3125   Loss: 0.789557 mae: 0.728055 (1849.307772349694 steps/sec)\n",
      "Step #5673\tEpoch   1 Batch 2547/3125   Loss: 1.041473 mae: 0.804815 (1847.531957255244 steps/sec)\n",
      "Step #5674\tEpoch   1 Batch 2548/3125   Loss: 0.855846 mae: 0.730453 (1923.6749894512832 steps/sec)\n",
      "Step #5675\tEpoch   1 Batch 2549/3125   Loss: 0.666223 mae: 0.655746 (1795.2609231612107 steps/sec)\n",
      "Step #5676\tEpoch   1 Batch 2550/3125   Loss: 0.915701 mae: 0.751116 (1862.8931823228959 steps/sec)\n",
      "Step #5677\tEpoch   1 Batch 2551/3125   Loss: 0.806154 mae: 0.715575 (1734.9040370615487 steps/sec)\n",
      "Step #5678\tEpoch   1 Batch 2552/3125   Loss: 0.822316 mae: 0.704388 (1927.2459932362888 steps/sec)\n",
      "Step #5679\tEpoch   1 Batch 2553/3125   Loss: 0.854716 mae: 0.738257 (1874.968261063925 steps/sec)\n",
      "Step #5680\tEpoch   1 Batch 2554/3125   Loss: 0.860361 mae: 0.721947 (2032.596727921222 steps/sec)\n",
      "Step #5681\tEpoch   1 Batch 2555/3125   Loss: 0.694472 mae: 0.650111 (2002.6279602750192 steps/sec)\n",
      "Step #5682\tEpoch   1 Batch 2556/3125   Loss: 1.020129 mae: 0.803691 (1865.6774044321082 steps/sec)\n",
      "Step #5683\tEpoch   1 Batch 2557/3125   Loss: 0.938988 mae: 0.757981 (1851.1850432971125 steps/sec)\n",
      "Step #5684\tEpoch   1 Batch 2558/3125   Loss: 0.865621 mae: 0.752658 (1763.0978503030763 steps/sec)\n",
      "Step #5685\tEpoch   1 Batch 2559/3125   Loss: 0.931397 mae: 0.770893 (1988.538051620489 steps/sec)\n",
      "Step #5686\tEpoch   1 Batch 2560/3125   Loss: 0.619821 mae: 0.622295 (2034.4502434954695 steps/sec)\n",
      "Step #5687\tEpoch   1 Batch 2561/3125   Loss: 0.651932 mae: 0.636252 (1996.393995068874 steps/sec)\n",
      "Step #5688\tEpoch   1 Batch 2562/3125   Loss: 0.890547 mae: 0.749716 (1910.270260422834 steps/sec)\n",
      "Step #5689\tEpoch   1 Batch 2563/3125   Loss: 0.806673 mae: 0.696379 (1777.2625191738914 steps/sec)\n",
      "Step #5690\tEpoch   1 Batch 2564/3125   Loss: 0.839547 mae: 0.726854 (1270.1005953354288 steps/sec)\n",
      "Step #5691\tEpoch   1 Batch 2565/3125   Loss: 0.830834 mae: 0.720400 (1114.7296298855586 steps/sec)\n",
      "Step #5692\tEpoch   1 Batch 2566/3125   Loss: 0.767158 mae: 0.674779 (1314.4415125323571 steps/sec)\n",
      "Step #5693\tEpoch   1 Batch 2567/3125   Loss: 0.833709 mae: 0.725175 (1656.622851365015 steps/sec)\n",
      "Step #5694\tEpoch   1 Batch 2568/3125   Loss: 0.882146 mae: 0.725634 (1967.4203050828378 steps/sec)\n",
      "Step #5695\tEpoch   1 Batch 2569/3125   Loss: 0.836088 mae: 0.716618 (2059.969549629193 steps/sec)\n",
      "Step #5696\tEpoch   1 Batch 2570/3125   Loss: 0.924443 mae: 0.779673 (1991.2380482154217 steps/sec)\n",
      "Step #5697\tEpoch   1 Batch 2571/3125   Loss: 0.744841 mae: 0.680917 (1869.3693452778891 steps/sec)\n",
      "Step #5698\tEpoch   1 Batch 2572/3125   Loss: 0.906197 mae: 0.752592 (2058.473286938427 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5699\tEpoch   1 Batch 2573/3125   Loss: 0.825355 mae: 0.741881 (1635.7929549780038 steps/sec)\n",
      "Step #5700\tEpoch   1 Batch 2574/3125   Loss: 0.905190 mae: 0.730067 (1733.0402446078836 steps/sec)\n",
      "Step #5701\tEpoch   1 Batch 2575/3125   Loss: 0.789269 mae: 0.682373 (2004.8870958490277 steps/sec)\n",
      "Step #5702\tEpoch   1 Batch 2576/3125   Loss: 0.893263 mae: 0.743139 (2221.841759545705 steps/sec)\n",
      "Step #5703\tEpoch   1 Batch 2577/3125   Loss: 0.793980 mae: 0.704116 (2064.5120642639863 steps/sec)\n",
      "Step #5704\tEpoch   1 Batch 2578/3125   Loss: 0.853074 mae: 0.745852 (2008.766283524904 steps/sec)\n",
      "Step #5705\tEpoch   1 Batch 2579/3125   Loss: 0.740684 mae: 0.674008 (1783.91445997329 steps/sec)\n",
      "Step #5706\tEpoch   1 Batch 2580/3125   Loss: 0.846147 mae: 0.721309 (2166.389818602537 steps/sec)\n",
      "Step #5707\tEpoch   1 Batch 2581/3125   Loss: 0.811069 mae: 0.722151 (1743.1960433897177 steps/sec)\n",
      "Step #5708\tEpoch   1 Batch 2582/3125   Loss: 0.918449 mae: 0.760859 (1576.9245807955485 steps/sec)\n",
      "Step #5709\tEpoch   1 Batch 2583/3125   Loss: 0.827464 mae: 0.729077 (1939.7240001479893 steps/sec)\n",
      "Step #5710\tEpoch   1 Batch 2584/3125   Loss: 1.005222 mae: 0.786616 (1816.3450545643514 steps/sec)\n",
      "Step #5711\tEpoch   1 Batch 2585/3125   Loss: 0.752478 mae: 0.707150 (2008.1892176577612 steps/sec)\n",
      "Step #5712\tEpoch   1 Batch 2586/3125   Loss: 0.917761 mae: 0.736708 (2117.5867117685666 steps/sec)\n",
      "Step #5713\tEpoch   1 Batch 2587/3125   Loss: 0.799184 mae: 0.731206 (1929.8175226141288 steps/sec)\n",
      "Step #5714\tEpoch   1 Batch 2588/3125   Loss: 0.824356 mae: 0.717658 (2046.740774718679 steps/sec)\n",
      "Step #5715\tEpoch   1 Batch 2589/3125   Loss: 0.841246 mae: 0.721419 (1745.6337348194145 steps/sec)\n",
      "Step #5716\tEpoch   1 Batch 2590/3125   Loss: 0.894347 mae: 0.742611 (1778.091297564946 steps/sec)\n",
      "Step #5717\tEpoch   1 Batch 2591/3125   Loss: 1.024278 mae: 0.787440 (1780.3706502084165 steps/sec)\n",
      "Step #5718\tEpoch   1 Batch 2592/3125   Loss: 0.800170 mae: 0.713092 (1823.547007060624 steps/sec)\n",
      "Step #5719\tEpoch   1 Batch 2593/3125   Loss: 0.811938 mae: 0.708310 (1651.1967750062988 steps/sec)\n",
      "Step #5720\tEpoch   1 Batch 2594/3125   Loss: 0.756819 mae: 0.712132 (1661.847631424632 steps/sec)\n",
      "Step #5721\tEpoch   1 Batch 2595/3125   Loss: 0.784460 mae: 0.698730 (1383.7746530916577 steps/sec)\n",
      "Step #5722\tEpoch   1 Batch 2596/3125   Loss: 0.854543 mae: 0.726144 (1352.6347699333085 steps/sec)\n",
      "Step #5723\tEpoch   1 Batch 2597/3125   Loss: 0.850572 mae: 0.708628 (1588.0056337174963 steps/sec)\n",
      "Step #5724\tEpoch   1 Batch 2598/3125   Loss: 0.846269 mae: 0.735836 (1708.3906285639805 steps/sec)\n",
      "Step #5725\tEpoch   1 Batch 2599/3125   Loss: 0.980840 mae: 0.745982 (1455.819733014932 steps/sec)\n",
      "Step #5726\tEpoch   1 Batch 2600/3125   Loss: 0.815738 mae: 0.701600 (1639.7194617543805 steps/sec)\n",
      "Step #5727\tEpoch   1 Batch 2601/3125   Loss: 0.860113 mae: 0.725564 (1802.311810861214 steps/sec)\n",
      "Step #5728\tEpoch   1 Batch 2602/3125   Loss: 0.804237 mae: 0.717464 (1784.8102127659574 steps/sec)\n",
      "Step #5729\tEpoch   1 Batch 2603/3125   Loss: 0.976655 mae: 0.778590 (1681.8522290746073 steps/sec)\n",
      "Step #5730\tEpoch   1 Batch 2604/3125   Loss: 0.933652 mae: 0.741518 (1475.4960177862831 steps/sec)\n",
      "Step #5731\tEpoch   1 Batch 2605/3125   Loss: 0.749325 mae: 0.694907 (1605.5489630145692 steps/sec)\n",
      "Step #5732\tEpoch   1 Batch 2606/3125   Loss: 0.807731 mae: 0.715150 (1672.343343806319 steps/sec)\n",
      "Step #5733\tEpoch   1 Batch 2607/3125   Loss: 0.783419 mae: 0.726693 (1692.6985971879187 steps/sec)\n",
      "Step #5734\tEpoch   1 Batch 2608/3125   Loss: 0.784211 mae: 0.687085 (1705.2650409412836 steps/sec)\n",
      "Step #5735\tEpoch   1 Batch 2609/3125   Loss: 0.884618 mae: 0.729524 (1951.9830226085985 steps/sec)\n",
      "Step #5736\tEpoch   1 Batch 2610/3125   Loss: 0.881565 mae: 0.750415 (1580.2754920577509 steps/sec)\n",
      "Step #5737\tEpoch   1 Batch 2611/3125   Loss: 0.788319 mae: 0.699874 (1423.0039016115352 steps/sec)\n",
      "Step #5738\tEpoch   1 Batch 2612/3125   Loss: 0.703220 mae: 0.668663 (1716.9083153904721 steps/sec)\n",
      "Step #5739\tEpoch   1 Batch 2613/3125   Loss: 0.973010 mae: 0.778119 (1783.2318628618075 steps/sec)\n",
      "Step #5740\tEpoch   1 Batch 2614/3125   Loss: 0.907342 mae: 0.767223 (1840.801924055966 steps/sec)\n",
      "Step #5741\tEpoch   1 Batch 2615/3125   Loss: 0.972963 mae: 0.778381 (1805.383906818984 steps/sec)\n",
      "Step #5742\tEpoch   1 Batch 2616/3125   Loss: 0.787129 mae: 0.701674 (1791.0292761247565 steps/sec)\n",
      "Step #5743\tEpoch   1 Batch 2617/3125   Loss: 0.758102 mae: 0.696086 (1750.2666522004022 steps/sec)\n",
      "Step #5744\tEpoch   1 Batch 2618/3125   Loss: 0.790094 mae: 0.723544 (1753.7795097801454 steps/sec)\n",
      "Step #5745\tEpoch   1 Batch 2619/3125   Loss: 0.964844 mae: 0.780380 (1769.4946716504805 steps/sec)\n",
      "Step #5746\tEpoch   1 Batch 2620/3125   Loss: 0.749841 mae: 0.691984 (1895.0282833029114 steps/sec)\n",
      "Step #5747\tEpoch   1 Batch 2621/3125   Loss: 0.781788 mae: 0.691911 (1933.2688035251713 steps/sec)\n",
      "Step #5748\tEpoch   1 Batch 2622/3125   Loss: 0.744367 mae: 0.671314 (1873.360370178479 steps/sec)\n",
      "Step #5749\tEpoch   1 Batch 2623/3125   Loss: 0.874186 mae: 0.740412 (2024.8643429564545 steps/sec)\n",
      "Step #5750\tEpoch   1 Batch 2624/3125   Loss: 0.970197 mae: 0.774747 (1989.5756448812697 steps/sec)\n",
      "Step #5751\tEpoch   1 Batch 2625/3125   Loss: 0.927472 mae: 0.777593 (1965.318439104847 steps/sec)\n",
      "Step #5752\tEpoch   1 Batch 2626/3125   Loss: 0.775907 mae: 0.695466 (1945.3558806341196 steps/sec)\n",
      "Step #5753\tEpoch   1 Batch 2627/3125   Loss: 0.756459 mae: 0.704440 (1787.6400088650971 steps/sec)\n",
      "Step #5754\tEpoch   1 Batch 2628/3125   Loss: 0.990407 mae: 0.802102 (1721.870355925941 steps/sec)\n",
      "Step #5755\tEpoch   1 Batch 2629/3125   Loss: 0.818822 mae: 0.721941 (1780.189295870294 steps/sec)\n",
      "Step #5756\tEpoch   1 Batch 2630/3125   Loss: 0.608923 mae: 0.616868 (1784.2483643448445 steps/sec)\n",
      "Step #5757\tEpoch   1 Batch 2631/3125   Loss: 0.643549 mae: 0.637566 (2027.8011989943918 steps/sec)\n",
      "Step #5758\tEpoch   1 Batch 2632/3125   Loss: 0.913716 mae: 0.748557 (1841.4808050296792 steps/sec)\n",
      "Step #5759\tEpoch   1 Batch 2633/3125   Loss: 0.697127 mae: 0.655798 (1803.613846484627 steps/sec)\n",
      "Step #5760\tEpoch   1 Batch 2634/3125   Loss: 0.874750 mae: 0.746094 (1952.891877042845 steps/sec)\n",
      "Step #5761\tEpoch   1 Batch 2635/3125   Loss: 0.809365 mae: 0.716374 (2354.4987088806556 steps/sec)\n",
      "Step #5762\tEpoch   1 Batch 2636/3125   Loss: 0.976539 mae: 0.770876 (2084.706303368888 steps/sec)\n",
      "Step #5763\tEpoch   1 Batch 2637/3125   Loss: 0.728449 mae: 0.674018 (2251.4434173940117 steps/sec)\n",
      "Step #5764\tEpoch   1 Batch 2638/3125   Loss: 0.865615 mae: 0.717208 (2011.6951883968998 steps/sec)\n",
      "Step #5765\tEpoch   1 Batch 2639/3125   Loss: 0.848744 mae: 0.741024 (1738.1681350650213 steps/sec)\n",
      "Step #5766\tEpoch   1 Batch 2640/3125   Loss: 0.779436 mae: 0.706923 (1690.4336611317106 steps/sec)\n",
      "Step #5767\tEpoch   1 Batch 2641/3125   Loss: 0.739715 mae: 0.699923 (1603.0820975386027 steps/sec)\n",
      "Step #5768\tEpoch   1 Batch 2642/3125   Loss: 0.811404 mae: 0.732433 (1654.5969529850804 steps/sec)\n",
      "Step #5769\tEpoch   1 Batch 2643/3125   Loss: 0.763273 mae: 0.682657 (1911.5239128255143 steps/sec)\n",
      "Step #5770\tEpoch   1 Batch 2644/3125   Loss: 0.969966 mae: 0.768950 (1656.5050828982394 steps/sec)\n",
      "Step #5771\tEpoch   1 Batch 2645/3125   Loss: 0.810304 mae: 0.733637 (1732.3673971765368 steps/sec)\n",
      "Step #5772\tEpoch   1 Batch 2646/3125   Loss: 0.747887 mae: 0.671604 (1624.6539048519169 steps/sec)\n",
      "Step #5773\tEpoch   1 Batch 2647/3125   Loss: 0.779377 mae: 0.704910 (1838.4458938214461 steps/sec)\n",
      "Step #5774\tEpoch   1 Batch 2648/3125   Loss: 0.833720 mae: 0.719214 (1827.5515895147796 steps/sec)\n",
      "Step #5775\tEpoch   1 Batch 2649/3125   Loss: 0.894037 mae: 0.754462 (1562.1593033736322 steps/sec)\n",
      "Step #5776\tEpoch   1 Batch 2650/3125   Loss: 0.756393 mae: 0.707287 (1829.7041451093642 steps/sec)\n",
      "Step #5777\tEpoch   1 Batch 2651/3125   Loss: 0.928848 mae: 0.749261 (1881.7482749647816 steps/sec)\n",
      "Step #5778\tEpoch   1 Batch 2652/3125   Loss: 0.848756 mae: 0.742968 (1783.7475546482947 steps/sec)\n",
      "Step #5779\tEpoch   1 Batch 2653/3125   Loss: 0.835150 mae: 0.738316 (1918.5011709601874 steps/sec)\n",
      "Step #5780\tEpoch   1 Batch 2654/3125   Loss: 0.897029 mae: 0.769388 (1967.8079813836525 steps/sec)\n",
      "Step #5781\tEpoch   1 Batch 2655/3125   Loss: 0.888878 mae: 0.724530 (2086.718407960199 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5782\tEpoch   1 Batch 2656/3125   Loss: 0.967032 mae: 0.791129 (1538.844006134384 steps/sec)\n",
      "Step #5783\tEpoch   1 Batch 2657/3125   Loss: 0.878164 mae: 0.754856 (1873.9797514051595 steps/sec)\n",
      "Step #5784\tEpoch   1 Batch 2658/3125   Loss: 0.895624 mae: 0.732709 (1997.439804937519 steps/sec)\n",
      "Step #5785\tEpoch   1 Batch 2659/3125   Loss: 0.718502 mae: 0.680488 (2117.629477043006 steps/sec)\n",
      "Step #5786\tEpoch   1 Batch 2660/3125   Loss: 0.892866 mae: 0.743076 (2107.224533269026 steps/sec)\n",
      "Step #5787\tEpoch   1 Batch 2661/3125   Loss: 0.807867 mae: 0.705534 (2086.1164440111806 steps/sec)\n",
      "Step #5788\tEpoch   1 Batch 2662/3125   Loss: 0.704598 mae: 0.688710 (1965.2447709723367 steps/sec)\n",
      "Step #5789\tEpoch   1 Batch 2663/3125   Loss: 0.768757 mae: 0.701284 (1910.0092897866991 steps/sec)\n",
      "Step #5790\tEpoch   1 Batch 2664/3125   Loss: 0.765404 mae: 0.710433 (2071.834186244097 steps/sec)\n",
      "Step #5791\tEpoch   1 Batch 2665/3125   Loss: 0.857810 mae: 0.745548 (2119.727093546268 steps/sec)\n",
      "Step #5792\tEpoch   1 Batch 2666/3125   Loss: 0.917438 mae: 0.764276 (1842.6457667029838 steps/sec)\n",
      "Step #5793\tEpoch   1 Batch 2667/3125   Loss: 0.686914 mae: 0.673428 (2000.9655843598232 steps/sec)\n",
      "Step #5794\tEpoch   1 Batch 2668/3125   Loss: 0.855317 mae: 0.751304 (2157.5859833948907 steps/sec)\n",
      "Step #5795\tEpoch   1 Batch 2669/3125   Loss: 0.798067 mae: 0.709271 (2110.3203992915796 steps/sec)\n",
      "Step #5796\tEpoch   1 Batch 2670/3125   Loss: 0.844199 mae: 0.726587 (1893.0269084606844 steps/sec)\n",
      "Step #5797\tEpoch   1 Batch 2671/3125   Loss: 0.758477 mae: 0.664361 (2012.0039910967841 steps/sec)\n",
      "Step #5798\tEpoch   1 Batch 2672/3125   Loss: 0.869410 mae: 0.745274 (1877.3012505482898 steps/sec)\n",
      "Step #5799\tEpoch   1 Batch 2673/3125   Loss: 0.830325 mae: 0.727871 (1878.2764457739604 steps/sec)\n",
      "Step #5800\tEpoch   1 Batch 2674/3125   Loss: 0.828787 mae: 0.721423 (1864.9142308342152 steps/sec)\n",
      "Step #5801\tEpoch   1 Batch 2675/3125   Loss: 0.822607 mae: 0.719324 (1757.159255628451 steps/sec)\n",
      "Step #5802\tEpoch   1 Batch 2676/3125   Loss: 0.779375 mae: 0.691274 (2071.609060286665 steps/sec)\n",
      "Step #5803\tEpoch   1 Batch 2677/3125   Loss: 0.811755 mae: 0.739546 (2042.8927680798006 steps/sec)\n",
      "Step #5804\tEpoch   1 Batch 2678/3125   Loss: 0.860567 mae: 0.724069 (1860.4803009199707 steps/sec)\n",
      "Step #5805\tEpoch   1 Batch 2679/3125   Loss: 0.753108 mae: 0.658546 (1974.6450227863356 steps/sec)\n",
      "Step #5806\tEpoch   1 Batch 2680/3125   Loss: 0.762883 mae: 0.692860 (1681.730846337669 steps/sec)\n",
      "Step #5807\tEpoch   1 Batch 2681/3125   Loss: 0.734611 mae: 0.672861 (1741.1987413133184 steps/sec)\n",
      "Step #5808\tEpoch   1 Batch 2682/3125   Loss: 0.925277 mae: 0.769086 (1969.674656247652 steps/sec)\n",
      "Step #5809\tEpoch   1 Batch 2683/3125   Loss: 0.865730 mae: 0.709943 (2057.463528534568 steps/sec)\n",
      "Step #5810\tEpoch   1 Batch 2684/3125   Loss: 0.890606 mae: 0.738611 (2056.575760250262 steps/sec)\n",
      "Step #5811\tEpoch   1 Batch 2685/3125   Loss: 0.820702 mae: 0.710442 (2088.027320608939 steps/sec)\n",
      "Step #5812\tEpoch   1 Batch 2686/3125   Loss: 0.765442 mae: 0.704109 (2125.4200871592175 steps/sec)\n",
      "Step #5813\tEpoch   1 Batch 2687/3125   Loss: 0.797206 mae: 0.709222 (1982.5600302514654 steps/sec)\n",
      "Step #5814\tEpoch   1 Batch 2688/3125   Loss: 0.687043 mae: 0.659012 (1862.6946272660255 steps/sec)\n",
      "Step #5815\tEpoch   1 Batch 2689/3125   Loss: 0.840559 mae: 0.733400 (1847.4668546007135 steps/sec)\n",
      "Step #5816\tEpoch   1 Batch 2690/3125   Loss: 0.888359 mae: 0.749104 (1974.9425547142803 steps/sec)\n",
      "Step #5817\tEpoch   1 Batch 2691/3125   Loss: 0.817019 mae: 0.720465 (2074.683181148165 steps/sec)\n",
      "Step #5818\tEpoch   1 Batch 2692/3125   Loss: 0.942114 mae: 0.771423 (1868.8029656297063 steps/sec)\n",
      "Step #5819\tEpoch   1 Batch 2693/3125   Loss: 0.766460 mae: 0.699466 (2094.0737116438836 steps/sec)\n",
      "Step #5820\tEpoch   1 Batch 2694/3125   Loss: 0.804370 mae: 0.718893 (1991.067902172262 steps/sec)\n",
      "Step #5821\tEpoch   1 Batch 2695/3125   Loss: 0.846868 mae: 0.732597 (1845.4834252928183 steps/sec)\n",
      "Step #5822\tEpoch   1 Batch 2696/3125   Loss: 0.685432 mae: 0.670376 (2052.208630981505 steps/sec)\n",
      "Step #5823\tEpoch   1 Batch 2697/3125   Loss: 0.712547 mae: 0.668817 (1807.1418723286915 steps/sec)\n",
      "Step #5824\tEpoch   1 Batch 2698/3125   Loss: 0.810163 mae: 0.705075 (1932.6452373930995 steps/sec)\n",
      "Step #5825\tEpoch   1 Batch 2699/3125   Loss: 0.787023 mae: 0.708193 (1949.6239553022767 steps/sec)\n",
      "Step #5826\tEpoch   1 Batch 2700/3125   Loss: 0.840428 mae: 0.722089 (1827.6152961271657 steps/sec)\n",
      "Step #5827\tEpoch   1 Batch 2701/3125   Loss: 0.870956 mae: 0.731111 (2124.3435980551053 steps/sec)\n",
      "Step #5828\tEpoch   1 Batch 2702/3125   Loss: 0.834868 mae: 0.741697 (2145.3801456747688 steps/sec)\n",
      "Step #5829\tEpoch   1 Batch 2703/3125   Loss: 0.822079 mae: 0.737998 (2051.6866244032244 steps/sec)\n",
      "Step #5830\tEpoch   1 Batch 2704/3125   Loss: 0.811073 mae: 0.702206 (1862.7442620621048 steps/sec)\n",
      "Step #5831\tEpoch   1 Batch 2705/3125   Loss: 0.774188 mae: 0.691107 (1781.2477173312948 steps/sec)\n",
      "Step #5832\tEpoch   1 Batch 2706/3125   Loss: 0.724353 mae: 0.689029 (2122.8598325724524 steps/sec)\n",
      "Step #5833\tEpoch   1 Batch 2707/3125   Loss: 0.866883 mae: 0.732595 (1920.4864513411294 steps/sec)\n",
      "Step #5834\tEpoch   1 Batch 2708/3125   Loss: 0.737677 mae: 0.700670 (2042.3953798657978 steps/sec)\n",
      "Step #5835\tEpoch   1 Batch 2709/3125   Loss: 0.733018 mae: 0.678354 (2095.559374875095 steps/sec)\n",
      "Step #5836\tEpoch   1 Batch 2710/3125   Loss: 0.748447 mae: 0.700695 (2187.883529988628 steps/sec)\n",
      "Step #5837\tEpoch   1 Batch 2711/3125   Loss: 0.838881 mae: 0.735279 (1998.3153240714273 steps/sec)\n",
      "Step #5838\tEpoch   1 Batch 2712/3125   Loss: 0.866655 mae: 0.725298 (1900.1794030770345 steps/sec)\n",
      "Step #5839\tEpoch   1 Batch 2713/3125   Loss: 0.861277 mae: 0.762062 (1813.753081081081 steps/sec)\n",
      "Step #5840\tEpoch   1 Batch 2714/3125   Loss: 0.858366 mae: 0.744611 (2058.897681085433 steps/sec)\n",
      "Step #5841\tEpoch   1 Batch 2715/3125   Loss: 0.812528 mae: 0.697131 (2098.1171338815857 steps/sec)\n",
      "Step #5842\tEpoch   1 Batch 2716/3125   Loss: 0.764865 mae: 0.699630 (1905.2720516757365 steps/sec)\n",
      "Step #5843\tEpoch   1 Batch 2717/3125   Loss: 0.833228 mae: 0.736040 (2027.0172047167987 steps/sec)\n",
      "Step #5844\tEpoch   1 Batch 2718/3125   Loss: 0.762427 mae: 0.705225 (2095.643136941402 steps/sec)\n",
      "Step #5845\tEpoch   1 Batch 2719/3125   Loss: 0.804334 mae: 0.716732 (1892.44610483951 steps/sec)\n",
      "Step #5846\tEpoch   1 Batch 2720/3125   Loss: 0.718318 mae: 0.672286 (1842.4515040764688 steps/sec)\n",
      "Step #5847\tEpoch   1 Batch 2721/3125   Loss: 0.823397 mae: 0.731104 (1779.479346977565 steps/sec)\n",
      "Step #5848\tEpoch   1 Batch 2722/3125   Loss: 0.816529 mae: 0.723223 (2073.657460967241 steps/sec)\n",
      "Step #5849\tEpoch   1 Batch 2723/3125   Loss: 0.776960 mae: 0.701097 (1942.5628485151633 steps/sec)\n",
      "Step #5850\tEpoch   1 Batch 2724/3125   Loss: 0.802875 mae: 0.713631 (1932.6630479859184 steps/sec)\n",
      "Step #5851\tEpoch   1 Batch 2725/3125   Loss: 0.886374 mae: 0.723834 (2042.9922747951798 steps/sec)\n",
      "Step #5852\tEpoch   1 Batch 2726/3125   Loss: 0.785776 mae: 0.704034 (2062.461399264373 steps/sec)\n",
      "Step #5853\tEpoch   1 Batch 2727/3125   Loss: 0.809229 mae: 0.709528 (1999.0391581194951 steps/sec)\n",
      "Step #5854\tEpoch   1 Batch 2728/3125   Loss: 0.911497 mae: 0.753407 (1847.1902194975867 steps/sec)\n",
      "Step #5855\tEpoch   1 Batch 2729/3125   Loss: 0.923712 mae: 0.764712 (1935.7492292639702 steps/sec)\n",
      "Step #5856\tEpoch   1 Batch 2730/3125   Loss: 0.948248 mae: 0.754420 (1722.0541623556846 steps/sec)\n",
      "Step #5857\tEpoch   1 Batch 2731/3125   Loss: 0.729140 mae: 0.669196 (1979.0987590242062 steps/sec)\n",
      "Step #5858\tEpoch   1 Batch 2732/3125   Loss: 0.789373 mae: 0.716963 (2093.99007498677 steps/sec)\n",
      "Step #5859\tEpoch   1 Batch 2733/3125   Loss: 0.836962 mae: 0.731141 (2132.4656308468234 steps/sec)\n",
      "Step #5860\tEpoch   1 Batch 2734/3125   Loss: 0.737936 mae: 0.698303 (2193.2836211134004 steps/sec)\n",
      "Step #5861\tEpoch   1 Batch 2735/3125   Loss: 0.694370 mae: 0.650261 (2105.701146655421 steps/sec)\n",
      "Step #5862\tEpoch   1 Batch 2736/3125   Loss: 0.857738 mae: 0.741739 (2061.8328040663437 steps/sec)\n",
      "Step #5863\tEpoch   1 Batch 2737/3125   Loss: 0.802391 mae: 0.717735 (1851.3157778582085 steps/sec)\n",
      "Step #5864\tEpoch   1 Batch 2738/3125   Loss: 0.832627 mae: 0.752127 (1873.427310571546 steps/sec)\n",
      "Step #5865\tEpoch   1 Batch 2739/3125   Loss: 0.870467 mae: 0.747147 (2036.1489766592877 steps/sec)\n",
      "Step #5866\tEpoch   1 Batch 2740/3125   Loss: 0.851960 mae: 0.725953 (2145.621591757809 steps/sec)\n",
      "Step #5867\tEpoch   1 Batch 2741/3125   Loss: 0.776402 mae: 0.701671 (2164.6009661037942 steps/sec)\n",
      "Step #5868\tEpoch   1 Batch 2742/3125   Loss: 0.793248 mae: 0.697004 (2152.6030546887832 steps/sec)\n",
      "Step #5869\tEpoch   1 Batch 2743/3125   Loss: 0.723500 mae: 0.667453 (1826.4374423019979 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5870\tEpoch   1 Batch 2744/3125   Loss: 0.866668 mae: 0.732041 (2014.8068442745011 steps/sec)\n",
      "Step #5871\tEpoch   1 Batch 2745/3125   Loss: 0.857246 mae: 0.719247 (1828.6192614552906 steps/sec)\n",
      "Step #5872\tEpoch   1 Batch 2746/3125   Loss: 0.685170 mae: 0.665989 (2022.1699386739692 steps/sec)\n",
      "Step #5873\tEpoch   1 Batch 2747/3125   Loss: 0.901982 mae: 0.744124 (2150.792771726868 steps/sec)\n",
      "Step #5874\tEpoch   1 Batch 2748/3125   Loss: 0.823933 mae: 0.717435 (1942.0591558164947 steps/sec)\n",
      "Step #5875\tEpoch   1 Batch 2749/3125   Loss: 0.986157 mae: 0.775393 (1956.0977884731976 steps/sec)\n",
      "Step #5876\tEpoch   1 Batch 2750/3125   Loss: 0.940318 mae: 0.769489 (2078.899264458058 steps/sec)\n",
      "Step #5877\tEpoch   1 Batch 2751/3125   Loss: 0.948874 mae: 0.765701 (1989.9154560722657 steps/sec)\n",
      "Step #5878\tEpoch   1 Batch 2752/3125   Loss: 0.819046 mae: 0.727161 (2004.331412296547 steps/sec)\n",
      "Step #5879\tEpoch   1 Batch 2753/3125   Loss: 0.809861 mae: 0.712982 (1917.9046329998355 steps/sec)\n",
      "Step #5880\tEpoch   1 Batch 2754/3125   Loss: 0.766726 mae: 0.704961 (1872.6577847626531 steps/sec)\n",
      "Step #5881\tEpoch   1 Batch 2755/3125   Loss: 0.882029 mae: 0.731215 (2234.269094318315 steps/sec)\n",
      "Step #5882\tEpoch   1 Batch 2756/3125   Loss: 0.959509 mae: 0.768087 (2204.789839988225 steps/sec)\n",
      "Step #5883\tEpoch   1 Batch 2757/3125   Loss: 0.783169 mae: 0.728684 (2044.8249300402695 steps/sec)\n",
      "Step #5884\tEpoch   1 Batch 2758/3125   Loss: 0.818432 mae: 0.731377 (2176.439698205631 steps/sec)\n",
      "Step #5885\tEpoch   1 Batch 2759/3125   Loss: 0.813006 mae: 0.705063 (2067.7486146989804 steps/sec)\n",
      "Step #5886\tEpoch   1 Batch 2760/3125   Loss: 0.727066 mae: 0.672527 (1938.2897704166512 steps/sec)\n",
      "Step #5887\tEpoch   1 Batch 2761/3125   Loss: 0.884678 mae: 0.750466 (1786.8632045328675 steps/sec)\n",
      "Step #5888\tEpoch   1 Batch 2762/3125   Loss: 0.704764 mae: 0.650901 (1997.6681272623357 steps/sec)\n",
      "Step #5889\tEpoch   1 Batch 2763/3125   Loss: 0.722160 mae: 0.669100 (1891.6438158464425 steps/sec)\n",
      "Step #5890\tEpoch   1 Batch 2764/3125   Loss: 0.766583 mae: 0.703550 (2055.829820605823 steps/sec)\n",
      "Step #5891\tEpoch   1 Batch 2765/3125   Loss: 0.777676 mae: 0.688629 (2096.9632733054027 steps/sec)\n",
      "Step #5892\tEpoch   1 Batch 2766/3125   Loss: 0.716921 mae: 0.670665 (1757.5863224941334 steps/sec)\n",
      "Step #5893\tEpoch   1 Batch 2767/3125   Loss: 0.752124 mae: 0.714057 (1973.4000809251818 steps/sec)\n",
      "Step #5894\tEpoch   1 Batch 2768/3125   Loss: 0.877278 mae: 0.736351 (1827.902030855051 steps/sec)\n",
      "Step #5895\tEpoch   1 Batch 2769/3125   Loss: 0.790971 mae: 0.703520 (1589.4740033348492 steps/sec)\n",
      "Step #5896\tEpoch   1 Batch 2770/3125   Loss: 0.818556 mae: 0.708920 (1813.376682893929 steps/sec)\n",
      "Step #5897\tEpoch   1 Batch 2771/3125   Loss: 0.906453 mae: 0.746406 (2058.392469793783 steps/sec)\n",
      "Step #5898\tEpoch   1 Batch 2772/3125   Loss: 0.914821 mae: 0.765311 (1985.8077589553723 steps/sec)\n",
      "Step #5899\tEpoch   1 Batch 2773/3125   Loss: 0.956727 mae: 0.767926 (2094.8685932333756 steps/sec)\n",
      "Step #5900\tEpoch   1 Batch 2774/3125   Loss: 0.800109 mae: 0.710059 (1996.0899650685778 steps/sec)\n",
      "Step #5901\tEpoch   1 Batch 2775/3125   Loss: 0.867766 mae: 0.737926 (2077.107908681226 steps/sec)\n",
      "Step #5902\tEpoch   1 Batch 2776/3125   Loss: 0.762250 mae: 0.692012 (1888.3564295812062 steps/sec)\n",
      "Step #5903\tEpoch   1 Batch 2777/3125   Loss: 0.936687 mae: 0.753348 (1754.3223302270333 steps/sec)\n",
      "Step #5904\tEpoch   1 Batch 2778/3125   Loss: 0.743182 mae: 0.671468 (1889.632553026617 steps/sec)\n",
      "Step #5905\tEpoch   1 Batch 2779/3125   Loss: 0.814224 mae: 0.692823 (2186.3780898466416 steps/sec)\n",
      "Step #5906\tEpoch   1 Batch 2780/3125   Loss: 0.813859 mae: 0.693922 (2000.5647346128897 steps/sec)\n",
      "Step #5907\tEpoch   1 Batch 2781/3125   Loss: 0.944498 mae: 0.766838 (2199.655968114118 steps/sec)\n",
      "Step #5908\tEpoch   1 Batch 2782/3125   Loss: 0.960635 mae: 0.772629 (1766.0970988252136 steps/sec)\n",
      "Step #5909\tEpoch   1 Batch 2783/3125   Loss: 0.801187 mae: 0.711498 (2007.228177641654 steps/sec)\n",
      "Step #5910\tEpoch   1 Batch 2784/3125   Loss: 0.817935 mae: 0.726763 (1772.8004328126058 steps/sec)\n",
      "Step #5911\tEpoch   1 Batch 2785/3125   Loss: 0.898403 mae: 0.773701 (1781.7168490450622 steps/sec)\n",
      "Step #5912\tEpoch   1 Batch 2786/3125   Loss: 0.931851 mae: 0.758909 (1966.1844535490948 steps/sec)\n",
      "Step #5913\tEpoch   1 Batch 2787/3125   Loss: 0.766748 mae: 0.689838 (1975.742616232512 steps/sec)\n",
      "Step #5914\tEpoch   1 Batch 2788/3125   Loss: 0.911452 mae: 0.746629 (1882.1873793988566 steps/sec)\n",
      "Step #5915\tEpoch   1 Batch 2789/3125   Loss: 0.808262 mae: 0.713792 (2009.151178386664 steps/sec)\n",
      "Step #5916\tEpoch   1 Batch 2790/3125   Loss: 0.811512 mae: 0.727706 (2027.0172047167987 steps/sec)\n",
      "Step #5917\tEpoch   1 Batch 2791/3125   Loss: 0.807088 mae: 0.713512 (2199.2638191218266 steps/sec)\n",
      "Step #5918\tEpoch   1 Batch 2792/3125   Loss: 0.693522 mae: 0.668093 (1914.5953348244852 steps/sec)\n",
      "Step #5919\tEpoch   1 Batch 2793/3125   Loss: 0.968272 mae: 0.794162 (1676.6217361410913 steps/sec)\n",
      "Step #5920\tEpoch   1 Batch 2794/3125   Loss: 0.773589 mae: 0.713833 (1828.6192614552906 steps/sec)\n",
      "Step #5921\tEpoch   1 Batch 2795/3125   Loss: 0.891527 mae: 0.746795 (1784.3090876604867 steps/sec)\n",
      "Step #5922\tEpoch   1 Batch 2796/3125   Loss: 0.830478 mae: 0.712128 (1692.1659283645195 steps/sec)\n",
      "Step #5923\tEpoch   1 Batch 2797/3125   Loss: 0.962416 mae: 0.777237 (1793.2652677304054 steps/sec)\n",
      "Step #5924\tEpoch   1 Batch 2798/3125   Loss: 0.778014 mae: 0.703643 (1087.779575916013 steps/sec)\n",
      "Step #5925\tEpoch   1 Batch 2799/3125   Loss: 0.852331 mae: 0.731204 (966.1222929114706 steps/sec)\n",
      "Step #5926\tEpoch   1 Batch 2800/3125   Loss: 1.020554 mae: 0.781702 (1559.0237665127827 steps/sec)\n",
      "Step #5927\tEpoch   1 Batch 2801/3125   Loss: 0.746330 mae: 0.699455 (1900.1277532640506 steps/sec)\n",
      "Step #5928\tEpoch   1 Batch 2802/3125   Loss: 0.806003 mae: 0.715814 (2021.5072005552236 steps/sec)\n",
      "Step #5929\tEpoch   1 Batch 2803/3125   Loss: 0.883258 mae: 0.753087 (1866.5076497236487 steps/sec)\n",
      "Step #5930\tEpoch   1 Batch 2804/3125   Loss: 0.715684 mae: 0.656183 (1762.9792778781891 steps/sec)\n",
      "Step #5931\tEpoch   1 Batch 2805/3125   Loss: 0.801699 mae: 0.703472 (1779.6152507997929 steps/sec)\n",
      "Step #5932\tEpoch   1 Batch 2806/3125   Loss: 0.821675 mae: 0.713861 (1794.6310447812284 steps/sec)\n",
      "Step #5933\tEpoch   1 Batch 2807/3125   Loss: 0.903973 mae: 0.765325 (1835.7262278866606 steps/sec)\n",
      "Step #5934\tEpoch   1 Batch 2808/3125   Loss: 0.920584 mae: 0.755051 (1829.1455883892127 steps/sec)\n",
      "Step #5935\tEpoch   1 Batch 2809/3125   Loss: 0.865252 mae: 0.751495 (1852.2805158099275 steps/sec)\n",
      "Step #5936\tEpoch   1 Batch 2810/3125   Loss: 0.793709 mae: 0.712520 (1690.1067019116083 steps/sec)\n",
      "Step #5937\tEpoch   1 Batch 2811/3125   Loss: 0.773519 mae: 0.689392 (1734.9040370615487 steps/sec)\n",
      "Step #5938\tEpoch   1 Batch 2812/3125   Loss: 0.882019 mae: 0.764059 (1953.3648159014913 steps/sec)\n",
      "Step #5939\tEpoch   1 Batch 2813/3125   Loss: 0.941785 mae: 0.784363 (2039.574803302763 steps/sec)\n",
      "Step #5940\tEpoch   1 Batch 2814/3125   Loss: 0.823795 mae: 0.723965 (1724.8017896502945 steps/sec)\n",
      "Step #5941\tEpoch   1 Batch 2815/3125   Loss: 0.776961 mae: 0.689277 (1885.7245620976153 steps/sec)\n",
      "Step #5942\tEpoch   1 Batch 2816/3125   Loss: 0.855283 mae: 0.712832 (2043.4103088765469 steps/sec)\n",
      "Step #5943\tEpoch   1 Batch 2817/3125   Loss: 0.755021 mae: 0.680322 (1845.3048007883992 steps/sec)\n",
      "Step #5944\tEpoch   1 Batch 2818/3125   Loss: 0.942870 mae: 0.745801 (1834.2651226253367 steps/sec)\n",
      "Step #5945\tEpoch   1 Batch 2819/3125   Loss: 0.919577 mae: 0.770815 (1980.7624012996334 steps/sec)\n",
      "Step #5946\tEpoch   1 Batch 2820/3125   Loss: 1.018662 mae: 0.793292 (1979.8835002785042 steps/sec)\n",
      "Step #5947\tEpoch   1 Batch 2821/3125   Loss: 0.812600 mae: 0.722683 (1833.5113963227516 steps/sec)\n",
      "Step #5948\tEpoch   1 Batch 2822/3125   Loss: 0.819093 mae: 0.723177 (2008.6700828504381 steps/sec)\n",
      "Step #5949\tEpoch   1 Batch 2823/3125   Loss: 0.765361 mae: 0.690374 (1548.227824738843 steps/sec)\n",
      "Step #5950\tEpoch   1 Batch 2824/3125   Loss: 0.871239 mae: 0.771596 (1822.6911644561874 steps/sec)\n",
      "Step #5951\tEpoch   1 Batch 2825/3125   Loss: 0.811411 mae: 0.690725 (1819.1495636786315 steps/sec)\n",
      "Step #5952\tEpoch   1 Batch 2826/3125   Loss: 0.849640 mae: 0.736165 (1895.1481578543091 steps/sec)\n",
      "Step #5953\tEpoch   1 Batch 2827/3125   Loss: 0.821011 mae: 0.748390 (1833.5915505272178 steps/sec)\n",
      "Step #5954\tEpoch   1 Batch 2828/3125   Loss: 0.972158 mae: 0.765817 (2093.551092121551 steps/sec)\n",
      "Step #5955\tEpoch   1 Batch 2829/3125   Loss: 0.707500 mae: 0.659558 (2159.096477952456 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #5956\tEpoch   1 Batch 2830/3125   Loss: 0.715646 mae: 0.659432 (2050.2624967004604 steps/sec)\n",
      "Step #5957\tEpoch   1 Batch 2831/3125   Loss: 0.880388 mae: 0.741401 (1933.4292141461076 steps/sec)\n",
      "Step #5958\tEpoch   1 Batch 2832/3125   Loss: 0.721452 mae: 0.675489 (1995.6151035322778 steps/sec)\n",
      "Step #5959\tEpoch   1 Batch 2833/3125   Loss: 0.721129 mae: 0.664582 (2214.638576482391 steps/sec)\n",
      "Step #5960\tEpoch   1 Batch 2834/3125   Loss: 0.757928 mae: 0.676621 (2197.0519522697036 steps/sec)\n",
      "Step #5961\tEpoch   1 Batch 2835/3125   Loss: 0.894868 mae: 0.725270 (1975.3518075466723 steps/sec)\n",
      "Step #5962\tEpoch   1 Batch 2836/3125   Loss: 0.814167 mae: 0.695959 (2015.6395371189112 steps/sec)\n",
      "Step #5963\tEpoch   1 Batch 2837/3125   Loss: 0.736283 mae: 0.681616 (1917.553901578187 steps/sec)\n",
      "Step #5964\tEpoch   1 Batch 2838/3125   Loss: 0.634262 mae: 0.612285 (1812.9847675363521 steps/sec)\n",
      "Step #5965\tEpoch   1 Batch 2839/3125   Loss: 0.824443 mae: 0.727264 (1676.5011071939628 steps/sec)\n",
      "Step #5966\tEpoch   1 Batch 2840/3125   Loss: 0.827317 mae: 0.715060 (1987.049582626657 steps/sec)\n",
      "Step #5967\tEpoch   1 Batch 2841/3125   Loss: 0.873387 mae: 0.724695 (2120.541573556326 steps/sec)\n",
      "Step #5968\tEpoch   1 Batch 2842/3125   Loss: 0.804853 mae: 0.721937 (2103.7577994904 steps/sec)\n",
      "Step #5969\tEpoch   1 Batch 2843/3125   Loss: 0.877580 mae: 0.745322 (2062.197748168543 steps/sec)\n",
      "Step #5970\tEpoch   1 Batch 2844/3125   Loss: 0.856302 mae: 0.725093 (2194.316326957686 steps/sec)\n",
      "Step #5971\tEpoch   1 Batch 2845/3125   Loss: 0.830483 mae: 0.725756 (2020.2803333172776 steps/sec)\n",
      "Step #5972\tEpoch   1 Batch 2846/3125   Loss: 0.769443 mae: 0.691815 (2127.0152947381234 steps/sec)\n",
      "Step #5973\tEpoch   1 Batch 2847/3125   Loss: 0.814237 mae: 0.711147 (1839.1875537158191 steps/sec)\n",
      "Step #5974\tEpoch   1 Batch 2848/3125   Loss: 0.736498 mae: 0.714803 (1966.5716429107276 steps/sec)\n",
      "Step #5975\tEpoch   1 Batch 2849/3125   Loss: 0.825197 mae: 0.728873 (2064.410450258894 steps/sec)\n",
      "Step #5976\tEpoch   1 Batch 2850/3125   Loss: 0.836825 mae: 0.726551 (2044.1671865252652 steps/sec)\n",
      "Step #5977\tEpoch   1 Batch 2851/3125   Loss: 0.825573 mae: 0.701060 (2188.066148468882 steps/sec)\n",
      "Step #5978\tEpoch   1 Batch 2852/3125   Loss: 0.858031 mae: 0.736653 (2045.1639328275244 steps/sec)\n",
      "Step #5979\tEpoch   1 Batch 2853/3125   Loss: 0.841168 mae: 0.741698 (2110.5964997031087 steps/sec)\n",
      "Step #5980\tEpoch   1 Batch 2854/3125   Loss: 0.887732 mae: 0.730584 (2057.5240861016814 steps/sec)\n",
      "Step #5981\tEpoch   1 Batch 2855/3125   Loss: 0.902638 mae: 0.764586 (2090.650078256622 steps/sec)\n",
      "Step #5982\tEpoch   1 Batch 2856/3125   Loss: 0.815006 mae: 0.717698 (1775.5772112673671 steps/sec)\n",
      "Step #5983\tEpoch   1 Batch 2857/3125   Loss: 0.685068 mae: 0.647194 (1895.3708222619887 steps/sec)\n",
      "Step #5984\tEpoch   1 Batch 2858/3125   Loss: 1.052662 mae: 0.802739 (2158.607557152121 steps/sec)\n",
      "Step #5985\tEpoch   1 Batch 2859/3125   Loss: 0.786033 mae: 0.706503 (1899.5425848935265 steps/sec)\n",
      "Step #5986\tEpoch   1 Batch 2860/3125   Loss: 0.784003 mae: 0.705106 (2025.2163164400495 steps/sec)\n",
      "Step #5987\tEpoch   1 Batch 2861/3125   Loss: 0.749544 mae: 0.691375 (2005.0596120199248 steps/sec)\n",
      "Step #5988\tEpoch   1 Batch 2862/3125   Loss: 0.787007 mae: 0.701719 (1966.49788079967 steps/sec)\n",
      "Step #5989\tEpoch   1 Batch 2863/3125   Loss: 0.769646 mae: 0.685878 (1881.9002494660708 steps/sec)\n",
      "Step #5990\tEpoch   1 Batch 2864/3125   Loss: 0.856444 mae: 0.734422 (1853.9179632248938 steps/sec)\n",
      "Step #5991\tEpoch   1 Batch 2865/3125   Loss: 0.769086 mae: 0.715068 (1942.580842372426 steps/sec)\n",
      "Step #5992\tEpoch   1 Batch 2866/3125   Loss: 0.871616 mae: 0.713296 (2064.8372963127063 steps/sec)\n",
      "Step #5993\tEpoch   1 Batch 2867/3125   Loss: 0.909208 mae: 0.783236 (2070.811280511889 steps/sec)\n",
      "Step #5994\tEpoch   1 Batch 2868/3125   Loss: 0.757717 mae: 0.673898 (2146.456096537466 steps/sec)\n",
      "Step #5995\tEpoch   1 Batch 2869/3125   Loss: 0.728407 mae: 0.699294 (2179.7877537444524 steps/sec)\n",
      "Step #5996\tEpoch   1 Batch 2870/3125   Loss: 0.683003 mae: 0.663173 (2039.1385094073605 steps/sec)\n",
      "Step #5997\tEpoch   1 Batch 2871/3125   Loss: 0.795335 mae: 0.715764 (2136.854761468077 steps/sec)\n",
      "Step #5998\tEpoch   1 Batch 2872/3125   Loss: 0.862414 mae: 0.732080 (2158.3409663973653 steps/sec)\n",
      "Step #5999\tEpoch   1 Batch 2873/3125   Loss: 0.780399 mae: 0.714254 (1799.8060435457985 steps/sec)\n",
      "Step #6000\tEpoch   1 Batch 2874/3125   Loss: 0.876930 mae: 0.723795 (2041.4212012070475 steps/sec)\n",
      "Step #6001\tEpoch   1 Batch 2875/3125   Loss: 0.874198 mae: 0.742197 (2061.0020244904376 steps/sec)\n",
      "Step #6002\tEpoch   1 Batch 2876/3125   Loss: 0.749211 mae: 0.699661 (1922.2820059213361 steps/sec)\n",
      "Step #6003\tEpoch   1 Batch 2877/3125   Loss: 0.783775 mae: 0.702949 (2111.6590980032825 steps/sec)\n",
      "Step #6004\tEpoch   1 Batch 2878/3125   Loss: 0.853539 mae: 0.719613 (1971.0816196097599 steps/sec)\n",
      "Step #6005\tEpoch   1 Batch 2879/3125   Loss: 0.711074 mae: 0.685228 (1838.0913983206829 steps/sec)\n",
      "Step #6006\tEpoch   1 Batch 2880/3125   Loss: 0.816137 mae: 0.735848 (2097.298810916764 steps/sec)\n",
      "Step #6007\tEpoch   1 Batch 2881/3125   Loss: 0.766608 mae: 0.695245 (2032.0255801560002 steps/sec)\n",
      "Step #6008\tEpoch   1 Batch 2882/3125   Loss: 0.814165 mae: 0.727653 (1916.8703441341804 steps/sec)\n",
      "Step #6009\tEpoch   1 Batch 2883/3125   Loss: 0.951314 mae: 0.784347 (1967.6602771600942 steps/sec)\n",
      "Step #6010\tEpoch   1 Batch 2884/3125   Loss: 0.767981 mae: 0.685630 (2099.419372922757 steps/sec)\n",
      "Step #6011\tEpoch   1 Batch 2885/3125   Loss: 0.845961 mae: 0.750878 (2138.9252093383784 steps/sec)\n",
      "Step #6012\tEpoch   1 Batch 2886/3125   Loss: 0.747537 mae: 0.682423 (2018.238860552401 steps/sec)\n",
      "Step #6013\tEpoch   1 Batch 2887/3125   Loss: 0.908611 mae: 0.761606 (2099.167200512492 steps/sec)\n",
      "Step #6014\tEpoch   1 Batch 2888/3125   Loss: 0.864352 mae: 0.749936 (2298.6518184010347 steps/sec)\n",
      "Step #6015\tEpoch   1 Batch 2889/3125   Loss: 0.788695 mae: 0.709192 (2083.6913538541025 steps/sec)\n",
      "Step #6016\tEpoch   1 Batch 2890/3125   Loss: 0.773106 mae: 0.690141 (1826.0875622583678 steps/sec)\n",
      "Step #6017\tEpoch   1 Batch 2891/3125   Loss: 0.903826 mae: 0.755212 (1970.4519402424128 steps/sec)\n",
      "Step #6018\tEpoch   1 Batch 2892/3125   Loss: 0.759921 mae: 0.696281 (2067.4632280452697 steps/sec)\n",
      "Step #6019\tEpoch   1 Batch 2893/3125   Loss: 0.740520 mae: 0.686687 (2293.6489013813393 steps/sec)\n",
      "Step #6020\tEpoch   1 Batch 2894/3125   Loss: 0.852657 mae: 0.737980 (2187.2217934544547 steps/sec)\n",
      "Step #6021\tEpoch   1 Batch 2895/3125   Loss: 0.794424 mae: 0.696177 (2135.4839366631027 steps/sec)\n",
      "Step #6022\tEpoch   1 Batch 2896/3125   Loss: 0.767961 mae: 0.705191 (2145.643544096583 steps/sec)\n",
      "Step #6023\tEpoch   1 Batch 2897/3125   Loss: 0.755989 mae: 0.697464 (2248.3055844420383 steps/sec)\n",
      "Step #6024\tEpoch   1 Batch 2898/3125   Loss: 0.828872 mae: 0.706127 (2004.8104315240043 steps/sec)\n",
      "Step #6025\tEpoch   1 Batch 2899/3125   Loss: 0.863929 mae: 0.744084 (1676.2197071424005 steps/sec)\n",
      "Step #6026\tEpoch   1 Batch 2900/3125   Loss: 0.751374 mae: 0.691590 (2016.9577594829575 steps/sec)\n",
      "Step #6027\tEpoch   1 Batch 2901/3125   Loss: 0.723238 mae: 0.683032 (2124.8601767042232 steps/sec)\n",
      "Step #6028\tEpoch   1 Batch 2902/3125   Loss: 0.800675 mae: 0.692338 (1972.4905944319037 steps/sec)\n",
      "Step #6029\tEpoch   1 Batch 2903/3125   Loss: 0.721570 mae: 0.668773 (2004.7337730618488 steps/sec)\n",
      "Step #6030\tEpoch   1 Batch 2904/3125   Loss: 0.791170 mae: 0.710525 (2120.734568400615 steps/sec)\n",
      "Step #6031\tEpoch   1 Batch 2905/3125   Loss: 0.921418 mae: 0.750910 (2231.938783112142 steps/sec)\n",
      "Step #6032\tEpoch   1 Batch 2906/3125   Loss: 0.978942 mae: 0.775911 (2244.5757342238207 steps/sec)\n",
      "Step #6033\tEpoch   1 Batch 2907/3125   Loss: 0.675101 mae: 0.658804 (1913.1456512616542 steps/sec)\n",
      "Step #6034\tEpoch   1 Batch 2908/3125   Loss: 0.882058 mae: 0.738276 (1697.5764542084216 steps/sec)\n",
      "Step #6035\tEpoch   1 Batch 2909/3125   Loss: 0.840459 mae: 0.733514 (1937.2154892107596 steps/sec)\n",
      "Step #6036\tEpoch   1 Batch 2910/3125   Loss: 0.886827 mae: 0.750177 (2041.9777609004693 steps/sec)\n",
      "Step #6037\tEpoch   1 Batch 2911/3125   Loss: 0.860531 mae: 0.753286 (1978.0907195879984 steps/sec)\n",
      "Step #6038\tEpoch   1 Batch 2912/3125   Loss: 0.744160 mae: 0.686833 (2061.8530753500077 steps/sec)\n",
      "Step #6039\tEpoch   1 Batch 2913/3125   Loss: 0.721953 mae: 0.672063 (2269.006556596629 steps/sec)\n",
      "Step #6040\tEpoch   1 Batch 2914/3125   Loss: 0.781324 mae: 0.700181 (2247.07697582719 steps/sec)\n",
      "Step #6041\tEpoch   1 Batch 2915/3125   Loss: 0.857988 mae: 0.735021 (2099.7977451589004 steps/sec)\n",
      "Step #6042\tEpoch   1 Batch 2916/3125   Loss: 1.090999 mae: 0.814684 (2048.94043164343 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6043\tEpoch   1 Batch 2917/3125   Loss: 0.920009 mae: 0.762274 (1638.2080224973636 steps/sec)\n",
      "Step #6044\tEpoch   1 Batch 2918/3125   Loss: 0.757757 mae: 0.698801 (2216.9797558010464 steps/sec)\n",
      "Step #6045\tEpoch   1 Batch 2919/3125   Loss: 0.782690 mae: 0.695066 (2062.785985481872 steps/sec)\n",
      "Step #6046\tEpoch   1 Batch 2920/3125   Loss: 0.801917 mae: 0.723768 (2204.002017824113 steps/sec)\n",
      "Step #6047\tEpoch   1 Batch 2921/3125   Loss: 0.775984 mae: 0.706414 (2360.3817756167837 steps/sec)\n",
      "Step #6048\tEpoch   1 Batch 2922/3125   Loss: 1.032938 mae: 0.810792 (2038.385350349426 steps/sec)\n",
      "Step #6049\tEpoch   1 Batch 2923/3125   Loss: 0.780122 mae: 0.712420 (2027.5071300816937 steps/sec)\n",
      "Step #6050\tEpoch   1 Batch 2924/3125   Loss: 0.858173 mae: 0.751117 (1959.0763022195652 steps/sec)\n",
      "Step #6051\tEpoch   1 Batch 2925/3125   Loss: 0.862026 mae: 0.736248 (2156.033268564496 steps/sec)\n",
      "Step #6052\tEpoch   1 Batch 2926/3125   Loss: 0.713269 mae: 0.670740 (1758.6917690469202 steps/sec)\n",
      "Step #6053\tEpoch   1 Batch 2927/3125   Loss: 0.760790 mae: 0.693642 (1772.1712383173622 steps/sec)\n",
      "Step #6054\tEpoch   1 Batch 2928/3125   Loss: 0.878882 mae: 0.745868 (2076.244220697575 steps/sec)\n",
      "Step #6055\tEpoch   1 Batch 2929/3125   Loss: 0.862097 mae: 0.738760 (2020.1051881249157 steps/sec)\n",
      "Step #6056\tEpoch   1 Batch 2930/3125   Loss: 0.646504 mae: 0.630659 (1884.7924362119945 steps/sec)\n",
      "Step #6057\tEpoch   1 Batch 2931/3125   Loss: 0.766971 mae: 0.693917 (2112.5737886571974 steps/sec)\n",
      "Step #6058\tEpoch   1 Batch 2932/3125   Loss: 0.649854 mae: 0.660640 (2044.984446763074 steps/sec)\n",
      "Step #6059\tEpoch   1 Batch 2933/3125   Loss: 0.799866 mae: 0.674463 (2135.570920866386 steps/sec)\n",
      "Step #6060\tEpoch   1 Batch 2934/3125   Loss: 0.711118 mae: 0.685743 (1854.0490840936418 steps/sec)\n",
      "Step #6061\tEpoch   1 Batch 2935/3125   Loss: 0.899613 mae: 0.773385 (1708.5298095253613 steps/sec)\n",
      "Step #6062\tEpoch   1 Batch 2936/3125   Loss: 0.760046 mae: 0.695404 (1857.5470110452707 steps/sec)\n",
      "Step #6063\tEpoch   1 Batch 2937/3125   Loss: 0.843291 mae: 0.723493 (1975.5006688144088 steps/sec)\n",
      "Step #6064\tEpoch   1 Batch 2938/3125   Loss: 0.831824 mae: 0.741149 (1993.2062918785343 steps/sec)\n",
      "Step #6065\tEpoch   1 Batch 2939/3125   Loss: 0.808116 mae: 0.705738 (1954.202115268136 steps/sec)\n",
      "Step #6066\tEpoch   1 Batch 2940/3125   Loss: 0.861074 mae: 0.741318 (2136.1147327249023 steps/sec)\n",
      "Step #6067\tEpoch   1 Batch 2941/3125   Loss: 0.815354 mae: 0.735904 (2145.555737436569 steps/sec)\n",
      "Step #6068\tEpoch   1 Batch 2942/3125   Loss: 0.948427 mae: 0.776576 (1938.6481289750036 steps/sec)\n",
      "Step #6069\tEpoch   1 Batch 2943/3125   Loss: 1.051738 mae: 0.828409 (1939.3293754276942 steps/sec)\n",
      "Step #6070\tEpoch   1 Batch 2944/3125   Loss: 0.864203 mae: 0.736033 (2052.9720416634036 steps/sec)\n",
      "Step #6071\tEpoch   1 Batch 2945/3125   Loss: 0.834678 mae: 0.742131 (2158.896438130533 steps/sec)\n",
      "Step #6072\tEpoch   1 Batch 2946/3125   Loss: 0.762580 mae: 0.687237 (2074.6010861930813 steps/sec)\n",
      "Step #6073\tEpoch   1 Batch 2947/3125   Loss: 0.893497 mae: 0.741681 (2138.3145551873567 steps/sec)\n",
      "Step #6074\tEpoch   1 Batch 2948/3125   Loss: 0.793257 mae: 0.710436 (2039.892225237581 steps/sec)\n",
      "Step #6075\tEpoch   1 Batch 2949/3125   Loss: 0.884295 mae: 0.749441 (2171.1895641370743 steps/sec)\n",
      "Step #6076\tEpoch   1 Batch 2950/3125   Loss: 0.933494 mae: 0.770354 (2037.8901543125899 steps/sec)\n",
      "Step #6077\tEpoch   1 Batch 2951/3125   Loss: 0.840367 mae: 0.737463 (1781.3838914088647 steps/sec)\n",
      "Step #6078\tEpoch   1 Batch 2952/3125   Loss: 0.853448 mae: 0.737183 (1582.4337682132698 steps/sec)\n",
      "Step #6079\tEpoch   1 Batch 2953/3125   Loss: 0.866755 mae: 0.709989 (2129.3909794285482 steps/sec)\n",
      "Step #6080\tEpoch   1 Batch 2954/3125   Loss: 0.793555 mae: 0.687344 (2301.9318580962417 steps/sec)\n",
      "Step #6081\tEpoch   1 Batch 2955/3125   Loss: 0.813535 mae: 0.697960 (2169.6845546623626 steps/sec)\n",
      "Step #6082\tEpoch   1 Batch 2956/3125   Loss: 0.903076 mae: 0.753052 (2318.370956687081 steps/sec)\n",
      "Step #6083\tEpoch   1 Batch 2957/3125   Loss: 0.891561 mae: 0.735548 (1985.939393939394 steps/sec)\n",
      "Step #6084\tEpoch   1 Batch 2958/3125   Loss: 1.006973 mae: 0.814120 (2016.143359803111 steps/sec)\n",
      "Step #6085\tEpoch   1 Batch 2959/3125   Loss: 0.773771 mae: 0.703684 (2194.614845278833 steps/sec)\n",
      "Step #6086\tEpoch   1 Batch 2960/3125   Loss: 0.788102 mae: 0.712162 (1766.1565929207266 steps/sec)\n",
      "Step #6087\tEpoch   1 Batch 2961/3125   Loss: 0.793418 mae: 0.715351 (1991.8621659099974 steps/sec)\n",
      "Step #6088\tEpoch   1 Batch 2962/3125   Loss: 0.834417 mae: 0.709692 (1894.3778002601532 steps/sec)\n",
      "Step #6089\tEpoch   1 Batch 2963/3125   Loss: 0.781985 mae: 0.706351 (1927.5294117647059 steps/sec)\n",
      "Step #6090\tEpoch   1 Batch 2964/3125   Loss: 1.007357 mae: 0.784633 (1757.851502908585 steps/sec)\n",
      "Step #6091\tEpoch   1 Batch 2965/3125   Loss: 0.878710 mae: 0.761915 (1729.6384270256003 steps/sec)\n",
      "Step #6092\tEpoch   1 Batch 2966/3125   Loss: 0.684819 mae: 0.673928 (1757.4095799953072 steps/sec)\n",
      "Step #6093\tEpoch   1 Batch 2967/3125   Loss: 0.973004 mae: 0.800070 (1804.079315239365 steps/sec)\n",
      "Step #6094\tEpoch   1 Batch 2968/3125   Loss: 0.711324 mae: 0.679464 (1611.8052139695033 steps/sec)\n",
      "Step #6095\tEpoch   1 Batch 2969/3125   Loss: 0.695237 mae: 0.652532 (1782.1407933648322 steps/sec)\n",
      "Step #6096\tEpoch   1 Batch 2970/3125   Loss: 0.869791 mae: 0.738055 (1799.9296215872907 steps/sec)\n",
      "Step #6097\tEpoch   1 Batch 2971/3125   Loss: 0.740740 mae: 0.695183 (2301.073097939389 steps/sec)\n",
      "Step #6098\tEpoch   1 Batch 2972/3125   Loss: 0.722857 mae: 0.680709 (2271.291954122578 steps/sec)\n",
      "Step #6099\tEpoch   1 Batch 2973/3125   Loss: 0.880959 mae: 0.729327 (2038.325914118539 steps/sec)\n",
      "Step #6100\tEpoch   1 Batch 2974/3125   Loss: 0.746008 mae: 0.685286 (2005.5389794200903 steps/sec)\n",
      "Step #6101\tEpoch   1 Batch 2975/3125   Loss: 0.737271 mae: 0.678358 (2050.042034057362 steps/sec)\n",
      "Step #6102\tEpoch   1 Batch 2976/3125   Loss: 0.921711 mae: 0.771880 (2100.050069095352 steps/sec)\n",
      "Step #6103\tEpoch   1 Batch 2977/3125   Loss: 0.910010 mae: 0.759874 (1782.080217539089 steps/sec)\n",
      "Step #6104\tEpoch   1 Batch 2978/3125   Loss: 0.738285 mae: 0.654337 (1936.7145654020908 steps/sec)\n",
      "Step #6105\tEpoch   1 Batch 2979/3125   Loss: 0.924193 mae: 0.755166 (1925.1406802221509 steps/sec)\n",
      "Step #6106\tEpoch   1 Batch 2980/3125   Loss: 0.773950 mae: 0.694204 (1910.0614782093903 steps/sec)\n",
      "Step #6107\tEpoch   1 Batch 2981/3125   Loss: 0.675807 mae: 0.686738 (1848.7860782481443 steps/sec)\n",
      "Step #6108\tEpoch   1 Batch 2982/3125   Loss: 0.842964 mae: 0.724519 (2028.5465554932193 steps/sec)\n",
      "Step #6109\tEpoch   1 Batch 2983/3125   Loss: 0.778322 mae: 0.695580 (1937.2870709086205 steps/sec)\n",
      "Step #6110\tEpoch   1 Batch 2984/3125   Loss: 0.748067 mae: 0.696450 (1642.4676738485155 steps/sec)\n",
      "Step #6111\tEpoch   1 Batch 2985/3125   Loss: 0.809736 mae: 0.714262 (1811.6691718930872 steps/sec)\n",
      "Step #6112\tEpoch   1 Batch 2986/3125   Loss: 0.845516 mae: 0.737158 (1859.160823042349 steps/sec)\n",
      "Step #6113\tEpoch   1 Batch 2987/3125   Loss: 0.791197 mae: 0.700144 (2117.0309203420115 steps/sec)\n",
      "Step #6114\tEpoch   1 Batch 2988/3125   Loss: 0.767406 mae: 0.696169 (2014.9036336734498 steps/sec)\n",
      "Step #6115\tEpoch   1 Batch 2989/3125   Loss: 0.846200 mae: 0.713252 (2002.07352814824 steps/sec)\n",
      "Step #6116\tEpoch   1 Batch 2990/3125   Loss: 0.785190 mae: 0.696068 (2102.0708457791234 steps/sec)\n",
      "Step #6117\tEpoch   1 Batch 2991/3125   Loss: 0.879824 mae: 0.748990 (1957.2479187665658 steps/sec)\n",
      "Step #6118\tEpoch   1 Batch 2992/3125   Loss: 0.834745 mae: 0.729908 (2158.141066540433 steps/sec)\n",
      "Step #6119\tEpoch   1 Batch 2993/3125   Loss: 0.677394 mae: 0.644858 (1858.5182559376108 steps/sec)\n",
      "Step #6120\tEpoch   1 Batch 2994/3125   Loss: 0.780391 mae: 0.679934 (1714.7183634089106 steps/sec)\n",
      "Step #6121\tEpoch   1 Batch 2995/3125   Loss: 0.758754 mae: 0.705838 (1753.3542906828975 steps/sec)\n",
      "Step #6122\tEpoch   1 Batch 2996/3125   Loss: 0.800131 mae: 0.708407 (1990.7654921020655 steps/sec)\n",
      "Step #6123\tEpoch   1 Batch 2997/3125   Loss: 0.990636 mae: 0.779745 (1799.5898228000171 steps/sec)\n",
      "Step #6124\tEpoch   1 Batch 2998/3125   Loss: 0.824526 mae: 0.738940 (2020.7475356760872 steps/sec)\n",
      "Step #6125\tEpoch   1 Batch 2999/3125   Loss: 0.757918 mae: 0.707036 (1877.2508369586624 steps/sec)\n",
      "Step #6126\tEpoch   1 Batch 3000/3125   Loss: 0.905216 mae: 0.730960 (1567.5070446748239 steps/sec)\n",
      "Step #6127\tEpoch   1 Batch 3001/3125   Loss: 0.811801 mae: 0.728698 (1850.40102351436 steps/sec)\n",
      "Step #6128\tEpoch   1 Batch 3002/3125   Loss: 0.734629 mae: 0.674084 (1975.295990355 steps/sec)\n",
      "Step #6129\tEpoch   1 Batch 3003/3125   Loss: 0.772767 mae: 0.688259 (2076.6554110925167 steps/sec)\n",
      "Step #6130\tEpoch   1 Batch 3004/3125   Loss: 0.799499 mae: 0.716575 (1896.3820340546358 steps/sec)\n",
      "Step #6131\tEpoch   1 Batch 3005/3125   Loss: 0.730794 mae: 0.676397 (1996.9643010179304 steps/sec)\n",
      "Step #6132\tEpoch   1 Batch 3006/3125   Loss: 0.700212 mae: 0.670031 (2089.0881198573506 steps/sec)\n",
      "Step #6133\tEpoch   1 Batch 3007/3125   Loss: 0.814712 mae: 0.698467 (1970.1557597279373 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6134\tEpoch   1 Batch 3008/3125   Loss: 0.818245 mae: 0.691832 (1685.7593003440404 steps/sec)\n",
      "Step #6135\tEpoch   1 Batch 3009/3125   Loss: 0.825838 mae: 0.703180 (1964.1036207316388 steps/sec)\n",
      "Step #6136\tEpoch   1 Batch 3010/3125   Loss: 0.834382 mae: 0.710069 (2069.0950708394175 steps/sec)\n",
      "Step #6137\tEpoch   1 Batch 3011/3125   Loss: 0.808392 mae: 0.697621 (2103.7789035461706 steps/sec)\n",
      "Step #6138\tEpoch   1 Batch 3012/3125   Loss: 0.841874 mae: 0.739038 (1807.8740700511203 steps/sec)\n",
      "Step #6139\tEpoch   1 Batch 3013/3125   Loss: 0.947556 mae: 0.765880 (1868.2535723194242 steps/sec)\n",
      "Step #6140\tEpoch   1 Batch 3014/3125   Loss: 0.898152 mae: 0.743736 (1815.999030151885 steps/sec)\n",
      "Step #6141\tEpoch   1 Batch 3015/3125   Loss: 0.897598 mae: 0.745915 (1784.1117519949637 steps/sec)\n",
      "Step #6142\tEpoch   1 Batch 3016/3125   Loss: 0.839130 mae: 0.736286 (1646.8529876004177 steps/sec)\n",
      "Step #6143\tEpoch   1 Batch 3017/3125   Loss: 0.852665 mae: 0.722237 (1829.672218393111 steps/sec)\n",
      "Step #6144\tEpoch   1 Batch 3018/3125   Loss: 0.848074 mae: 0.731130 (2118.1000090898992 steps/sec)\n",
      "Step #6145\tEpoch   1 Batch 3019/3125   Loss: 0.918934 mae: 0.733954 (1989.8965746275737 steps/sec)\n",
      "Step #6146\tEpoch   1 Batch 3020/3125   Loss: 0.939081 mae: 0.756261 (2199.7944070321187 steps/sec)\n",
      "Step #6147\tEpoch   1 Batch 3021/3125   Loss: 0.756831 mae: 0.695580 (2150.9030676608445 steps/sec)\n",
      "Step #6148\tEpoch   1 Batch 3022/3125   Loss: 0.622375 mae: 0.630452 (2062.461399264373 steps/sec)\n",
      "Step #6149\tEpoch   1 Batch 3023/3125   Loss: 0.893522 mae: 0.738325 (2039.0790292470442 steps/sec)\n",
      "Step #6150\tEpoch   1 Batch 3024/3125   Loss: 0.722210 mae: 0.698582 (2064.410450258894 steps/sec)\n",
      "Step #6151\tEpoch   1 Batch 3025/3125   Loss: 0.823475 mae: 0.730150 (2038.1476262209048 steps/sec)\n",
      "Step #6152\tEpoch   1 Batch 3026/3125   Loss: 0.711861 mae: 0.657579 (1841.4969749655347 steps/sec)\n",
      "Step #6153\tEpoch   1 Batch 3027/3125   Loss: 0.836299 mae: 0.742265 (2037.672344270737 steps/sec)\n",
      "Step #6154\tEpoch   1 Batch 3028/3125   Loss: 0.757966 mae: 0.679568 (1984.9995267392333 steps/sec)\n",
      "Step #6155\tEpoch   1 Batch 3029/3125   Loss: 0.762339 mae: 0.706316 (1869.8693772011948 steps/sec)\n",
      "Step #6156\tEpoch   1 Batch 3030/3125   Loss: 0.849449 mae: 0.709388 (2305.2212720117836 steps/sec)\n",
      "Step #6157\tEpoch   1 Batch 3031/3125   Loss: 0.830660 mae: 0.723601 (1939.7060591766328 steps/sec)\n",
      "Step #6158\tEpoch   1 Batch 3032/3125   Loss: 0.783913 mae: 0.690594 (1846.2470287877454 steps/sec)\n",
      "Step #6159\tEpoch   1 Batch 3033/3125   Loss: 0.779422 mae: 0.720745 (1837.7531437584892 steps/sec)\n",
      "Step #6160\tEpoch   1 Batch 3034/3125   Loss: 0.764614 mae: 0.700787 (1958.9299058436707 steps/sec)\n",
      "Step #6161\tEpoch   1 Batch 3035/3125   Loss: 0.884014 mae: 0.757417 (1974.4033440974608 steps/sec)\n",
      "Step #6162\tEpoch   1 Batch 3036/3125   Loss: 0.608881 mae: 0.621214 (2133.6371960524975 steps/sec)\n",
      "Step #6163\tEpoch   1 Batch 3037/3125   Loss: 0.848277 mae: 0.740074 (1894.4462511291779 steps/sec)\n",
      "Step #6164\tEpoch   1 Batch 3038/3125   Loss: 0.822281 mae: 0.751887 (1952.1465539710318 steps/sec)\n",
      "Step #6165\tEpoch   1 Batch 3039/3125   Loss: 0.742453 mae: 0.693930 (1941.6635804755203 steps/sec)\n",
      "Step #6166\tEpoch   1 Batch 3040/3125   Loss: 0.895243 mae: 0.733459 (1694.6272009567444 steps/sec)\n",
      "Step #6167\tEpoch   1 Batch 3041/3125   Loss: 0.832700 mae: 0.729075 (1149.4518986231694 steps/sec)\n",
      "Step #6168\tEpoch   1 Batch 3042/3125   Loss: 0.813784 mae: 0.709488 (766.6009906238006 steps/sec)\n",
      "Step #6169\tEpoch   1 Batch 3043/3125   Loss: 0.928316 mae: 0.766493 (1927.210571780404 steps/sec)\n",
      "Step #6170\tEpoch   1 Batch 3044/3125   Loss: 0.841267 mae: 0.713244 (1480.3391050844587 steps/sec)\n",
      "Step #6171\tEpoch   1 Batch 3045/3125   Loss: 0.885337 mae: 0.727170 (1239.7079774184967 steps/sec)\n",
      "Step #6172\tEpoch   1 Batch 3046/3125   Loss: 0.634068 mae: 0.633706 (1482.0024309579671 steps/sec)\n",
      "Step #6173\tEpoch   1 Batch 3047/3125   Loss: 0.657176 mae: 0.646416 (1204.007325712908 steps/sec)\n",
      "Step #6174\tEpoch   1 Batch 3048/3125   Loss: 0.730055 mae: 0.681917 (1625.749635647617 steps/sec)\n",
      "Step #6175\tEpoch   1 Batch 3049/3125   Loss: 0.781552 mae: 0.721734 (1437.6067672987017 steps/sec)\n",
      "Step #6176\tEpoch   1 Batch 3050/3125   Loss: 0.799649 mae: 0.720196 (1779.6605566870332 steps/sec)\n",
      "Step #6177\tEpoch   1 Batch 3051/3125   Loss: 0.816119 mae: 0.700192 (1499.250786388333 steps/sec)\n",
      "Step #6178\tEpoch   1 Batch 3052/3125   Loss: 0.727476 mae: 0.676100 (1511.6788005478268 steps/sec)\n",
      "Step #6179\tEpoch   1 Batch 3053/3125   Loss: 0.851060 mae: 0.710839 (1410.4759086384547 steps/sec)\n",
      "Step #6180\tEpoch   1 Batch 3054/3125   Loss: 0.766255 mae: 0.682681 (1596.7838216482915 steps/sec)\n",
      "Step #6181\tEpoch   1 Batch 3055/3125   Loss: 0.899434 mae: 0.748204 (1651.8991130645745 steps/sec)\n",
      "Step #6182\tEpoch   1 Batch 3056/3125   Loss: 0.851376 mae: 0.733351 (1744.4720796559554 steps/sec)\n",
      "Step #6183\tEpoch   1 Batch 3057/3125   Loss: 0.958162 mae: 0.761233 (1549.9098353386348 steps/sec)\n",
      "Step #6184\tEpoch   1 Batch 3058/3125   Loss: 0.914874 mae: 0.760542 (1296.747545200465 steps/sec)\n",
      "Step #6185\tEpoch   1 Batch 3059/3125   Loss: 0.687033 mae: 0.649062 (1692.34344738541 steps/sec)\n",
      "Step #6186\tEpoch   1 Batch 3060/3125   Loss: 0.719338 mae: 0.693481 (1553.4344190043037 steps/sec)\n",
      "Step #6187\tEpoch   1 Batch 3061/3125   Loss: 0.886587 mae: 0.733363 (1680.4371864933732 steps/sec)\n",
      "Step #6188\tEpoch   1 Batch 3062/3125   Loss: 0.736432 mae: 0.670702 (1784.0662191936979 steps/sec)\n",
      "Step #6189\tEpoch   1 Batch 3063/3125   Loss: 0.889384 mae: 0.784052 (2044.226963904512 steps/sec)\n",
      "Step #6190\tEpoch   1 Batch 3064/3125   Loss: 0.841700 mae: 0.718611 (1950.6939018491646 steps/sec)\n",
      "Step #6191\tEpoch   1 Batch 3065/3125   Loss: 0.798499 mae: 0.706785 (1887.1499532071125 steps/sec)\n",
      "Step #6192\tEpoch   1 Batch 3066/3125   Loss: 0.842485 mae: 0.714282 (1781.5049525136342 steps/sec)\n",
      "Step #6193\tEpoch   1 Batch 3067/3125   Loss: 0.733973 mae: 0.668973 (1844.6230978977921 steps/sec)\n",
      "Step #6194\tEpoch   1 Batch 3068/3125   Loss: 0.937565 mae: 0.753860 (2067.4020840110807 steps/sec)\n",
      "Step #6195\tEpoch   1 Batch 3069/3125   Loss: 0.714427 mae: 0.679621 (1984.0230080793174 steps/sec)\n",
      "Step #6196\tEpoch   1 Batch 3070/3125   Loss: 0.804726 mae: 0.710721 (2015.000432372186 steps/sec)\n",
      "Step #6197\tEpoch   1 Batch 3071/3125   Loss: 0.870507 mae: 0.755115 (2043.5895187144931 steps/sec)\n",
      "Step #6198\tEpoch   1 Batch 3072/3125   Loss: 0.837838 mae: 0.707560 (1816.7226861637616 steps/sec)\n",
      "Step #6199\tEpoch   1 Batch 3073/3125   Loss: 1.015043 mae: 0.783604 (2041.5801873016492 steps/sec)\n",
      "Step #6200\tEpoch   1 Batch 3074/3125   Loss: 0.834418 mae: 0.735079 (2043.2311304669765 steps/sec)\n",
      "Step #6201\tEpoch   1 Batch 3075/3125   Loss: 0.790366 mae: 0.705293 (1710.2854346762356 steps/sec)\n",
      "Step #6202\tEpoch   1 Batch 3076/3125   Loss: 0.913467 mae: 0.768668 (2139.121565107407 steps/sec)\n",
      "Step #6203\tEpoch   1 Batch 3077/3125   Loss: 0.718428 mae: 0.679289 (1995.918988883813 steps/sec)\n",
      "Step #6204\tEpoch   1 Batch 3078/3125   Loss: 0.666021 mae: 0.646001 (2043.8882716410346 steps/sec)\n",
      "Step #6205\tEpoch   1 Batch 3079/3125   Loss: 0.820436 mae: 0.709305 (2098.180108253044 steps/sec)\n",
      "Step #6206\tEpoch   1 Batch 3080/3125   Loss: 0.966089 mae: 0.785310 (1941.7534698110237 steps/sec)\n",
      "Step #6207\tEpoch   1 Batch 3081/3125   Loss: 0.770273 mae: 0.668281 (1987.369697888632 steps/sec)\n",
      "Step #6208\tEpoch   1 Batch 3082/3125   Loss: 0.800206 mae: 0.702307 (1925.6709976585098 steps/sec)\n",
      "Step #6209\tEpoch   1 Batch 3083/3125   Loss: 0.785042 mae: 0.718345 (1680.329471339519 steps/sec)\n",
      "Step #6210\tEpoch   1 Batch 3084/3125   Loss: 0.856105 mae: 0.731758 (2078.6519972247 steps/sec)\n",
      "Step #6211\tEpoch   1 Batch 3085/3125   Loss: 0.829558 mae: 0.724074 (2001.6913399956095 steps/sec)\n",
      "Step #6212\tEpoch   1 Batch 3086/3125   Loss: 0.825149 mae: 0.713722 (2196.844817833274 steps/sec)\n",
      "Step #6213\tEpoch   1 Batch 3087/3125   Loss: 0.932032 mae: 0.748375 (2145.555737436569 steps/sec)\n",
      "Step #6214\tEpoch   1 Batch 3088/3125   Loss: 0.669863 mae: 0.659331 (2254.1780424360986 steps/sec)\n",
      "Step #6215\tEpoch   1 Batch 3089/3125   Loss: 0.804150 mae: 0.724456 (2041.878352984704 steps/sec)\n",
      "Step #6216\tEpoch   1 Batch 3090/3125   Loss: 0.677087 mae: 0.664867 (1993.926428781958 steps/sec)\n",
      "Step #6217\tEpoch   1 Batch 3091/3125   Loss: 0.922754 mae: 0.774682 (1802.4512247529008 steps/sec)\n",
      "Step #6218\tEpoch   1 Batch 3092/3125   Loss: 0.664631 mae: 0.652698 (1949.0079088484308 steps/sec)\n",
      "Step #6219\tEpoch   1 Batch 3093/3125   Loss: 0.812450 mae: 0.724126 (2010.9428787864258 steps/sec)\n",
      "Step #6220\tEpoch   1 Batch 3094/3125   Loss: 0.883547 mae: 0.765479 (1786.5283208532462 steps/sec)\n",
      "Step #6221\tEpoch   1 Batch 3095/3125   Loss: 0.761620 mae: 0.684963 (1822.2952130201681 steps/sec)\n",
      "Step #6222\tEpoch   1 Batch 3096/3125   Loss: 0.762613 mae: 0.698030 (2130.4509483222773 steps/sec)\n",
      "Step #6223\tEpoch   1 Batch 3097/3125   Loss: 0.710115 mae: 0.646665 (2113.617077030064 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6224\tEpoch   1 Batch 3098/3125   Loss: 0.958356 mae: 0.756231 (2004.4655146046796 steps/sec)\n",
      "Step #6225\tEpoch   1 Batch 3099/3125   Loss: 0.892672 mae: 0.765166 (1713.7935261381558 steps/sec)\n",
      "Step #6226\tEpoch   1 Batch 3100/3125   Loss: 0.999624 mae: 0.772279 (1873.7788261362925 steps/sec)\n",
      "Step #6227\tEpoch   1 Batch 3101/3125   Loss: 0.861739 mae: 0.752545 (2129.109940202439 steps/sec)\n",
      "Step #6228\tEpoch   1 Batch 3102/3125   Loss: 0.840743 mae: 0.702182 (2147.1153747709195 steps/sec)\n",
      "Step #6229\tEpoch   1 Batch 3103/3125   Loss: 0.875115 mae: 0.734939 (2037.395198818649 steps/sec)\n",
      "Step #6230\tEpoch   1 Batch 3104/3125   Loss: 0.776630 mae: 0.672695 (2215.1298138876564 steps/sec)\n",
      "Step #6231\tEpoch   1 Batch 3105/3125   Loss: 0.813530 mae: 0.737155 (2061.427462082117 steps/sec)\n",
      "Step #6232\tEpoch   1 Batch 3106/3125   Loss: 0.812683 mae: 0.721927 (2151.2781584670306 steps/sec)\n",
      "Step #6233\tEpoch   1 Batch 3107/3125   Loss: 0.705386 mae: 0.669362 (1963.864515343628 steps/sec)\n",
      "Step #6234\tEpoch   1 Batch 3108/3125   Loss: 0.810185 mae: 0.692718 (1786.4826646221995 steps/sec)\n",
      "Step #6235\tEpoch   1 Batch 3109/3125   Loss: 0.686951 mae: 0.657740 (2153.7968573482594 steps/sec)\n",
      "Step #6236\tEpoch   1 Batch 3110/3125   Loss: 0.723230 mae: 0.678275 (2120.455809344698 steps/sec)\n",
      "Step #6237\tEpoch   1 Batch 3111/3125   Loss: 0.919769 mae: 0.758643 (2098.9150886744865 steps/sec)\n",
      "Step #6238\tEpoch   1 Batch 3112/3125   Loss: 0.865324 mae: 0.733547 (2005.7883410645115 steps/sec)\n",
      "Step #6239\tEpoch   1 Batch 3113/3125   Loss: 0.872144 mae: 0.734387 (2008.9972027436104 steps/sec)\n",
      "Step #6240\tEpoch   1 Batch 3114/3125   Loss: 0.873128 mae: 0.735697 (1849.8623950321078 steps/sec)\n",
      "Step #6241\tEpoch   1 Batch 3115/3125   Loss: 0.746113 mae: 0.664039 (1697.0956438704247 steps/sec)\n",
      "Step #6242\tEpoch   1 Batch 3116/3125   Loss: 0.765647 mae: 0.705340 (2101.712716595011 steps/sec)\n",
      "Step #6243\tEpoch   1 Batch 3117/3125   Loss: 0.744280 mae: 0.692723 (1956.2437618350232 steps/sec)\n",
      "Step #6244\tEpoch   1 Batch 3118/3125   Loss: 0.835315 mae: 0.724583 (1854.2621950680377 steps/sec)\n",
      "Step #6245\tEpoch   1 Batch 3119/3125   Loss: 0.867988 mae: 0.751122 (1891.797392990844 steps/sec)\n",
      "Step #6246\tEpoch   1 Batch 3120/3125   Loss: 0.778955 mae: 0.697944 (1906.207222520156 steps/sec)\n",
      "Step #6247\tEpoch   1 Batch 3121/3125   Loss: 0.836202 mae: 0.722594 (1936.9292153095907 steps/sec)\n",
      "Step #6248\tEpoch   1 Batch 3122/3125   Loss: 0.960050 mae: 0.786821 (1861.7190135468636 steps/sec)\n",
      "Step #6249\tEpoch   1 Batch 3123/3125   Loss: 0.811211 mae: 0.725925 (1787.5638217168575 steps/sec)\n",
      "Step #6250\tEpoch   1 Batch 3124/3125   Loss: 0.853249 mae: 0.735181 (1876.1088547351094 steps/sec)\n",
      "\n",
      "Train time for epoch #2 (6250 total steps): 88.33581805229187\n",
      "Model test set loss: 0.836908 mae: 0.724520\n",
      "best loss = 0.8369083404541016\n",
      "Step #6251\tEpoch   2 Batch    0/3125   Loss: 0.914103 mae: 0.724551 (673.7881046625199 steps/sec)\n",
      "Step #6252\tEpoch   2 Batch    1/3125   Loss: 0.938665 mae: 0.755662 (2189.3224762501304 steps/sec)\n",
      "Step #6253\tEpoch   2 Batch    2/3125   Loss: 0.780540 mae: 0.701897 (2292.620854013162 steps/sec)\n",
      "Step #6254\tEpoch   2 Batch    3/3125   Loss: 0.877457 mae: 0.743263 (2120.9704987004056 steps/sec)\n",
      "Step #6255\tEpoch   2 Batch    4/3125   Loss: 0.882099 mae: 0.755487 (2127.8569761660765 steps/sec)\n",
      "Step #6256\tEpoch   2 Batch    5/3125   Loss: 0.771804 mae: 0.714624 (2411.628334866605 steps/sec)\n",
      "Step #6257\tEpoch   2 Batch    6/3125   Loss: 0.719909 mae: 0.681129 (2231.938783112142 steps/sec)\n",
      "Step #6258\tEpoch   2 Batch    7/3125   Loss: 0.841952 mae: 0.731128 (2350.619276595268 steps/sec)\n",
      "Step #6259\tEpoch   2 Batch    8/3125   Loss: 0.812342 mae: 0.712922 (2301.047849987382 steps/sec)\n",
      "Step #6260\tEpoch   2 Batch    9/3125   Loss: 0.707189 mae: 0.652718 (2008.6893222481897 steps/sec)\n",
      "Step #6261\tEpoch   2 Batch   10/3125   Loss: 0.802416 mae: 0.708705 (2102.0708457791234 steps/sec)\n",
      "Step #6262\tEpoch   2 Batch   11/3125   Loss: 0.883470 mae: 0.735240 (1637.9265368604388 steps/sec)\n",
      "Step #6263\tEpoch   2 Batch   12/3125   Loss: 0.856919 mae: 0.736394 (2150.7265995959347 steps/sec)\n",
      "Step #6264\tEpoch   2 Batch   13/3125   Loss: 0.721766 mae: 0.681172 (2315.555162971469 steps/sec)\n",
      "Step #6265\tEpoch   2 Batch   14/3125   Loss: 0.759566 mae: 0.699662 (2151.432645649743 steps/sec)\n",
      "Step #6266\tEpoch   2 Batch   15/3125   Loss: 0.897622 mae: 0.731295 (2304.6892686411343 steps/sec)\n",
      "Step #6267\tEpoch   2 Batch   16/3125   Loss: 0.909142 mae: 0.760067 (2137.203187738214 steps/sec)\n",
      "Step #6268\tEpoch   2 Batch   17/3125   Loss: 0.812329 mae: 0.710634 (2240.2597957526814 steps/sec)\n",
      "Step #6269\tEpoch   2 Batch   18/3125   Loss: 0.761824 mae: 0.680732 (1757.4095799953072 steps/sec)\n",
      "Step #6270\tEpoch   2 Batch   19/3125   Loss: 0.887810 mae: 0.765786 (1634.581719265154 steps/sec)\n",
      "Step #6271\tEpoch   2 Batch   20/3125   Loss: 0.791830 mae: 0.707113 (2284.977119198082 steps/sec)\n",
      "Step #6272\tEpoch   2 Batch   21/3125   Loss: 0.820959 mae: 0.712604 (2231.8675237324933 steps/sec)\n",
      "Step #6273\tEpoch   2 Batch   22/3125   Loss: 0.838342 mae: 0.732221 (2054.1383430955784 steps/sec)\n",
      "Step #6274\tEpoch   2 Batch   23/3125   Loss: 0.818954 mae: 0.711930 (2425.378468086091 steps/sec)\n",
      "Step #6275\tEpoch   2 Batch   24/3125   Loss: 0.739982 mae: 0.683218 (2335.982890750312 steps/sec)\n",
      "Step #6276\tEpoch   2 Batch   25/3125   Loss: 0.875082 mae: 0.740662 (2199.9328633769724 steps/sec)\n",
      "Step #6277\tEpoch   2 Batch   26/3125   Loss: 0.755807 mae: 0.688117 (2136.027704216745 steps/sec)\n",
      "Step #6278\tEpoch   2 Batch   27/3125   Loss: 0.832252 mae: 0.722470 (2247.4863628082435 steps/sec)\n",
      "Step #6279\tEpoch   2 Batch   28/3125   Loss: 0.918996 mae: 0.750212 (1601.9432753049734 steps/sec)\n",
      "Step #6280\tEpoch   2 Batch   29/3125   Loss: 0.759697 mae: 0.694552 (2263.0811067466657 steps/sec)\n",
      "Step #6281\tEpoch   2 Batch   30/3125   Loss: 0.845724 mae: 0.733325 (2135.114332837857 steps/sec)\n",
      "Step #6282\tEpoch   2 Batch   31/3125   Loss: 0.763386 mae: 0.703932 (2336.451346955146 steps/sec)\n",
      "Step #6283\tEpoch   2 Batch   32/3125   Loss: 0.758884 mae: 0.695758 (2207.9932617393138 steps/sec)\n",
      "Step #6284\tEpoch   2 Batch   33/3125   Loss: 0.746808 mae: 0.687547 (1955.5688176053711 steps/sec)\n",
      "Step #6285\tEpoch   2 Batch   34/3125   Loss: 0.837689 mae: 0.745078 (2256.6521757844444 steps/sec)\n",
      "Step #6286\tEpoch   2 Batch   35/3125   Loss: 0.883907 mae: 0.738568 (2233.650374378255 steps/sec)\n",
      "Step #6287\tEpoch   2 Batch   36/3125   Loss: 0.966981 mae: 0.790782 (1915.627169425262 steps/sec)\n",
      "Step #6288\tEpoch   2 Batch   37/3125   Loss: 0.803796 mae: 0.697649 (1628.817969290036 steps/sec)\n",
      "Step #6289\tEpoch   2 Batch   38/3125   Loss: 0.874633 mae: 0.742171 (2200.8101584636374 steps/sec)\n",
      "Step #6290\tEpoch   2 Batch   39/3125   Loss: 0.878188 mae: 0.742887 (2202.6362500131286 steps/sec)\n",
      "Step #6291\tEpoch   2 Batch   40/3125   Loss: 0.848543 mae: 0.738091 (2302.9682747109146 steps/sec)\n",
      "Step #6292\tEpoch   2 Batch   41/3125   Loss: 0.773127 mae: 0.712227 (2184.0555711771385 steps/sec)\n",
      "Step #6293\tEpoch   2 Batch   42/3125   Loss: 0.734117 mae: 0.658991 (2322.8647696687085 steps/sec)\n",
      "Step #6294\tEpoch   2 Batch   43/3125   Loss: 0.829344 mae: 0.711821 (2294.426817793921 steps/sec)\n",
      "Step #6295\tEpoch   2 Batch   44/3125   Loss: 0.688564 mae: 0.670970 (2259.0587398877556 steps/sec)\n",
      "Step #6296\tEpoch   2 Batch   45/3125   Loss: 0.749906 mae: 0.666300 (1926.4847187646405 steps/sec)\n",
      "Step #6297\tEpoch   2 Batch   46/3125   Loss: 0.842145 mae: 0.719432 (1894.6516334200635 steps/sec)\n",
      "Step #6298\tEpoch   2 Batch   47/3125   Loss: 0.933749 mae: 0.763839 (2258.9614054741105 steps/sec)\n",
      "Step #6299\tEpoch   2 Batch   48/3125   Loss: 0.971903 mae: 0.802795 (2007.535610353806 steps/sec)\n",
      "Step #6300\tEpoch   2 Batch   49/3125   Loss: 0.926622 mae: 0.748635 (2134.375508874776 steps/sec)\n",
      "Step #6301\tEpoch   2 Batch   50/3125   Loss: 0.877503 mae: 0.731996 (2159.741302959774 steps/sec)\n",
      "Step #6302\tEpoch   2 Batch   51/3125   Loss: 0.837497 mae: 0.729764 (2167.6661808634894 steps/sec)\n",
      "Step #6303\tEpoch   2 Batch   52/3125   Loss: 0.798491 mae: 0.717486 (1957.3940638417025 steps/sec)\n",
      "Step #6304\tEpoch   2 Batch   53/3125   Loss: 0.809051 mae: 0.706537 (2067.9729023478717 steps/sec)\n",
      "Step #6305\tEpoch   2 Batch   54/3125   Loss: 0.777872 mae: 0.690024 (1928.80582738577 steps/sec)\n",
      "Step #6306\tEpoch   2 Batch   55/3125   Loss: 0.870003 mae: 0.736752 (1726.4488935721813 steps/sec)\n",
      "Step #6307\tEpoch   2 Batch   56/3125   Loss: 0.846163 mae: 0.727082 (2373.9283005625925 steps/sec)\n",
      "Step #6308\tEpoch   2 Batch   57/3125   Loss: 0.671227 mae: 0.649815 (2163.9309078151764 steps/sec)\n",
      "Step #6309\tEpoch   2 Batch   58/3125   Loss: 0.731635 mae: 0.698559 (2366.3210155148095 steps/sec)\n",
      "Step #6310\tEpoch   2 Batch   59/3125   Loss: 0.794246 mae: 0.712927 (1983.68520620507 steps/sec)\n",
      "Step #6311\tEpoch   2 Batch   60/3125   Loss: 0.701842 mae: 0.657699 (2177.2079357993416 steps/sec)\n",
      "Step #6312\tEpoch   2 Batch   61/3125   Loss: 0.757600 mae: 0.698217 (2134.1148695404404 steps/sec)\n",
      "Step #6313\tEpoch   2 Batch   62/3125   Loss: 0.785531 mae: 0.710751 (2019.4826955299193 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6314\tEpoch   2 Batch   63/3125   Loss: 0.645113 mae: 0.657813 (1619.0098276114966 steps/sec)\n",
      "Step #6315\tEpoch   2 Batch   64/3125   Loss: 0.748716 mae: 0.680523 (1870.703358458588 steps/sec)\n",
      "Step #6316\tEpoch   2 Batch   65/3125   Loss: 0.890438 mae: 0.745521 (2354.2872538673973 steps/sec)\n",
      "Step #6317\tEpoch   2 Batch   66/3125   Loss: 0.848848 mae: 0.741346 (2237.7498212704204 steps/sec)\n",
      "Step #6318\tEpoch   2 Batch   67/3125   Loss: 0.820666 mae: 0.685857 (2195.2810635402493 steps/sec)\n",
      "Step #6319\tEpoch   2 Batch   68/3125   Loss: 0.789146 mae: 0.690327 (2337.4148750013933 steps/sec)\n",
      "Step #6320\tEpoch   2 Batch   69/3125   Loss: 0.755458 mae: 0.685195 (2060.921008667623 steps/sec)\n",
      "Step #6321\tEpoch   2 Batch   70/3125   Loss: 0.758861 mae: 0.698214 (2276.0247880965044 steps/sec)\n",
      "Step #6322\tEpoch   2 Batch   71/3125   Loss: 0.843966 mae: 0.738322 (2296.612823741992 steps/sec)\n",
      "Step #6323\tEpoch   2 Batch   72/3125   Loss: 0.845011 mae: 0.708909 (2014.7487750984724 steps/sec)\n",
      "Step #6324\tEpoch   2 Batch   73/3125   Loss: 0.830408 mae: 0.712188 (1826.2147758542617 steps/sec)\n",
      "Step #6325\tEpoch   2 Batch   74/3125   Loss: 0.874710 mae: 0.741202 (2116.667675972466 steps/sec)\n",
      "Step #6326\tEpoch   2 Batch   75/3125   Loss: 0.789322 mae: 0.715032 (2326.446574369897 steps/sec)\n",
      "Step #6327\tEpoch   2 Batch   76/3125   Loss: 0.738974 mae: 0.676364 (2132.660802359282 steps/sec)\n",
      "Step #6328\tEpoch   2 Batch   77/3125   Loss: 0.739114 mae: 0.658881 (2220.9476203587997 steps/sec)\n",
      "Step #6329\tEpoch   2 Batch   78/3125   Loss: 0.639094 mae: 0.621746 (2335.046541664811 steps/sec)\n",
      "Step #6330\tEpoch   2 Batch   79/3125   Loss: 0.790053 mae: 0.710106 (2199.955941128956 steps/sec)\n",
      "Step #6331\tEpoch   2 Batch   80/3125   Loss: 0.788631 mae: 0.720690 (2278.0026286918455 steps/sec)\n",
      "Step #6332\tEpoch   2 Batch   81/3125   Loss: 0.880317 mae: 0.730595 (2166.591249548014 steps/sec)\n",
      "Step #6333\tEpoch   2 Batch   82/3125   Loss: 0.835515 mae: 0.740678 (1871.6884136872357 steps/sec)\n",
      "Step #6334\tEpoch   2 Batch   83/3125   Loss: 0.739848 mae: 0.678177 (2068.5236329203817 steps/sec)\n",
      "Step #6335\tEpoch   2 Batch   84/3125   Loss: 0.826664 mae: 0.724922 (2083.9191136284594 steps/sec)\n",
      "Step #6336\tEpoch   2 Batch   85/3125   Loss: 0.775700 mae: 0.689037 (2079.9508068275363 steps/sec)\n",
      "Step #6337\tEpoch   2 Batch   86/3125   Loss: 0.961340 mae: 0.781375 (2126.3898605830163 steps/sec)\n",
      "Step #6338\tEpoch   2 Batch   87/3125   Loss: 0.729753 mae: 0.655229 (2213.2595985393755 steps/sec)\n",
      "Step #6339\tEpoch   2 Batch   88/3125   Loss: 1.110484 mae: 0.804392 (2373.1492587982348 steps/sec)\n",
      "Step #6340\tEpoch   2 Batch   89/3125   Loss: 0.871313 mae: 0.745483 (2227.6712590688435 steps/sec)\n",
      "Step #6341\tEpoch   2 Batch   90/3125   Loss: 0.897132 mae: 0.756238 (1833.2389244379174 steps/sec)\n",
      "Step #6342\tEpoch   2 Batch   91/3125   Loss: 0.858506 mae: 0.733861 (2069.789384339038 steps/sec)\n",
      "Step #6343\tEpoch   2 Batch   92/3125   Loss: 0.956633 mae: 0.771209 (2272.202478980671 steps/sec)\n",
      "Step #6344\tEpoch   2 Batch   93/3125   Loss: 0.835088 mae: 0.732309 (2318.3453277175295 steps/sec)\n",
      "Step #6345\tEpoch   2 Batch   94/3125   Loss: 0.849468 mae: 0.734957 (2150.0871455227707 steps/sec)\n",
      "Step #6346\tEpoch   2 Batch   95/3125   Loss: 0.703689 mae: 0.647116 (2139.885513708764 steps/sec)\n",
      "Step #6347\tEpoch   2 Batch   96/3125   Loss: 0.624061 mae: 0.626814 (2103.1248746439887 steps/sec)\n",
      "Step #6348\tEpoch   2 Batch   97/3125   Loss: 0.862133 mae: 0.747263 (2261.202221143997 steps/sec)\n",
      "Step #6349\tEpoch   2 Batch   98/3125   Loss: 0.778897 mae: 0.717562 (2367.763714985718 steps/sec)\n",
      "Step #6350\tEpoch   2 Batch   99/3125   Loss: 1.036304 mae: 0.819901 (2266.7257536289844 steps/sec)\n",
      "Step #6351\tEpoch   2 Batch  100/3125   Loss: 0.915582 mae: 0.769391 (1992.7517365235321 steps/sec)\n",
      "Step #6352\tEpoch   2 Batch  101/3125   Loss: 0.802706 mae: 0.698978 (2374.4121010382346 steps/sec)\n",
      "Step #6353\tEpoch   2 Batch  102/3125   Loss: 0.895542 mae: 0.751530 (2208.272260129728 steps/sec)\n",
      "Step #6354\tEpoch   2 Batch  103/3125   Loss: 0.822556 mae: 0.704878 (2363.3876148081367 steps/sec)\n",
      "Step #6355\tEpoch   2 Batch  104/3125   Loss: 0.827371 mae: 0.732350 (1955.258864223314 steps/sec)\n",
      "Step #6356\tEpoch   2 Batch  105/3125   Loss: 0.688178 mae: 0.648612 (2417.3269552187194 steps/sec)\n",
      "Step #6357\tEpoch   2 Batch  106/3125   Loss: 0.755885 mae: 0.701069 (2336.789793303248 steps/sec)\n",
      "Step #6358\tEpoch   2 Batch  107/3125   Loss: 0.930232 mae: 0.752992 (2386.76165980015 steps/sec)\n",
      "Step #6359\tEpoch   2 Batch  108/3125   Loss: 0.787533 mae: 0.705461 (2044.5857016115667 steps/sec)\n",
      "Step #6360\tEpoch   2 Batch  109/3125   Loss: 0.893293 mae: 0.718023 (2134.831780933476 steps/sec)\n",
      "Step #6361\tEpoch   2 Batch  110/3125   Loss: 0.704800 mae: 0.682887 (2167.3301502656 steps/sec)\n",
      "Step #6362\tEpoch   2 Batch  111/3125   Loss: 0.773044 mae: 0.700445 (2232.841795939227 steps/sec)\n",
      "Step #6363\tEpoch   2 Batch  112/3125   Loss: 0.800850 mae: 0.706899 (2347.094044834417 steps/sec)\n",
      "Step #6364\tEpoch   2 Batch  113/3125   Loss: 0.862645 mae: 0.728871 (2308.5201003918805 steps/sec)\n",
      "Step #6365\tEpoch   2 Batch  114/3125   Loss: 0.917305 mae: 0.761250 (2084.3747825827677 steps/sec)\n",
      "Step #6366\tEpoch   2 Batch  115/3125   Loss: 0.810022 mae: 0.720495 (2274.1736791879935 steps/sec)\n",
      "Step #6367\tEpoch   2 Batch  116/3125   Loss: 0.938368 mae: 0.769235 (2322.556066227366 steps/sec)\n",
      "Step #6368\tEpoch   2 Batch  117/3125   Loss: 0.855798 mae: 0.713618 (2213.3763944738203 steps/sec)\n",
      "Step #6369\tEpoch   2 Batch  118/3125   Loss: 0.769136 mae: 0.706148 (2146.456096537466 steps/sec)\n",
      "Step #6370\tEpoch   2 Batch  119/3125   Loss: 0.806889 mae: 0.725501 (2084.4369346983403 steps/sec)\n",
      "Step #6371\tEpoch   2 Batch  120/3125   Loss: 0.900200 mae: 0.767454 (2110.957663116784 steps/sec)\n",
      "Step #6372\tEpoch   2 Batch  121/3125   Loss: 1.001140 mae: 0.822372 (2231.938783112142 steps/sec)\n",
      "Step #6373\tEpoch   2 Batch  122/3125   Loss: 1.048441 mae: 0.836540 (2421.4298910031407 steps/sec)\n",
      "Step #6374\tEpoch   2 Batch  123/3125   Loss: 0.809607 mae: 0.716134 (2350.0924504409604 steps/sec)\n",
      "Step #6375\tEpoch   2 Batch  124/3125   Loss: 0.700323 mae: 0.684052 (2260.0812578806135 steps/sec)\n",
      "Step #6376\tEpoch   2 Batch  125/3125   Loss: 0.826215 mae: 0.744369 (2253.3787486434503 steps/sec)\n",
      "Step #6377\tEpoch   2 Batch  126/3125   Loss: 0.926071 mae: 0.761647 (2215.317009274713 steps/sec)\n",
      "Step #6378\tEpoch   2 Batch  127/3125   Loss: 0.920756 mae: 0.764420 (2103.251429144519 steps/sec)\n",
      "Step #6379\tEpoch   2 Batch  128/3125   Loss: 0.873456 mae: 0.747350 (2148.3251039767256 steps/sec)\n",
      "Step #6380\tEpoch   2 Batch  129/3125   Loss: 0.958701 mae: 0.799795 (2191.6333120839386 steps/sec)\n",
      "Step #6381\tEpoch   2 Batch  130/3125   Loss: 0.772635 mae: 0.700238 (2358.841922929836 steps/sec)\n",
      "Step #6382\tEpoch   2 Batch  131/3125   Loss: 0.815976 mae: 0.716517 (2440.109372272965 steps/sec)\n",
      "Step #6383\tEpoch   2 Batch  132/3125   Loss: 0.845738 mae: 0.730425 (2279.8847638201883 steps/sec)\n",
      "Step #6384\tEpoch   2 Batch  133/3125   Loss: 0.841757 mae: 0.735243 (2380.0710451352243 steps/sec)\n",
      "Step #6385\tEpoch   2 Batch  134/3125   Loss: 0.793795 mae: 0.683972 (2146.01680259509 steps/sec)\n",
      "Step #6386\tEpoch   2 Batch  135/3125   Loss: 0.786691 mae: 0.742916 (2183.5098131084387 steps/sec)\n",
      "Step #6387\tEpoch   2 Batch  136/3125   Loss: 0.722364 mae: 0.686388 (1948.247447581357 steps/sec)\n",
      "Step #6388\tEpoch   2 Batch  137/3125   Loss: 0.722929 mae: 0.673013 (2315.580730288074 steps/sec)\n",
      "Step #6389\tEpoch   2 Batch  138/3125   Loss: 0.790424 mae: 0.710300 (2503.0160529927794 steps/sec)\n",
      "Step #6390\tEpoch   2 Batch  139/3125   Loss: 0.902860 mae: 0.761274 (2097.844288615243 steps/sec)\n",
      "Step #6391\tEpoch   2 Batch  140/3125   Loss: 0.846940 mae: 0.729331 (2111.7654166834495 steps/sec)\n",
      "Step #6392\tEpoch   2 Batch  141/3125   Loss: 0.808806 mae: 0.732723 (2370.0114141060267 steps/sec)\n",
      "Step #6393\tEpoch   2 Batch  142/3125   Loss: 0.795454 mae: 0.716954 (2390.952207223641 steps/sec)\n",
      "Step #6394\tEpoch   2 Batch  143/3125   Loss: 0.794619 mae: 0.700893 (2167.218163216799 steps/sec)\n",
      "Step #6395\tEpoch   2 Batch  144/3125   Loss: 0.685023 mae: 0.644828 (2200.002098085497 steps/sec)\n",
      "Step #6396\tEpoch   2 Batch  145/3125   Loss: 0.836436 mae: 0.716847 (2009.3244291996818 steps/sec)\n",
      "Step #6397\tEpoch   2 Batch  146/3125   Loss: 0.718461 mae: 0.682467 (2161.4330179539506 steps/sec)\n",
      "Step #6398\tEpoch   2 Batch  147/3125   Loss: 0.919598 mae: 0.761934 (2004.1781744856125 steps/sec)\n",
      "Step #6399\tEpoch   2 Batch  148/3125   Loss: 0.688095 mae: 0.666240 (2022.09194692996 steps/sec)\n",
      "Step #6400\tEpoch   2 Batch  149/3125   Loss: 0.784134 mae: 0.702217 (2041.6596896357016 steps/sec)\n",
      "Step #6401\tEpoch   2 Batch  150/3125   Loss: 0.842272 mae: 0.722905 (2090.816825020189 steps/sec)\n",
      "Step #6402\tEpoch   2 Batch  151/3125   Loss: 0.834395 mae: 0.712265 (2201.1104463826528 steps/sec)\n",
      "Step #6403\tEpoch   2 Batch  152/3125   Loss: 0.727244 mae: 0.681559 (2197.78874671194 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6404\tEpoch   2 Batch  153/3125   Loss: 0.970993 mae: 0.779953 (2107.83874242409 steps/sec)\n",
      "Step #6405\tEpoch   2 Batch  154/3125   Loss: 0.879518 mae: 0.750718 (2003.1252984889297 steps/sec)\n",
      "Step #6406\tEpoch   2 Batch  155/3125   Loss: 0.771757 mae: 0.698802 (2340.179657423422 steps/sec)\n",
      "Step #6407\tEpoch   2 Batch  156/3125   Loss: 0.922837 mae: 0.764909 (2296.185345771471 steps/sec)\n",
      "Step #6408\tEpoch   2 Batch  157/3125   Loss: 0.850770 mae: 0.735644 (2263.667371875135 steps/sec)\n",
      "Step #6409\tEpoch   2 Batch  158/3125   Loss: 0.804629 mae: 0.715049 (1942.9587903943077 steps/sec)\n",
      "Step #6410\tEpoch   2 Batch  159/3125   Loss: 0.760704 mae: 0.681090 (2220.1247075512647 steps/sec)\n",
      "Step #6411\tEpoch   2 Batch  160/3125   Loss: 0.731976 mae: 0.649636 (2202.728790950245 steps/sec)\n",
      "Step #6412\tEpoch   2 Batch  161/3125   Loss: 0.777141 mae: 0.676533 (2157.031185715461 steps/sec)\n",
      "Step #6413\tEpoch   2 Batch  162/3125   Loss: 0.879269 mae: 0.772591 (2090.650078256622 steps/sec)\n",
      "Step #6414\tEpoch   2 Batch  163/3125   Loss: 0.802110 mae: 0.709916 (1910.4964926664845 steps/sec)\n",
      "Step #6415\tEpoch   2 Batch  164/3125   Loss: 0.812642 mae: 0.708951 (2147.7530621441156 steps/sec)\n",
      "Step #6416\tEpoch   2 Batch  165/3125   Loss: 0.791988 mae: 0.710314 (2181.397574320248 steps/sec)\n",
      "Step #6417\tEpoch   2 Batch  166/3125   Loss: 0.964984 mae: 0.770881 (1916.8703441341804 steps/sec)\n",
      "Step #6418\tEpoch   2 Batch  167/3125   Loss: 0.823191 mae: 0.721338 (1942.5088689434147 steps/sec)\n",
      "Step #6419\tEpoch   2 Batch  168/3125   Loss: 0.974268 mae: 0.797875 (1872.6577847626531 steps/sec)\n",
      "Step #6420\tEpoch   2 Batch  169/3125   Loss: 0.948028 mae: 0.752927 (1999.2869059535726 steps/sec)\n",
      "Step #6421\tEpoch   2 Batch  170/3125   Loss: 0.851707 mae: 0.747792 (1831.3818639094593 steps/sec)\n",
      "Step #6422\tEpoch   2 Batch  171/3125   Loss: 0.886842 mae: 0.758191 (1597.0270187943586 steps/sec)\n",
      "Step #6423\tEpoch   2 Batch  172/3125   Loss: 0.703490 mae: 0.672060 (1866.7901014776571 steps/sec)\n",
      "Step #6424\tEpoch   2 Batch  173/3125   Loss: 0.798965 mae: 0.699169 (1936.7682234186977 steps/sec)\n",
      "Step #6425\tEpoch   2 Batch  174/3125   Loss: 0.746447 mae: 0.675558 (1735.3347124534546 steps/sec)\n",
      "Step #6426\tEpoch   2 Batch  175/3125   Loss: 0.816865 mae: 0.733324 (2048.8403446726197 steps/sec)\n",
      "Step #6427\tEpoch   2 Batch  176/3125   Loss: 0.877177 mae: 0.740292 (1911.036185859175 steps/sec)\n",
      "Step #6428\tEpoch   2 Batch  177/3125   Loss: 0.656761 mae: 0.637909 (1978.1653539593453 steps/sec)\n",
      "Step #6429\tEpoch   2 Batch  178/3125   Loss: 0.703801 mae: 0.682306 (1795.9835230240903 steps/sec)\n",
      "Step #6430\tEpoch   2 Batch  179/3125   Loss: 0.846344 mae: 0.728622 (1821.7569950572026 steps/sec)\n",
      "Step #6431\tEpoch   2 Batch  180/3125   Loss: 0.795852 mae: 0.698394 (2047.2402819266288 steps/sec)\n",
      "Step #6432\tEpoch   2 Batch  181/3125   Loss: 0.788966 mae: 0.701514 (1935.7492292639702 steps/sec)\n",
      "Step #6433\tEpoch   2 Batch  182/3125   Loss: 0.833877 mae: 0.732292 (1695.4355102106813 steps/sec)\n",
      "Step #6434\tEpoch   2 Batch  183/3125   Loss: 0.783400 mae: 0.699527 (1404.0263244223959 steps/sec)\n",
      "Step #6435\tEpoch   2 Batch  184/3125   Loss: 0.948714 mae: 0.779944 (1549.7265821288167 steps/sec)\n",
      "Step #6436\tEpoch   2 Batch  185/3125   Loss: 0.712978 mae: 0.659741 (1444.7971781305114 steps/sec)\n",
      "Step #6437\tEpoch   2 Batch  186/3125   Loss: 0.807513 mae: 0.684741 (1381.2955705582085 steps/sec)\n",
      "Step #6438\tEpoch   2 Batch  187/3125   Loss: 0.831185 mae: 0.726776 (1470.375174405968 steps/sec)\n",
      "Step #6439\tEpoch   2 Batch  188/3125   Loss: 0.797737 mae: 0.701199 (1617.9730897420072 steps/sec)\n",
      "Step #6440\tEpoch   2 Batch  189/3125   Loss: 0.890784 mae: 0.741179 (1897.600347461001 steps/sec)\n",
      "Step #6441\tEpoch   2 Batch  190/3125   Loss: 0.774419 mae: 0.715426 (2133.0295571512847 steps/sec)\n",
      "Step #6442\tEpoch   2 Batch  191/3125   Loss: 0.851929 mae: 0.739346 (1971.063093884226 steps/sec)\n",
      "Step #6443\tEpoch   2 Batch  192/3125   Loss: 0.935553 mae: 0.743414 (1827.6152961271657 steps/sec)\n",
      "Step #6444\tEpoch   2 Batch  193/3125   Loss: 0.864896 mae: 0.724701 (1747.4519214745192 steps/sec)\n",
      "Step #6445\tEpoch   2 Batch  194/3125   Loss: 0.801185 mae: 0.691935 (1986.9554507039584 steps/sec)\n",
      "Step #6446\tEpoch   2 Batch  195/3125   Loss: 0.858860 mae: 0.732008 (2014.0328637145026 steps/sec)\n",
      "Step #6447\tEpoch   2 Batch  196/3125   Loss: 0.889179 mae: 0.765414 (2031.9468263426638 steps/sec)\n",
      "Step #6448\tEpoch   2 Batch  197/3125   Loss: 0.784461 mae: 0.709622 (2071.69091861028 steps/sec)\n",
      "Step #6449\tEpoch   2 Batch  198/3125   Loss: 0.938401 mae: 0.795356 (2028.8212986610943 steps/sec)\n",
      "Step #6450\tEpoch   2 Batch  199/3125   Loss: 0.881716 mae: 0.736947 (2065.7729095046247 steps/sec)\n",
      "Step #6451\tEpoch   2 Batch  200/3125   Loss: 0.972952 mae: 0.753050 (1882.119811532421 steps/sec)\n",
      "Step #6452\tEpoch   2 Batch  201/3125   Loss: 0.869874 mae: 0.733538 (1489.2007044253821 steps/sec)\n",
      "Step #6453\tEpoch   2 Batch  202/3125   Loss: 0.904923 mae: 0.760863 (1744.7768644547239 steps/sec)\n",
      "Step #6454\tEpoch   2 Batch  203/3125   Loss: 0.900877 mae: 0.734763 (1987.2567042547144 steps/sec)\n",
      "Step #6455\tEpoch   2 Batch  204/3125   Loss: 0.763848 mae: 0.681481 (2100.3865953568497 steps/sec)\n",
      "Step #6456\tEpoch   2 Batch  205/3125   Loss: 0.718745 mae: 0.664879 (2186.150173565866 steps/sec)\n",
      "Step #6457\tEpoch   2 Batch  206/3125   Loss: 0.862976 mae: 0.730660 (2170.3357204950944 steps/sec)\n",
      "Step #6458\tEpoch   2 Batch  207/3125   Loss: 0.863487 mae: 0.716491 (2124.946297572245 steps/sec)\n",
      "Step #6459\tEpoch   2 Batch  208/3125   Loss: 0.839991 mae: 0.710614 (1749.4781977592952 steps/sec)\n",
      "Step #6460\tEpoch   2 Batch  209/3125   Loss: 0.728971 mae: 0.689714 (1951.728694940019 steps/sec)\n",
      "Step #6461\tEpoch   2 Batch  210/3125   Loss: 0.780924 mae: 0.692964 (2125.053958474774 steps/sec)\n",
      "Step #6462\tEpoch   2 Batch  211/3125   Loss: 0.808643 mae: 0.700451 (1941.3041063428 steps/sec)\n",
      "Step #6463\tEpoch   2 Batch  212/3125   Loss: 0.879135 mae: 0.744454 (2008.7470426528482 steps/sec)\n",
      "Step #6464\tEpoch   2 Batch  213/3125   Loss: 0.709970 mae: 0.668808 (2201.018041372362 steps/sec)\n",
      "Step #6465\tEpoch   2 Batch  214/3125   Loss: 0.895785 mae: 0.738711 (2151.3664341403364 steps/sec)\n",
      "Step #6466\tEpoch   2 Batch  215/3125   Loss: 0.853734 mae: 0.732140 (2104.4755750010036 steps/sec)\n",
      "Step #6467\tEpoch   2 Batch  216/3125   Loss: 0.875497 mae: 0.748754 (2050.6634593759472 steps/sec)\n",
      "Step #6468\tEpoch   2 Batch  217/3125   Loss: 0.766734 mae: 0.686409 (1773.2951134336183 steps/sec)\n",
      "Step #6469\tEpoch   2 Batch  218/3125   Loss: 0.832097 mae: 0.736533 (1978.79997358017 steps/sec)\n",
      "Step #6470\tEpoch   2 Batch  219/3125   Loss: 1.000416 mae: 0.783964 (1967.0881326679923 steps/sec)\n",
      "Step #6471\tEpoch   2 Batch  220/3125   Loss: 0.813500 mae: 0.701561 (2036.9004836923793 steps/sec)\n",
      "Step #6472\tEpoch   2 Batch  221/3125   Loss: 0.731260 mae: 0.655309 (1917.7467879840885 steps/sec)\n",
      "Step #6473\tEpoch   2 Batch  222/3125   Loss: 0.876990 mae: 0.756675 (2010.7693487765591 steps/sec)\n",
      "Step #6474\tEpoch   2 Batch  223/3125   Loss: 0.886383 mae: 0.734607 (1941.5197748481708 steps/sec)\n",
      "Step #6475\tEpoch   2 Batch  224/3125   Loss: 0.599540 mae: 0.619764 (2103.3358072733836 steps/sec)\n",
      "Step #6476\tEpoch   2 Batch  225/3125   Loss: 0.733306 mae: 0.675547 (1995.6910661946633 steps/sec)\n",
      "Step #6477\tEpoch   2 Batch  226/3125   Loss: 0.762580 mae: 0.686972 (1655.968793923027 steps/sec)\n",
      "Step #6478\tEpoch   2 Batch  227/3125   Loss: 0.842859 mae: 0.748820 (2022.6770316930615 steps/sec)\n",
      "Step #6479\tEpoch   2 Batch  228/3125   Loss: 0.931100 mae: 0.764517 (1914.6827353236556 steps/sec)\n",
      "Step #6480\tEpoch   2 Batch  229/3125   Loss: 0.797890 mae: 0.686661 (2042.7335774955193 steps/sec)\n",
      "Step #6481\tEpoch   2 Batch  230/3125   Loss: 0.871988 mae: 0.751128 (2044.087488790986 steps/sec)\n",
      "Step #6482\tEpoch   2 Batch  231/3125   Loss: 0.749694 mae: 0.665177 (2023.477195318455 steps/sec)\n",
      "Step #6483\tEpoch   2 Batch  232/3125   Loss: 0.777341 mae: 0.714880 (2249.945820682552 steps/sec)\n",
      "Step #6484\tEpoch   2 Batch  233/3125   Loss: 0.718015 mae: 0.686008 (1908.4623294839243 steps/sec)\n",
      "Step #6485\tEpoch   2 Batch  234/3125   Loss: 0.769608 mae: 0.694696 (1762.994098559107 steps/sec)\n",
      "Step #6486\tEpoch   2 Batch  235/3125   Loss: 0.794566 mae: 0.715685 (1968.9904139556281 steps/sec)\n",
      "Step #6487\tEpoch   2 Batch  236/3125   Loss: 0.769057 mae: 0.706046 (2085.9296982235574 steps/sec)\n",
      "Step #6488\tEpoch   2 Batch  237/3125   Loss: 0.877809 mae: 0.728653 (1881.461278977966 steps/sec)\n",
      "Step #6489\tEpoch   2 Batch  238/3125   Loss: 0.685455 mae: 0.643322 (2035.3387618040122 steps/sec)\n",
      "Step #6490\tEpoch   2 Batch  239/3125   Loss: 0.781188 mae: 0.714277 (1971.3039554820273 steps/sec)\n",
      "Step #6491\tEpoch   2 Batch  240/3125   Loss: 0.896582 mae: 0.736432 (2106.4202490960224 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6492\tEpoch   2 Batch  241/3125   Loss: 0.739472 mae: 0.688860 (1930.794726375488 steps/sec)\n",
      "Step #6493\tEpoch   2 Batch  242/3125   Loss: 0.786504 mae: 0.692057 (1856.116687023171 steps/sec)\n",
      "Step #6494\tEpoch   2 Batch  243/3125   Loss: 0.838933 mae: 0.730962 (2227.1744440432444 steps/sec)\n",
      "Step #6495\tEpoch   2 Batch  244/3125   Loss: 0.791385 mae: 0.698951 (1997.1164376386787 steps/sec)\n",
      "Step #6496\tEpoch   2 Batch  245/3125   Loss: 0.961836 mae: 0.785909 (2102.6398901131956 steps/sec)\n",
      "Step #6497\tEpoch   2 Batch  246/3125   Loss: 0.967858 mae: 0.789420 (1958.6188861804562 steps/sec)\n",
      "Step #6498\tEpoch   2 Batch  247/3125   Loss: 0.814840 mae: 0.720869 (2042.5744117188717 steps/sec)\n",
      "Step #6499\tEpoch   2 Batch  248/3125   Loss: 0.748782 mae: 0.688750 (2111.85047933618 steps/sec)\n",
      "Step #6500\tEpoch   2 Batch  249/3125   Loss: 0.818350 mae: 0.693931 (2143.3629042148727 steps/sec)\n",
      "Step #6501\tEpoch   2 Batch  250/3125   Loss: 0.786600 mae: 0.700053 (2001.3474953954212 steps/sec)\n",
      "Step #6502\tEpoch   2 Batch  251/3125   Loss: 0.898710 mae: 0.749888 (1881.461278977966 steps/sec)\n",
      "Step #6503\tEpoch   2 Batch  252/3125   Loss: 0.830875 mae: 0.713238 (2098.012184995848 steps/sec)\n",
      "Step #6504\tEpoch   2 Batch  253/3125   Loss: 0.916166 mae: 0.729516 (2147.8630465285387 steps/sec)\n",
      "Step #6505\tEpoch   2 Batch  254/3125   Loss: 0.797540 mae: 0.729604 (1901.9889172055396 steps/sec)\n",
      "Step #6506\tEpoch   2 Batch  255/3125   Loss: 0.971257 mae: 0.797886 (2060.758996128373 steps/sec)\n",
      "Step #6507\tEpoch   2 Batch  256/3125   Loss: 0.818671 mae: 0.714594 (1949.3340025840514 steps/sec)\n",
      "Step #6508\tEpoch   2 Batch  257/3125   Loss: 0.857423 mae: 0.737626 (1876.5285395993092 steps/sec)\n",
      "Step #6509\tEpoch   2 Batch  258/3125   Loss: 0.963630 mae: 0.786196 (1786.5283208532462 steps/sec)\n",
      "Step #6510\tEpoch   2 Batch  259/3125   Loss: 0.766437 mae: 0.682042 (1917.6240375998975 steps/sec)\n",
      "Step #6511\tEpoch   2 Batch  260/3125   Loss: 0.928597 mae: 0.753845 (1809.511976254573 steps/sec)\n",
      "Step #6512\tEpoch   2 Batch  261/3125   Loss: 1.012002 mae: 0.783845 (1799.7751516867916 steps/sec)\n",
      "Step #6513\tEpoch   2 Batch  262/3125   Loss: 0.726476 mae: 0.664291 (1963.625468164794 steps/sec)\n",
      "Step #6514\tEpoch   2 Batch  263/3125   Loss: 0.720250 mae: 0.678648 (1936.3211641090984 steps/sec)\n",
      "Step #6515\tEpoch   2 Batch  264/3125   Loss: 0.853999 mae: 0.747896 (1811.0746485198108 steps/sec)\n",
      "Step #6516\tEpoch   2 Batch  265/3125   Loss: 0.775585 mae: 0.692054 (1601.9432753049734 steps/sec)\n",
      "Step #6517\tEpoch   2 Batch  266/3125   Loss: 0.741348 mae: 0.686118 (1988.3118114416823 steps/sec)\n",
      "Step #6518\tEpoch   2 Batch  267/3125   Loss: 0.860272 mae: 0.739442 (2151.543007222587 steps/sec)\n",
      "Step #6519\tEpoch   2 Batch  268/3125   Loss: 0.912883 mae: 0.732796 (2069.013417521705 steps/sec)\n",
      "Step #6520\tEpoch   2 Batch  269/3125   Loss: 0.858489 mae: 0.736973 (2071.7727834033094 steps/sec)\n",
      "Step #6521\tEpoch   2 Batch  270/3125   Loss: 0.676309 mae: 0.654284 (2162.7481514329615 steps/sec)\n",
      "Step #6522\tEpoch   2 Batch  271/3125   Loss: 0.827015 mae: 0.717153 (1930.3504201912722 steps/sec)\n",
      "Step #6523\tEpoch   2 Batch  272/3125   Loss: 0.857528 mae: 0.744780 (2116.8385989704248 steps/sec)\n",
      "Step #6524\tEpoch   2 Batch  273/3125   Loss: 0.693037 mae: 0.646802 (1988.538051620489 steps/sec)\n",
      "Step #6525\tEpoch   2 Batch  274/3125   Loss: 0.832746 mae: 0.731149 (1589.3776336132416 steps/sec)\n",
      "Step #6526\tEpoch   2 Batch  275/3125   Loss: 0.732275 mae: 0.688584 (2011.7530816825747 steps/sec)\n",
      "Step #6527\tEpoch   2 Batch  276/3125   Loss: 0.819155 mae: 0.701842 (2152.0287326834273 steps/sec)\n",
      "Step #6528\tEpoch   2 Batch  277/3125   Loss: 0.782462 mae: 0.696879 (2008.7470426528482 steps/sec)\n",
      "Step #6529\tEpoch   2 Batch  278/3125   Loss: 0.753817 mae: 0.682360 (1890.2797807903087 steps/sec)\n",
      "Step #6530\tEpoch   2 Batch  279/3125   Loss: 0.809918 mae: 0.696512 (2149.337924814496 steps/sec)\n",
      "Step #6531\tEpoch   2 Batch  280/3125   Loss: 0.893809 mae: 0.745327 (1993.6989609179668 steps/sec)\n",
      "Step #6532\tEpoch   2 Batch  281/3125   Loss: 0.839838 mae: 0.717266 (1860.3317661669475 steps/sec)\n",
      "Step #6533\tEpoch   2 Batch  282/3125   Loss: 0.926351 mae: 0.759756 (1867.6878684787062 steps/sec)\n",
      "Step #6534\tEpoch   2 Batch  283/3125   Loss: 0.767437 mae: 0.699155 (1939.0245481022607 steps/sec)\n",
      "Step #6535\tEpoch   2 Batch  284/3125   Loss: 0.877314 mae: 0.733556 (2064.7559786942866 steps/sec)\n",
      "Step #6536\tEpoch   2 Batch  285/3125   Loss: 0.838020 mae: 0.715706 (2097.928233446375 steps/sec)\n",
      "Step #6537\tEpoch   2 Batch  286/3125   Loss: 0.867887 mae: 0.709599 (2049.601250977326 steps/sec)\n",
      "Step #6538\tEpoch   2 Batch  287/3125   Loss: 0.774444 mae: 0.708068 (2034.2529003220425 steps/sec)\n",
      "Step #6539\tEpoch   2 Batch  288/3125   Loss: 0.904429 mae: 0.745495 (1948.1750536475704 steps/sec)\n",
      "Step #6540\tEpoch   2 Batch  289/3125   Loss: 0.763865 mae: 0.685970 (2124.10691677386 steps/sec)\n",
      "Step #6541\tEpoch   2 Batch  290/3125   Loss: 0.949582 mae: 0.797716 (1953.6559690716847 steps/sec)\n",
      "Step #6542\tEpoch   2 Batch  291/3125   Loss: 0.753305 mae: 0.682587 (1818.455508731769 steps/sec)\n",
      "Step #6543\tEpoch   2 Batch  292/3125   Loss: 0.909558 mae: 0.746304 (1903.6636287716494 steps/sec)\n",
      "Step #6544\tEpoch   2 Batch  293/3125   Loss: 0.972034 mae: 0.772957 (1954.0382393500056 steps/sec)\n",
      "Step #6545\tEpoch   2 Batch  294/3125   Loss: 0.790779 mae: 0.703550 (1812.890733056708 steps/sec)\n",
      "Step #6546\tEpoch   2 Batch  295/3125   Loss: 0.745401 mae: 0.665547 (2071.8751234933807 steps/sec)\n",
      "Step #6547\tEpoch   2 Batch  296/3125   Loss: 0.899419 mae: 0.742172 (2087.2583952067203 steps/sec)\n",
      "Step #6548\tEpoch   2 Batch  297/3125   Loss: 0.801686 mae: 0.712621 (1791.7331647386497 steps/sec)\n",
      "Step #6549\tEpoch   2 Batch  298/3125   Loss: 0.827185 mae: 0.708520 (2100.197286064794 steps/sec)\n",
      "Step #6550\tEpoch   2 Batch  299/3125   Loss: 0.797770 mae: 0.719707 (1840.7534517111535 steps/sec)\n",
      "Step #6551\tEpoch   2 Batch  300/3125   Loss: 1.002107 mae: 0.801768 (1894.3606883157943 steps/sec)\n",
      "Step #6552\tEpoch   2 Batch  301/3125   Loss: 0.840655 mae: 0.730670 (2042.3953798657978 steps/sec)\n",
      "Step #6553\tEpoch   2 Batch  302/3125   Loss: 0.767096 mae: 0.708479 (1923.0046948356808 steps/sec)\n",
      "Step #6554\tEpoch   2 Batch  303/3125   Loss: 0.816023 mae: 0.725614 (2038.2268614358884 steps/sec)\n",
      "Step #6555\tEpoch   2 Batch  304/3125   Loss: 0.882449 mae: 0.748519 (2193.168936018908 steps/sec)\n",
      "Step #6556\tEpoch   2 Batch  305/3125   Loss: 0.738795 mae: 0.666807 (2170.785028155018 steps/sec)\n",
      "Step #6557\tEpoch   2 Batch  306/3125   Loss: 0.811670 mae: 0.711037 (2110.5115380357665 steps/sec)\n",
      "Step #6558\tEpoch   2 Batch  307/3125   Loss: 0.777130 mae: 0.705423 (1887.7954811414168 steps/sec)\n",
      "Step #6559\tEpoch   2 Batch  308/3125   Loss: 0.719710 mae: 0.684411 (1939.0962635574335 steps/sec)\n",
      "Step #6560\tEpoch   2 Batch  309/3125   Loss: 0.741189 mae: 0.687667 (2122.9672821509557 steps/sec)\n",
      "Step #6561\tEpoch   2 Batch  310/3125   Loss: 0.904291 mae: 0.749137 (2194.7296816459802 steps/sec)\n",
      "Step #6562\tEpoch   2 Batch  311/3125   Loss: 0.908919 mae: 0.741446 (1884.301322599195 steps/sec)\n",
      "Step #6563\tEpoch   2 Batch  312/3125   Loss: 0.773013 mae: 0.700847 (1762.4310878042222 steps/sec)\n",
      "Step #6564\tEpoch   2 Batch  313/3125   Loss: 0.842330 mae: 0.705838 (1591.4883929181244 steps/sec)\n",
      "Step #6565\tEpoch   2 Batch  314/3125   Loss: 0.953993 mae: 0.779275 (1897.1025374281967 steps/sec)\n",
      "Step #6566\tEpoch   2 Batch  315/3125   Loss: 0.800990 mae: 0.701914 (1792.6367887030183 steps/sec)\n",
      "Step #6567\tEpoch   2 Batch  316/3125   Loss: 0.753945 mae: 0.702129 (1893.0781729554071 steps/sec)\n",
      "Step #6568\tEpoch   2 Batch  317/3125   Loss: 0.950683 mae: 0.763923 (1853.213506181349 steps/sec)\n",
      "Step #6569\tEpoch   2 Batch  318/3125   Loss: 0.795087 mae: 0.703737 (2024.7079495645794 steps/sec)\n",
      "Step #6570\tEpoch   2 Batch  319/3125   Loss: 0.911048 mae: 0.761561 (2341.564502802528 steps/sec)\n",
      "Step #6571\tEpoch   2 Batch  320/3125   Loss: 0.958429 mae: 0.772445 (2074.703705902139 steps/sec)\n",
      "Step #6572\tEpoch   2 Batch  321/3125   Loss: 0.865376 mae: 0.729676 (2201.6650394213307 steps/sec)\n",
      "Step #6573\tEpoch   2 Batch  322/3125   Loss: 0.682724 mae: 0.655601 (1819.2915947359745 steps/sec)\n",
      "Step #6574\tEpoch   2 Batch  323/3125   Loss: 0.793623 mae: 0.685785 (1932.5205724343202 steps/sec)\n",
      "Step #6575\tEpoch   2 Batch  324/3125   Loss: 0.796315 mae: 0.712902 (1939.042476468739 steps/sec)\n",
      "Step #6576\tEpoch   2 Batch  325/3125   Loss: 0.818202 mae: 0.718618 (2205.5782255689705 steps/sec)\n",
      "Step #6577\tEpoch   2 Batch  326/3125   Loss: 0.732897 mae: 0.647550 (2129.563963524848 steps/sec)\n",
      "Step #6578\tEpoch   2 Batch  327/3125   Loss: 0.960684 mae: 0.793875 (2216.6985529611975 steps/sec)\n",
      "Step #6579\tEpoch   2 Batch  328/3125   Loss: 0.807617 mae: 0.707778 (2204.117838712729 steps/sec)\n",
      "Step #6580\tEpoch   2 Batch  329/3125   Loss: 0.930678 mae: 0.768459 (2089.44195917066 steps/sec)\n",
      "Step #6581\tEpoch   2 Batch  330/3125   Loss: 0.886824 mae: 0.748992 (1851.2340665936938 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6582\tEpoch   2 Batch  331/3125   Loss: 0.811505 mae: 0.709467 (1844.671774257391 steps/sec)\n",
      "Step #6583\tEpoch   2 Batch  332/3125   Loss: 0.840336 mae: 0.719969 (1909.6962191301814 steps/sec)\n",
      "Step #6584\tEpoch   2 Batch  333/3125   Loss: 0.877438 mae: 0.754470 (2058.6349402675933 steps/sec)\n",
      "Step #6585\tEpoch   2 Batch  334/3125   Loss: 0.840113 mae: 0.713952 (2275.8271929158213 steps/sec)\n",
      "Step #6586\tEpoch   2 Batch  335/3125   Loss: 0.743489 mae: 0.680712 (2068.421623647536 steps/sec)\n",
      "Step #6587\tEpoch   2 Batch  336/3125   Loss: 0.689066 mae: 0.666302 (2115.578690393326 steps/sec)\n",
      "Step #6588\tEpoch   2 Batch  337/3125   Loss: 0.718430 mae: 0.657740 (2282.9125981080524 steps/sec)\n",
      "Step #6589\tEpoch   2 Batch  338/3125   Loss: 0.904168 mae: 0.764317 (2023.145343340601 steps/sec)\n",
      "Step #6590\tEpoch   2 Batch  339/3125   Loss: 0.796591 mae: 0.682739 (2076.347003029643 steps/sec)\n",
      "Step #6591\tEpoch   2 Batch  340/3125   Loss: 0.665205 mae: 0.652572 (1863.5222193589664 steps/sec)\n",
      "Step #6592\tEpoch   2 Batch  341/3125   Loss: 0.828830 mae: 0.683301 (1869.136088556939 steps/sec)\n",
      "Step #6593\tEpoch   2 Batch  342/3125   Loss: 0.905436 mae: 0.752022 (2062.2991444586487 steps/sec)\n",
      "Step #6594\tEpoch   2 Batch  343/3125   Loss: 0.899521 mae: 0.746576 (2023.9458776069564 steps/sec)\n",
      "Step #6595\tEpoch   2 Batch  344/3125   Loss: 0.939529 mae: 0.751454 (2059.9897842913833 steps/sec)\n",
      "Step #6596\tEpoch   2 Batch  345/3125   Loss: 0.798011 mae: 0.724198 (1923.533835965733 steps/sec)\n",
      "Step #6597\tEpoch   2 Batch  346/3125   Loss: 0.901781 mae: 0.738325 (1902.7482148851811 steps/sec)\n",
      "Step #6598\tEpoch   2 Batch  347/3125   Loss: 0.744483 mae: 0.685827 (2111.9355488418933 steps/sec)\n",
      "Step #6599\tEpoch   2 Batch  348/3125   Loss: 0.937950 mae: 0.767573 (1990.5198515523412 steps/sec)\n",
      "Step #6600\tEpoch   2 Batch  349/3125   Loss: 0.908978 mae: 0.770916 (1700.853203568532 steps/sec)\n",
      "Step #6601\tEpoch   2 Batch  350/3125   Loss: 0.785211 mae: 0.677529 (1872.239829305527 steps/sec)\n",
      "Step #6602\tEpoch   2 Batch  351/3125   Loss: 1.026418 mae: 0.805365 (1834.5378996632112 steps/sec)\n",
      "Step #6603\tEpoch   2 Batch  352/3125   Loss: 0.741466 mae: 0.701403 (1986.6542884750195 steps/sec)\n",
      "Step #6604\tEpoch   2 Batch  353/3125   Loss: 0.869897 mae: 0.753329 (1793.7867798001917 steps/sec)\n",
      "Step #6605\tEpoch   2 Batch  354/3125   Loss: 0.789998 mae: 0.716295 (1705.1402553053094 steps/sec)\n",
      "Step #6606\tEpoch   2 Batch  355/3125   Loss: 0.781172 mae: 0.721367 (1984.4360333081 steps/sec)\n",
      "Step #6607\tEpoch   2 Batch  356/3125   Loss: 0.845846 mae: 0.716486 (1683.9995503236064 steps/sec)\n",
      "Step #6608\tEpoch   2 Batch  357/3125   Loss: 0.862519 mae: 0.720248 (1939.5446053678115 steps/sec)\n",
      "Step #6609\tEpoch   2 Batch  358/3125   Loss: 0.897269 mae: 0.749102 (1967.3649352233176 steps/sec)\n",
      "Step #6610\tEpoch   2 Batch  359/3125   Loss: 0.756854 mae: 0.683784 (1870.4864517740239 steps/sec)\n",
      "Step #6611\tEpoch   2 Batch  360/3125   Loss: 0.780489 mae: 0.688243 (2070.2184578631995 steps/sec)\n",
      "Step #6612\tEpoch   2 Batch  361/3125   Loss: 0.844347 mae: 0.739301 (2179.493255180728 steps/sec)\n",
      "Step #6613\tEpoch   2 Batch  362/3125   Loss: 0.726476 mae: 0.673439 (2090.0458441299584 steps/sec)\n",
      "Step #6614\tEpoch   2 Batch  363/3125   Loss: 0.871166 mae: 0.737503 (1888.4244459852503 steps/sec)\n",
      "Step #6615\tEpoch   2 Batch  364/3125   Loss: 0.711728 mae: 0.686482 (1925.7594123048668 steps/sec)\n",
      "Step #6616\tEpoch   2 Batch  365/3125   Loss: 0.794336 mae: 0.723538 (1931.9864761536264 steps/sec)\n",
      "Step #6617\tEpoch   2 Batch  366/3125   Loss: 0.910863 mae: 0.750045 (2068.3400234730207 steps/sec)\n",
      "Step #6618\tEpoch   2 Batch  367/3125   Loss: 0.863320 mae: 0.727493 (2060.819747845484 steps/sec)\n",
      "Step #6619\tEpoch   2 Batch  368/3125   Loss: 0.859580 mae: 0.727228 (2132.639114871461 steps/sec)\n",
      "Step #6620\tEpoch   2 Batch  369/3125   Loss: 0.891189 mae: 0.744546 (2041.1629016088687 steps/sec)\n",
      "Step #6621\tEpoch   2 Batch  370/3125   Loss: 0.967579 mae: 0.801070 (2000.7174203396298 steps/sec)\n",
      "Step #6622\tEpoch   2 Batch  371/3125   Loss: 0.779102 mae: 0.712746 (2091.9639294549515 steps/sec)\n",
      "Step #6623\tEpoch   2 Batch  372/3125   Loss: 0.780326 mae: 0.719067 (2082.7394430540658 steps/sec)\n",
      "Step #6624\tEpoch   2 Batch  373/3125   Loss: 0.864398 mae: 0.737501 (1782.9741287695224 steps/sec)\n",
      "Step #6625\tEpoch   2 Batch  374/3125   Loss: 0.797814 mae: 0.707088 (1948.4646616681068 steps/sec)\n",
      "Step #6626\tEpoch   2 Batch  375/3125   Loss: 0.819193 mae: 0.738313 (2098.201100550275 steps/sec)\n",
      "Step #6627\tEpoch   2 Batch  376/3125   Loss: 0.791898 mae: 0.699155 (1941.1423864046576 steps/sec)\n",
      "Step #6628\tEpoch   2 Batch  377/3125   Loss: 0.781984 mae: 0.712683 (2213.960559098011 steps/sec)\n",
      "Step #6629\tEpoch   2 Batch  378/3125   Loss: 0.876906 mae: 0.752805 (2212.675803711793 steps/sec)\n",
      "Step #6630\tEpoch   2 Batch  379/3125   Loss: 0.798530 mae: 0.706985 (1841.7557325651858 steps/sec)\n",
      "Step #6631\tEpoch   2 Batch  380/3125   Loss: 0.795989 mae: 0.711868 (2019.3660208758618 steps/sec)\n",
      "Step #6632\tEpoch   2 Batch  381/3125   Loss: 0.781480 mae: 0.711384 (1732.324467206344 steps/sec)\n",
      "Step #6633\tEpoch   2 Batch  382/3125   Loss: 0.771885 mae: 0.700151 (2253.7662142266067 steps/sec)\n",
      "Step #6634\tEpoch   2 Batch  383/3125   Loss: 0.852540 mae: 0.714107 (2142.093113521685 steps/sec)\n",
      "Step #6635\tEpoch   2 Batch  384/3125   Loss: 0.806231 mae: 0.718924 (2234.031084550403 steps/sec)\n",
      "Step #6636\tEpoch   2 Batch  385/3125   Loss: 0.806392 mae: 0.722690 (2326.03371783496 steps/sec)\n",
      "Step #6637\tEpoch   2 Batch  386/3125   Loss: 0.737146 mae: 0.695606 (2072.284584980237 steps/sec)\n",
      "Step #6638\tEpoch   2 Batch  387/3125   Loss: 0.818774 mae: 0.732900 (2111.1064133925247 steps/sec)\n",
      "Step #6639\tEpoch   2 Batch  388/3125   Loss: 0.834950 mae: 0.726463 (2116.753134021035 steps/sec)\n",
      "Step #6640\tEpoch   2 Batch  389/3125   Loss: 0.657855 mae: 0.653613 (1718.5544538228305 steps/sec)\n",
      "Step #6641\tEpoch   2 Batch  390/3125   Loss: 0.915299 mae: 0.755430 (1754.1462435384847 steps/sec)\n",
      "Step #6642\tEpoch   2 Batch  391/3125   Loss: 0.869798 mae: 0.714738 (2201.3646001721495 steps/sec)\n",
      "Step #6643\tEpoch   2 Batch  392/3125   Loss: 0.691900 mae: 0.660842 (2035.7339079957676 steps/sec)\n",
      "Step #6644\tEpoch   2 Batch  393/3125   Loss: 0.895118 mae: 0.735328 (2130.2778201025953 steps/sec)\n",
      "Step #6645\tEpoch   2 Batch  394/3125   Loss: 0.834387 mae: 0.728192 (1851.5773023847153 steps/sec)\n",
      "Step #6646\tEpoch   2 Batch  395/3125   Loss: 0.829602 mae: 0.715018 (2135.9406827996413 steps/sec)\n",
      "Step #6647\tEpoch   2 Batch  396/3125   Loss: 0.808184 mae: 0.696662 (2234.3167023577416 steps/sec)\n",
      "Step #6648\tEpoch   2 Batch  397/3125   Loss: 0.782245 mae: 0.699874 (1807.0795850136146 steps/sec)\n",
      "Step #6649\tEpoch   2 Batch  398/3125   Loss: 0.872645 mae: 0.756449 (1956.7730979528617 steps/sec)\n",
      "Step #6650\tEpoch   2 Batch  399/3125   Loss: 0.911431 mae: 0.753968 (2220.336255452505 steps/sec)\n",
      "Step #6651\tEpoch   2 Batch  400/3125   Loss: 0.833922 mae: 0.717934 (2002.819214974692 steps/sec)\n",
      "Step #6652\tEpoch   2 Batch  401/3125   Loss: 0.914618 mae: 0.773019 (2116.4113432233326 steps/sec)\n",
      "Step #6653\tEpoch   2 Batch  402/3125   Loss: 0.861078 mae: 0.724220 (2228.0499335989375 steps/sec)\n",
      "Step #6654\tEpoch   2 Batch  403/3125   Loss: 0.958405 mae: 0.785738 (1939.2935084150176 steps/sec)\n",
      "Step #6655\tEpoch   2 Batch  404/3125   Loss: 0.789467 mae: 0.695644 (2121.700069807675 steps/sec)\n",
      "Step #6656\tEpoch   2 Batch  405/3125   Loss: 0.805438 mae: 0.696296 (2132.378899418392 steps/sec)\n",
      "Step #6657\tEpoch   2 Batch  406/3125   Loss: 0.976536 mae: 0.778023 (1711.6953288877644 steps/sec)\n",
      "Step #6658\tEpoch   2 Batch  407/3125   Loss: 0.771936 mae: 0.689134 (1869.0694544709143 steps/sec)\n",
      "Step #6659\tEpoch   2 Batch  408/3125   Loss: 0.821462 mae: 0.706233 (2104.919151669661 steps/sec)\n",
      "Step #6660\tEpoch   2 Batch  409/3125   Loss: 0.759296 mae: 0.678752 (2162.257575601357 steps/sec)\n",
      "Step #6661\tEpoch   2 Batch  410/3125   Loss: 0.822998 mae: 0.733866 (2017.3263945669844 steps/sec)\n",
      "Step #6662\tEpoch   2 Batch  411/3125   Loss: 0.897966 mae: 0.776555 (2103.7577994904 steps/sec)\n",
      "Step #6663\tEpoch   2 Batch  412/3125   Loss: 0.669628 mae: 0.653421 (2132.010369542012 steps/sec)\n",
      "Step #6664\tEpoch   2 Batch  413/3125   Loss: 0.771945 mae: 0.695145 (2139.0342914261237 steps/sec)\n",
      "Step #6665\tEpoch   2 Batch  414/3125   Loss: 0.776381 mae: 0.682757 (1979.565791957712 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6666\tEpoch   2 Batch  415/3125   Loss: 0.957111 mae: 0.763154 (1894.95979036776 steps/sec)\n",
      "Step #6667\tEpoch   2 Batch  416/3125   Loss: 0.871945 mae: 0.716283 (1953.055560729386 steps/sec)\n",
      "Step #6668\tEpoch   2 Batch  417/3125   Loss: 0.945530 mae: 0.757635 (2325.6983797810876 steps/sec)\n",
      "Step #6669\tEpoch   2 Batch  418/3125   Loss: 0.814721 mae: 0.710467 (2153.1334702258728 steps/sec)\n",
      "Step #6670\tEpoch   2 Batch  419/3125   Loss: 0.872558 mae: 0.747858 (2083.6706508887496 steps/sec)\n",
      "Step #6671\tEpoch   2 Batch  420/3125   Loss: 0.745380 mae: 0.688097 (2170.066225165563 steps/sec)\n",
      "Step #6672\tEpoch   2 Batch  421/3125   Loss: 0.822266 mae: 0.724784 (1981.6233582160069 steps/sec)\n",
      "Step #6673\tEpoch   2 Batch  422/3125   Loss: 0.924194 mae: 0.776399 (2124.1284310746482 steps/sec)\n",
      "Step #6674\tEpoch   2 Batch  423/3125   Loss: 0.863370 mae: 0.739099 (2078.6519972247 steps/sec)\n",
      "Step #6675\tEpoch   2 Batch  424/3125   Loss: 0.891409 mae: 0.771715 (1654.101037188942 steps/sec)\n",
      "Step #6676\tEpoch   2 Batch  425/3125   Loss: 0.860786 mae: 0.742807 (2122.88132161801 steps/sec)\n",
      "Step #6677\tEpoch   2 Batch  426/3125   Loss: 0.745409 mae: 0.682683 (2039.892225237581 steps/sec)\n",
      "Step #6678\tEpoch   2 Batch  427/3125   Loss: 0.725311 mae: 0.696960 (2077.787024927674 steps/sec)\n",
      "Step #6679\tEpoch   2 Batch  428/3125   Loss: 0.755215 mae: 0.690965 (2231.5112950765597 steps/sec)\n",
      "Step #6680\tEpoch   2 Batch  429/3125   Loss: 0.766089 mae: 0.708567 (2133.3767369941606 steps/sec)\n",
      "Step #6681\tEpoch   2 Batch  430/3125   Loss: 0.943963 mae: 0.774655 (2219.3728635983616 steps/sec)\n",
      "Step #6682\tEpoch   2 Batch  431/3125   Loss: 0.778841 mae: 0.713554 (2041.9976436451445 steps/sec)\n",
      "Step #6683\tEpoch   2 Batch  432/3125   Loss: 0.868534 mae: 0.727514 (2153.8853398516935 steps/sec)\n",
      "Step #6684\tEpoch   2 Batch  433/3125   Loss: 0.861256 mae: 0.737950 (1890.2968190872791 steps/sec)\n",
      "Step #6685\tEpoch   2 Batch  434/3125   Loss: 0.838559 mae: 0.743241 (1691.770058566335 steps/sec)\n",
      "Step #6686\tEpoch   2 Batch  435/3125   Loss: 0.856911 mae: 0.719797 (2042.4749457035168 steps/sec)\n",
      "Step #6687\tEpoch   2 Batch  436/3125   Loss: 0.773765 mae: 0.702339 (1952.891877042845 steps/sec)\n",
      "Step #6688\tEpoch   2 Batch  437/3125   Loss: 0.817567 mae: 0.726715 (2026.5862662105487 steps/sec)\n",
      "Step #6689\tEpoch   2 Batch  438/3125   Loss: 0.909834 mae: 0.773537 (2115.8561685298037 steps/sec)\n",
      "Step #6690\tEpoch   2 Batch  439/3125   Loss: 0.949617 mae: 0.736832 (2049.120612835144 steps/sec)\n",
      "Step #6691\tEpoch   2 Batch  440/3125   Loss: 0.773981 mae: 0.698354 (1890.9956538204901 steps/sec)\n",
      "Step #6692\tEpoch   2 Batch  441/3125   Loss: 0.798031 mae: 0.706816 (1786.8632045328675 steps/sec)\n",
      "Step #6693\tEpoch   2 Batch  442/3125   Loss: 0.934868 mae: 0.759191 (1831.2379388933034 steps/sec)\n",
      "Step #6694\tEpoch   2 Batch  443/3125   Loss: 0.754670 mae: 0.706721 (1971.470740305523 steps/sec)\n",
      "Step #6695\tEpoch   2 Batch  444/3125   Loss: 0.837834 mae: 0.714659 (2098.5370344427324 steps/sec)\n",
      "Step #6696\tEpoch   2 Batch  445/3125   Loss: 0.911334 mae: 0.757169 (2160.965305470545 steps/sec)\n",
      "Step #6697\tEpoch   2 Batch  446/3125   Loss: 0.877547 mae: 0.732914 (2066.0171219718836 steps/sec)\n",
      "Step #6698\tEpoch   2 Batch  447/3125   Loss: 0.799995 mae: 0.689559 (2305.119919101321 steps/sec)\n",
      "Step #6699\tEpoch   2 Batch  448/3125   Loss: 0.805689 mae: 0.704540 (2081.602429849028 steps/sec)\n",
      "Step #6700\tEpoch   2 Batch  449/3125   Loss: 0.805017 mae: 0.718831 (2138.663457714233 steps/sec)\n",
      "Step #6701\tEpoch   2 Batch  450/3125   Loss: 0.773127 mae: 0.684279 (1746.9715523345412 steps/sec)\n",
      "Step #6702\tEpoch   2 Batch  451/3125   Loss: 0.752630 mae: 0.690025 (1855.8538786924125 steps/sec)\n",
      "Step #6703\tEpoch   2 Batch  452/3125   Loss: 0.812078 mae: 0.724502 (2233.0319970185806 steps/sec)\n",
      "Step #6704\tEpoch   2 Batch  453/3125   Loss: 0.755807 mae: 0.694303 (2103.5678820402227 steps/sec)\n",
      "Step #6705\tEpoch   2 Batch  454/3125   Loss: 0.855397 mae: 0.735043 (2020.2219482120838 steps/sec)\n",
      "Step #6706\tEpoch   2 Batch  455/3125   Loss: 0.741589 mae: 0.675820 (2215.3404109227276 steps/sec)\n",
      "Step #6707\tEpoch   2 Batch  456/3125   Loss: 0.901656 mae: 0.762948 (2104.919151669661 steps/sec)\n",
      "Step #6708\tEpoch   2 Batch  457/3125   Loss: 0.902268 mae: 0.778314 (2174.5665698880134 steps/sec)\n",
      "Step #6709\tEpoch   2 Batch  458/3125   Loss: 0.897244 mae: 0.742686 (2155.3685032734147 steps/sec)\n",
      "Step #6710\tEpoch   2 Batch  459/3125   Loss: 0.855581 mae: 0.729749 (1891.729133403693 steps/sec)\n",
      "Step #6711\tEpoch   2 Batch  460/3125   Loss: 0.809106 mae: 0.714902 (2044.0675653284209 steps/sec)\n",
      "Step #6712\tEpoch   2 Batch  461/3125   Loss: 0.778031 mae: 0.703158 (2194.821559392988 steps/sec)\n",
      "Step #6713\tEpoch   2 Batch  462/3125   Loss: 0.761127 mae: 0.713001 (2049.260775672533 steps/sec)\n",
      "Step #6714\tEpoch   2 Batch  463/3125   Loss: 0.755304 mae: 0.664506 (2101.4389354282735 steps/sec)\n",
      "Step #6715\tEpoch   2 Batch  464/3125   Loss: 0.730764 mae: 0.671060 (2161.4330179539506 steps/sec)\n",
      "Step #6716\tEpoch   2 Batch  465/3125   Loss: 0.821241 mae: 0.732474 (2229.8503971334094 steps/sec)\n",
      "Step #6717\tEpoch   2 Batch  466/3125   Loss: 0.858214 mae: 0.737651 (2046.5809838881244 steps/sec)\n",
      "Step #6718\tEpoch   2 Batch  467/3125   Loss: 0.872704 mae: 0.742416 (2114.235018953141 steps/sec)\n",
      "Step #6719\tEpoch   2 Batch  468/3125   Loss: 0.686752 mae: 0.684304 (1623.9493878688854 steps/sec)\n",
      "Step #6720\tEpoch   2 Batch  469/3125   Loss: 0.771591 mae: 0.687841 (2052.469733893146 steps/sec)\n",
      "Step #6721\tEpoch   2 Batch  470/3125   Loss: 0.770300 mae: 0.696595 (2005.8650802000936 steps/sec)\n",
      "Step #6722\tEpoch   2 Batch  471/3125   Loss: 0.848382 mae: 0.732880 (2062.9483169057035 steps/sec)\n",
      "Step #6723\tEpoch   2 Batch  472/3125   Loss: 0.874141 mae: 0.750734 (2220.547843672903 steps/sec)\n",
      "Step #6724\tEpoch   2 Batch  473/3125   Loss: 0.826663 mae: 0.736250 (2354.7366411785183 steps/sec)\n",
      "Step #6725\tEpoch   2 Batch  474/3125   Loss: 0.791269 mae: 0.690843 (2113.1698272908648 steps/sec)\n",
      "Step #6726\tEpoch   2 Batch  475/3125   Loss: 0.771876 mae: 0.685943 (2275.0865164516863 steps/sec)\n",
      "Step #6727\tEpoch   2 Batch  476/3125   Loss: 0.712768 mae: 0.673795 (2164.310556570379 steps/sec)\n",
      "Step #6728\tEpoch   2 Batch  477/3125   Loss: 0.903543 mae: 0.758782 (1835.5334214418876 steps/sec)\n",
      "Step #6729\tEpoch   2 Batch  478/3125   Loss: 0.882286 mae: 0.734286 (1842.8400702987697 steps/sec)\n",
      "Step #6730\tEpoch   2 Batch  479/3125   Loss: 0.902361 mae: 0.732107 (2160.7871825253724 steps/sec)\n",
      "Step #6731\tEpoch   2 Batch  480/3125   Loss: 0.930574 mae: 0.756845 (2074.8063357638234 steps/sec)\n",
      "Step #6732\tEpoch   2 Batch  481/3125   Loss: 0.812701 mae: 0.688311 (2196.5456925896833 steps/sec)\n",
      "Step #6733\tEpoch   2 Batch  482/3125   Loss: 0.898754 mae: 0.723996 (2108.8985650070895 steps/sec)\n",
      "Step #6734\tEpoch   2 Batch  483/3125   Loss: 0.830047 mae: 0.725663 (2101.7969713065877 steps/sec)\n",
      "Step #6735\tEpoch   2 Batch  484/3125   Loss: 0.716382 mae: 0.682315 (2275.7284081907264 steps/sec)\n",
      "Step #6736\tEpoch   2 Batch  485/3125   Loss: 0.854362 mae: 0.753277 (2289.0424266238797 steps/sec)\n",
      "Step #6737\tEpoch   2 Batch  486/3125   Loss: 0.854023 mae: 0.742853 (1665.1596357082171 steps/sec)\n",
      "Step #6738\tEpoch   2 Batch  487/3125   Loss: 0.782497 mae: 0.709713 (1957.1748544124234 steps/sec)\n",
      "Step #6739\tEpoch   2 Batch  488/3125   Loss: 0.748910 mae: 0.677360 (2294.000153141032 steps/sec)\n",
      "Step #6740\tEpoch   2 Batch  489/3125   Loss: 0.746943 mae: 0.699669 (2197.9730225440976 steps/sec)\n",
      "Step #6741\tEpoch   2 Batch  490/3125   Loss: 0.818258 mae: 0.714189 (2027.0759832587453 steps/sec)\n",
      "Step #6742\tEpoch   2 Batch  491/3125   Loss: 0.781115 mae: 0.715284 (2203.1453214131884 steps/sec)\n",
      "Step #6743\tEpoch   2 Batch  492/3125   Loss: 0.719656 mae: 0.674573 (1791.0292761247565 steps/sec)\n",
      "Step #6744\tEpoch   2 Batch  493/3125   Loss: 0.713988 mae: 0.675598 (2161.588967109535 steps/sec)\n",
      "Step #6745\tEpoch   2 Batch  494/3125   Loss: 0.778525 mae: 0.723951 (1985.6385396152098 steps/sec)\n",
      "Step #6746\tEpoch   2 Batch  495/3125   Loss: 0.773674 mae: 0.700510 (1778.9057596064129 steps/sec)\n",
      "Step #6747\tEpoch   2 Batch  496/3125   Loss: 0.707635 mae: 0.685024 (2062.785985481872 steps/sec)\n",
      "Step #6748\tEpoch   2 Batch  497/3125   Loss: 0.967743 mae: 0.772572 (2013.1241960565976 steps/sec)\n",
      "Step #6749\tEpoch   2 Batch  498/3125   Loss: 0.742750 mae: 0.683970 (2085.3489250840244 steps/sec)\n",
      "Step #6750\tEpoch   2 Batch  499/3125   Loss: 0.806006 mae: 0.701684 (2123.956328870344 steps/sec)\n",
      "Step #6751\tEpoch   2 Batch  500/3125   Loss: 0.668884 mae: 0.666338 (2223.4200231125624 steps/sec)\n",
      "Step #6752\tEpoch   2 Batch  501/3125   Loss: 0.874198 mae: 0.735204 (2090.816825020189 steps/sec)\n",
      "Step #6753\tEpoch   2 Batch  502/3125   Loss: 0.911241 mae: 0.753819 (2203.1453214131884 steps/sec)\n",
      "Step #6754\tEpoch   2 Batch  503/3125   Loss: 0.767701 mae: 0.667888 (2191.7478366289033 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6755\tEpoch   2 Batch  504/3125   Loss: 0.946602 mae: 0.733427 (1985.7701521650617 steps/sec)\n",
      "Step #6756\tEpoch   2 Batch  505/3125   Loss: 0.729571 mae: 0.681039 (1940.0111008325625 steps/sec)\n",
      "Step #6757\tEpoch   2 Batch  506/3125   Loss: 0.829415 mae: 0.687366 (2065.0609527935876 steps/sec)\n",
      "Step #6758\tEpoch   2 Batch  507/3125   Loss: 0.781333 mae: 0.711636 (2104.4755750010036 steps/sec)\n",
      "Step #6759\tEpoch   2 Batch  508/3125   Loss: 0.846361 mae: 0.726568 (1966.8851936261408 steps/sec)\n",
      "Step #6760\tEpoch   2 Batch  509/3125   Loss: 0.724209 mae: 0.668845 (2115.301285025519 steps/sec)\n",
      "Step #6761\tEpoch   2 Batch  510/3125   Loss: 0.885354 mae: 0.737729 (2008.4392388212648 steps/sec)\n",
      "Step #6762\tEpoch   2 Batch  511/3125   Loss: 0.960003 mae: 0.784691 (1926.325458353235 steps/sec)\n",
      "Step #6763\tEpoch   2 Batch  512/3125   Loss: 0.760748 mae: 0.689840 (1879.0674336505206 steps/sec)\n",
      "Step #6764\tEpoch   2 Batch  513/3125   Loss: 0.779410 mae: 0.702216 (1869.319356793953 steps/sec)\n",
      "Step #6765\tEpoch   2 Batch  514/3125   Loss: 0.747543 mae: 0.695815 (2000.9655843598232 steps/sec)\n",
      "Step #6766\tEpoch   2 Batch  515/3125   Loss: 0.888718 mae: 0.753159 (2078.404788804979 steps/sec)\n",
      "Step #6767\tEpoch   2 Batch  516/3125   Loss: 0.945095 mae: 0.799479 (1997.534932896454 steps/sec)\n",
      "Step #6768\tEpoch   2 Batch  517/3125   Loss: 0.788639 mae: 0.713077 (2049.8616907933965 steps/sec)\n",
      "Step #6769\tEpoch   2 Batch  518/3125   Loss: 0.801132 mae: 0.700577 (1876.3102800393665 steps/sec)\n",
      "Step #6770\tEpoch   2 Batch  519/3125   Loss: 0.809408 mae: 0.733616 (2054.9837337827776 steps/sec)\n",
      "Step #6771\tEpoch   2 Batch  520/3125   Loss: 0.890631 mae: 0.736493 (1948.8811240799942 steps/sec)\n",
      "Step #6772\tEpoch   2 Batch  521/3125   Loss: 0.873674 mae: 0.725665 (1914.1584519897774 steps/sec)\n",
      "Step #6773\tEpoch   2 Batch  522/3125   Loss: 0.825196 mae: 0.724008 (1904.4069705142524 steps/sec)\n",
      "Step #6774\tEpoch   2 Batch  523/3125   Loss: 0.670098 mae: 0.622719 (1906.6404829442142 steps/sec)\n",
      "Step #6775\tEpoch   2 Batch  524/3125   Loss: 0.833164 mae: 0.707060 (2003.5845992165855 steps/sec)\n",
      "Step #6776\tEpoch   2 Batch  525/3125   Loss: 0.901032 mae: 0.730288 (1929.0187276941756 steps/sec)\n",
      "Step #6777\tEpoch   2 Batch  526/3125   Loss: 0.738645 mae: 0.694198 (1968.6577111905901 steps/sec)\n",
      "Step #6778\tEpoch   2 Batch  527/3125   Loss: 0.818829 mae: 0.698573 (1907.576997944296 steps/sec)\n",
      "Step #6779\tEpoch   2 Batch  528/3125   Loss: 0.846712 mae: 0.719371 (1794.3700053048583 steps/sec)\n",
      "Step #6780\tEpoch   2 Batch  529/3125   Loss: 0.726182 mae: 0.693080 (1834.184909521852 steps/sec)\n",
      "Step #6781\tEpoch   2 Batch  530/3125   Loss: 0.730554 mae: 0.682338 (1957.3209885761219 steps/sec)\n",
      "Step #6782\tEpoch   2 Batch  531/3125   Loss: 0.905342 mae: 0.766450 (2137.9221758943045 steps/sec)\n",
      "Step #6783\tEpoch   2 Batch  532/3125   Loss: 0.795229 mae: 0.708865 (2119.27725453737 steps/sec)\n",
      "Step #6784\tEpoch   2 Batch  533/3125   Loss: 0.806021 mae: 0.710364 (2095.2871943969867 steps/sec)\n",
      "Step #6785\tEpoch   2 Batch  534/3125   Loss: 0.770059 mae: 0.699865 (2089.5876925529583 steps/sec)\n",
      "Step #6786\tEpoch   2 Batch  535/3125   Loss: 0.896284 mae: 0.740979 (2087.424601357673 steps/sec)\n",
      "Step #6787\tEpoch   2 Batch  536/3125   Loss: 0.919849 mae: 0.750036 (2124.149945811261 steps/sec)\n",
      "Step #6788\tEpoch   2 Batch  537/3125   Loss: 0.781499 mae: 0.700895 (1823.816605355388 steps/sec)\n",
      "Step #6789\tEpoch   2 Batch  538/3125   Loss: 0.790774 mae: 0.717253 (1856.4617359359092 steps/sec)\n",
      "Step #6790\tEpoch   2 Batch  539/3125   Loss: 0.913525 mae: 0.734228 (2095.2243935579268 steps/sec)\n",
      "Step #6791\tEpoch   2 Batch  540/3125   Loss: 0.744344 mae: 0.697346 (2052.891655898821 steps/sec)\n",
      "Step #6792\tEpoch   2 Batch  541/3125   Loss: 0.622937 mae: 0.626305 (2083.2773726978326 steps/sec)\n",
      "Step #6793\tEpoch   2 Batch  542/3125   Loss: 0.726279 mae: 0.665161 (2164.5116010238626 steps/sec)\n",
      "Step #6794\tEpoch   2 Batch  543/3125   Loss: 0.915888 mae: 0.769554 (2135.9406827996413 steps/sec)\n",
      "Step #6795\tEpoch   2 Batch  544/3125   Loss: 0.847976 mae: 0.750712 (2210.5300882250635 steps/sec)\n",
      "Step #6796\tEpoch   2 Batch  545/3125   Loss: 0.712869 mae: 0.668254 (1737.6208665103445 steps/sec)\n",
      "Step #6797\tEpoch   2 Batch  546/3125   Loss: 0.962915 mae: 0.789378 (2041.2225034066576 steps/sec)\n",
      "Step #6798\tEpoch   2 Batch  547/3125   Loss: 0.916492 mae: 0.772071 (2003.8334750661686 steps/sec)\n",
      "Step #6799\tEpoch   2 Batch  548/3125   Loss: 0.752180 mae: 0.689345 (2066.8723205046076 steps/sec)\n",
      "Step #6800\tEpoch   2 Batch  549/3125   Loss: 0.787307 mae: 0.683646 (2026.1555108981295 steps/sec)\n",
      "Step #6801\tEpoch   2 Batch  550/3125   Loss: 0.858888 mae: 0.727534 (1940.836988912951 steps/sec)\n",
      "Step #6802\tEpoch   2 Batch  551/3125   Loss: 0.819119 mae: 0.728889 (2284.255356228692 steps/sec)\n",
      "Step #6803\tEpoch   2 Batch  552/3125   Loss: 0.860682 mae: 0.739834 (2045.4032965961183 steps/sec)\n",
      "Step #6804\tEpoch   2 Batch  553/3125   Loss: 0.816205 mae: 0.698979 (1745.1398423912592 steps/sec)\n",
      "Step #6805\tEpoch   2 Batch  554/3125   Loss: 0.830292 mae: 0.726132 (2072.796639486039 steps/sec)\n",
      "Step #6806\tEpoch   2 Batch  555/3125   Loss: 0.927675 mae: 0.764616 (1896.7593723149278 steps/sec)\n",
      "Step #6807\tEpoch   2 Batch  556/3125   Loss: 0.767111 mae: 0.681301 (2023.71150932654 steps/sec)\n",
      "Step #6808\tEpoch   2 Batch  557/3125   Loss: 0.945204 mae: 0.764964 (2043.5696049580013 steps/sec)\n",
      "Step #6809\tEpoch   2 Batch  558/3125   Loss: 0.752981 mae: 0.698844 (2255.0748948890823 steps/sec)\n",
      "Step #6810\tEpoch   2 Batch  559/3125   Loss: 0.943458 mae: 0.776983 (2239.7573505067658 steps/sec)\n",
      "Step #6811\tEpoch   2 Batch  560/3125   Loss: 0.977350 mae: 0.785253 (1873.795568263045 steps/sec)\n",
      "Step #6812\tEpoch   2 Batch  561/3125   Loss: 0.898123 mae: 0.768041 (1966.6269681255098 steps/sec)\n",
      "Step #6813\tEpoch   2 Batch  562/3125   Loss: 0.653729 mae: 0.662701 (1933.3400938482387 steps/sec)\n",
      "Step #6814\tEpoch   2 Batch  563/3125   Loss: 0.791650 mae: 0.739807 (2046.8406566593142 steps/sec)\n",
      "Step #6815\tEpoch   2 Batch  564/3125   Loss: 0.932106 mae: 0.742297 (2189.2539120812585 steps/sec)\n",
      "Step #6816\tEpoch   2 Batch  565/3125   Loss: 0.914061 mae: 0.748474 (2057.5240861016814 steps/sec)\n",
      "Step #6817\tEpoch   2 Batch  566/3125   Loss: 0.720948 mae: 0.663062 (1916.012206040894 steps/sec)\n",
      "Step #6818\tEpoch   2 Batch  567/3125   Loss: 0.795063 mae: 0.718847 (1958.4725581569091 steps/sec)\n",
      "Step #6819\tEpoch   2 Batch  568/3125   Loss: 1.121492 mae: 0.859970 (1997.6871564789149 steps/sec)\n",
      "Step #6820\tEpoch   2 Batch  569/3125   Loss: 0.801205 mae: 0.708835 (1552.928283164871 steps/sec)\n",
      "Step #6821\tEpoch   2 Batch  570/3125   Loss: 0.902748 mae: 0.746623 (1931.470463629833 steps/sec)\n",
      "Step #6822\tEpoch   2 Batch  571/3125   Loss: 0.873237 mae: 0.720630 (1898.424883224101 steps/sec)\n",
      "Step #6823\tEpoch   2 Batch  572/3125   Loss: 0.889551 mae: 0.754022 (1906.9352125483065 steps/sec)\n",
      "Step #6824\tEpoch   2 Batch  573/3125   Loss: 0.787266 mae: 0.696352 (1947.650358482856 steps/sec)\n",
      "Step #6825\tEpoch   2 Batch  574/3125   Loss: 0.898516 mae: 0.729180 (1340.8814521646282 steps/sec)\n",
      "Step #6826\tEpoch   2 Batch  575/3125   Loss: 0.908924 mae: 0.777382 (1618.0230225596395 steps/sec)\n",
      "Step #6827\tEpoch   2 Batch  576/3125   Loss: 0.785913 mae: 0.716656 (1750.6903748226061 steps/sec)\n",
      "Step #6828\tEpoch   2 Batch  577/3125   Loss: 0.821688 mae: 0.748693 (1794.4928379511578 steps/sec)\n",
      "Step #6829\tEpoch   2 Batch  578/3125   Loss: 0.849720 mae: 0.733308 (1848.900173679988 steps/sec)\n",
      "Step #6830\tEpoch   2 Batch  579/3125   Loss: 0.862608 mae: 0.718220 (1616.5637598378157 steps/sec)\n",
      "Step #6831\tEpoch   2 Batch  580/3125   Loss: 0.932917 mae: 0.760149 (1714.4520200781544 steps/sec)\n",
      "Step #6832\tEpoch   2 Batch  581/3125   Loss: 0.791855 mae: 0.712004 (1657.4476997368192 steps/sec)\n",
      "Step #6833\tEpoch   2 Batch  582/3125   Loss: 0.876907 mae: 0.726559 (1992.5813317117666 steps/sec)\n",
      "Step #6834\tEpoch   2 Batch  583/3125   Loss: 0.848843 mae: 0.731042 (1797.5229066847235 steps/sec)\n",
      "Step #6835\tEpoch   2 Batch  584/3125   Loss: 0.699324 mae: 0.660905 (1941.3041063428 steps/sec)\n",
      "Step #6836\tEpoch   2 Batch  585/3125   Loss: 0.738304 mae: 0.669583 (1824.4830136152073 steps/sec)\n",
      "Step #6837\tEpoch   2 Batch  586/3125   Loss: 0.905905 mae: 0.732959 (2033.168196845279 steps/sec)\n",
      "Step #6838\tEpoch   2 Batch  587/3125   Loss: 0.808584 mae: 0.709696 (1919.9589852511695 steps/sec)\n",
      "Step #6839\tEpoch   2 Batch  588/3125   Loss: 0.864501 mae: 0.736355 (1986.823680994382 steps/sec)\n",
      "Step #6840\tEpoch   2 Batch  589/3125   Loss: 0.764378 mae: 0.691053 (1986.1839052156042 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6841\tEpoch   2 Batch  590/3125   Loss: 0.952488 mae: 0.731379 (1993.2252361852986 steps/sec)\n",
      "Step #6842\tEpoch   2 Batch  591/3125   Loss: 0.786229 mae: 0.684632 (1641.657664427849 steps/sec)\n",
      "Step #6843\tEpoch   2 Batch  592/3125   Loss: 0.770371 mae: 0.700790 (1830.2310968372547 steps/sec)\n",
      "Step #6844\tEpoch   2 Batch  593/3125   Loss: 0.800255 mae: 0.681441 (1987.1248945867326 steps/sec)\n",
      "Step #6845\tEpoch   2 Batch  594/3125   Loss: 0.832330 mae: 0.693538 (1974.7379918831627 steps/sec)\n",
      "Step #6846\tEpoch   2 Batch  595/3125   Loss: 0.955028 mae: 0.767404 (2096.774580575496 steps/sec)\n",
      "Step #6847\tEpoch   2 Batch  596/3125   Loss: 0.781337 mae: 0.711573 (2018.6468249766579 steps/sec)\n",
      "Step #6848\tEpoch   2 Batch  597/3125   Loss: 0.847777 mae: 0.749542 (2080.65242625976 steps/sec)\n",
      "Step #6849\tEpoch   2 Batch  598/3125   Loss: 0.893551 mae: 0.757424 (1795.5837150562952 steps/sec)\n",
      "Step #6850\tEpoch   2 Batch  599/3125   Loss: 0.736430 mae: 0.689067 (1636.2779520309598 steps/sec)\n",
      "Step #6851\tEpoch   2 Batch  600/3125   Loss: 0.783547 mae: 0.704956 (1761.3653161325 steps/sec)\n",
      "Step #6852\tEpoch   2 Batch  601/3125   Loss: 0.769232 mae: 0.682132 (1929.9063184436714 steps/sec)\n",
      "Step #6853\tEpoch   2 Batch  602/3125   Loss: 0.951496 mae: 0.773801 (1926.1131520940485 steps/sec)\n",
      "Step #6854\tEpoch   2 Batch  603/3125   Loss: 0.808489 mae: 0.706738 (1975.1843654344243 steps/sec)\n",
      "Step #6855\tEpoch   2 Batch  604/3125   Loss: 0.875776 mae: 0.734075 (1706.7916758226106 steps/sec)\n",
      "Step #6856\tEpoch   2 Batch  605/3125   Loss: 0.849103 mae: 0.732910 (1977.9787785899553 steps/sec)\n",
      "Step #6857\tEpoch   2 Batch  606/3125   Loss: 0.730897 mae: 0.681551 (1955.8606282175633 steps/sec)\n",
      "Step #6858\tEpoch   2 Batch  607/3125   Loss: 0.918378 mae: 0.762473 (1928.1142258221703 steps/sec)\n",
      "Step #6859\tEpoch   2 Batch  608/3125   Loss: 0.829978 mae: 0.733042 (1963.7909561667182 steps/sec)\n",
      "Step #6860\tEpoch   2 Batch  609/3125   Loss: 0.764276 mae: 0.684027 (2022.8136001929106 steps/sec)\n",
      "Step #6861\tEpoch   2 Batch  610/3125   Loss: 0.793601 mae: 0.693529 (1782.413435550494 steps/sec)\n",
      "Step #6862\tEpoch   2 Batch  611/3125   Loss: 0.828974 mae: 0.726857 (1909.1399024106038 steps/sec)\n",
      "Step #6863\tEpoch   2 Batch  612/3125   Loss: 0.895352 mae: 0.743846 (1931.7907148120855 steps/sec)\n",
      "Step #6864\tEpoch   2 Batch  613/3125   Loss: 0.880669 mae: 0.728908 (1936.0351544469268 steps/sec)\n",
      "Step #6865\tEpoch   2 Batch  614/3125   Loss: 0.907055 mae: 0.752510 (1809.621275530896 steps/sec)\n",
      "Step #6866\tEpoch   2 Batch  615/3125   Loss: 0.845456 mae: 0.716404 (1839.9782412241066 steps/sec)\n",
      "Step #6867\tEpoch   2 Batch  616/3125   Loss: 0.819432 mae: 0.715453 (2106.8646460181435 steps/sec)\n",
      "Step #6868\tEpoch   2 Batch  617/3125   Loss: 0.921368 mae: 0.761027 (2055.829820605823 steps/sec)\n",
      "Step #6869\tEpoch   2 Batch  618/3125   Loss: 0.719387 mae: 0.711142 (1989.1038773806813 steps/sec)\n",
      "Step #6870\tEpoch   2 Batch  619/3125   Loss: 0.814723 mae: 0.711072 (2051.6866244032244 steps/sec)\n",
      "Step #6871\tEpoch   2 Batch  620/3125   Loss: 0.864849 mae: 0.730481 (1990.2742716143114 steps/sec)\n",
      "Step #6872\tEpoch   2 Batch  621/3125   Loss: 0.633178 mae: 0.630295 (1721.884493489006 steps/sec)\n",
      "Step #6873\tEpoch   2 Batch  622/3125   Loss: 0.772982 mae: 0.709696 (1671.2371996652987 steps/sec)\n",
      "Step #6874\tEpoch   2 Batch  623/3125   Loss: 0.791016 mae: 0.705992 (1690.447287177875 steps/sec)\n",
      "Step #6875\tEpoch   2 Batch  624/3125   Loss: 0.661576 mae: 0.631610 (1871.8888908724136 steps/sec)\n",
      "Step #6876\tEpoch   2 Batch  625/3125   Loss: 0.802352 mae: 0.711001 (1931.096971426993 steps/sec)\n",
      "Step #6877\tEpoch   2 Batch  626/3125   Loss: 0.758118 mae: 0.690674 (1904.1821780739826 steps/sec)\n",
      "Step #6878\tEpoch   2 Batch  627/3125   Loss: 0.891210 mae: 0.765868 (2112.5525077817288 steps/sec)\n",
      "Step #6879\tEpoch   2 Batch  628/3125   Loss: 0.802034 mae: 0.709781 (2000.5647346128897 steps/sec)\n",
      "Step #6880\tEpoch   2 Batch  629/3125   Loss: 0.941269 mae: 0.765091 (1982.4101032253186 steps/sec)\n",
      "Step #6881\tEpoch   2 Batch  630/3125   Loss: 0.859254 mae: 0.755943 (1769.9725703675572 steps/sec)\n",
      "Step #6882\tEpoch   2 Batch  631/3125   Loss: 0.827817 mae: 0.719694 (1938.8990588191787 steps/sec)\n",
      "Step #6883\tEpoch   2 Batch  632/3125   Loss: 0.818956 mae: 0.681849 (2055.910436640982 steps/sec)\n",
      "Step #6884\tEpoch   2 Batch  633/3125   Loss: 0.798105 mae: 0.707801 (1981.9979208014365 steps/sec)\n",
      "Step #6885\tEpoch   2 Batch  634/3125   Loss: 0.733790 mae: 0.683850 (2073.308947108255 steps/sec)\n",
      "Step #6886\tEpoch   2 Batch  635/3125   Loss: 0.807713 mae: 0.734782 (1949.7870916156864 steps/sec)\n",
      "Step #6887\tEpoch   2 Batch  636/3125   Loss: 0.858033 mae: 0.743605 (1971.767316353106 steps/sec)\n",
      "Step #6888\tEpoch   2 Batch  637/3125   Loss: 0.832356 mae: 0.726573 (2032.3603519789124 steps/sec)\n",
      "Step #6889\tEpoch   2 Batch  638/3125   Loss: 0.866509 mae: 0.732501 (2103.6733875012537 steps/sec)\n",
      "Step #6890\tEpoch   2 Batch  639/3125   Loss: 0.868603 mae: 0.743674 (1788.2649885310345 steps/sec)\n",
      "Step #6891\tEpoch   2 Batch  640/3125   Loss: 0.792809 mae: 0.721064 (1813.0631370548722 steps/sec)\n",
      "Step #6892\tEpoch   2 Batch  641/3125   Loss: 0.962033 mae: 0.783479 (1973.6229401744793 steps/sec)\n",
      "Step #6893\tEpoch   2 Batch  642/3125   Loss: 0.806216 mae: 0.700406 (1867.7710387331783 steps/sec)\n",
      "Step #6894\tEpoch   2 Batch  643/3125   Loss: 0.721431 mae: 0.677875 (1900.28271112722 steps/sec)\n",
      "Step #6895\tEpoch   2 Batch  644/3125   Loss: 0.956398 mae: 0.775302 (2105.701146655421 steps/sec)\n",
      "Step #6896\tEpoch   2 Batch  645/3125   Loss: 0.866018 mae: 0.735438 (2122.236839442207 steps/sec)\n",
      "Step #6897\tEpoch   2 Batch  646/3125   Loss: 0.907204 mae: 0.763492 (2122.065043612005 steps/sec)\n",
      "Step #6898\tEpoch   2 Batch  647/3125   Loss: 0.793653 mae: 0.706049 (2027.3307296701596 steps/sec)\n",
      "Step #6899\tEpoch   2 Batch  648/3125   Loss: 0.880607 mae: 0.753208 (1907.4728952921487 steps/sec)\n",
      "Step #6900\tEpoch   2 Batch  649/3125   Loss: 0.784049 mae: 0.697473 (2174.498926827245 steps/sec)\n",
      "Step #6901\tEpoch   2 Batch  650/3125   Loss: 0.817562 mae: 0.736385 (2035.7536693329191 steps/sec)\n",
      "Step #6902\tEpoch   2 Batch  651/3125   Loss: 0.792586 mae: 0.684094 (2112.1907984852146 steps/sec)\n",
      "Step #6903\tEpoch   2 Batch  652/3125   Loss: 0.855070 mae: 0.739800 (2075.463407392746 steps/sec)\n",
      "Step #6904\tEpoch   2 Batch  653/3125   Loss: 0.790270 mae: 0.710626 (2041.1629016088687 steps/sec)\n",
      "Step #6905\tEpoch   2 Batch  654/3125   Loss: 0.795386 mae: 0.701364 (2120.3486138353587 steps/sec)\n",
      "Step #6906\tEpoch   2 Batch  655/3125   Loss: 0.748803 mae: 0.688658 (2161.232544958005 steps/sec)\n",
      "Step #6907\tEpoch   2 Batch  656/3125   Loss: 0.866483 mae: 0.723413 (1862.1157500310774 steps/sec)\n",
      "Step #6908\tEpoch   2 Batch  657/3125   Loss: 0.850465 mae: 0.733810 (2016.9383613683794 steps/sec)\n",
      "Step #6909\tEpoch   2 Batch  658/3125   Loss: 0.789497 mae: 0.703601 (2279.2652972502988 steps/sec)\n",
      "Step #6910\tEpoch   2 Batch  659/3125   Loss: 0.899256 mae: 0.751449 (1994.078102863011 steps/sec)\n",
      "Step #6911\tEpoch   2 Batch  660/3125   Loss: 0.808577 mae: 0.699546 (1959.955140186916 steps/sec)\n",
      "Step #6912\tEpoch   2 Batch  661/3125   Loss: 0.843486 mae: 0.736096 (1954.6574704073073 steps/sec)\n",
      "Step #6913\tEpoch   2 Batch  662/3125   Loss: 0.839509 mae: 0.719802 (2055.6484576402436 steps/sec)\n",
      "Step #6914\tEpoch   2 Batch  663/3125   Loss: 0.766300 mae: 0.694981 (2157.9634088617236 steps/sec)\n",
      "Step #6915\tEpoch   2 Batch  664/3125   Loss: 0.777040 mae: 0.699429 (1937.9852697919844 steps/sec)\n",
      "Step #6916\tEpoch   2 Batch  665/3125   Loss: 0.923455 mae: 0.738150 (1934.7491558573354 steps/sec)\n",
      "Step #6917\tEpoch   2 Batch  666/3125   Loss: 0.743154 mae: 0.696089 (1993.14946111882 steps/sec)\n",
      "Step #6918\tEpoch   2 Batch  667/3125   Loss: 0.750002 mae: 0.666455 (2072.878591691295 steps/sec)\n",
      "Step #6919\tEpoch   2 Batch  668/3125   Loss: 0.789007 mae: 0.722970 (2242.247858952838 steps/sec)\n",
      "Step #6920\tEpoch   2 Batch  669/3125   Loss: 0.834947 mae: 0.712888 (2124.236009116232 steps/sec)\n",
      "Step #6921\tEpoch   2 Batch  670/3125   Loss: 0.879184 mae: 0.730300 (2197.328192286334 steps/sec)\n",
      "Step #6922\tEpoch   2 Batch  671/3125   Loss: 0.778553 mae: 0.692531 (1994.893746551757 steps/sec)\n",
      "Step #6923\tEpoch   2 Batch  672/3125   Loss: 0.868630 mae: 0.731022 (2067.728227325163 steps/sec)\n",
      "Step #6924\tEpoch   2 Batch  673/3125   Loss: 0.888785 mae: 0.751837 (1867.7710387331783 steps/sec)\n",
      "Step #6925\tEpoch   2 Batch  674/3125   Loss: 0.845175 mae: 0.740993 (1998.5819387794 steps/sec)\n",
      "Step #6926\tEpoch   2 Batch  675/3125   Loss: 0.697584 mae: 0.644209 (2274.2723289810438 steps/sec)\n",
      "Step #6927\tEpoch   2 Batch  676/3125   Loss: 0.984300 mae: 0.781096 (2068.60524758335 steps/sec)\n",
      "Step #6928\tEpoch   2 Batch  677/3125   Loss: 0.832879 mae: 0.712229 (2222.33619803533 steps/sec)\n",
      "Step #6929\tEpoch   2 Batch  678/3125   Loss: 0.795548 mae: 0.676511 (2082.4705823941213 steps/sec)\n",
      "Step #6930\tEpoch   2 Batch  679/3125   Loss: 0.887084 mae: 0.745161 (2147.1153747709195 steps/sec)\n",
      "Step #6931\tEpoch   2 Batch  680/3125   Loss: 0.897602 mae: 0.764057 (2273.754513026791 steps/sec)\n",
      "Step #6932\tEpoch   2 Batch  681/3125   Loss: 0.767986 mae: 0.678435 (2022.579494054221 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #6933\tEpoch   2 Batch  682/3125   Loss: 0.839057 mae: 0.698329 (1848.0692292779217 steps/sec)\n",
      "Step #6934\tEpoch   2 Batch  683/3125   Loss: 0.827072 mae: 0.729470 (1956.6453009395323 steps/sec)\n",
      "Step #6935\tEpoch   2 Batch  684/3125   Loss: 0.870653 mae: 0.752621 (2299.9122653097033 steps/sec)\n",
      "Step #6936\tEpoch   2 Batch  685/3125   Loss: 0.845159 mae: 0.739556 (1983.7414985290918 steps/sec)\n",
      "Step #6937\tEpoch   2 Batch  686/3125   Loss: 0.904392 mae: 0.763805 (2031.0415960486175 steps/sec)\n",
      "Step #6938\tEpoch   2 Batch  687/3125   Loss: 0.857677 mae: 0.739976 (1911.0884304148137 steps/sec)\n",
      "Step #6939\tEpoch   2 Batch  688/3125   Loss: 0.876248 mae: 0.750227 (2243.6632074462395 steps/sec)\n",
      "Step #6940\tEpoch   2 Batch  689/3125   Loss: 0.887896 mae: 0.745417 (2174.65676718238 steps/sec)\n",
      "Step #6941\tEpoch   2 Batch  690/3125   Loss: 0.705229 mae: 0.672171 (2207.0173223043084 steps/sec)\n",
      "Step #6942\tEpoch   2 Batch  691/3125   Loss: 0.845934 mae: 0.764980 (2011.5215285304585 steps/sec)\n",
      "Step #6943\tEpoch   2 Batch  692/3125   Loss: 0.941824 mae: 0.785856 (1906.7098228897698 steps/sec)\n",
      "Step #6944\tEpoch   2 Batch  693/3125   Loss: 0.862139 mae: 0.756512 (2088.027320608939 steps/sec)\n",
      "Step #6945\tEpoch   2 Batch  694/3125   Loss: 0.786566 mae: 0.698218 (2150.9913125532066 steps/sec)\n",
      "Step #6946\tEpoch   2 Batch  695/3125   Loss: 0.754685 mae: 0.706034 (2253.136650300289 steps/sec)\n",
      "Step #6947\tEpoch   2 Batch  696/3125   Loss: 0.723866 mae: 0.697314 (2156.1884394727645 steps/sec)\n",
      "Step #6948\tEpoch   2 Batch  697/3125   Loss: 0.906275 mae: 0.744214 (2259.3507934626864 steps/sec)\n",
      "Step #6949\tEpoch   2 Batch  698/3125   Loss: 0.872596 mae: 0.739078 (1888.7135703735726 steps/sec)\n",
      "Step #6950\tEpoch   2 Batch  699/3125   Loss: 0.758400 mae: 0.690931 (1695.0792111218882 steps/sec)\n",
      "Step #6951\tEpoch   2 Batch  700/3125   Loss: 0.854322 mae: 0.710592 (1693.2999596285829 steps/sec)\n",
      "Step #6952\tEpoch   2 Batch  701/3125   Loss: 0.791997 mae: 0.692597 (1910.6531463816839 steps/sec)\n",
      "Step #6953\tEpoch   2 Batch  702/3125   Loss: 0.779521 mae: 0.709962 (1920.0468761444372 steps/sec)\n",
      "Step #6954\tEpoch   2 Batch  703/3125   Loss: 0.811700 mae: 0.692559 (2009.979202008875 steps/sec)\n",
      "Step #6955\tEpoch   2 Batch  704/3125   Loss: 0.774242 mae: 0.688321 (2122.451623350336 steps/sec)\n",
      "Step #6956\tEpoch   2 Batch  705/3125   Loss: 0.937744 mae: 0.789418 (1848.900173679988 steps/sec)\n",
      "Step #6957\tEpoch   2 Batch  706/3125   Loss: 0.851654 mae: 0.748368 (2028.3111205679247 steps/sec)\n",
      "Step #6958\tEpoch   2 Batch  707/3125   Loss: 0.880562 mae: 0.741997 (1788.2802373968211 steps/sec)\n",
      "Step #6959\tEpoch   2 Batch  708/3125   Loss: 0.781822 mae: 0.680503 (1864.9971542401822 steps/sec)\n",
      "Step #6960\tEpoch   2 Batch  709/3125   Loss: 0.768113 mae: 0.695974 (2060.5767624662244 steps/sec)\n",
      "Step #6961\tEpoch   2 Batch  710/3125   Loss: 0.780290 mae: 0.686839 (2096.8584398184253 steps/sec)\n",
      "Step #6962\tEpoch   2 Batch  711/3125   Loss: 0.870270 mae: 0.745462 (2166.3003057598544 steps/sec)\n",
      "Step #6963\tEpoch   2 Batch  712/3125   Loss: 0.698658 mae: 0.665318 (2071.8546546665216 steps/sec)\n",
      "Step #6964\tEpoch   2 Batch  713/3125   Loss: 0.781324 mae: 0.704083 (2169.190827377197 steps/sec)\n",
      "Step #6965\tEpoch   2 Batch  714/3125   Loss: 0.782351 mae: 0.719065 (2227.0798377333645 steps/sec)\n",
      "Step #6966\tEpoch   2 Batch  715/3125   Loss: 0.771134 mae: 0.695612 (2082.03641562258 steps/sec)\n",
      "Step #6967\tEpoch   2 Batch  716/3125   Loss: 0.854921 mae: 0.742082 (1957.7046946033997 steps/sec)\n",
      "Step #6968\tEpoch   2 Batch  717/3125   Loss: 0.732432 mae: 0.700627 (1992.9979282686788 steps/sec)\n",
      "Step #6969\tEpoch   2 Batch  718/3125   Loss: 0.811522 mae: 0.692826 (2122.451623350336 steps/sec)\n",
      "Step #6970\tEpoch   2 Batch  719/3125   Loss: 0.847066 mae: 0.722584 (2181.420264830398 steps/sec)\n",
      "Step #6971\tEpoch   2 Batch  720/3125   Loss: 0.753281 mae: 0.685177 (2026.096785724637 steps/sec)\n",
      "Step #6972\tEpoch   2 Batch  721/3125   Loss: 0.704882 mae: 0.651990 (2144.0641230114916 steps/sec)\n",
      "Step #6973\tEpoch   2 Batch  722/3125   Loss: 0.847569 mae: 0.738625 (2145.3581987253588 steps/sec)\n",
      "Step #6974\tEpoch   2 Batch  723/3125   Loss: 1.013833 mae: 0.797123 (2118.913238964162 steps/sec)\n",
      "Step #6975\tEpoch   2 Batch  724/3125   Loss: 0.664185 mae: 0.648818 (2250.4286986661514 steps/sec)\n",
      "Step #6976\tEpoch   2 Batch  725/3125   Loss: 0.764741 mae: 0.706093 (1789.7300664806232 steps/sec)\n",
      "Step #6977\tEpoch   2 Batch  726/3125   Loss: 0.726380 mae: 0.693035 (1979.565791957712 steps/sec)\n",
      "Step #6978\tEpoch   2 Batch  727/3125   Loss: 0.729960 mae: 0.694611 (1832.3579523114695 steps/sec)\n",
      "Step #6979\tEpoch   2 Batch  728/3125   Loss: 0.838948 mae: 0.702003 (2108.177769735718 steps/sec)\n",
      "Step #6980\tEpoch   2 Batch  729/3125   Loss: 0.733723 mae: 0.680165 (2192.8020242999646 steps/sec)\n",
      "Step #6981\tEpoch   2 Batch  730/3125   Loss: 0.705361 mae: 0.674417 (2098.894082088133 steps/sec)\n",
      "Step #6982\tEpoch   2 Batch  731/3125   Loss: 0.861295 mae: 0.741192 (2289.7922194197868 steps/sec)\n",
      "Step #6983\tEpoch   2 Batch  732/3125   Loss: 0.900830 mae: 0.759548 (2167.0166156898404 steps/sec)\n",
      "Step #6984\tEpoch   2 Batch  733/3125   Loss: 0.819302 mae: 0.714150 (2139.492557717224 steps/sec)\n",
      "Step #6985\tEpoch   2 Batch  734/3125   Loss: 0.899484 mae: 0.757144 (1771.8418384589388 steps/sec)\n",
      "Step #6986\tEpoch   2 Batch  735/3125   Loss: 0.782079 mae: 0.689487 (1803.8930989101732 steps/sec)\n",
      "Step #6987\tEpoch   2 Batch  736/3125   Loss: 0.855396 mae: 0.717693 (2322.6589582571905 steps/sec)\n",
      "Step #6988\tEpoch   2 Batch  737/3125   Loss: 0.671352 mae: 0.656725 (2225.6853276731226 steps/sec)\n",
      "Step #6989\tEpoch   2 Batch  738/3125   Loss: 0.890414 mae: 0.763714 (2029.7049059744684 steps/sec)\n",
      "Step #6990\tEpoch   2 Batch  739/3125   Loss: 0.793571 mae: 0.712213 (1950.6939018491646 steps/sec)\n",
      "Step #6991\tEpoch   2 Batch  740/3125   Loss: 0.875637 mae: 0.742126 (2098.64203584545 steps/sec)\n",
      "Step #6992\tEpoch   2 Batch  741/3125   Loss: 0.893412 mae: 0.725855 (1999.2869059535726 steps/sec)\n",
      "Step #6993\tEpoch   2 Batch  742/3125   Loss: 0.809087 mae: 0.718160 (1990.3687182650785 steps/sec)\n",
      "Step #6994\tEpoch   2 Batch  743/3125   Loss: 0.709541 mae: 0.672067 (1933.0371462807632 steps/sec)\n",
      "Step #6995\tEpoch   2 Batch  744/3125   Loss: 0.812285 mae: 0.713031 (2007.1705444904912 steps/sec)\n",
      "Step #6996\tEpoch   2 Batch  745/3125   Loss: 0.773771 mae: 0.704569 (2136.6588216115983 steps/sec)\n",
      "Step #6997\tEpoch   2 Batch  746/3125   Loss: 0.754833 mae: 0.685207 (2035.674626286158 steps/sec)\n",
      "Step #6998\tEpoch   2 Batch  747/3125   Loss: 0.733938 mae: 0.689422 (1792.3150553808287 steps/sec)\n",
      "Step #6999\tEpoch   2 Batch  748/3125   Loss: 0.751111 mae: 0.694197 (2176.078362196881 steps/sec)\n",
      "Step #7000\tEpoch   2 Batch  749/3125   Loss: 0.781011 mae: 0.695850 (2257.259409947582 steps/sec)\n",
      "Step #7001\tEpoch   2 Batch  750/3125   Loss: 0.791329 mae: 0.707062 (2185.3300682540507 steps/sec)\n",
      "Step #7002\tEpoch   2 Batch  751/3125   Loss: 0.767036 mae: 0.686448 (1799.9141734040545 steps/sec)\n",
      "Step #7003\tEpoch   2 Batch  752/3125   Loss: 0.850457 mae: 0.749715 (2105.5108781864 steps/sec)\n",
      "Step #7004\tEpoch   2 Batch  753/3125   Loss: 0.892259 mae: 0.773231 (2144.45876025114 steps/sec)\n",
      "Step #7005\tEpoch   2 Batch  754/3125   Loss: 0.819481 mae: 0.711006 (2178.927135391233 steps/sec)\n",
      "Step #7006\tEpoch   2 Batch  755/3125   Loss: 0.921150 mae: 0.759861 (2191.9311007985284 steps/sec)\n",
      "Step #7007\tEpoch   2 Batch  756/3125   Loss: 0.813478 mae: 0.716208 (2230.1586626398403 steps/sec)\n",
      "Step #7008\tEpoch   2 Batch  757/3125   Loss: 0.824723 mae: 0.709166 (2160.030487490859 steps/sec)\n",
      "Step #7009\tEpoch   2 Batch  758/3125   Loss: 0.746787 mae: 0.678988 (2154.327861442689 steps/sec)\n",
      "Step #7010\tEpoch   2 Batch  759/3125   Loss: 0.886151 mae: 0.741102 (1974.8123734639107 steps/sec)\n",
      "Step #7011\tEpoch   2 Batch  760/3125   Loss: 0.757189 mae: 0.704465 (1885.7076061251832 steps/sec)\n",
      "Step #7012\tEpoch   2 Batch  761/3125   Loss: 0.786946 mae: 0.696141 (2239.2312209705833 steps/sec)\n",
      "Step #7013\tEpoch   2 Batch  762/3125   Loss: 0.830137 mae: 0.716536 (1938.5943667440076 steps/sec)\n",
      "Step #7014\tEpoch   2 Batch  763/3125   Loss: 0.891965 mae: 0.749753 (1983.8165599311344 steps/sec)\n",
      "Step #7015\tEpoch   2 Batch  764/3125   Loss: 0.855316 mae: 0.743246 (2167.48695157873 steps/sec)\n",
      "Step #7016\tEpoch   2 Batch  765/3125   Loss: 0.787797 mae: 0.723922 (2068.095261574873 steps/sec)\n",
      "Step #7017\tEpoch   2 Batch  766/3125   Loss: 0.827802 mae: 0.718441 (2265.6237846246922 steps/sec)\n",
      "Step #7018\tEpoch   2 Batch  767/3125   Loss: 0.871270 mae: 0.725673 (2143.603896435763 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7019\tEpoch   2 Batch  768/3125   Loss: 0.711531 mae: 0.683710 (1785.64604708587 steps/sec)\n",
      "Step #7020\tEpoch   2 Batch  769/3125   Loss: 0.892410 mae: 0.722307 (1945.42806519541 steps/sec)\n",
      "Step #7021\tEpoch   2 Batch  770/3125   Loss: 0.747050 mae: 0.664492 (2125.958740939733 steps/sec)\n",
      "Step #7022\tEpoch   2 Batch  771/3125   Loss: 0.762506 mae: 0.700877 (2147.951042146771 steps/sec)\n",
      "Step #7023\tEpoch   2 Batch  772/3125   Loss: 0.822577 mae: 0.716715 (2072.0184166065624 steps/sec)\n",
      "Step #7024\tEpoch   2 Batch  773/3125   Loss: 0.715315 mae: 0.686326 (2010.6151250191747 steps/sec)\n",
      "Step #7025\tEpoch   2 Batch  774/3125   Loss: 0.777742 mae: 0.705123 (2321.9131975199293 steps/sec)\n",
      "Step #7026\tEpoch   2 Batch  775/3125   Loss: 0.788082 mae: 0.700688 (2112.914340983739 steps/sec)\n",
      "Step #7027\tEpoch   2 Batch  776/3125   Loss: 0.811902 mae: 0.691728 (2030.1961315804758 steps/sec)\n",
      "Step #7028\tEpoch   2 Batch  777/3125   Loss: 0.732279 mae: 0.694317 (1798.494074061369 steps/sec)\n",
      "Step #7029\tEpoch   2 Batch  778/3125   Loss: 0.716016 mae: 0.655451 (1731.6664739979853 steps/sec)\n",
      "Step #7030\tEpoch   2 Batch  779/3125   Loss: 0.780618 mae: 0.671882 (2221.0652291333495 steps/sec)\n",
      "Step #7031\tEpoch   2 Batch  780/3125   Loss: 0.837855 mae: 0.747134 (2067.422464953962 steps/sec)\n",
      "Step #7032\tEpoch   2 Batch  781/3125   Loss: 0.791981 mae: 0.697330 (2025.6858048064291 steps/sec)\n",
      "Step #7033\tEpoch   2 Batch  782/3125   Loss: 0.925540 mae: 0.782797 (1968.6577111905901 steps/sec)\n",
      "Step #7034\tEpoch   2 Batch  783/3125   Loss: 0.772016 mae: 0.689267 (2057.342424093785 steps/sec)\n",
      "Step #7035\tEpoch   2 Batch  784/3125   Loss: 0.909959 mae: 0.749147 (2125.1400950518328 steps/sec)\n",
      "Step #7036\tEpoch   2 Batch  785/3125   Loss: 0.973922 mae: 0.781582 (2059.1402706046383 steps/sec)\n",
      "Step #7037\tEpoch   2 Batch  786/3125   Loss: 0.896587 mae: 0.734717 (1612.338066718434 steps/sec)\n",
      "Step #7038\tEpoch   2 Batch  787/3125   Loss: 0.856859 mae: 0.733400 (1895.8163080817212 steps/sec)\n",
      "Step #7039\tEpoch   2 Batch  788/3125   Loss: 0.795362 mae: 0.711997 (1864.6986644852666 steps/sec)\n",
      "Step #7040\tEpoch   2 Batch  789/3125   Loss: 0.877789 mae: 0.736113 (1855.4433896325656 steps/sec)\n",
      "Step #7041\tEpoch   2 Batch  790/3125   Loss: 0.759901 mae: 0.699189 (1897.462994462741 steps/sec)\n",
      "Step #7042\tEpoch   2 Batch  791/3125   Loss: 0.863086 mae: 0.743645 (2038.325914118539 steps/sec)\n",
      "Step #7043\tEpoch   2 Batch  792/3125   Loss: 0.904024 mae: 0.747161 (1938.5943667440076 steps/sec)\n",
      "Step #7044\tEpoch   2 Batch  793/3125   Loss: 0.810519 mae: 0.739284 (1492.0401832721477 steps/sec)\n",
      "Step #7045\tEpoch   2 Batch  794/3125   Loss: 0.881576 mae: 0.742232 (1499.3901349138825 steps/sec)\n",
      "Step #7046\tEpoch   2 Batch  795/3125   Loss: 0.811310 mae: 0.719432 (1723.2994231432938 steps/sec)\n",
      "Step #7047\tEpoch   2 Batch  796/3125   Loss: 0.942805 mae: 0.757382 (1794.2318386762831 steps/sec)\n",
      "Step #7048\tEpoch   2 Batch  797/3125   Loss: 0.811820 mae: 0.714570 (1816.7226861637616 steps/sec)\n",
      "Step #7049\tEpoch   2 Batch  798/3125   Loss: 0.718580 mae: 0.656375 (1629.514056162489 steps/sec)\n",
      "Step #7050\tEpoch   2 Batch  799/3125   Loss: 0.897141 mae: 0.758617 (1925.6886799382942 steps/sec)\n",
      "Step #7051\tEpoch   2 Batch  800/3125   Loss: 0.702475 mae: 0.669592 (1916.3798854093372 steps/sec)\n",
      "Step #7052\tEpoch   2 Batch  801/3125   Loss: 0.971678 mae: 0.769346 (1982.2601988733034 steps/sec)\n",
      "Step #7053\tEpoch   2 Batch  802/3125   Loss: 0.722658 mae: 0.683827 (2118.1000090898992 steps/sec)\n",
      "Step #7054\tEpoch   2 Batch  803/3125   Loss: 0.839663 mae: 0.725130 (2091.6926820997196 steps/sec)\n",
      "Step #7055\tEpoch   2 Batch  804/3125   Loss: 0.804897 mae: 0.718262 (2030.6089448764003 steps/sec)\n",
      "Step #7056\tEpoch   2 Batch  805/3125   Loss: 0.733245 mae: 0.666310 (2029.0372205075612 steps/sec)\n",
      "Step #7057\tEpoch   2 Batch  806/3125   Loss: 0.862339 mae: 0.728989 (2033.8386042497066 steps/sec)\n",
      "Step #7058\tEpoch   2 Batch  807/3125   Loss: 0.759878 mae: 0.716847 (2229.6370324693276 steps/sec)\n",
      "Step #7059\tEpoch   2 Batch  808/3125   Loss: 0.833154 mae: 0.730000 (1762.1052985363067 steps/sec)\n",
      "Step #7060\tEpoch   2 Batch  809/3125   Loss: 0.855752 mae: 0.721189 (1805.5859765127252 steps/sec)\n",
      "Step #7061\tEpoch   2 Batch  810/3125   Loss: 0.775567 mae: 0.687346 (1848.0366584420162 steps/sec)\n",
      "Step #7062\tEpoch   2 Batch  811/3125   Loss: 0.819623 mae: 0.730142 (2066.8723205046076 steps/sec)\n",
      "Step #7063\tEpoch   2 Batch  812/3125   Loss: 0.876827 mae: 0.732937 (1984.6050477425215 steps/sec)\n",
      "Step #7064\tEpoch   2 Batch  813/3125   Loss: 0.941097 mae: 0.770220 (1922.3524882439754 steps/sec)\n",
      "Step #7065\tEpoch   2 Batch  814/3125   Loss: 0.908321 mae: 0.742924 (2044.1472615090697 steps/sec)\n",
      "Step #7066\tEpoch   2 Batch  815/3125   Loss: 0.766920 mae: 0.682101 (1898.95777683204 steps/sec)\n",
      "Step #7067\tEpoch   2 Batch  816/3125   Loss: 0.902093 mae: 0.746832 (1713.2754930313874 steps/sec)\n",
      "Step #7068\tEpoch   2 Batch  817/3125   Loss: 0.717883 mae: 0.656412 (1926.1131520940485 steps/sec)\n",
      "Step #7069\tEpoch   2 Batch  818/3125   Loss: 0.666818 mae: 0.654470 (2055.3261135884745 steps/sec)\n",
      "Step #7070\tEpoch   2 Batch  819/3125   Loss: 0.718834 mae: 0.669102 (1907.0912827601258 steps/sec)\n",
      "Step #7071\tEpoch   2 Batch  820/3125   Loss: 0.793476 mae: 0.705417 (2013.9361579533668 steps/sec)\n",
      "Step #7072\tEpoch   2 Batch  821/3125   Loss: 0.986538 mae: 0.796936 (2019.249359702671 steps/sec)\n",
      "Step #7073\tEpoch   2 Batch  822/3125   Loss: 0.766997 mae: 0.700646 (2066.4649948268216 steps/sec)\n",
      "Step #7074\tEpoch   2 Batch  823/3125   Loss: 0.775590 mae: 0.693282 (2032.2618782281743 steps/sec)\n",
      "Step #7075\tEpoch   2 Batch  824/3125   Loss: 0.833550 mae: 0.713635 (1988.4814867491584 steps/sec)\n",
      "Step #7076\tEpoch   2 Batch  825/3125   Loss: 0.836856 mae: 0.736220 (1798.170235022765 steps/sec)\n",
      "Step #7077\tEpoch   2 Batch  826/3125   Loss: 0.875341 mae: 0.743937 (2039.4756292060529 steps/sec)\n",
      "Step #7078\tEpoch   2 Batch  827/3125   Loss: 0.699883 mae: 0.664704 (1812.4363705502597 steps/sec)\n",
      "Step #7079\tEpoch   2 Batch  828/3125   Loss: 0.949596 mae: 0.778827 (1803.7534618891164 steps/sec)\n",
      "Step #7080\tEpoch   2 Batch  829/3125   Loss: 0.963503 mae: 0.768156 (2032.1043400742242 steps/sec)\n",
      "Step #7081\tEpoch   2 Batch  830/3125   Loss: 0.725486 mae: 0.667725 (1981.9979208014365 steps/sec)\n",
      "Step #7082\tEpoch   2 Batch  831/3125   Loss: 0.927967 mae: 0.779723 (2103.1459660031087 steps/sec)\n",
      "Step #7083\tEpoch   2 Batch  832/3125   Loss: 0.886261 mae: 0.746405 (2037.8901543125899 steps/sec)\n",
      "Step #7084\tEpoch   2 Batch  833/3125   Loss: 0.970178 mae: 0.779559 (1754.8801713750167 steps/sec)\n",
      "Step #7085\tEpoch   2 Batch  834/3125   Loss: 0.845558 mae: 0.730034 (2065.0812876035175 steps/sec)\n",
      "Step #7086\tEpoch   2 Batch  835/3125   Loss: 0.753667 mae: 0.699776 (2076.0592381404927 steps/sec)\n",
      "Step #7087\tEpoch   2 Batch  836/3125   Loss: 0.787526 mae: 0.678103 (2099.167200512492 steps/sec)\n",
      "Step #7088\tEpoch   2 Batch  837/3125   Loss: 0.844504 mae: 0.743195 (2112.360999194198 steps/sec)\n",
      "Step #7089\tEpoch   2 Batch  838/3125   Loss: 0.898767 mae: 0.737079 (2064.308846256066 steps/sec)\n",
      "Step #7090\tEpoch   2 Batch  839/3125   Loss: 0.825854 mae: 0.727078 (2015.8720393725007 steps/sec)\n",
      "Step #7091\tEpoch   2 Batch  840/3125   Loss: 0.760741 mae: 0.707335 (2030.864580105361 steps/sec)\n",
      "Step #7092\tEpoch   2 Batch  841/3125   Loss: 0.904932 mae: 0.758726 (1874.6330562259766 steps/sec)\n",
      "Step #7093\tEpoch   2 Batch  842/3125   Loss: 0.727071 mae: 0.672877 (2054.7219908881593 steps/sec)\n",
      "Step #7094\tEpoch   2 Batch  843/3125   Loss: 0.760786 mae: 0.712618 (2079.187817258883 steps/sec)\n",
      "Step #7095\tEpoch   2 Batch  844/3125   Loss: 0.926465 mae: 0.727775 (1950.6213259915173 steps/sec)\n",
      "Step #7096\tEpoch   2 Batch  845/3125   Loss: 0.754866 mae: 0.692575 (2043.5696049580013 steps/sec)\n",
      "Step #7097\tEpoch   2 Batch  846/3125   Loss: 0.659960 mae: 0.648786 (2051.0445192081997 steps/sec)\n",
      "Step #7098\tEpoch   2 Batch  847/3125   Loss: 0.859361 mae: 0.721438 (2096.103948025987 steps/sec)\n",
      "Step #7099\tEpoch   2 Batch  848/3125   Loss: 0.977591 mae: 0.766167 (2008.9972027436104 steps/sec)\n",
      "Step #7100\tEpoch   2 Batch  849/3125   Loss: 0.951892 mae: 0.789258 (1846.328300391777 steps/sec)\n",
      "Step #7101\tEpoch   2 Batch  850/3125   Loss: 0.896027 mae: 0.752529 (1975.4262353761233 steps/sec)\n",
      "Step #7102\tEpoch   2 Batch  851/3125   Loss: 0.832427 mae: 0.719415 (2120.8846997906576 steps/sec)\n",
      "Step #7103\tEpoch   2 Batch  852/3125   Loss: 0.818973 mae: 0.733618 (2100.9336806251254 steps/sec)\n",
      "Step #7104\tEpoch   2 Batch  853/3125   Loss: 0.861635 mae: 0.733441 (1879.3368581414106 steps/sec)\n",
      "Step #7105\tEpoch   2 Batch  854/3125   Loss: 0.755583 mae: 0.669392 (1800.8260701558543 steps/sec)\n",
      "Step #7106\tEpoch   2 Batch  855/3125   Loss: 0.995376 mae: 0.805101 (1973.715813051744 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7107\tEpoch   2 Batch  856/3125   Loss: 0.718401 mae: 0.664620 (1902.6619005280252 steps/sec)\n",
      "Step #7108\tEpoch   2 Batch  857/3125   Loss: 0.727950 mae: 0.687211 (1987.765276816773 steps/sec)\n",
      "Step #7109\tEpoch   2 Batch  858/3125   Loss: 0.879226 mae: 0.739366 (1990.2176079261292 steps/sec)\n",
      "Step #7110\tEpoch   2 Batch  859/3125   Loss: 0.950998 mae: 0.732092 (2172.111570290733 steps/sec)\n",
      "Step #7111\tEpoch   2 Batch  860/3125   Loss: 0.812843 mae: 0.697631 (2128.4832737901916 steps/sec)\n",
      "Step #7112\tEpoch   2 Batch  861/3125   Loss: 0.666876 mae: 0.666218 (2067.6466818500003 steps/sec)\n",
      "Step #7113\tEpoch   2 Batch  862/3125   Loss: 0.804385 mae: 0.715461 (1950.839069767442 steps/sec)\n",
      "Step #7114\tEpoch   2 Batch  863/3125   Loss: 0.712475 mae: 0.663807 (2001.8250892499188 steps/sec)\n",
      "Step #7115\tEpoch   2 Batch  864/3125   Loss: 0.800263 mae: 0.709824 (1844.93142490169 steps/sec)\n",
      "Step #7116\tEpoch   2 Batch  865/3125   Loss: 0.915816 mae: 0.727533 (1846.783553633858 steps/sec)\n",
      "Step #7117\tEpoch   2 Batch  866/3125   Loss: 0.845499 mae: 0.723976 (2024.8643429564545 steps/sec)\n",
      "Step #7118\tEpoch   2 Batch  867/3125   Loss: 0.691995 mae: 0.665899 (1983.1224586288415 steps/sec)\n",
      "Step #7119\tEpoch   2 Batch  868/3125   Loss: 0.870850 mae: 0.736602 (1987.6899163088706 steps/sec)\n",
      "Step #7120\tEpoch   2 Batch  869/3125   Loss: 0.920158 mae: 0.769848 (2071.424902708362 steps/sec)\n",
      "Step #7121\tEpoch   2 Batch  870/3125   Loss: 0.745806 mae: 0.669928 (2156.676264911559 steps/sec)\n",
      "Step #7122\tEpoch   2 Batch  871/3125   Loss: 0.727197 mae: 0.674949 (2001.385694517345 steps/sec)\n",
      "Step #7123\tEpoch   2 Batch  872/3125   Loss: 0.699776 mae: 0.683005 (1919.5372209458778 steps/sec)\n",
      "Step #7124\tEpoch   2 Batch  873/3125   Loss: 0.835680 mae: 0.739407 (1706.6666666666667 steps/sec)\n",
      "Step #7125\tEpoch   2 Batch  874/3125   Loss: 0.774895 mae: 0.708137 (2053.7159085344956 steps/sec)\n",
      "Step #7126\tEpoch   2 Batch  875/3125   Loss: 0.890776 mae: 0.766995 (1864.7815687216012 steps/sec)\n",
      "Step #7127\tEpoch   2 Batch  876/3125   Loss: 0.913540 mae: 0.777856 (1955.0948110305224 steps/sec)\n",
      "Step #7128\tEpoch   2 Batch  877/3125   Loss: 0.768347 mae: 0.696496 (1849.3730048148998 steps/sec)\n",
      "Step #7129\tEpoch   2 Batch  878/3125   Loss: 0.863501 mae: 0.722079 (1947.6322705870334 steps/sec)\n",
      "Step #7130\tEpoch   2 Batch  879/3125   Loss: 0.721280 mae: 0.694977 (1818.108679821063 steps/sec)\n",
      "Step #7131\tEpoch   2 Batch  880/3125   Loss: 0.773924 mae: 0.715244 (1782.534636634084 steps/sec)\n",
      "Step #7132\tEpoch   2 Batch  881/3125   Loss: 0.755779 mae: 0.704821 (1562.3455088615895 steps/sec)\n",
      "Step #7133\tEpoch   2 Batch  882/3125   Loss: 0.775318 mae: 0.700701 (2061.346412809499 steps/sec)\n",
      "Step #7134\tEpoch   2 Batch  883/3125   Loss: 0.858112 mae: 0.709192 (1955.7329503595042 steps/sec)\n",
      "Step #7135\tEpoch   2 Batch  884/3125   Loss: 0.981173 mae: 0.798344 (2194.316326957686 steps/sec)\n",
      "Step #7136\tEpoch   2 Batch  885/3125   Loss: 0.848462 mae: 0.735881 (2043.3107614361572 steps/sec)\n",
      "Step #7137\tEpoch   2 Batch  886/3125   Loss: 0.845695 mae: 0.735734 (1966.1844535490948 steps/sec)\n",
      "Step #7138\tEpoch   2 Batch  887/3125   Loss: 0.827174 mae: 0.716983 (2036.0699029126213 steps/sec)\n",
      "Step #7139\tEpoch   2 Batch  888/3125   Loss: 0.847625 mae: 0.739116 (2026.8408895417951 steps/sec)\n",
      "Step #7140\tEpoch   2 Batch  889/3125   Loss: 0.973625 mae: 0.772737 (1996.4130000190394 steps/sec)\n",
      "Step #7141\tEpoch   2 Batch  890/3125   Loss: 0.762341 mae: 0.662589 (1887.2178827256039 steps/sec)\n",
      "Step #7142\tEpoch   2 Batch  891/3125   Loss: 0.778224 mae: 0.703978 (2048.080003125122 steps/sec)\n",
      "Step #7143\tEpoch   2 Batch  892/3125   Loss: 0.790410 mae: 0.703280 (1944.0034112608687 steps/sec)\n",
      "Step #7144\tEpoch   2 Batch  893/3125   Loss: 0.790329 mae: 0.708415 (2094.1364435213295 steps/sec)\n",
      "Step #7145\tEpoch   2 Batch  894/3125   Loss: 0.996024 mae: 0.783140 (1955.0948110305224 steps/sec)\n",
      "Step #7146\tEpoch   2 Batch  895/3125   Loss: 0.745532 mae: 0.691926 (2081.251240522409 steps/sec)\n",
      "Step #7147\tEpoch   2 Batch  896/3125   Loss: 0.834844 mae: 0.726998 (1897.8922886179967 steps/sec)\n",
      "Step #7148\tEpoch   2 Batch  897/3125   Loss: 0.848265 mae: 0.710351 (1871.3209836884748 steps/sec)\n",
      "Step #7149\tEpoch   2 Batch  898/3125   Loss: 0.701670 mae: 0.662384 (1915.2772704025717 steps/sec)\n",
      "Step #7150\tEpoch   2 Batch  899/3125   Loss: 0.906508 mae: 0.759028 (2145.292360571218 steps/sec)\n",
      "Step #7151\tEpoch   2 Batch  900/3125   Loss: 0.828733 mae: 0.722404 (1907.1433118412558 steps/sec)\n",
      "Step #7152\tEpoch   2 Batch  901/3125   Loss: 0.720446 mae: 0.680600 (2026.7625370869696 steps/sec)\n",
      "Step #7153\tEpoch   2 Batch  902/3125   Loss: 0.777798 mae: 0.716056 (2116.667675972466 steps/sec)\n",
      "Step #7154\tEpoch   2 Batch  903/3125   Loss: 0.862620 mae: 0.734348 (2119.9842301588105 steps/sec)\n",
      "Step #7155\tEpoch   2 Batch  904/3125   Loss: 0.922032 mae: 0.750485 (1908.8271166693974 steps/sec)\n",
      "Step #7156\tEpoch   2 Batch  905/3125   Loss: 0.855368 mae: 0.717350 (1727.7005206617016 steps/sec)\n",
      "Step #7157\tEpoch   2 Batch  906/3125   Loss: 0.881417 mae: 0.751077 (1985.788955382168 steps/sec)\n",
      "Step #7158\tEpoch   2 Batch  907/3125   Loss: 0.793552 mae: 0.709682 (2157.4971965885825 steps/sec)\n",
      "Step #7159\tEpoch   2 Batch  908/3125   Loss: 0.821732 mae: 0.740881 (2182.260145681582 steps/sec)\n",
      "Step #7160\tEpoch   2 Batch  909/3125   Loss: 0.812555 mae: 0.707870 (1890.7910633463764 steps/sec)\n",
      "Step #7161\tEpoch   2 Batch  910/3125   Loss: 0.895615 mae: 0.750566 (2206.436815470241 steps/sec)\n",
      "Step #7162\tEpoch   2 Batch  911/3125   Loss: 0.784850 mae: 0.694083 (1973.4186506069445 steps/sec)\n",
      "Step #7163\tEpoch   2 Batch  912/3125   Loss: 0.945378 mae: 0.763233 (1885.2160155337012 steps/sec)\n",
      "Step #7164\tEpoch   2 Batch  913/3125   Loss: 0.802650 mae: 0.716185 (1775.1263320947005 steps/sec)\n",
      "Step #7165\tEpoch   2 Batch  914/3125   Loss: 0.881321 mae: 0.753291 (1937.2333841393008 steps/sec)\n",
      "Step #7166\tEpoch   2 Batch  915/3125   Loss: 0.874869 mae: 0.730099 (2118.1000090898992 steps/sec)\n",
      "Step #7167\tEpoch   2 Batch  916/3125   Loss: 0.893105 mae: 0.748579 (2112.658916446719 steps/sec)\n",
      "Step #7168\tEpoch   2 Batch  917/3125   Loss: 0.650818 mae: 0.650894 (2229.6370324693276 steps/sec)\n",
      "Step #7169\tEpoch   2 Batch  918/3125   Loss: 0.840118 mae: 0.718995 (2026.3317068457413 steps/sec)\n",
      "Step #7170\tEpoch   2 Batch  919/3125   Loss: 0.807918 mae: 0.706827 (2179.991683991684 steps/sec)\n",
      "Step #7171\tEpoch   2 Batch  920/3125   Loss: 0.817963 mae: 0.692225 (2066.1392498596074 steps/sec)\n",
      "Step #7172\tEpoch   2 Batch  921/3125   Loss: 0.878346 mae: 0.735286 (1660.794779606253 steps/sec)\n",
      "Step #7173\tEpoch   2 Batch  922/3125   Loss: 0.784437 mae: 0.703853 (1849.307772349694 steps/sec)\n",
      "Step #7174\tEpoch   2 Batch  923/3125   Loss: 0.788843 mae: 0.690968 (2031.9271388431353 steps/sec)\n",
      "Step #7175\tEpoch   2 Batch  924/3125   Loss: 0.938360 mae: 0.733899 (2234.530963644888 steps/sec)\n",
      "Step #7176\tEpoch   2 Batch  925/3125   Loss: 0.838102 mae: 0.749260 (1969.4526877277337 steps/sec)\n",
      "Step #7177\tEpoch   2 Batch  926/3125   Loss: 0.796589 mae: 0.724176 (2068.1768424374513 steps/sec)\n",
      "Step #7178\tEpoch   2 Batch  927/3125   Loss: 0.712984 mae: 0.657739 (2037.8307469561078 steps/sec)\n",
      "Step #7179\tEpoch   2 Batch  928/3125   Loss: 0.848790 mae: 0.728210 (1799.651594854588 steps/sec)\n",
      "Step #7180\tEpoch   2 Batch  929/3125   Loss: 0.807974 mae: 0.716621 (1785.0228963450963 steps/sec)\n",
      "Step #7181\tEpoch   2 Batch  930/3125   Loss: 0.813886 mae: 0.700844 (2139.5798687982697 steps/sec)\n",
      "Step #7182\tEpoch   2 Batch  931/3125   Loss: 0.927598 mae: 0.768563 (2173.349638319481 steps/sec)\n",
      "Step #7183\tEpoch   2 Batch  932/3125   Loss: 0.856407 mae: 0.737900 (2265.525883674704 steps/sec)\n",
      "Step #7184\tEpoch   2 Batch  933/3125   Loss: 0.769316 mae: 0.708348 (2079.4352119938126 steps/sec)\n",
      "Step #7185\tEpoch   2 Batch  934/3125   Loss: 0.819966 mae: 0.734014 (2236.9621333333334 steps/sec)\n",
      "Step #7186\tEpoch   2 Batch  935/3125   Loss: 0.913663 mae: 0.766466 (2208.760677009279 steps/sec)\n",
      "Step #7187\tEpoch   2 Batch  936/3125   Loss: 0.809991 mae: 0.733738 (2239.3268625001333 steps/sec)\n",
      "Step #7188\tEpoch   2 Batch  937/3125   Loss: 0.766213 mae: 0.676727 (1828.4279449331718 steps/sec)\n",
      "Step #7189\tEpoch   2 Batch  938/3125   Loss: 0.852505 mae: 0.756479 (1988.70776554484 steps/sec)\n",
      "Step #7190\tEpoch   2 Batch  939/3125   Loss: 0.907349 mae: 0.731805 (2085.058659773315 steps/sec)\n",
      "Step #7191\tEpoch   2 Batch  940/3125   Loss: 0.814726 mae: 0.710949 (2192.1143956181795 steps/sec)\n",
      "Step #7192\tEpoch   2 Batch  941/3125   Loss: 0.807184 mae: 0.717606 (2095.8944633220067 steps/sec)\n",
      "Step #7193\tEpoch   2 Batch  942/3125   Loss: 0.788428 mae: 0.712213 (2210.926265629283 steps/sec)\n",
      "Step #7194\tEpoch   2 Batch  943/3125   Loss: 0.808482 mae: 0.704374 (2208.667628566313 steps/sec)\n",
      "Step #7195\tEpoch   2 Batch  944/3125   Loss: 0.795970 mae: 0.700904 (2149.1396890788164 steps/sec)\n",
      "Step #7196\tEpoch   2 Batch  945/3125   Loss: 0.832558 mae: 0.729273 (2369.8775030511233 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7197\tEpoch   2 Batch  946/3125   Loss: 0.910597 mae: 0.741864 (1786.3609260805124 steps/sec)\n",
      "Step #7198\tEpoch   2 Batch  947/3125   Loss: 0.708170 mae: 0.658145 (1920.7854774596544 steps/sec)\n",
      "Step #7199\tEpoch   2 Batch  948/3125   Loss: 0.647852 mae: 0.647483 (2086.6353577967047 steps/sec)\n",
      "Step #7200\tEpoch   2 Batch  949/3125   Loss: 0.711699 mae: 0.670209 (2112.9994962216624 steps/sec)\n",
      "Step #7201\tEpoch   2 Batch  950/3125   Loss: 0.846485 mae: 0.720896 (1888.7816125081058 steps/sec)\n",
      "Step #7202\tEpoch   2 Batch  951/3125   Loss: 0.909698 mae: 0.740297 (2055.003870613713 steps/sec)\n",
      "Step #7203\tEpoch   2 Batch  952/3125   Loss: 0.912265 mae: 0.737874 (2205.1375876681072 steps/sec)\n",
      "Step #7204\tEpoch   2 Batch  953/3125   Loss: 0.852833 mae: 0.740045 (2003.6994573110144 steps/sec)\n",
      "Step #7205\tEpoch   2 Batch  954/3125   Loss: 0.896688 mae: 0.756935 (2040.6663552856921 steps/sec)\n",
      "Step #7206\tEpoch   2 Batch  955/3125   Loss: 0.896828 mae: 0.745768 (1825.562993462573 steps/sec)\n",
      "Step #7207\tEpoch   2 Batch  956/3125   Loss: 0.772960 mae: 0.691748 (2089.6085132671055 steps/sec)\n",
      "Step #7208\tEpoch   2 Batch  957/3125   Loss: 0.898559 mae: 0.758033 (2197.5354178891776 steps/sec)\n",
      "Step #7209\tEpoch   2 Batch  958/3125   Loss: 0.924553 mae: 0.738995 (2075.8331931067933 steps/sec)\n",
      "Step #7210\tEpoch   2 Batch  959/3125   Loss: 0.951386 mae: 0.760911 (2258.450537379655 steps/sec)\n",
      "Step #7211\tEpoch   2 Batch  960/3125   Loss: 0.910561 mae: 0.726806 (2094.0737116438836 steps/sec)\n",
      "Step #7212\tEpoch   2 Batch  961/3125   Loss: 0.859204 mae: 0.735326 (2023.965410747375 steps/sec)\n",
      "Step #7213\tEpoch   2 Batch  962/3125   Loss: 0.733936 mae: 0.686796 (2057.867312994927 steps/sec)\n",
      "Step #7214\tEpoch   2 Batch  963/3125   Loss: 0.794274 mae: 0.696418 (1767.2877428053764 steps/sec)\n",
      "Step #7215\tEpoch   2 Batch  964/3125   Loss: 0.760701 mae: 0.692399 (2028.5465554932193 steps/sec)\n",
      "Step #7216\tEpoch   2 Batch  965/3125   Loss: 0.703630 mae: 0.680157 (2004.1781744856125 steps/sec)\n",
      "Step #7217\tEpoch   2 Batch  966/3125   Loss: 0.823092 mae: 0.721883 (1959.662106600882 steps/sec)\n",
      "Step #7218\tEpoch   2 Batch  967/3125   Loss: 0.789281 mae: 0.703223 (2063.63851058805 steps/sec)\n",
      "Step #7219\tEpoch   2 Batch  968/3125   Loss: 0.975681 mae: 0.757889 (1983.2724934273988 steps/sec)\n",
      "Step #7220\tEpoch   2 Batch  969/3125   Loss: 0.812694 mae: 0.717913 (1756.7176806641035 steps/sec)\n",
      "Step #7221\tEpoch   2 Batch  970/3125   Loss: 0.907359 mae: 0.767140 (1821.0293236542987 steps/sec)\n",
      "Step #7222\tEpoch   2 Batch  971/3125   Loss: 0.819587 mae: 0.712986 (1980.8372374187697 steps/sec)\n",
      "Step #7223\tEpoch   2 Batch  972/3125   Loss: 0.803642 mae: 0.733225 (1846.3120454985649 steps/sec)\n",
      "Step #7224\tEpoch   2 Batch  973/3125   Loss: 0.893750 mae: 0.745054 (2166.0094401008046 steps/sec)\n",
      "Step #7225\tEpoch   2 Batch  974/3125   Loss: 0.748328 mae: 0.671155 (2263.3742013469177 steps/sec)\n",
      "Step #7226\tEpoch   2 Batch  975/3125   Loss: 0.764910 mae: 0.683964 (2301.2498491183023 steps/sec)\n",
      "Step #7227\tEpoch   2 Batch  976/3125   Loss: 0.833986 mae: 0.713992 (2143.2752841141364 steps/sec)\n",
      "Step #7228\tEpoch   2 Batch  977/3125   Loss: 0.703506 mae: 0.683110 (2158.607557152121 steps/sec)\n",
      "Step #7229\tEpoch   2 Batch  978/3125   Loss: 0.654038 mae: 0.639384 (2243.951293629224 steps/sec)\n",
      "Step #7230\tEpoch   2 Batch  979/3125   Loss: 0.899217 mae: 0.743974 (2307.4532931364565 steps/sec)\n",
      "Step #7231\tEpoch   2 Batch  980/3125   Loss: 0.868918 mae: 0.730569 (2006.0377647261387 steps/sec)\n",
      "Step #7232\tEpoch   2 Batch  981/3125   Loss: 0.851349 mae: 0.764193 (1901.5060432137384 steps/sec)\n",
      "Step #7233\tEpoch   2 Batch  982/3125   Loss: 0.925425 mae: 0.734588 (2165.9870690545536 steps/sec)\n",
      "Step #7234\tEpoch   2 Batch  983/3125   Loss: 0.836726 mae: 0.716296 (2181.1253250130007 steps/sec)\n",
      "Step #7235\tEpoch   2 Batch  984/3125   Loss: 0.850571 mae: 0.742566 (2211.6957214119234 steps/sec)\n",
      "Step #7236\tEpoch   2 Batch  985/3125   Loss: 0.880512 mae: 0.743318 (1822.02606429192 steps/sec)\n",
      "Step #7237\tEpoch   2 Batch  986/3125   Loss: 0.928868 mae: 0.773845 (2023.6333986278503 steps/sec)\n",
      "Step #7238\tEpoch   2 Batch  987/3125   Loss: 0.732702 mae: 0.681134 (2046.9205692310695 steps/sec)\n",
      "Step #7239\tEpoch   2 Batch  988/3125   Loss: 0.743298 mae: 0.673027 (2153.708382114322 steps/sec)\n",
      "Step #7240\tEpoch   2 Batch  989/3125   Loss: 0.836865 mae: 0.737338 (2062.3599870189896 steps/sec)\n",
      "Step #7241\tEpoch   2 Batch  990/3125   Loss: 0.690885 mae: 0.641450 (1867.6213376079793 steps/sec)\n",
      "Step #7242\tEpoch   2 Batch  991/3125   Loss: 0.836293 mae: 0.707578 (2204.002017824113 steps/sec)\n",
      "Step #7243\tEpoch   2 Batch  992/3125   Loss: 0.856562 mae: 0.698346 (2063.130970299757 steps/sec)\n",
      "Step #7244\tEpoch   2 Batch  993/3125   Loss: 0.864763 mae: 0.729235 (2030.7072585018204 steps/sec)\n",
      "Step #7245\tEpoch   2 Batch  994/3125   Loss: 0.742617 mae: 0.675511 (2192.206054524168 steps/sec)\n",
      "Step #7246\tEpoch   2 Batch  995/3125   Loss: 0.721237 mae: 0.673001 (1953.1283178423082 steps/sec)\n",
      "Step #7247\tEpoch   2 Batch  996/3125   Loss: 0.837852 mae: 0.735142 (2148.4131374597905 steps/sec)\n",
      "Step #7248\tEpoch   2 Batch  997/3125   Loss: 0.790478 mae: 0.714228 (2255.390174653704 steps/sec)\n",
      "Step #7249\tEpoch   2 Batch  998/3125   Loss: 0.733621 mae: 0.704540 (1770.5404104789484 steps/sec)\n",
      "Step #7250\tEpoch   2 Batch  999/3125   Loss: 0.881165 mae: 0.734478 (1962.706598034628 steps/sec)\n",
      "Step #7251\tEpoch   2 Batch 1000/3125   Loss: 0.907516 mae: 0.756877 (2073.554944728984 steps/sec)\n",
      "Step #7252\tEpoch   2 Batch 1001/3125   Loss: 0.783246 mae: 0.715081 (1996.1849644958022 steps/sec)\n",
      "Step #7253\tEpoch   2 Batch 1002/3125   Loss: 0.790589 mae: 0.698572 (2325.6983797810876 steps/sec)\n",
      "Step #7254\tEpoch   2 Batch 1003/3125   Loss: 1.031494 mae: 0.821603 (2314.584023133126 steps/sec)\n",
      "Step #7255\tEpoch   2 Batch 1004/3125   Loss: 0.767686 mae: 0.689747 (2241.04980818346 steps/sec)\n",
      "Step #7256\tEpoch   2 Batch 1005/3125   Loss: 0.680049 mae: 0.670003 (2276.148302509334 steps/sec)\n",
      "Step #7257\tEpoch   2 Batch 1006/3125   Loss: 0.860255 mae: 0.703516 (2211.206005778031 steps/sec)\n",
      "Step #7258\tEpoch   2 Batch 1007/3125   Loss: 0.854335 mae: 0.748253 (1850.6786212252246 steps/sec)\n",
      "Step #7259\tEpoch   2 Batch 1008/3125   Loss: 0.774243 mae: 0.681691 (2033.2667584495161 steps/sec)\n",
      "Step #7260\tEpoch   2 Batch 1009/3125   Loss: 0.857835 mae: 0.739607 (1833.8480910823903 steps/sec)\n",
      "Step #7261\tEpoch   2 Batch 1010/3125   Loss: 0.973084 mae: 0.755378 (2132.8343181425244 steps/sec)\n",
      "Step #7262\tEpoch   2 Batch 1011/3125   Loss: 0.853333 mae: 0.731575 (2220.641895826936 steps/sec)\n",
      "Step #7263\tEpoch   2 Batch 1012/3125   Loss: 0.801335 mae: 0.718360 (2056.2732870533787 steps/sec)\n",
      "Step #7264\tEpoch   2 Batch 1013/3125   Loss: 0.813464 mae: 0.709204 (2065.020284375123 steps/sec)\n",
      "Step #7265\tEpoch   2 Batch 1014/3125   Loss: 0.856655 mae: 0.733023 (1924.7696317779655 steps/sec)\n",
      "Step #7266\tEpoch   2 Batch 1015/3125   Loss: 0.889441 mae: 0.738973 (1937.3228637413395 steps/sec)\n",
      "Step #7267\tEpoch   2 Batch 1016/3125   Loss: 0.919184 mae: 0.744843 (2041.6398134717044 steps/sec)\n",
      "Step #7268\tEpoch   2 Batch 1017/3125   Loss: 0.851654 mae: 0.755337 (1969.1198287356106 steps/sec)\n",
      "Step #7269\tEpoch   2 Batch 1018/3125   Loss: 0.757032 mae: 0.682435 (2183.873621510169 steps/sec)\n",
      "Step #7270\tEpoch   2 Batch 1019/3125   Loss: 0.879023 mae: 0.745193 (2153.5756828917642 steps/sec)\n",
      "Step #7271\tEpoch   2 Batch 1020/3125   Loss: 0.843431 mae: 0.736832 (2104.9825350303126 steps/sec)\n",
      "Step #7272\tEpoch   2 Batch 1021/3125   Loss: 0.893530 mae: 0.723944 (2229.1393403416278 steps/sec)\n",
      "Step #7273\tEpoch   2 Batch 1022/3125   Loss: 0.833610 mae: 0.720263 (2143.428623991987 steps/sec)\n",
      "Step #7274\tEpoch   2 Batch 1023/3125   Loss: 0.775325 mae: 0.704044 (2290.2173200829966 steps/sec)\n",
      "Step #7275\tEpoch   2 Batch 1024/3125   Loss: 0.834193 mae: 0.718169 (2151.9183212764865 steps/sec)\n",
      "Step #7276\tEpoch   2 Batch 1025/3125   Loss: 0.800306 mae: 0.713393 (1961.8619967070797 steps/sec)\n",
      "Step #7277\tEpoch   2 Batch 1026/3125   Loss: 0.828437 mae: 0.738248 (2291.593727804185 steps/sec)\n",
      "Step #7278\tEpoch   2 Batch 1027/3125   Loss: 0.866615 mae: 0.739976 (2159.652338681441 steps/sec)\n",
      "Step #7279\tEpoch   2 Batch 1028/3125   Loss: 0.946713 mae: 0.757414 (2015.0585160558833 steps/sec)\n",
      "Step #7280\tEpoch   2 Batch 1029/3125   Loss: 0.801995 mae: 0.707389 (2153.221898229907 steps/sec)\n",
      "Step #7281\tEpoch   2 Batch 1030/3125   Loss: 0.741067 mae: 0.677825 (1828.8903617399799 steps/sec)\n",
      "Step #7282\tEpoch   2 Batch 1031/3125   Loss: 0.785174 mae: 0.681641 (2235.745887570495 steps/sec)\n",
      "Step #7283\tEpoch   2 Batch 1032/3125   Loss: 0.938912 mae: 0.755033 (2062.8062755127135 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7284\tEpoch   2 Batch 1033/3125   Loss: 0.850785 mae: 0.738665 (1966.4056859417342 steps/sec)\n",
      "Step #7285\tEpoch   2 Batch 1034/3125   Loss: 0.823426 mae: 0.707150 (1931.1681016621392 steps/sec)\n",
      "Step #7286\tEpoch   2 Batch 1035/3125   Loss: 0.836134 mae: 0.732025 (2134.4624028009607 steps/sec)\n",
      "Step #7287\tEpoch   2 Batch 1036/3125   Loss: 0.922818 mae: 0.746735 (1981.04306590718 steps/sec)\n",
      "Step #7288\tEpoch   2 Batch 1037/3125   Loss: 0.692132 mae: 0.686524 (2112.4673885671114 steps/sec)\n",
      "Step #7289\tEpoch   2 Batch 1038/3125   Loss: 0.911119 mae: 0.751815 (2117.308779581617 steps/sec)\n",
      "Step #7290\tEpoch   2 Batch 1039/3125   Loss: 0.833649 mae: 0.702205 (2329.910009998889 steps/sec)\n",
      "Step #7291\tEpoch   2 Batch 1040/3125   Loss: 0.700699 mae: 0.679570 (2204.5812440211507 steps/sec)\n",
      "Step #7292\tEpoch   2 Batch 1041/3125   Loss: 0.992885 mae: 0.772806 (2177.524426585262 steps/sec)\n",
      "Step #7293\tEpoch   2 Batch 1042/3125   Loss: 0.811647 mae: 0.691284 (1823.3567503651666 steps/sec)\n",
      "Step #7294\tEpoch   2 Batch 1043/3125   Loss: 0.847793 mae: 0.728171 (2002.7809611123844 steps/sec)\n",
      "Step #7295\tEpoch   2 Batch 1044/3125   Loss: 0.858693 mae: 0.742754 (2222.5246134443987 steps/sec)\n",
      "Step #7296\tEpoch   2 Batch 1045/3125   Loss: 1.017092 mae: 0.799158 (2171.3469244069865 steps/sec)\n",
      "Step #7297\tEpoch   2 Batch 1046/3125   Loss: 0.774276 mae: 0.703407 (1962.174047287119 steps/sec)\n",
      "Step #7298\tEpoch   2 Batch 1047/3125   Loss: 0.805580 mae: 0.716695 (2251.0325876948177 steps/sec)\n",
      "Step #7299\tEpoch   2 Batch 1048/3125   Loss: 0.794728 mae: 0.703780 (2019.8911630146881 steps/sec)\n",
      "Step #7300\tEpoch   2 Batch 1049/3125   Loss: 0.899161 mae: 0.753551 (2018.5691047520045 steps/sec)\n",
      "Step #7301\tEpoch   2 Batch 1050/3125   Loss: 0.790443 mae: 0.719312 (1821.503826009919 steps/sec)\n",
      "Step #7302\tEpoch   2 Batch 1051/3125   Loss: 0.860616 mae: 0.749685 (1777.8199759244503 steps/sec)\n",
      "Step #7303\tEpoch   2 Batch 1052/3125   Loss: 0.738683 mae: 0.661134 (1904.6145183409167 steps/sec)\n",
      "Step #7304\tEpoch   2 Batch 1053/3125   Loss: 0.874000 mae: 0.743189 (1959.7902980123167 steps/sec)\n",
      "Step #7305\tEpoch   2 Batch 1054/3125   Loss: 0.758258 mae: 0.711124 (1960.0284122770945 steps/sec)\n",
      "Step #7306\tEpoch   2 Batch 1055/3125   Loss: 0.800030 mae: 0.723298 (2030.0389135190599 steps/sec)\n",
      "Step #7307\tEpoch   2 Batch 1056/3125   Loss: 0.984389 mae: 0.800251 (2105.701146655421 steps/sec)\n",
      "Step #7308\tEpoch   2 Batch 1057/3125   Loss: 0.860101 mae: 0.744615 (1949.098479497379 steps/sec)\n",
      "Step #7309\tEpoch   2 Batch 1058/3125   Loss: 0.848462 mae: 0.734345 (2018.8217173661917 steps/sec)\n",
      "Step #7310\tEpoch   2 Batch 1059/3125   Loss: 0.739064 mae: 0.692147 (1971.6190171763799 steps/sec)\n",
      "Step #7311\tEpoch   2 Batch 1060/3125   Loss: 0.894062 mae: 0.747371 (1930.563661637316 steps/sec)\n",
      "Step #7312\tEpoch   2 Batch 1061/3125   Loss: 0.797580 mae: 0.683364 (2081.3338626439063 steps/sec)\n",
      "Step #7313\tEpoch   2 Batch 1062/3125   Loss: 0.746344 mae: 0.687404 (2057.5442727495706 steps/sec)\n",
      "Step #7314\tEpoch   2 Batch 1063/3125   Loss: 0.648248 mae: 0.639568 (1931.7729202936598 steps/sec)\n",
      "Step #7315\tEpoch   2 Batch 1064/3125   Loss: 0.683429 mae: 0.662835 (2300.4168312053002 steps/sec)\n",
      "Step #7316\tEpoch   2 Batch 1065/3125   Loss: 0.842409 mae: 0.721836 (2090.316664507062 steps/sec)\n",
      "Step #7317\tEpoch   2 Batch 1066/3125   Loss: 0.751927 mae: 0.691766 (2232.722936717486 steps/sec)\n",
      "Step #7318\tEpoch   2 Batch 1067/3125   Loss: 0.846405 mae: 0.729240 (2055.1649794694395 steps/sec)\n",
      "Step #7319\tEpoch   2 Batch 1068/3125   Loss: 0.839757 mae: 0.745228 (1754.703972689849 steps/sec)\n",
      "Step #7320\tEpoch   2 Batch 1069/3125   Loss: 0.833898 mae: 0.713837 (1957.8874646401464 steps/sec)\n",
      "Step #7321\tEpoch   2 Batch 1070/3125   Loss: 0.827422 mae: 0.724500 (2034.1542430914576 steps/sec)\n",
      "Step #7322\tEpoch   2 Batch 1071/3125   Loss: 0.764748 mae: 0.690236 (2191.839464882943 steps/sec)\n",
      "Step #7323\tEpoch   2 Batch 1072/3125   Loss: 0.784487 mae: 0.693840 (2330.5573151080735 steps/sec)\n",
      "Step #7324\tEpoch   2 Batch 1073/3125   Loss: 0.655699 mae: 0.644073 (2317.6279465558587 steps/sec)\n",
      "Step #7325\tEpoch   2 Batch 1074/3125   Loss: 0.845275 mae: 0.724971 (2083.774169829693 steps/sec)\n",
      "Step #7326\tEpoch   2 Batch 1075/3125   Loss: 0.794649 mae: 0.718919 (2164.9585002271133 steps/sec)\n",
      "Step #7327\tEpoch   2 Batch 1076/3125   Loss: 0.767593 mae: 0.697641 (2081.0034135111537 steps/sec)\n",
      "Step #7328\tEpoch   2 Batch 1077/3125   Loss: 0.749714 mae: 0.669439 (1902.8690681426367 steps/sec)\n",
      "Step #7329\tEpoch   2 Batch 1078/3125   Loss: 0.873992 mae: 0.735248 (1870.9870815787594 steps/sec)\n",
      "Step #7330\tEpoch   2 Batch 1079/3125   Loss: 0.836728 mae: 0.718857 (1723.3702306699865 steps/sec)\n",
      "Step #7331\tEpoch   2 Batch 1080/3125   Loss: 0.860856 mae: 0.735081 (1393.808403450705 steps/sec)\n",
      "Step #7332\tEpoch   2 Batch 1081/3125   Loss: 0.973330 mae: 0.784370 (1632.7618691705206 steps/sec)\n",
      "Step #7333\tEpoch   2 Batch 1082/3125   Loss: 0.938626 mae: 0.779401 (994.9624484644909 steps/sec)\n",
      "Step #7334\tEpoch   2 Batch 1083/3125   Loss: 0.761796 mae: 0.684211 (1095.2214829592338 steps/sec)\n",
      "Step #7335\tEpoch   2 Batch 1084/3125   Loss: 0.847863 mae: 0.699512 (1210.2398374922093 steps/sec)\n",
      "Step #7336\tEpoch   2 Batch 1085/3125   Loss: 0.740086 mae: 0.679791 (1132.1697537695768 steps/sec)\n",
      "Step #7337\tEpoch   2 Batch 1086/3125   Loss: 0.909885 mae: 0.740658 (1553.4114056724666 steps/sec)\n",
      "Step #7338\tEpoch   2 Batch 1087/3125   Loss: 0.796387 mae: 0.701954 (1662.5195214954456 steps/sec)\n",
      "Step #7339\tEpoch   2 Batch 1088/3125   Loss: 0.818233 mae: 0.743172 (1472.967354048435 steps/sec)\n",
      "Step #7340\tEpoch   2 Batch 1089/3125   Loss: 0.829579 mae: 0.731269 (1167.7833214540271 steps/sec)\n",
      "Step #7341\tEpoch   2 Batch 1090/3125   Loss: 0.889914 mae: 0.730546 (1697.2192547991324 steps/sec)\n",
      "Step #7342\tEpoch   2 Batch 1091/3125   Loss: 0.916882 mae: 0.762157 (1582.183058212874 steps/sec)\n",
      "Step #7343\tEpoch   2 Batch 1092/3125   Loss: 0.799732 mae: 0.710681 (1589.2090147163578 steps/sec)\n",
      "Step #7344\tEpoch   2 Batch 1093/3125   Loss: 0.836077 mae: 0.716228 (1684.2970958622461 steps/sec)\n",
      "Step #7345\tEpoch   2 Batch 1094/3125   Loss: 0.831451 mae: 0.720253 (1635.218988062285 steps/sec)\n",
      "Step #7346\tEpoch   2 Batch 1095/3125   Loss: 0.763985 mae: 0.691063 (1707.1251231206297 steps/sec)\n",
      "Step #7347\tEpoch   2 Batch 1096/3125   Loss: 0.714044 mae: 0.665240 (1701.5570106045486 steps/sec)\n",
      "Step #7348\tEpoch   2 Batch 1097/3125   Loss: 0.958627 mae: 0.753118 (1573.9539631194602 steps/sec)\n",
      "Step #7349\tEpoch   2 Batch 1098/3125   Loss: 0.960705 mae: 0.778945 (1429.6781582553328 steps/sec)\n",
      "Step #7350\tEpoch   2 Batch 1099/3125   Loss: 0.791811 mae: 0.724125 (1492.0826455688998 steps/sec)\n",
      "Step #7351\tEpoch   2 Batch 1100/3125   Loss: 0.772733 mae: 0.691150 (1578.384399437031 steps/sec)\n",
      "Step #7352\tEpoch   2 Batch 1101/3125   Loss: 0.878726 mae: 0.728260 (1391.1640618781012 steps/sec)\n",
      "Step #7353\tEpoch   2 Batch 1102/3125   Loss: 0.974138 mae: 0.781099 (1359.0248391256732 steps/sec)\n",
      "Step #7354\tEpoch   2 Batch 1103/3125   Loss: 0.940894 mae: 0.762325 (1385.4658844669943 steps/sec)\n",
      "Step #7355\tEpoch   2 Batch 1104/3125   Loss: 0.896307 mae: 0.748511 (1475.5790717964594 steps/sec)\n",
      "Step #7356\tEpoch   2 Batch 1105/3125   Loss: 0.701942 mae: 0.659173 (1537.6592905430175 steps/sec)\n",
      "Step #7357\tEpoch   2 Batch 1106/3125   Loss: 0.895993 mae: 0.729203 (1625.283454620136 steps/sec)\n",
      "Step #7358\tEpoch   2 Batch 1107/3125   Loss: 0.786621 mae: 0.707487 (1476.846804974578 steps/sec)\n",
      "Step #7359\tEpoch   2 Batch 1108/3125   Loss: 0.954270 mae: 0.788673 (1409.1964063728421 steps/sec)\n",
      "Step #7360\tEpoch   2 Batch 1109/3125   Loss: 0.773376 mae: 0.698528 (1333.2265303657366 steps/sec)\n",
      "Step #7361\tEpoch   2 Batch 1110/3125   Loss: 0.940429 mae: 0.789981 (1532.0539138693064 steps/sec)\n",
      "Step #7362\tEpoch   2 Batch 1111/3125   Loss: 0.793154 mae: 0.710444 (1524.536202384414 steps/sec)\n",
      "Step #7363\tEpoch   2 Batch 1112/3125   Loss: 0.787606 mae: 0.707650 (1546.2187847910875 steps/sec)\n",
      "Step #7364\tEpoch   2 Batch 1113/3125   Loss: 0.883457 mae: 0.756123 (1317.4141104487176 steps/sec)\n",
      "Step #7365\tEpoch   2 Batch 1114/3125   Loss: 0.780175 mae: 0.700901 (1787.4267012136916 steps/sec)\n",
      "Step #7366\tEpoch   2 Batch 1115/3125   Loss: 0.859392 mae: 0.740915 (1422.5598795286967 steps/sec)\n",
      "Step #7367\tEpoch   2 Batch 1116/3125   Loss: 0.863405 mae: 0.722982 (1820.1758421066336 steps/sec)\n",
      "Step #7368\tEpoch   2 Batch 1117/3125   Loss: 0.753941 mae: 0.692664 (1663.7197347126582 steps/sec)\n",
      "Step #7369\tEpoch   2 Batch 1118/3125   Loss: 0.762120 mae: 0.688647 (1309.8849483454298 steps/sec)\n",
      "Step #7370\tEpoch   2 Batch 1119/3125   Loss: 0.805629 mae: 0.681489 (1397.1978120815206 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7371\tEpoch   2 Batch 1120/3125   Loss: 0.818431 mae: 0.730205 (1225.759541761646 steps/sec)\n",
      "Step #7372\tEpoch   2 Batch 1121/3125   Loss: 0.841815 mae: 0.743991 (1317.4554911987536 steps/sec)\n",
      "Step #7373\tEpoch   2 Batch 1122/3125   Loss: 0.805049 mae: 0.702988 (1157.1962235207777 steps/sec)\n",
      "Step #7374\tEpoch   2 Batch 1123/3125   Loss: 0.634099 mae: 0.630149 (1628.5649942146258 steps/sec)\n",
      "Step #7375\tEpoch   2 Batch 1124/3125   Loss: 0.845564 mae: 0.710339 (1476.88840687897 steps/sec)\n",
      "Step #7376\tEpoch   2 Batch 1125/3125   Loss: 0.765898 mae: 0.684459 (1392.7161641652278 steps/sec)\n",
      "Step #7377\tEpoch   2 Batch 1126/3125   Loss: 0.905263 mae: 0.775090 (1249.814954975357 steps/sec)\n",
      "Step #7378\tEpoch   2 Batch 1127/3125   Loss: 0.789420 mae: 0.680752 (1253.7676064758352 steps/sec)\n",
      "Step #7379\tEpoch   2 Batch 1128/3125   Loss: 0.879701 mae: 0.732205 (1404.7787148244656 steps/sec)\n",
      "Step #7380\tEpoch   2 Batch 1129/3125   Loss: 0.874711 mae: 0.754971 (1469.4273362341385 steps/sec)\n",
      "Step #7381\tEpoch   2 Batch 1130/3125   Loss: 0.768858 mae: 0.709819 (1730.2948796224484 steps/sec)\n",
      "Step #7382\tEpoch   2 Batch 1131/3125   Loss: 0.749109 mae: 0.673213 (1946.746375062659 steps/sec)\n",
      "Step #7383\tEpoch   2 Batch 1132/3125   Loss: 0.874425 mae: 0.733008 (1725.0146002813124 steps/sec)\n",
      "Step #7384\tEpoch   2 Batch 1133/3125   Loss: 0.938324 mae: 0.776749 (1121.1778731776164 steps/sec)\n",
      "Step #7385\tEpoch   2 Batch 1134/3125   Loss: 0.729462 mae: 0.687645 (1196.3422077959121 steps/sec)\n",
      "Step #7386\tEpoch   2 Batch 1135/3125   Loss: 0.768825 mae: 0.719956 (1397.2629755480045 steps/sec)\n",
      "Step #7387\tEpoch   2 Batch 1136/3125   Loss: 0.820049 mae: 0.709764 (1267.2302421279708 steps/sec)\n",
      "Step #7388\tEpoch   2 Batch 1137/3125   Loss: 0.859824 mae: 0.742093 (1374.911165016718 steps/sec)\n",
      "Step #7389\tEpoch   2 Batch 1138/3125   Loss: 0.812630 mae: 0.716657 (1353.4728227898752 steps/sec)\n",
      "Step #7390\tEpoch   2 Batch 1139/3125   Loss: 0.838149 mae: 0.749933 (1169.815699049489 steps/sec)\n",
      "Step #7391\tEpoch   2 Batch 1140/3125   Loss: 0.814530 mae: 0.714446 (1346.218088213582 steps/sec)\n",
      "Step #7392\tEpoch   2 Batch 1141/3125   Loss: 0.928515 mae: 0.751659 (1382.9625037918254 steps/sec)\n",
      "Step #7393\tEpoch   2 Batch 1142/3125   Loss: 0.646757 mae: 0.609792 (1324.3609174497321 steps/sec)\n",
      "Step #7394\tEpoch   2 Batch 1143/3125   Loss: 0.985930 mae: 0.798650 (1512.4092223592452 steps/sec)\n",
      "Step #7395\tEpoch   2 Batch 1144/3125   Loss: 0.832279 mae: 0.718559 (1475.1120145742038 steps/sec)\n",
      "Step #7396\tEpoch   2 Batch 1145/3125   Loss: 0.785469 mae: 0.691288 (1327.3533972594068 steps/sec)\n",
      "Step #7397\tEpoch   2 Batch 1146/3125   Loss: 0.907525 mae: 0.749926 (1654.4794723721166 steps/sec)\n",
      "Step #7398\tEpoch   2 Batch 1147/3125   Loss: 0.917762 mae: 0.735428 (1740.201805629315 steps/sec)\n",
      "Step #7399\tEpoch   2 Batch 1148/3125   Loss: 0.982294 mae: 0.800616 (1740.5195451904722 steps/sec)\n",
      "Step #7400\tEpoch   2 Batch 1149/3125   Loss: 0.755556 mae: 0.697425 (1267.3910678672871 steps/sec)\n",
      "Step #7401\tEpoch   2 Batch 1150/3125   Loss: 0.790809 mae: 0.736325 (1441.0741649315935 steps/sec)\n",
      "Step #7402\tEpoch   2 Batch 1151/3125   Loss: 0.763234 mae: 0.686340 (1240.5439778528373 steps/sec)\n",
      "Step #7403\tEpoch   2 Batch 1152/3125   Loss: 0.864917 mae: 0.764779 (1432.5004439951365 steps/sec)\n",
      "Step #7404\tEpoch   2 Batch 1153/3125   Loss: 0.840392 mae: 0.728494 (1308.6588622918903 steps/sec)\n",
      "Step #7405\tEpoch   2 Batch 1154/3125   Loss: 0.751632 mae: 0.696491 (1537.7945942775016 steps/sec)\n",
      "Step #7406\tEpoch   2 Batch 1155/3125   Loss: 0.872217 mae: 0.734268 (1297.7586356266786 steps/sec)\n",
      "Step #7407\tEpoch   2 Batch 1156/3125   Loss: 0.853760 mae: 0.748399 (1468.9435860078731 steps/sec)\n",
      "Step #7408\tEpoch   2 Batch 1157/3125   Loss: 0.830816 mae: 0.749047 (1176.2193206802172 steps/sec)\n",
      "Step #7409\tEpoch   2 Batch 1158/3125   Loss: 0.789543 mae: 0.713714 (1420.3439191065418 steps/sec)\n",
      "Step #7410\tEpoch   2 Batch 1159/3125   Loss: 0.861218 mae: 0.732103 (1389.7811766888892 steps/sec)\n",
      "Step #7411\tEpoch   2 Batch 1160/3125   Loss: 0.737429 mae: 0.694397 (1538.03143311845 steps/sec)\n",
      "Step #7412\tEpoch   2 Batch 1161/3125   Loss: 0.835356 mae: 0.728948 (1287.1332825964205 steps/sec)\n",
      "Step #7413\tEpoch   2 Batch 1162/3125   Loss: 0.893775 mae: 0.760519 (1761.483671549523 steps/sec)\n",
      "Step #7414\tEpoch   2 Batch 1163/3125   Loss: 0.767774 mae: 0.711681 (1890.859255252006 steps/sec)\n",
      "Step #7415\tEpoch   2 Batch 1164/3125   Loss: 0.829784 mae: 0.698049 (1528.7592943577781 steps/sec)\n",
      "Step #7416\tEpoch   2 Batch 1165/3125   Loss: 0.912402 mae: 0.747990 (1771.2432432432433 steps/sec)\n",
      "Step #7417\tEpoch   2 Batch 1166/3125   Loss: 0.822562 mae: 0.728902 (1266.0062420389856 steps/sec)\n",
      "Step #7418\tEpoch   2 Batch 1167/3125   Loss: 0.826956 mae: 0.713685 (1460.8802262563217 steps/sec)\n",
      "Step #7419\tEpoch   2 Batch 1168/3125   Loss: 0.818173 mae: 0.727397 (1142.3143124822973 steps/sec)\n",
      "Step #7420\tEpoch   2 Batch 1169/3125   Loss: 0.724528 mae: 0.662789 (1177.2163125543798 steps/sec)\n",
      "Step #7421\tEpoch   2 Batch 1170/3125   Loss: 0.840566 mae: 0.694354 (1565.482748839223 steps/sec)\n",
      "Step #7422\tEpoch   2 Batch 1171/3125   Loss: 0.811161 mae: 0.716000 (1447.7295006143947 steps/sec)\n",
      "Step #7423\tEpoch   2 Batch 1172/3125   Loss: 0.805986 mae: 0.697468 (1365.2533380205587 steps/sec)\n",
      "Step #7424\tEpoch   2 Batch 1173/3125   Loss: 0.792716 mae: 0.711108 (1602.0534131882907 steps/sec)\n",
      "Step #7425\tEpoch   2 Batch 1174/3125   Loss: 0.753375 mae: 0.665756 (1402.0926236687103 steps/sec)\n",
      "Step #7426\tEpoch   2 Batch 1175/3125   Loss: 0.763613 mae: 0.691337 (1522.7539736133197 steps/sec)\n",
      "Step #7427\tEpoch   2 Batch 1176/3125   Loss: 0.840241 mae: 0.696517 (1282.081504395564 steps/sec)\n",
      "Step #7428\tEpoch   2 Batch 1177/3125   Loss: 0.874005 mae: 0.736492 (1294.47434694583 steps/sec)\n",
      "Step #7429\tEpoch   2 Batch 1178/3125   Loss: 0.807133 mae: 0.702166 (1638.2080224973636 steps/sec)\n",
      "Step #7430\tEpoch   2 Batch 1179/3125   Loss: 0.756415 mae: 0.707763 (1515.3379818635067 steps/sec)\n",
      "Step #7431\tEpoch   2 Batch 1180/3125   Loss: 0.801392 mae: 0.706973 (1670.3985726574697 steps/sec)\n",
      "Step #7432\tEpoch   2 Batch 1181/3125   Loss: 0.818415 mae: 0.728866 (1317.8363161069776 steps/sec)\n",
      "Step #7433\tEpoch   2 Batch 1182/3125   Loss: 0.833986 mae: 0.724348 (1099.5973154362416 steps/sec)\n",
      "Step #7434\tEpoch   2 Batch 1183/3125   Loss: 0.756309 mae: 0.679030 (1603.9035433221418 steps/sec)\n",
      "Step #7435\tEpoch   2 Batch 1184/3125   Loss: 0.830270 mae: 0.739134 (1483.6380100741412 steps/sec)\n",
      "Step #7436\tEpoch   2 Batch 1185/3125   Loss: 0.786331 mae: 0.702204 (1241.71186320252 steps/sec)\n",
      "Step #7437\tEpoch   2 Batch 1186/3125   Loss: 0.877134 mae: 0.759235 (1355.7828318744262 steps/sec)\n",
      "Step #7438\tEpoch   2 Batch 1187/3125   Loss: 0.699643 mae: 0.663293 (1209.0118759368154 steps/sec)\n",
      "Step #7439\tEpoch   2 Batch 1188/3125   Loss: 0.812164 mae: 0.725602 (1554.9202206536568 steps/sec)\n",
      "Step #7440\tEpoch   2 Batch 1189/3125   Loss: 0.684633 mae: 0.654009 (1340.1274210966906 steps/sec)\n",
      "Step #7441\tEpoch   2 Batch 1190/3125   Loss: 0.868530 mae: 0.722029 (1546.7775958460563 steps/sec)\n",
      "Step #7442\tEpoch   2 Batch 1191/3125   Loss: 0.827506 mae: 0.704277 (1425.063535423547 steps/sec)\n",
      "Step #7443\tEpoch   2 Batch 1192/3125   Loss: 0.890845 mae: 0.719791 (1475.4856366923937 steps/sec)\n",
      "Step #7444\tEpoch   2 Batch 1193/3125   Loss: 0.736979 mae: 0.659085 (1336.4380803079257 steps/sec)\n",
      "Step #7445\tEpoch   2 Batch 1194/3125   Loss: 0.904033 mae: 0.743804 (1261.065544197234 steps/sec)\n",
      "Step #7446\tEpoch   2 Batch 1195/3125   Loss: 0.970603 mae: 0.794703 (1137.555598949858 steps/sec)\n",
      "Step #7447\tEpoch   2 Batch 1196/3125   Loss: 0.765862 mae: 0.686281 (1346.218088213582 steps/sec)\n",
      "Step #7448\tEpoch   2 Batch 1197/3125   Loss: 0.900551 mae: 0.769190 (1446.0823455589803 steps/sec)\n",
      "Step #7449\tEpoch   2 Batch 1198/3125   Loss: 0.788561 mae: 0.679172 (1365.902459358066 steps/sec)\n",
      "Step #7450\tEpoch   2 Batch 1199/3125   Loss: 0.819277 mae: 0.706542 (1078.559967084962 steps/sec)\n",
      "Step #7451\tEpoch   2 Batch 1200/3125   Loss: 0.931176 mae: 0.761924 (1455.1025505814437 steps/sec)\n",
      "Step #7452\tEpoch   2 Batch 1201/3125   Loss: 0.867885 mae: 0.721020 (1382.7801294984902 steps/sec)\n",
      "Step #7453\tEpoch   2 Batch 1202/3125   Loss: 0.914762 mae: 0.789739 (1331.736466105731 steps/sec)\n",
      "Step #7454\tEpoch   2 Batch 1203/3125   Loss: 0.791290 mae: 0.723420 (1503.89535884344 steps/sec)\n",
      "Step #7455\tEpoch   2 Batch 1204/3125   Loss: 0.904100 mae: 0.769462 (1632.5457928210558 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7456\tEpoch   2 Batch 1205/3125   Loss: 0.804776 mae: 0.715533 (1941.3041063428 steps/sec)\n",
      "Step #7457\tEpoch   2 Batch 1206/3125   Loss: 0.827785 mae: 0.729592 (1765.1457381174828 steps/sec)\n",
      "Step #7458\tEpoch   2 Batch 1207/3125   Loss: 0.876778 mae: 0.714655 (1681.2994051341254 steps/sec)\n",
      "Step #7459\tEpoch   2 Batch 1208/3125   Loss: 0.847008 mae: 0.714779 (1382.5066582285158 steps/sec)\n",
      "Step #7460\tEpoch   2 Batch 1209/3125   Loss: 0.853212 mae: 0.716694 (1228.6752126737128 steps/sec)\n",
      "Step #7461\tEpoch   2 Batch 1210/3125   Loss: 0.764744 mae: 0.684811 (1465.3716617521695 steps/sec)\n",
      "Step #7462\tEpoch   2 Batch 1211/3125   Loss: 0.709953 mae: 0.665507 (1302.5222505853783 steps/sec)\n",
      "Step #7463\tEpoch   2 Batch 1212/3125   Loss: 0.860897 mae: 0.723082 (1237.1994242159662 steps/sec)\n",
      "Step #7464\tEpoch   2 Batch 1213/3125   Loss: 0.885513 mae: 0.739206 (1515.1518654451927 steps/sec)\n",
      "Step #7465\tEpoch   2 Batch 1214/3125   Loss: 0.884619 mae: 0.767492 (1298.7713040031708 steps/sec)\n",
      "Step #7466\tEpoch   2 Batch 1215/3125   Loss: 0.667368 mae: 0.650527 (1239.1585913495628 steps/sec)\n",
      "Step #7467\tEpoch   2 Batch 1216/3125   Loss: 0.835106 mae: 0.699457 (1509.839523682676 steps/sec)\n",
      "Step #7468\tEpoch   2 Batch 1217/3125   Loss: 0.785909 mae: 0.694643 (1228.862403168911 steps/sec)\n",
      "Step #7469\tEpoch   2 Batch 1218/3125   Loss: 0.811356 mae: 0.681638 (1209.283766095225 steps/sec)\n",
      "Step #7470\tEpoch   2 Batch 1219/3125   Loss: 0.986182 mae: 0.799049 (1376.156228673421 steps/sec)\n",
      "Step #7471\tEpoch   2 Batch 1220/3125   Loss: 0.879593 mae: 0.730324 (1646.6848833191998 steps/sec)\n",
      "Step #7472\tEpoch   2 Batch 1221/3125   Loss: 0.840854 mae: 0.733975 (1066.460543311331 steps/sec)\n",
      "Step #7473\tEpoch   2 Batch 1222/3125   Loss: 0.729611 mae: 0.653850 (1331.237701067706 steps/sec)\n",
      "Step #7474\tEpoch   2 Batch 1223/3125   Loss: 0.594260 mae: 0.641670 (1005.0040494366731 steps/sec)\n",
      "Step #7475\tEpoch   2 Batch 1224/3125   Loss: 0.768708 mae: 0.699246 (1408.9313187366893 steps/sec)\n",
      "Step #7476\tEpoch   2 Batch 1225/3125   Loss: 0.840368 mae: 0.729165 (1325.206159834693 steps/sec)\n",
      "Step #7477\tEpoch   2 Batch 1226/3125   Loss: 0.835609 mae: 0.723573 (1476.0152587942175 steps/sec)\n",
      "Step #7478\tEpoch   2 Batch 1227/3125   Loss: 0.899004 mae: 0.762179 (1286.943714867817 steps/sec)\n",
      "Step #7479\tEpoch   2 Batch 1228/3125   Loss: 0.888927 mae: 0.758285 (1574.2966099150226 steps/sec)\n",
      "Step #7480\tEpoch   2 Batch 1229/3125   Loss: 0.738864 mae: 0.689542 (1222.6081582920872 steps/sec)\n",
      "Step #7481\tEpoch   2 Batch 1230/3125   Loss: 0.717661 mae: 0.665335 (1419.9207826940656 steps/sec)\n",
      "Step #7482\tEpoch   2 Batch 1231/3125   Loss: 0.782017 mae: 0.701326 (1219.4233016821822 steps/sec)\n",
      "Step #7483\tEpoch   2 Batch 1232/3125   Loss: 0.827903 mae: 0.695141 (1595.107777963704 steps/sec)\n",
      "Step #7484\tEpoch   2 Batch 1233/3125   Loss: 0.781008 mae: 0.688697 (1698.4288444718002 steps/sec)\n",
      "Step #7485\tEpoch   2 Batch 1234/3125   Loss: 0.847889 mae: 0.713292 (1356.7213326863982 steps/sec)\n",
      "Step #7486\tEpoch   2 Batch 1235/3125   Loss: 0.928808 mae: 0.761159 (1053.0091685998052 steps/sec)\n",
      "Step #7487\tEpoch   2 Batch 1236/3125   Loss: 0.771754 mae: 0.705387 (1325.1810380780264 steps/sec)\n",
      "Step #7488\tEpoch   2 Batch 1237/3125   Loss: 0.948529 mae: 0.789471 (1279.71881178452 steps/sec)\n",
      "Step #7489\tEpoch   2 Batch 1238/3125   Loss: 0.950071 mae: 0.751394 (1328.1940530099116 steps/sec)\n",
      "Step #7490\tEpoch   2 Batch 1239/3125   Loss: 0.864194 mae: 0.744150 (1275.0582155342756 steps/sec)\n",
      "Step #7491\tEpoch   2 Batch 1240/3125   Loss: 0.771037 mae: 0.690893 (1364.1876288793917 steps/sec)\n",
      "Step #7492\tEpoch   2 Batch 1241/3125   Loss: 0.846044 mae: 0.730794 (1152.420622273022 steps/sec)\n",
      "Step #7493\tEpoch   2 Batch 1242/3125   Loss: 0.793919 mae: 0.729285 (1328.9347113879612 steps/sec)\n",
      "Step #7494\tEpoch   2 Batch 1243/3125   Loss: 0.869714 mae: 0.726591 (1575.5976619434718 steps/sec)\n",
      "Step #7495\tEpoch   2 Batch 1244/3125   Loss: 0.914765 mae: 0.728852 (1414.6241433273974 steps/sec)\n",
      "Step #7496\tEpoch   2 Batch 1245/3125   Loss: 0.981478 mae: 0.787421 (1694.5587355968908 steps/sec)\n",
      "Step #7497\tEpoch   2 Batch 1246/3125   Loss: 0.812592 mae: 0.707483 (1775.186436086916 steps/sec)\n",
      "Step #7498\tEpoch   2 Batch 1247/3125   Loss: 0.961011 mae: 0.799309 (1527.400911858531 steps/sec)\n",
      "Step #7499\tEpoch   2 Batch 1248/3125   Loss: 0.823461 mae: 0.721401 (956.4244994755325 steps/sec)\n",
      "Step #7500\tEpoch   2 Batch 1249/3125   Loss: 0.930973 mae: 0.763674 (1218.2969477976972 steps/sec)\n",
      "Step #7501\tEpoch   2 Batch 1250/3125   Loss: 0.827326 mae: 0.709661 (1261.0048825071553 steps/sec)\n",
      "Step #7502\tEpoch   2 Batch 1251/3125   Loss: 0.866983 mae: 0.748863 (1548.227824738843 steps/sec)\n",
      "Step #7503\tEpoch   2 Batch 1252/3125   Loss: 0.783572 mae: 0.703092 (1473.4848165479252 steps/sec)\n",
      "Step #7504\tEpoch   2 Batch 1253/3125   Loss: 0.987876 mae: 0.787272 (1433.3326498670658 steps/sec)\n",
      "Step #7505\tEpoch   2 Batch 1254/3125   Loss: 0.928024 mae: 0.755015 (1494.7306900066285 steps/sec)\n",
      "Step #7506\tEpoch   2 Batch 1255/3125   Loss: 0.852183 mae: 0.724298 (1431.6105646157732 steps/sec)\n",
      "Step #7507\tEpoch   2 Batch 1256/3125   Loss: 0.838356 mae: 0.734366 (1649.4046214587954 steps/sec)\n",
      "Step #7508\tEpoch   2 Batch 1257/3125   Loss: 1.078058 mae: 0.784254 (1410.030188729989 steps/sec)\n",
      "Step #7509\tEpoch   2 Batch 1258/3125   Loss: 0.813376 mae: 0.721924 (1570.3945545629497 steps/sec)\n",
      "Step #7510\tEpoch   2 Batch 1259/3125   Loss: 0.836572 mae: 0.728266 (1451.0752539370624 steps/sec)\n",
      "Step #7511\tEpoch   2 Batch 1260/3125   Loss: 0.817721 mae: 0.738001 (1384.6151814658558 steps/sec)\n",
      "Step #7512\tEpoch   2 Batch 1261/3125   Loss: 0.778183 mae: 0.694902 (1378.471893568912 steps/sec)\n",
      "Step #7513\tEpoch   2 Batch 1262/3125   Loss: 0.857423 mae: 0.730342 (1228.4952755859385 steps/sec)\n",
      "Step #7514\tEpoch   2 Batch 1263/3125   Loss: 0.724667 mae: 0.677965 (1327.8744784180633 steps/sec)\n",
      "Step #7515\tEpoch   2 Batch 1264/3125   Loss: 0.841796 mae: 0.714862 (1054.89481997163 steps/sec)\n",
      "Step #7516\tEpoch   2 Batch 1265/3125   Loss: 0.788212 mae: 0.706172 (1294.865984601041 steps/sec)\n",
      "Step #7517\tEpoch   2 Batch 1266/3125   Loss: 0.807822 mae: 0.715859 (1185.6220531201593 steps/sec)\n",
      "Step #7518\tEpoch   2 Batch 1267/3125   Loss: 0.805138 mae: 0.711506 (1403.9041371000135 steps/sec)\n",
      "Step #7519\tEpoch   2 Batch 1268/3125   Loss: 0.942280 mae: 0.749624 (1409.2153450210662 steps/sec)\n",
      "Step #7520\tEpoch   2 Batch 1269/3125   Loss: 0.825357 mae: 0.693017 (1598.2075766466746 steps/sec)\n",
      "Step #7521\tEpoch   2 Batch 1270/3125   Loss: 0.834639 mae: 0.725979 (1149.9813559693798 steps/sec)\n",
      "Step #7522\tEpoch   2 Batch 1271/3125   Loss: 0.956173 mae: 0.766454 (1488.8412443737666 steps/sec)\n",
      "Step #7523\tEpoch   2 Batch 1272/3125   Loss: 0.890921 mae: 0.749309 (1468.593837535014 steps/sec)\n",
      "Step #7524\tEpoch   2 Batch 1273/3125   Loss: 0.763396 mae: 0.705287 (1199.6133143422626 steps/sec)\n",
      "Step #7525\tEpoch   2 Batch 1274/3125   Loss: 0.940656 mae: 0.749852 (900.169547544136 steps/sec)\n",
      "Step #7526\tEpoch   2 Batch 1275/3125   Loss: 0.837347 mae: 0.731950 (1144.9773696365492 steps/sec)\n",
      "Step #7527\tEpoch   2 Batch 1276/3125   Loss: 0.873650 mae: 0.737950 (1186.2457505840296 steps/sec)\n",
      "Step #7528\tEpoch   2 Batch 1277/3125   Loss: 0.923015 mae: 0.779777 (1162.2756117161305 steps/sec)\n",
      "Step #7529\tEpoch   2 Batch 1278/3125   Loss: 0.884530 mae: 0.742380 (1255.336138729431 steps/sec)\n",
      "Step #7530\tEpoch   2 Batch 1279/3125   Loss: 0.959951 mae: 0.769147 (1349.2755488071648 steps/sec)\n",
      "Step #7531\tEpoch   2 Batch 1280/3125   Loss: 0.865102 mae: 0.759732 (1178.8309228166227 steps/sec)\n",
      "Step #7532\tEpoch   2 Batch 1281/3125   Loss: 0.909554 mae: 0.734571 (1347.8533600699266 steps/sec)\n",
      "Step #7533\tEpoch   2 Batch 1282/3125   Loss: 0.758557 mae: 0.714279 (1370.4636497304361 steps/sec)\n",
      "Step #7534\tEpoch   2 Batch 1283/3125   Loss: 0.714418 mae: 0.676106 (1409.1301251125476 steps/sec)\n",
      "Step #7535\tEpoch   2 Batch 1284/3125   Loss: 0.874792 mae: 0.747772 (1529.9861384693952 steps/sec)\n",
      "Step #7536\tEpoch   2 Batch 1285/3125   Loss: 0.872657 mae: 0.752486 (1661.123652464574 steps/sec)\n",
      "Step #7537\tEpoch   2 Batch 1286/3125   Loss: 0.756284 mae: 0.703364 (1039.331149425857 steps/sec)\n",
      "Step #7538\tEpoch   2 Batch 1287/3125   Loss: 0.768645 mae: 0.694073 (1515.1956534304375 steps/sec)\n",
      "Step #7539\tEpoch   2 Batch 1288/3125   Loss: 0.937994 mae: 0.753435 (1472.6673922966188 steps/sec)\n",
      "Step #7540\tEpoch   2 Batch 1289/3125   Loss: 0.846072 mae: 0.718025 (1362.211598420287 steps/sec)\n",
      "Step #7541\tEpoch   2 Batch 1290/3125   Loss: 0.741163 mae: 0.706609 (1354.4256217829072 steps/sec)\n",
      "Step #7542\tEpoch   2 Batch 1291/3125   Loss: 0.909150 mae: 0.755493 (1578.4913215613662 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7543\tEpoch   2 Batch 1292/3125   Loss: 0.817641 mae: 0.688336 (1577.5295436252718 steps/sec)\n",
      "Step #7544\tEpoch   2 Batch 1293/3125   Loss: 0.857783 mae: 0.731020 (1229.5540624523633 steps/sec)\n",
      "Step #7545\tEpoch   2 Batch 1294/3125   Loss: 0.778842 mae: 0.701273 (1422.1643542065074 steps/sec)\n",
      "Step #7546\tEpoch   2 Batch 1295/3125   Loss: 0.800538 mae: 0.724058 (1643.0467415659914 steps/sec)\n",
      "Step #7547\tEpoch   2 Batch 1296/3125   Loss: 0.722870 mae: 0.684740 (1456.1533120399945 steps/sec)\n",
      "Step #7548\tEpoch   2 Batch 1297/3125   Loss: 0.872756 mae: 0.724112 (1487.510639505192 steps/sec)\n",
      "Step #7549\tEpoch   2 Batch 1298/3125   Loss: 0.742296 mae: 0.676213 (1104.6072002317558 steps/sec)\n",
      "Step #7550\tEpoch   2 Batch 1299/3125   Loss: 0.805794 mae: 0.711266 (1184.0489619855803 steps/sec)\n",
      "Step #7551\tEpoch   2 Batch 1300/3125   Loss: 0.754001 mae: 0.674827 (1220.1682627971654 steps/sec)\n",
      "Step #7552\tEpoch   2 Batch 1301/3125   Loss: 0.764947 mae: 0.692690 (1346.3650137387328 steps/sec)\n",
      "Step #7553\tEpoch   2 Batch 1302/3125   Loss: 0.765670 mae: 0.693584 (1366.9084817791336 steps/sec)\n",
      "Step #7554\tEpoch   2 Batch 1303/3125   Loss: 0.724311 mae: 0.668895 (1334.0491851248712 steps/sec)\n",
      "Step #7555\tEpoch   2 Batch 1304/3125   Loss: 0.737526 mae: 0.696585 (961.3128280351126 steps/sec)\n",
      "Step #7556\tEpoch   2 Batch 1305/3125   Loss: 0.724424 mae: 0.680332 (1234.0761576348882 steps/sec)\n",
      "Step #7557\tEpoch   2 Batch 1306/3125   Loss: 0.707096 mae: 0.656599 (1277.761733291902 steps/sec)\n",
      "Step #7558\tEpoch   2 Batch 1307/3125   Loss: 1.014667 mae: 0.786114 (1572.184030406849 steps/sec)\n",
      "Step #7559\tEpoch   2 Batch 1308/3125   Loss: 0.845606 mae: 0.702437 (1552.8477919616148 steps/sec)\n",
      "Step #7560\tEpoch   2 Batch 1309/3125   Loss: 0.857417 mae: 0.748733 (1049.8883604505631 steps/sec)\n",
      "Step #7561\tEpoch   2 Batch 1310/3125   Loss: 0.790624 mae: 0.694775 (1434.391436681372 steps/sec)\n",
      "Step #7562\tEpoch   2 Batch 1311/3125   Loss: 0.829225 mae: 0.731510 (1168.6618482131414 steps/sec)\n",
      "Step #7563\tEpoch   2 Batch 1312/3125   Loss: 0.815397 mae: 0.722414 (1269.3856304097815 steps/sec)\n",
      "Step #7564\tEpoch   2 Batch 1313/3125   Loss: 0.793819 mae: 0.722092 (1236.2804626431023 steps/sec)\n",
      "Step #7565\tEpoch   2 Batch 1314/3125   Loss: 0.887192 mae: 0.769724 (1067.073722987997 steps/sec)\n",
      "Step #7566\tEpoch   2 Batch 1315/3125   Loss: 0.919045 mae: 0.766873 (1484.2995562287226 steps/sec)\n",
      "Step #7567\tEpoch   2 Batch 1316/3125   Loss: 0.811006 mae: 0.698610 (1257.7000809619478 steps/sec)\n",
      "Step #7568\tEpoch   2 Batch 1317/3125   Loss: 0.700879 mae: 0.667020 (1243.34617892927 steps/sec)\n",
      "Step #7569\tEpoch   2 Batch 1318/3125   Loss: 0.858990 mae: 0.718383 (1879.623206331281 steps/sec)\n",
      "Step #7570\tEpoch   2 Batch 1319/3125   Loss: 0.844386 mae: 0.722234 (1028.9790931705666 steps/sec)\n",
      "Step #7571\tEpoch   2 Batch 1320/3125   Loss: 0.781307 mae: 0.674035 (1342.7530525089157 steps/sec)\n",
      "Step #7572\tEpoch   2 Batch 1321/3125   Loss: 0.854183 mae: 0.752388 (1271.3248220759224 steps/sec)\n",
      "Step #7573\tEpoch   2 Batch 1322/3125   Loss: 0.679093 mae: 0.649500 (1013.7290634441088 steps/sec)\n",
      "Step #7574\tEpoch   2 Batch 1323/3125   Loss: 0.834570 mae: 0.706549 (1006.5959172702444 steps/sec)\n",
      "Step #7575\tEpoch   2 Batch 1324/3125   Loss: 0.779962 mae: 0.710608 (932.0882686281917 steps/sec)\n",
      "Step #7576\tEpoch   2 Batch 1325/3125   Loss: 0.773329 mae: 0.678470 (1261.1944696691785 steps/sec)\n",
      "Step #7577\tEpoch   2 Batch 1326/3125   Loss: 0.780991 mae: 0.690295 (1278.727828149485 steps/sec)\n",
      "Step #7578\tEpoch   2 Batch 1327/3125   Loss: 0.717871 mae: 0.667837 (1520.821488658119 steps/sec)\n",
      "Step #7579\tEpoch   2 Batch 1328/3125   Loss: 0.871914 mae: 0.738913 (1532.4233478502324 steps/sec)\n",
      "Step #7580\tEpoch   2 Batch 1329/3125   Loss: 0.748767 mae: 0.702818 (1424.821485593157 steps/sec)\n",
      "Step #7581\tEpoch   2 Batch 1330/3125   Loss: 0.934079 mae: 0.767759 (1105.8770176706023 steps/sec)\n",
      "Step #7582\tEpoch   2 Batch 1331/3125   Loss: 0.644578 mae: 0.620870 (1305.5223889888382 steps/sec)\n",
      "Step #7583\tEpoch   2 Batch 1332/3125   Loss: 0.847772 mae: 0.737917 (1430.1266357976283 steps/sec)\n",
      "Step #7584\tEpoch   2 Batch 1333/3125   Loss: 0.717206 mae: 0.653950 (1267.7434955024664 steps/sec)\n",
      "Step #7585\tEpoch   2 Batch 1334/3125   Loss: 0.691773 mae: 0.653960 (1344.914449888413 steps/sec)\n",
      "Step #7586\tEpoch   2 Batch 1335/3125   Loss: 0.835148 mae: 0.737510 (1439.1457707140994 steps/sec)\n",
      "Step #7587\tEpoch   2 Batch 1336/3125   Loss: 0.782717 mae: 0.716581 (1105.3640795888787 steps/sec)\n",
      "Step #7588\tEpoch   2 Batch 1337/3125   Loss: 0.702364 mae: 0.658174 (1355.7127157540888 steps/sec)\n",
      "Step #7589\tEpoch   2 Batch 1338/3125   Loss: 0.777657 mae: 0.725335 (1274.8256891887784 steps/sec)\n",
      "Step #7590\tEpoch   2 Batch 1339/3125   Loss: 0.893139 mae: 0.761831 (1520.4025113460061 steps/sec)\n",
      "Step #7591\tEpoch   2 Batch 1340/3125   Loss: 0.729988 mae: 0.692934 (1500.3233652883102 steps/sec)\n",
      "Step #7592\tEpoch   2 Batch 1341/3125   Loss: 0.877761 mae: 0.728651 (1829.8957288076435 steps/sec)\n",
      "Step #7593\tEpoch   2 Batch 1342/3125   Loss: 0.840528 mae: 0.726599 (1908.045600531339 steps/sec)\n",
      "Step #7594\tEpoch   2 Batch 1343/3125   Loss: 0.714194 mae: 0.664945 (1672.2366637429232 steps/sec)\n",
      "Step #7595\tEpoch   2 Batch 1344/3125   Loss: 0.747650 mae: 0.678377 (1672.1299972890654 steps/sec)\n",
      "Step #7596\tEpoch   2 Batch 1345/3125   Loss: 0.847381 mae: 0.738071 (1879.3368581414106 steps/sec)\n",
      "Step #7597\tEpoch   2 Batch 1346/3125   Loss: 0.762252 mae: 0.702014 (1890.5183449021906 steps/sec)\n",
      "Step #7598\tEpoch   2 Batch 1347/3125   Loss: 0.794858 mae: 0.713858 (1728.1418671149456 steps/sec)\n",
      "Step #7599\tEpoch   2 Batch 1348/3125   Loss: 0.781529 mae: 0.726336 (1522.4444460576847 steps/sec)\n",
      "Step #7600\tEpoch   2 Batch 1349/3125   Loss: 0.851095 mae: 0.750266 (1146.504698852485 steps/sec)\n",
      "Step #7601\tEpoch   2 Batch 1350/3125   Loss: 0.784249 mae: 0.676124 (1065.073996201156 steps/sec)\n",
      "Step #7602\tEpoch   2 Batch 1351/3125   Loss: 0.857305 mae: 0.738484 (1569.3251717377314 steps/sec)\n",
      "Step #7603\tEpoch   2 Batch 1352/3125   Loss: 0.830341 mae: 0.727707 (1505.7958527198575 steps/sec)\n",
      "Step #7604\tEpoch   2 Batch 1353/3125   Loss: 0.678378 mae: 0.648205 (1366.080408556763 steps/sec)\n",
      "Step #7605\tEpoch   2 Batch 1354/3125   Loss: 0.717925 mae: 0.670093 (1301.536036343551 steps/sec)\n",
      "Step #7606\tEpoch   2 Batch 1355/3125   Loss: 0.845734 mae: 0.719812 (1476.025647342009 steps/sec)\n",
      "Step #7607\tEpoch   2 Batch 1356/3125   Loss: 0.894252 mae: 0.746801 (1303.2345465731207 steps/sec)\n",
      "Step #7608\tEpoch   2 Batch 1357/3125   Loss: 0.768016 mae: 0.701225 (1522.941962470226 steps/sec)\n",
      "Step #7609\tEpoch   2 Batch 1358/3125   Loss: 1.031958 mae: 0.793153 (1733.8856231035709 steps/sec)\n",
      "Step #7610\tEpoch   2 Batch 1359/3125   Loss: 1.003396 mae: 0.798563 (1712.967621785866 steps/sec)\n",
      "Step #7611\tEpoch   2 Batch 1360/3125   Loss: 0.721550 mae: 0.679571 (1970.3778867655074 steps/sec)\n",
      "Step #7612\tEpoch   2 Batch 1361/3125   Loss: 0.790775 mae: 0.716482 (1805.8347397788723 steps/sec)\n",
      "Step #7613\tEpoch   2 Batch 1362/3125   Loss: 0.827130 mae: 0.721310 (1348.6247853739155 steps/sec)\n",
      "Step #7614\tEpoch   2 Batch 1363/3125   Loss: 0.987973 mae: 0.777419 (1495.711463437248 steps/sec)\n",
      "Step #7615\tEpoch   2 Batch 1364/3125   Loss: 0.832273 mae: 0.720457 (1650.5989579234026 steps/sec)\n",
      "Step #7616\tEpoch   2 Batch 1365/3125   Loss: 0.740089 mae: 0.697521 (1219.4020304450466 steps/sec)\n",
      "Step #7617\tEpoch   2 Batch 1366/3125   Loss: 0.914921 mae: 0.756415 (1483.9949617175448 steps/sec)\n",
      "Step #7618\tEpoch   2 Batch 1367/3125   Loss: 0.811913 mae: 0.716915 (1470.663889648595 steps/sec)\n",
      "Step #7619\tEpoch   2 Batch 1368/3125   Loss: 0.898789 mae: 0.741104 (1416.037812288994 steps/sec)\n",
      "Step #7620\tEpoch   2 Batch 1369/3125   Loss: 0.731145 mae: 0.687589 (1200.018310826276 steps/sec)\n",
      "Step #7621\tEpoch   2 Batch 1370/3125   Loss: 0.749546 mae: 0.702636 (1222.5012533081506 steps/sec)\n",
      "Step #7622\tEpoch   2 Batch 1371/3125   Loss: 0.816004 mae: 0.724562 (1403.3498618165272 steps/sec)\n",
      "Step #7623\tEpoch   2 Batch 1372/3125   Loss: 0.931156 mae: 0.755646 (1616.4017819981193 steps/sec)\n",
      "Step #7624\tEpoch   2 Batch 1373/3125   Loss: 0.976037 mae: 0.777114 (1863.7872041663334 steps/sec)\n",
      "Step #7625\tEpoch   2 Batch 1374/3125   Loss: 0.782317 mae: 0.671823 (1838.7844034686236 steps/sec)\n",
      "Step #7626\tEpoch   2 Batch 1375/3125   Loss: 0.862219 mae: 0.724817 (1656.5705077569592 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7627\tEpoch   2 Batch 1376/3125   Loss: 0.818695 mae: 0.696725 (1079.4760005147343 steps/sec)\n",
      "Step #7628\tEpoch   2 Batch 1377/3125   Loss: 0.883639 mae: 0.746484 (1234.2286776995693 steps/sec)\n",
      "Step #7629\tEpoch   2 Batch 1378/3125   Loss: 0.781772 mae: 0.693055 (1434.7643448931701 steps/sec)\n",
      "Step #7630\tEpoch   2 Batch 1379/3125   Loss: 0.868054 mae: 0.742074 (1361.3538549422585 steps/sec)\n",
      "Step #7631\tEpoch   2 Batch 1380/3125   Loss: 0.791315 mae: 0.698977 (1536.57771720813 steps/sec)\n",
      "Step #7632\tEpoch   2 Batch 1381/3125   Loss: 0.867785 mae: 0.767073 (1403.4343839925048 steps/sec)\n",
      "Step #7633\tEpoch   2 Batch 1382/3125   Loss: 0.649639 mae: 0.649749 (1066.1189879466624 steps/sec)\n",
      "Step #7634\tEpoch   2 Batch 1383/3125   Loss: 0.864011 mae: 0.734163 (1394.9765857811835 steps/sec)\n",
      "Step #7635\tEpoch   2 Batch 1384/3125   Loss: 0.896676 mae: 0.747387 (1356.782775218674 steps/sec)\n",
      "Step #7636\tEpoch   2 Batch 1385/3125   Loss: 0.715846 mae: 0.659307 (1627.3518068736469 steps/sec)\n",
      "Step #7637\tEpoch   2 Batch 1386/3125   Loss: 0.877641 mae: 0.752336 (1929.2316750073594 steps/sec)\n",
      "Step #7638\tEpoch   2 Batch 1387/3125   Loss: 0.767556 mae: 0.690199 (1733.0402446078836 steps/sec)\n",
      "Step #7639\tEpoch   2 Batch 1388/3125   Loss: 0.788031 mae: 0.716887 (1314.655750653519 steps/sec)\n",
      "Step #7640\tEpoch   2 Batch 1389/3125   Loss: 0.785070 mae: 0.702931 (1132.7568233254292 steps/sec)\n",
      "Step #7641\tEpoch   2 Batch 1390/3125   Loss: 0.846002 mae: 0.718255 (1328.5221974457734 steps/sec)\n",
      "Step #7642\tEpoch   2 Batch 1391/3125   Loss: 0.803568 mae: 0.692707 (1162.4946646637213 steps/sec)\n",
      "Step #7643\tEpoch   2 Batch 1392/3125   Loss: 0.766313 mae: 0.720324 (1205.9251081056216 steps/sec)\n",
      "Step #7644\tEpoch   2 Batch 1393/3125   Loss: 0.769586 mae: 0.688552 (1186.4873580646438 steps/sec)\n",
      "Step #7645\tEpoch   2 Batch 1394/3125   Loss: 0.861858 mae: 0.731054 (1461.1245035881 steps/sec)\n",
      "Step #7646\tEpoch   2 Batch 1395/3125   Loss: 0.815975 mae: 0.711477 (1584.1789984967631 steps/sec)\n",
      "Step #7647\tEpoch   2 Batch 1396/3125   Loss: 0.786532 mae: 0.713436 (1582.0278965910034 steps/sec)\n",
      "Step #7648\tEpoch   2 Batch 1397/3125   Loss: 0.960562 mae: 0.779543 (1458.4013685864893 steps/sec)\n",
      "Step #7649\tEpoch   2 Batch 1398/3125   Loss: 0.866600 mae: 0.745698 (1576.948296086865 steps/sec)\n",
      "Step #7650\tEpoch   2 Batch 1399/3125   Loss: 0.832873 mae: 0.727155 (1831.63778647289 steps/sec)\n",
      "Step #7651\tEpoch   2 Batch 1400/3125   Loss: 0.875326 mae: 0.745983 (1522.95302208376 steps/sec)\n",
      "Step #7652\tEpoch   2 Batch 1401/3125   Loss: 0.800872 mae: 0.718801 (1337.648537112751 steps/sec)\n",
      "Step #7653\tEpoch   2 Batch 1402/3125   Loss: 0.718378 mae: 0.693991 (1424.0571482894898 steps/sec)\n",
      "Step #7654\tEpoch   2 Batch 1403/3125   Loss: 0.755760 mae: 0.706784 (1423.1294363540126 steps/sec)\n",
      "Step #7655\tEpoch   2 Batch 1404/3125   Loss: 0.862112 mae: 0.744804 (1288.4618220245263 steps/sec)\n",
      "Step #7656\tEpoch   2 Batch 1405/3125   Loss: 0.836199 mae: 0.735902 (1467.997592014448 steps/sec)\n",
      "Step #7657\tEpoch   2 Batch 1406/3125   Loss: 0.938011 mae: 0.774256 (1227.3880243235808 steps/sec)\n",
      "Step #7658\tEpoch   2 Batch 1407/3125   Loss: 0.916642 mae: 0.774172 (1344.3368226719401 steps/sec)\n",
      "Step #7659\tEpoch   2 Batch 1408/3125   Loss: 0.896442 mae: 0.752902 (1439.1062679274803 steps/sec)\n",
      "Step #7660\tEpoch   2 Batch 1409/3125   Loss: 0.804675 mae: 0.687295 (1664.9613363184553 steps/sec)\n",
      "Step #7661\tEpoch   2 Batch 1410/3125   Loss: 0.863339 mae: 0.714907 (1840.2688686281908 steps/sec)\n",
      "Step #7662\tEpoch   2 Batch 1411/3125   Loss: 0.642702 mae: 0.642523 (2039.5549677118183 steps/sec)\n",
      "Step #7663\tEpoch   2 Batch 1412/3125   Loss: 0.953687 mae: 0.765790 (1773.8200764624285 steps/sec)\n",
      "Step #7664\tEpoch   2 Batch 1413/3125   Loss: 0.758252 mae: 0.686762 (1097.0663318685918 steps/sec)\n",
      "Step #7665\tEpoch   2 Batch 1414/3125   Loss: 0.734934 mae: 0.671963 (1537.2309857503078 steps/sec)\n",
      "Step #7666\tEpoch   2 Batch 1415/3125   Loss: 0.803246 mae: 0.718065 (1467.473707044343 steps/sec)\n",
      "Step #7667\tEpoch   2 Batch 1416/3125   Loss: 0.807245 mae: 0.703727 (1426.15862739631 steps/sec)\n",
      "Step #7668\tEpoch   2 Batch 1417/3125   Loss: 0.839994 mae: 0.717897 (1540.879200005878 steps/sec)\n",
      "Step #7669\tEpoch   2 Batch 1418/3125   Loss: 0.895109 mae: 0.722623 (1341.8852857618183 steps/sec)\n",
      "Step #7670\tEpoch   2 Batch 1419/3125   Loss: 0.858234 mae: 0.697195 (1399.4354618069226 steps/sec)\n",
      "Step #7671\tEpoch   2 Batch 1420/3125   Loss: 0.821186 mae: 0.725846 (1285.0747274699283 steps/sec)\n",
      "Step #7672\tEpoch   2 Batch 1421/3125   Loss: 0.773477 mae: 0.719504 (1574.8995576782993 steps/sec)\n",
      "Step #7673\tEpoch   2 Batch 1422/3125   Loss: 0.830147 mae: 0.704491 (1528.4918807031866 steps/sec)\n",
      "Step #7674\tEpoch   2 Batch 1423/3125   Loss: 0.770193 mae: 0.711852 (1597.6475069515866 steps/sec)\n",
      "Step #7675\tEpoch   2 Batch 1424/3125   Loss: 0.816046 mae: 0.710298 (2037.2368638346238 steps/sec)\n",
      "Step #7676\tEpoch   2 Batch 1425/3125   Loss: 1.020824 mae: 0.793136 (1851.7080923579533 steps/sec)\n",
      "Step #7677\tEpoch   2 Batch 1426/3125   Loss: 0.855707 mae: 0.729384 (1184.3030511805466 steps/sec)\n",
      "Step #7678\tEpoch   2 Batch 1427/3125   Loss: 0.680333 mae: 0.634792 (1286.1386745820505 steps/sec)\n",
      "Step #7679\tEpoch   2 Batch 1428/3125   Loss: 0.788105 mae: 0.694880 (1255.1407966005327 steps/sec)\n",
      "Step #7680\tEpoch   2 Batch 1429/3125   Loss: 0.905948 mae: 0.752891 (1299.4472947183185 steps/sec)\n",
      "Step #7681\tEpoch   2 Batch 1430/3125   Loss: 0.789009 mae: 0.699278 (1525.6452786265095 steps/sec)\n",
      "Step #7682\tEpoch   2 Batch 1431/3125   Loss: 0.843668 mae: 0.734608 (1282.5441091031403 steps/sec)\n",
      "Step #7683\tEpoch   2 Batch 1432/3125   Loss: 0.891679 mae: 0.731336 (1089.1806548113677 steps/sec)\n",
      "Step #7684\tEpoch   2 Batch 1433/3125   Loss: 0.814331 mae: 0.718070 (1403.5470960660696 steps/sec)\n",
      "Step #7685\tEpoch   2 Batch 1434/3125   Loss: 0.775333 mae: 0.705383 (1287.694414255101 steps/sec)\n",
      "Step #7686\tEpoch   2 Batch 1435/3125   Loss: 0.826846 mae: 0.727382 (1446.1720937288812 steps/sec)\n",
      "Step #7687\tEpoch   2 Batch 1436/3125   Loss: 0.838236 mae: 0.712515 (1860.8765095787821 steps/sec)\n",
      "Step #7688\tEpoch   2 Batch 1437/3125   Loss: 0.822660 mae: 0.719730 (1915.2597788067253 steps/sec)\n",
      "Step #7689\tEpoch   2 Batch 1438/3125   Loss: 0.922052 mae: 0.765066 (1817.3996689573892 steps/sec)\n",
      "Step #7690\tEpoch   2 Batch 1439/3125   Loss: 0.906122 mae: 0.745842 (1668.8964754378844 steps/sec)\n",
      "Step #7691\tEpoch   2 Batch 1440/3125   Loss: 0.717596 mae: 0.668235 (1353.8485374718373 steps/sec)\n",
      "Step #7692\tEpoch   2 Batch 1441/3125   Loss: 0.769617 mae: 0.697708 (1553.6645898311615 steps/sec)\n",
      "Step #7693\tEpoch   2 Batch 1442/3125   Loss: 0.859354 mae: 0.754866 (1256.381500119818 steps/sec)\n",
      "Step #7694\tEpoch   2 Batch 1443/3125   Loss: 0.779409 mae: 0.710165 (1508.8944210208222 steps/sec)\n",
      "Step #7695\tEpoch   2 Batch 1444/3125   Loss: 0.786680 mae: 0.708265 (1400.949931527439 steps/sec)\n",
      "Step #7696\tEpoch   2 Batch 1445/3125   Loss: 0.868899 mae: 0.746811 (1597.4892975212906 steps/sec)\n",
      "Step #7697\tEpoch   2 Batch 1446/3125   Loss: 0.697045 mae: 0.656833 (1254.330025778591 steps/sec)\n",
      "Step #7698\tEpoch   2 Batch 1447/3125   Loss: 0.850289 mae: 0.744489 (1038.0810010790904 steps/sec)\n",
      "Step #7699\tEpoch   2 Batch 1448/3125   Loss: 0.858243 mae: 0.746683 (1102.099460288932 steps/sec)\n",
      "Step #7700\tEpoch   2 Batch 1449/3125   Loss: 0.819239 mae: 0.710941 (1488.809535641519 steps/sec)\n",
      "Step #7701\tEpoch   2 Batch 1450/3125   Loss: 0.846194 mae: 0.738671 (1845.0288127391898 steps/sec)\n",
      "Step #7702\tEpoch   2 Batch 1451/3125   Loss: 0.839370 mae: 0.717944 (1681.6903893187923 steps/sec)\n",
      "Step #7703\tEpoch   2 Batch 1452/3125   Loss: 0.800617 mae: 0.742486 (1588.149943203332 steps/sec)\n",
      "Step #7704\tEpoch   2 Batch 1453/3125   Loss: 0.855281 mae: 0.724988 (1430.4973295225882 steps/sec)\n",
      "Step #7705\tEpoch   2 Batch 1454/3125   Loss: 0.854831 mae: 0.726161 (1275.1512498251886 steps/sec)\n",
      "Step #7706\tEpoch   2 Batch 1455/3125   Loss: 0.788415 mae: 0.713757 (1463.3062602919422 steps/sec)\n",
      "Step #7707\tEpoch   2 Batch 1456/3125   Loss: 0.841724 mae: 0.711245 (1162.005130847698 steps/sec)\n",
      "Step #7708\tEpoch   2 Batch 1457/3125   Loss: 0.815241 mae: 0.738663 (1014.0525799167348 steps/sec)\n",
      "Step #7709\tEpoch   2 Batch 1458/3125   Loss: 0.842248 mae: 0.704341 (1472.708759067703 steps/sec)\n",
      "Step #7710\tEpoch   2 Batch 1459/3125   Loss: 0.815479 mae: 0.685107 (1226.1178671655753 steps/sec)\n",
      "Step #7711\tEpoch   2 Batch 1460/3125   Loss: 0.818244 mae: 0.727972 (1304.0935745244476 steps/sec)\n",
      "Step #7712\tEpoch   2 Batch 1461/3125   Loss: 0.785169 mae: 0.709385 (1584.382460487746 steps/sec)\n",
      "Step #7713\tEpoch   2 Batch 1462/3125   Loss: 0.915589 mae: 0.730847 (1388.5389285785225 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7714\tEpoch   2 Batch 1463/3125   Loss: 0.813162 mae: 0.735292 (1300.9385681407914 steps/sec)\n",
      "Step #7715\tEpoch   2 Batch 1464/3125   Loss: 0.835196 mae: 0.730894 (1582.326311341824 steps/sec)\n",
      "Step #7716\tEpoch   2 Batch 1465/3125   Loss: 0.824377 mae: 0.715793 (1775.0662321723305 steps/sec)\n",
      "Step #7717\tEpoch   2 Batch 1466/3125   Loss: 0.700312 mae: 0.646723 (1693.8059816012858 steps/sec)\n",
      "Step #7718\tEpoch   2 Batch 1467/3125   Loss: 0.761044 mae: 0.672719 (1220.9994352486362 steps/sec)\n",
      "Step #7719\tEpoch   2 Batch 1468/3125   Loss: 0.961871 mae: 0.773973 (1442.2039294973627 steps/sec)\n",
      "Step #7720\tEpoch   2 Batch 1469/3125   Loss: 0.863558 mae: 0.735856 (960.3179763807291 steps/sec)\n",
      "Step #7721\tEpoch   2 Batch 1470/3125   Loss: 0.896751 mae: 0.738455 (1177.130412330628 steps/sec)\n",
      "Step #7722\tEpoch   2 Batch 1471/3125   Loss: 0.769580 mae: 0.696371 (1324.9968409613587 steps/sec)\n",
      "Step #7723\tEpoch   2 Batch 1472/3125   Loss: 0.781309 mae: 0.705603 (1260.6486129061345 steps/sec)\n",
      "Step #7724\tEpoch   2 Batch 1473/3125   Loss: 0.877320 mae: 0.733355 (1264.2280147573003 steps/sec)\n",
      "Step #7725\tEpoch   2 Batch 1474/3125   Loss: 0.719489 mae: 0.659551 (1143.0365396354757 steps/sec)\n",
      "Step #7726\tEpoch   2 Batch 1475/3125   Loss: 0.898159 mae: 0.745234 (1246.0500047533035 steps/sec)\n",
      "Step #7727\tEpoch   2 Batch 1476/3125   Loss: 0.720200 mae: 0.675982 (1794.7539131699887 steps/sec)\n",
      "Step #7728\tEpoch   2 Batch 1477/3125   Loss: 0.852286 mae: 0.753795 (1824.9593177566028 steps/sec)\n",
      "Step #7729\tEpoch   2 Batch 1478/3125   Loss: 0.774070 mae: 0.686414 (1787.5638217168575 steps/sec)\n",
      "Step #7730\tEpoch   2 Batch 1479/3125   Loss: 0.694037 mae: 0.677960 (1791.090461874829 steps/sec)\n",
      "Step #7731\tEpoch   2 Batch 1480/3125   Loss: 0.765197 mae: 0.670071 (1394.197580109028 steps/sec)\n",
      "Step #7732\tEpoch   2 Batch 1481/3125   Loss: 0.775446 mae: 0.717183 (965.4195841216791 steps/sec)\n",
      "Step #7733\tEpoch   2 Batch 1482/3125   Loss: 0.867072 mae: 0.727792 (1324.9131319257547 steps/sec)\n",
      "Step #7734\tEpoch   2 Batch 1483/3125   Loss: 0.895584 mae: 0.734986 (1370.8398972434845 steps/sec)\n",
      "Step #7735\tEpoch   2 Batch 1484/3125   Loss: 0.836638 mae: 0.745975 (1255.7495628847214 steps/sec)\n",
      "Step #7736\tEpoch   2 Batch 1485/3125   Loss: 0.883801 mae: 0.723598 (1227.3233765625732 steps/sec)\n",
      "Step #7737\tEpoch   2 Batch 1486/3125   Loss: 0.856907 mae: 0.725731 (1207.1468041996684 steps/sec)\n",
      "Step #7738\tEpoch   2 Batch 1487/3125   Loss: 0.816073 mae: 0.708738 (1324.2856511388536 steps/sec)\n",
      "Step #7739\tEpoch   2 Batch 1488/3125   Loss: 0.892769 mae: 0.729936 (1187.7574137423258 steps/sec)\n",
      "Step #7740\tEpoch   2 Batch 1489/3125   Loss: 0.744456 mae: 0.670355 (1495.6154614177722 steps/sec)\n",
      "Step #7741\tEpoch   2 Batch 1490/3125   Loss: 0.912102 mae: 0.756956 (1243.869773842075 steps/sec)\n",
      "Step #7742\tEpoch   2 Batch 1491/3125   Loss: 0.828394 mae: 0.705845 (1698.4976229236015 steps/sec)\n",
      "Step #7743\tEpoch   2 Batch 1492/3125   Loss: 0.785779 mae: 0.715682 (1759.7100087265892 steps/sec)\n",
      "Step #7744\tEpoch   2 Batch 1493/3125   Loss: 0.856644 mae: 0.756446 (1696.985782604122 steps/sec)\n",
      "Step #7745\tEpoch   2 Batch 1494/3125   Loss: 0.750645 mae: 0.704505 (1783.1712128426639 steps/sec)\n",
      "Step #7746\tEpoch   2 Batch 1495/3125   Loss: 0.851503 mae: 0.726512 (1906.1379191245308 steps/sec)\n",
      "Step #7747\tEpoch   2 Batch 1496/3125   Loss: 0.767915 mae: 0.715490 (1642.2490211433046 steps/sec)\n",
      "Step #7748\tEpoch   2 Batch 1497/3125   Loss: 0.914575 mae: 0.749672 (1750.2082237967668 steps/sec)\n",
      "Step #7749\tEpoch   2 Batch 1498/3125   Loss: 0.834684 mae: 0.729959 (1269.816050474103 steps/sec)\n",
      "Step #7750\tEpoch   2 Batch 1499/3125   Loss: 0.857198 mae: 0.731409 (1309.1490211745904 steps/sec)\n",
      "Step #7751\tEpoch   2 Batch 1500/3125   Loss: 0.856756 mae: 0.731899 (1134.969909512058 steps/sec)\n",
      "Step #7752\tEpoch   2 Batch 1501/3125   Loss: 0.713441 mae: 0.675438 (1267.031181087139 steps/sec)\n",
      "Step #7753\tEpoch   2 Batch 1502/3125   Loss: 0.762238 mae: 0.684814 (1199.530975627892 steps/sec)\n",
      "Step #7754\tEpoch   2 Batch 1503/3125   Loss: 0.923640 mae: 0.758915 (1583.377627446243 steps/sec)\n",
      "Step #7755\tEpoch   2 Batch 1504/3125   Loss: 0.903336 mae: 0.750971 (1478.5022877406711 steps/sec)\n",
      "Step #7756\tEpoch   2 Batch 1505/3125   Loss: 0.637589 mae: 0.619204 (1249.0631216571967 steps/sec)\n",
      "Step #7757\tEpoch   2 Batch 1506/3125   Loss: 0.758307 mae: 0.677714 (1582.7920632165255 steps/sec)\n",
      "Step #7758\tEpoch   2 Batch 1507/3125   Loss: 0.865532 mae: 0.740087 (1863.7872041663334 steps/sec)\n",
      "Step #7759\tEpoch   2 Batch 1508/3125   Loss: 0.820605 mae: 0.712237 (1708.0566867568007 steps/sec)\n",
      "Step #7760\tEpoch   2 Batch 1509/3125   Loss: 0.756866 mae: 0.678269 (1811.199778906277 steps/sec)\n",
      "Step #7761\tEpoch   2 Batch 1510/3125   Loss: 0.894248 mae: 0.741626 (1972.230895104106 steps/sec)\n",
      "Step #7762\tEpoch   2 Batch 1511/3125   Loss: 0.868698 mae: 0.751375 (2037.474375540421 steps/sec)\n",
      "Step #7763\tEpoch   2 Batch 1512/3125   Loss: 0.854964 mae: 0.714793 (1885.775431844545 steps/sec)\n",
      "Step #7764\tEpoch   2 Batch 1513/3125   Loss: 0.804280 mae: 0.697297 (1616.2398366151594 steps/sec)\n",
      "Step #7765\tEpoch   2 Batch 1514/3125   Loss: 0.652481 mae: 0.668111 (1483.9844606882302 steps/sec)\n",
      "Step #7766\tEpoch   2 Batch 1515/3125   Loss: 0.743596 mae: 0.699233 (1240.6320434929216 steps/sec)\n",
      "Step #7767\tEpoch   2 Batch 1516/3125   Loss: 0.797182 mae: 0.731665 (1434.7643448931701 steps/sec)\n",
      "Step #7768\tEpoch   2 Batch 1517/3125   Loss: 0.811913 mae: 0.703048 (1196.485542801068 steps/sec)\n",
      "Step #7769\tEpoch   2 Batch 1518/3125   Loss: 0.903405 mae: 0.771864 (1363.8061545665012 steps/sec)\n",
      "Step #7770\tEpoch   2 Batch 1519/3125   Loss: 0.755682 mae: 0.707456 (1120.3213812556091 steps/sec)\n",
      "Step #7771\tEpoch   2 Batch 1520/3125   Loss: 0.764338 mae: 0.695445 (1122.3599426283904 steps/sec)\n",
      "Step #7772\tEpoch   2 Batch 1521/3125   Loss: 0.842668 mae: 0.720905 (1367.9784479103475 steps/sec)\n",
      "Step #7773\tEpoch   2 Batch 1522/3125   Loss: 0.888502 mae: 0.747177 (1205.315186905145 steps/sec)\n",
      "Step #7774\tEpoch   2 Batch 1523/3125   Loss: 0.764698 mae: 0.705137 (1522.3449817796425 steps/sec)\n",
      "Step #7775\tEpoch   2 Batch 1524/3125   Loss: 0.756716 mae: 0.692357 (1317.3065326633166 steps/sec)\n",
      "Step #7776\tEpoch   2 Batch 1525/3125   Loss: 0.759611 mae: 0.682618 (1291.723589955221 steps/sec)\n",
      "Step #7777\tEpoch   2 Batch 1526/3125   Loss: 0.906872 mae: 0.760379 (1877.2844457175595 steps/sec)\n",
      "Step #7778\tEpoch   2 Batch 1527/3125   Loss: 0.677508 mae: 0.636203 (1754.0728845172675 steps/sec)\n",
      "Step #7779\tEpoch   2 Batch 1528/3125   Loss: 0.867863 mae: 0.732899 (1273.239026167203 steps/sec)\n",
      "Step #7780\tEpoch   2 Batch 1529/3125   Loss: 0.900687 mae: 0.722246 (1384.7706082776472 steps/sec)\n",
      "Step #7781\tEpoch   2 Batch 1530/3125   Loss: 0.890188 mae: 0.746471 (987.9597307215776 steps/sec)\n",
      "Step #7782\tEpoch   2 Batch 1531/3125   Loss: 0.742374 mae: 0.691115 (934.0768586648613 steps/sec)\n",
      "Step #7783\tEpoch   2 Batch 1532/3125   Loss: 0.787019 mae: 0.686479 (1215.5148030510281 steps/sec)\n",
      "Step #7784\tEpoch   2 Batch 1533/3125   Loss: 0.940162 mae: 0.752318 (1211.2114124000116 steps/sec)\n",
      "Step #7785\tEpoch   2 Batch 1534/3125   Loss: 0.919892 mae: 0.756146 (1284.2641583382324 steps/sec)\n",
      "Step #7786\tEpoch   2 Batch 1535/3125   Loss: 0.674263 mae: 0.655969 (1202.012953516364 steps/sec)\n",
      "Step #7787\tEpoch   2 Batch 1536/3125   Loss: 0.835029 mae: 0.730510 (1668.7769555184213 steps/sec)\n",
      "Step #7788\tEpoch   2 Batch 1537/3125   Loss: 0.989045 mae: 0.779930 (1598.2075766466746 steps/sec)\n",
      "Step #7789\tEpoch   2 Batch 1538/3125   Loss: 0.790447 mae: 0.700857 (1778.483352838413 steps/sec)\n",
      "Step #7790\tEpoch   2 Batch 1539/3125   Loss: 0.794080 mae: 0.685571 (1661.4131683395788 steps/sec)\n",
      "Step #7791\tEpoch   2 Batch 1540/3125   Loss: 0.902787 mae: 0.751956 (1828.2844838107858 steps/sec)\n",
      "Step #7792\tEpoch   2 Batch 1541/3125   Loss: 0.794798 mae: 0.719280 (1977.2328289256589 steps/sec)\n",
      "Step #7793\tEpoch   2 Batch 1542/3125   Loss: 0.804702 mae: 0.710759 (1815.024579381015 steps/sec)\n",
      "Step #7794\tEpoch   2 Batch 1543/3125   Loss: 0.867729 mae: 0.743202 (1997.5159065797995 steps/sec)\n",
      "Step #7795\tEpoch   2 Batch 1544/3125   Loss: 0.942737 mae: 0.789429 (1545.4439605302914 steps/sec)\n",
      "Step #7796\tEpoch   2 Batch 1545/3125   Loss: 0.813173 mae: 0.727007 (1022.7265855502183 steps/sec)\n",
      "Step #7797\tEpoch   2 Batch 1546/3125   Loss: 0.866669 mae: 0.749603 (1184.9454467378223 steps/sec)\n",
      "Step #7798\tEpoch   2 Batch 1547/3125   Loss: 0.734129 mae: 0.674011 (1053.0303182461814 steps/sec)\n",
      "Step #7799\tEpoch   2 Batch 1548/3125   Loss: 0.926087 mae: 0.789720 (1082.5742441371265 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7800\tEpoch   2 Batch 1549/3125   Loss: 0.909918 mae: 0.734822 (973.3686697330741 steps/sec)\n",
      "Step #7801\tEpoch   2 Batch 1550/3125   Loss: 0.810457 mae: 0.727590 (1375.5875504247156 steps/sec)\n",
      "Step #7802\tEpoch   2 Batch 1551/3125   Loss: 0.932352 mae: 0.768144 (1399.781070618075 steps/sec)\n",
      "Step #7803\tEpoch   2 Batch 1552/3125   Loss: 0.693313 mae: 0.660296 (1063.7881708430557 steps/sec)\n",
      "Step #7804\tEpoch   2 Batch 1553/3125   Loss: 0.946392 mae: 0.768739 (1247.0280427181694 steps/sec)\n",
      "Step #7805\tEpoch   2 Batch 1554/3125   Loss: 0.743314 mae: 0.691538 (1537.185914914827 steps/sec)\n",
      "Step #7806\tEpoch   2 Batch 1555/3125   Loss: 0.800290 mae: 0.720321 (1417.87597695865 steps/sec)\n",
      "Step #7807\tEpoch   2 Batch 1556/3125   Loss: 0.889082 mae: 0.754551 (1085.9882968256434 steps/sec)\n",
      "Step #7808\tEpoch   2 Batch 1557/3125   Loss: 1.045183 mae: 0.796875 (1172.8316490596217 steps/sec)\n",
      "Step #7809\tEpoch   2 Batch 1558/3125   Loss: 0.764135 mae: 0.692812 (1236.5501748262056 steps/sec)\n",
      "Step #7810\tEpoch   2 Batch 1559/3125   Loss: 0.769511 mae: 0.711262 (1036.2498085275647 steps/sec)\n",
      "Step #7811\tEpoch   2 Batch 1560/3125   Loss: 0.739605 mae: 0.699793 (847.9885487386225 steps/sec)\n",
      "Step #7812\tEpoch   2 Batch 1561/3125   Loss: 0.978436 mae: 0.748207 (1133.5344035457542 steps/sec)\n",
      "Step #7813\tEpoch   2 Batch 1562/3125   Loss: 0.735991 mae: 0.679550 (1062.543129436442 steps/sec)\n",
      "Step #7814\tEpoch   2 Batch 1563/3125   Loss: 0.886755 mae: 0.748093 (1217.193866298304 steps/sec)\n",
      "Step #7815\tEpoch   2 Batch 1564/3125   Loss: 0.763115 mae: 0.695825 (1736.340453717503 steps/sec)\n",
      "Step #7816\tEpoch   2 Batch 1565/3125   Loss: 0.868296 mae: 0.740194 (1882.052248516993 steps/sec)\n",
      "Step #7817\tEpoch   2 Batch 1566/3125   Loss: 0.874571 mae: 0.745998 (1683.5534290783273 steps/sec)\n",
      "Step #7818\tEpoch   2 Batch 1567/3125   Loss: 0.777300 mae: 0.703929 (1705.0293500707328 steps/sec)\n",
      "Step #7819\tEpoch   2 Batch 1568/3125   Loss: 0.789093 mae: 0.685330 (2042.7335774955193 steps/sec)\n",
      "Step #7820\tEpoch   2 Batch 1569/3125   Loss: 0.792441 mae: 0.710726 (1831.7657745789952 steps/sec)\n",
      "Step #7821\tEpoch   2 Batch 1570/3125   Loss: 0.814701 mae: 0.684836 (1341.1215491165356 steps/sec)\n",
      "Step #7822\tEpoch   2 Batch 1571/3125   Loss: 0.978491 mae: 0.777843 (952.2207793387154 steps/sec)\n",
      "Step #7823\tEpoch   2 Batch 1572/3125   Loss: 0.910075 mae: 0.747113 (1035.6050467889681 steps/sec)\n",
      "Step #7824\tEpoch   2 Batch 1573/3125   Loss: 0.690784 mae: 0.657193 (959.8960073600425 steps/sec)\n",
      "Step #7825\tEpoch   2 Batch 1574/3125   Loss: 0.791786 mae: 0.720283 (1103.8513566860543 steps/sec)\n",
      "Step #7826\tEpoch   2 Batch 1575/3125   Loss: 0.744668 mae: 0.675197 (1196.376332063072 steps/sec)\n",
      "Step #7827\tEpoch   2 Batch 1576/3125   Loss: 0.842433 mae: 0.705425 (1293.524212500077 steps/sec)\n",
      "Step #7828\tEpoch   2 Batch 1577/3125   Loss: 0.674234 mae: 0.643445 (1008.9009693791644 steps/sec)\n",
      "Step #7829\tEpoch   2 Batch 1578/3125   Loss: 0.717299 mae: 0.667464 (1270.1621353166695 steps/sec)\n",
      "Step #7830\tEpoch   2 Batch 1579/3125   Loss: 0.769651 mae: 0.706842 (1439.629855910155 steps/sec)\n",
      "Step #7831\tEpoch   2 Batch 1580/3125   Loss: 0.908314 mae: 0.770462 (1866.3581510421302 steps/sec)\n",
      "Step #7832\tEpoch   2 Batch 1581/3125   Loss: 0.958725 mae: 0.768686 (1933.0549640977426 steps/sec)\n",
      "Step #7833\tEpoch   2 Batch 1582/3125   Loss: 0.792878 mae: 0.699850 (2011.5794118211293 steps/sec)\n",
      "Step #7834\tEpoch   2 Batch 1583/3125   Loss: 0.725874 mae: 0.684453 (2007.458743347245 steps/sec)\n",
      "Step #7835\tEpoch   2 Batch 1584/3125   Loss: 0.711990 mae: 0.697857 (1694.380751549232 steps/sec)\n",
      "Step #7836\tEpoch   2 Batch 1585/3125   Loss: 0.734858 mae: 0.710500 (1848.166948674563 steps/sec)\n",
      "Step #7837\tEpoch   2 Batch 1586/3125   Loss: 0.823715 mae: 0.703987 (1856.0345514244498 steps/sec)\n",
      "Step #7838\tEpoch   2 Batch 1587/3125   Loss: 0.740078 mae: 0.711436 (1494.0172401510295 steps/sec)\n",
      "Step #7839\tEpoch   2 Batch 1588/3125   Loss: 0.750492 mae: 0.703743 (1026.777513390715 steps/sec)\n",
      "Step #7840\tEpoch   2 Batch 1589/3125   Loss: 0.752731 mae: 0.697703 (826.1838352321971 steps/sec)\n",
      "Step #7841\tEpoch   2 Batch 1590/3125   Loss: 0.896159 mae: 0.762589 (754.2409943606859 steps/sec)\n",
      "Step #7842\tEpoch   2 Batch 1591/3125   Loss: 0.781592 mae: 0.717012 (814.5291842092998 steps/sec)\n",
      "Step #7843\tEpoch   2 Batch 1592/3125   Loss: 0.751160 mae: 0.678724 (1219.2673298411055 steps/sec)\n",
      "Step #7844\tEpoch   2 Batch 1593/3125   Loss: 0.905609 mae: 0.751580 (1672.6234437434698 steps/sec)\n",
      "Step #7845\tEpoch   2 Batch 1594/3125   Loss: 0.676456 mae: 0.663155 (1571.241692952027 steps/sec)\n",
      "Step #7846\tEpoch   2 Batch 1595/3125   Loss: 0.948050 mae: 0.771031 (1807.2041639378167 steps/sec)\n",
      "Step #7847\tEpoch   2 Batch 1596/3125   Loss: 0.914675 mae: 0.739257 (1884.9449028384477 steps/sec)\n",
      "Step #7848\tEpoch   2 Batch 1597/3125   Loss: 0.697720 mae: 0.678609 (1992.032447733123 steps/sec)\n",
      "Step #7849\tEpoch   2 Batch 1598/3125   Loss: 0.854473 mae: 0.745066 (1953.3466217097298 steps/sec)\n",
      "Step #7850\tEpoch   2 Batch 1599/3125   Loss: 0.817881 mae: 0.709163 (2009.882885127753 steps/sec)\n",
      "Step #7851\tEpoch   2 Batch 1600/3125   Loss: 0.745730 mae: 0.681611 (2012.8536875647867 steps/sec)\n",
      "Step #7852\tEpoch   2 Batch 1601/3125   Loss: 0.722110 mae: 0.660345 (1587.2964933659298 steps/sec)\n",
      "Step #7853\tEpoch   2 Batch 1602/3125   Loss: 0.773432 mae: 0.705472 (1769.4050943698692 steps/sec)\n",
      "Step #7854\tEpoch   2 Batch 1603/3125   Loss: 0.723384 mae: 0.702716 (1944.0034112608687 steps/sec)\n",
      "Step #7855\tEpoch   2 Batch 1604/3125   Loss: 0.816385 mae: 0.711483 (1601.062717105012 steps/sec)\n",
      "Step #7856\tEpoch   2 Batch 1605/3125   Loss: 0.791455 mae: 0.695111 (1266.9775984147313 steps/sec)\n",
      "Step #7857\tEpoch   2 Batch 1606/3125   Loss: 0.766132 mae: 0.683560 (1162.894326795646 steps/sec)\n",
      "Step #7858\tEpoch   2 Batch 1607/3125   Loss: 0.970229 mae: 0.768590 (1377.3040422946835 steps/sec)\n",
      "Step #7859\tEpoch   2 Batch 1608/3125   Loss: 0.803011 mae: 0.722721 (1122.684397049219 steps/sec)\n",
      "Step #7860\tEpoch   2 Batch 1609/3125   Loss: 0.871040 mae: 0.761327 (896.3605356402508 steps/sec)\n",
      "Step #7861\tEpoch   2 Batch 1610/3125   Loss: 0.751081 mae: 0.665711 (1178.744786807109 steps/sec)\n",
      "Step #7862\tEpoch   2 Batch 1611/3125   Loss: 0.736434 mae: 0.689669 (1268.425923125775 steps/sec)\n",
      "Step #7863\tEpoch   2 Batch 1612/3125   Loss: 0.858559 mae: 0.743107 (1038.919245612036 steps/sec)\n",
      "Step #7864\tEpoch   2 Batch 1613/3125   Loss: 0.721275 mae: 0.679484 (1575.5148036571532 steps/sec)\n",
      "Step #7865\tEpoch   2 Batch 1614/3125   Loss: 0.827119 mae: 0.708816 (1134.3314582431847 steps/sec)\n",
      "Step #7866\tEpoch   2 Batch 1615/3125   Loss: 0.844185 mae: 0.730279 (1688.1068332380808 steps/sec)\n",
      "Step #7867\tEpoch   2 Batch 1616/3125   Loss: 0.902947 mae: 0.746631 (1803.5518021310813 steps/sec)\n",
      "Step #7868\tEpoch   2 Batch 1617/3125   Loss: 0.869891 mae: 0.730093 (1981.3050913110433 steps/sec)\n",
      "Step #7869\tEpoch   2 Batch 1618/3125   Loss: 0.699042 mae: 0.647938 (1669.002729739839 steps/sec)\n",
      "Step #7870\tEpoch   2 Batch 1619/3125   Loss: 0.915077 mae: 0.744929 (1459.4670582422248 steps/sec)\n",
      "Step #7871\tEpoch   2 Batch 1620/3125   Loss: 0.775873 mae: 0.701514 (1105.6263180092787 steps/sec)\n",
      "Step #7872\tEpoch   2 Batch 1621/3125   Loss: 0.855814 mae: 0.725822 (977.0600869366704 steps/sec)\n",
      "Step #7873\tEpoch   2 Batch 1622/3125   Loss: 0.796393 mae: 0.689207 (989.8623166857828 steps/sec)\n",
      "Step #7874\tEpoch   2 Batch 1623/3125   Loss: 0.868941 mae: 0.754494 (1021.0783597713573 steps/sec)\n",
      "Step #7875\tEpoch   2 Batch 1624/3125   Loss: 0.897349 mae: 0.754640 (999.8865256342407 steps/sec)\n",
      "Step #7876\tEpoch   2 Batch 1625/3125   Loss: 0.854026 mae: 0.751147 (933.0296127562642 steps/sec)\n",
      "Step #7877\tEpoch   2 Batch 1626/3125   Loss: 0.751617 mae: 0.690574 (1024.400156311059 steps/sec)\n",
      "Step #7878\tEpoch   2 Batch 1627/3125   Loss: 0.860799 mae: 0.749259 (1269.2934354989045 steps/sec)\n",
      "Step #7879\tEpoch   2 Batch 1628/3125   Loss: 0.849890 mae: 0.756364 (1552.5144172755606 steps/sec)\n",
      "Step #7880\tEpoch   2 Batch 1629/3125   Loss: 0.772485 mae: 0.695242 (1723.9085580883018 steps/sec)\n",
      "Step #7881\tEpoch   2 Batch 1630/3125   Loss: 0.947895 mae: 0.773402 (1908.306034796535 steps/sec)\n",
      "Step #7882\tEpoch   2 Batch 1631/3125   Loss: 0.919063 mae: 0.751662 (1910.9491179472227 steps/sec)\n",
      "Step #7883\tEpoch   2 Batch 1632/3125   Loss: 0.814416 mae: 0.696396 (1941.2861361301132 steps/sec)\n",
      "Step #7884\tEpoch   2 Batch 1633/3125   Loss: 0.960721 mae: 0.750528 (2178.723404255319 steps/sec)\n",
      "Step #7885\tEpoch   2 Batch 1634/3125   Loss: 0.985964 mae: 0.794197 (2057.180973681371 steps/sec)\n",
      "Step #7886\tEpoch   2 Batch 1635/3125   Loss: 0.867535 mae: 0.748232 (1958.4176907848046 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7887\tEpoch   2 Batch 1636/3125   Loss: 0.852901 mae: 0.732934 (1695.490338750101 steps/sec)\n",
      "Step #7888\tEpoch   2 Batch 1637/3125   Loss: 0.824816 mae: 0.712598 (1679.4816968182656 steps/sec)\n",
      "Step #7889\tEpoch   2 Batch 1638/3125   Loss: 0.842235 mae: 0.706899 (1897.94381697075 steps/sec)\n",
      "Step #7890\tEpoch   2 Batch 1639/3125   Loss: 0.764406 mae: 0.674555 (1842.5810079426442 steps/sec)\n",
      "Step #7891\tEpoch   2 Batch 1640/3125   Loss: 0.895263 mae: 0.772427 (1835.7422969187676 steps/sec)\n",
      "Step #7892\tEpoch   2 Batch 1641/3125   Loss: 0.648690 mae: 0.628555 (1846.9299327156798 steps/sec)\n",
      "Step #7893\tEpoch   2 Batch 1642/3125   Loss: 0.826569 mae: 0.733189 (1901.075112859655 steps/sec)\n",
      "Step #7894\tEpoch   2 Batch 1643/3125   Loss: 0.849504 mae: 0.707928 (1814.741870164933 steps/sec)\n",
      "Step #7895\tEpoch   2 Batch 1644/3125   Loss: 0.760792 mae: 0.686229 (1528.068667026129 steps/sec)\n",
      "Step #7896\tEpoch   2 Batch 1645/3125   Loss: 0.890503 mae: 0.739776 (1559.3483481920455 steps/sec)\n",
      "Step #7897\tEpoch   2 Batch 1646/3125   Loss: 0.740191 mae: 0.680229 (1823.3567503651666 steps/sec)\n",
      "Step #7898\tEpoch   2 Batch 1647/3125   Loss: 0.853907 mae: 0.733298 (1552.2730973634734 steps/sec)\n",
      "Step #7899\tEpoch   2 Batch 1648/3125   Loss: 0.884773 mae: 0.718715 (1240.93302879324 steps/sec)\n",
      "Step #7900\tEpoch   2 Batch 1649/3125   Loss: 0.865048 mae: 0.746738 (1344.6643712210105 steps/sec)\n",
      "Step #7901\tEpoch   2 Batch 1650/3125   Loss: 0.734546 mae: 0.675407 (1462.8163276694288 steps/sec)\n",
      "Step #7902\tEpoch   2 Batch 1651/3125   Loss: 0.830559 mae: 0.696548 (1052.0530352816056 steps/sec)\n",
      "Step #7903\tEpoch   2 Batch 1652/3125   Loss: 0.705439 mae: 0.667434 (1227.8623160828352 steps/sec)\n",
      "Step #7904\tEpoch   2 Batch 1653/3125   Loss: 0.788268 mae: 0.713092 (1286.5410687884569 steps/sec)\n",
      "Step #7905\tEpoch   2 Batch 1654/3125   Loss: 0.739431 mae: 0.688533 (1426.3720269066225 steps/sec)\n",
      "Step #7906\tEpoch   2 Batch 1655/3125   Loss: 0.889235 mae: 0.722647 (1317.0004458762726 steps/sec)\n",
      "Step #7907\tEpoch   2 Batch 1656/3125   Loss: 0.850558 mae: 0.713514 (1251.5304326030782 steps/sec)\n",
      "Step #7908\tEpoch   2 Batch 1657/3125   Loss: 0.792882 mae: 0.705377 (1485.2666841363484 steps/sec)\n",
      "Step #7909\tEpoch   2 Batch 1658/3125   Loss: 0.657760 mae: 0.650038 (1309.7213374801713 steps/sec)\n",
      "Step #7910\tEpoch   2 Batch 1659/3125   Loss: 0.728947 mae: 0.686391 (1430.165647142263 steps/sec)\n",
      "Step #7911\tEpoch   2 Batch 1660/3125   Loss: 0.918996 mae: 0.752167 (2044.326600639476 steps/sec)\n",
      "Step #7912\tEpoch   2 Batch 1661/3125   Loss: 0.972310 mae: 0.778281 (1802.4357332554082 steps/sec)\n",
      "Step #7913\tEpoch   2 Batch 1662/3125   Loss: 0.855989 mae: 0.732615 (1881.123748699365 steps/sec)\n",
      "Step #7914\tEpoch   2 Batch 1663/3125   Loss: 0.851973 mae: 0.749963 (1606.4836873673808 steps/sec)\n",
      "Step #7915\tEpoch   2 Batch 1664/3125   Loss: 0.850955 mae: 0.733316 (1451.4669342838356 steps/sec)\n",
      "Step #7916\tEpoch   2 Batch 1665/3125   Loss: 0.840513 mae: 0.725502 (1613.206255432734 steps/sec)\n",
      "Step #7917\tEpoch   2 Batch 1666/3125   Loss: 0.796281 mae: 0.715745 (1379.3422783478031 steps/sec)\n",
      "Step #7918\tEpoch   2 Batch 1667/3125   Loss: 0.771498 mae: 0.706105 (1515.8856481983448 steps/sec)\n",
      "Step #7919\tEpoch   2 Batch 1668/3125   Loss: 0.761824 mae: 0.673031 (1156.947248794589 steps/sec)\n",
      "Step #7920\tEpoch   2 Batch 1669/3125   Loss: 0.848678 mae: 0.730009 (1492.5392679472488 steps/sec)\n",
      "Step #7921\tEpoch   2 Batch 1670/3125   Loss: 0.729263 mae: 0.644008 (1562.8927442914207 steps/sec)\n",
      "Step #7922\tEpoch   2 Batch 1671/3125   Loss: 0.833032 mae: 0.740132 (1310.408777915245 steps/sec)\n",
      "Step #7923\tEpoch   2 Batch 1672/3125   Loss: 0.833744 mae: 0.720763 (1490.8098271155595 steps/sec)\n",
      "Step #7924\tEpoch   2 Batch 1673/3125   Loss: 0.735644 mae: 0.705721 (1328.9347113879612 steps/sec)\n",
      "Step #7925\tEpoch   2 Batch 1674/3125   Loss: 0.788104 mae: 0.698232 (1247.257956120161 steps/sec)\n",
      "Step #7926\tEpoch   2 Batch 1675/3125   Loss: 0.818030 mae: 0.723283 (1191.4959377308107 steps/sec)\n",
      "Step #7927\tEpoch   2 Batch 1676/3125   Loss: 0.747766 mae: 0.675772 (1659.2048736105066 steps/sec)\n",
      "Step #7928\tEpoch   2 Batch 1677/3125   Loss: 0.715646 mae: 0.670742 (1870.8201752038394 steps/sec)\n",
      "Step #7929\tEpoch   2 Batch 1678/3125   Loss: 0.886534 mae: 0.730933 (1708.5158904087268 steps/sec)\n",
      "Step #7930\tEpoch   2 Batch 1679/3125   Loss: 0.816452 mae: 0.685000 (1378.4447117438658 steps/sec)\n",
      "Step #7931\tEpoch   2 Batch 1680/3125   Loss: 0.807293 mae: 0.709985 (1543.7261685682738 steps/sec)\n",
      "Step #7932\tEpoch   2 Batch 1681/3125   Loss: 0.870091 mae: 0.729322 (906.6530628126256 steps/sec)\n",
      "Step #7933\tEpoch   2 Batch 1682/3125   Loss: 0.882728 mae: 0.758173 (931.3597221667088 steps/sec)\n",
      "Step #7934\tEpoch   2 Batch 1683/3125   Loss: 0.741898 mae: 0.689051 (1027.4113266705858 steps/sec)\n",
      "Step #7935\tEpoch   2 Batch 1684/3125   Loss: 0.662133 mae: 0.649452 (878.439782856377 steps/sec)\n",
      "Step #7936\tEpoch   2 Batch 1685/3125   Loss: 0.864306 mae: 0.735195 (1373.965342156124 steps/sec)\n",
      "Step #7937\tEpoch   2 Batch 1686/3125   Loss: 0.671055 mae: 0.647881 (1494.198200252221 steps/sec)\n",
      "Step #7938\tEpoch   2 Batch 1687/3125   Loss: 0.952301 mae: 0.773100 (1217.1726727685334 steps/sec)\n",
      "Step #7939\tEpoch   2 Batch 1688/3125   Loss: 0.845952 mae: 0.737381 (1737.5632793404864 steps/sec)\n",
      "Step #7940\tEpoch   2 Batch 1689/3125   Loss: 0.825818 mae: 0.741227 (1556.5705972730443 steps/sec)\n",
      "Step #7941\tEpoch   2 Batch 1690/3125   Loss: 0.795776 mae: 0.701174 (1302.9025844930418 steps/sec)\n",
      "Step #7942\tEpoch   2 Batch 1691/3125   Loss: 0.927211 mae: 0.765406 (1197.5719090665098 steps/sec)\n",
      "Step #7943\tEpoch   2 Batch 1692/3125   Loss: 0.828995 mae: 0.732778 (1484.7829626954965 steps/sec)\n",
      "Step #7944\tEpoch   2 Batch 1693/3125   Loss: 1.029876 mae: 0.789537 (1371.2880804012214 steps/sec)\n",
      "Step #7945\tEpoch   2 Batch 1694/3125   Loss: 0.868296 mae: 0.721210 (1324.7875882022224 steps/sec)\n",
      "Step #7946\tEpoch   2 Batch 1695/3125   Loss: 0.866836 mae: 0.697972 (1500.8387484613402 steps/sec)\n",
      "Step #7947\tEpoch   2 Batch 1696/3125   Loss: 0.835640 mae: 0.719085 (1024.1100118176757 steps/sec)\n",
      "Step #7948\tEpoch   2 Batch 1697/3125   Loss: 0.784737 mae: 0.718869 (1640.1041707399095 steps/sec)\n",
      "Step #7949\tEpoch   2 Batch 1698/3125   Loss: 0.772540 mae: 0.696235 (2136.0059481977164 steps/sec)\n",
      "Step #7950\tEpoch   2 Batch 1699/3125   Loss: 0.899418 mae: 0.728667 (1911.9595937494303 steps/sec)\n",
      "Step #7951\tEpoch   2 Batch 1700/3125   Loss: 0.775266 mae: 0.702694 (1829.5604836599027 steps/sec)\n",
      "Step #7952\tEpoch   2 Batch 1701/3125   Loss: 0.834712 mae: 0.731404 (1992.013526092821 steps/sec)\n",
      "Step #7953\tEpoch   2 Batch 1702/3125   Loss: 0.794682 mae: 0.705948 (1729.995132936819 steps/sec)\n",
      "Step #7954\tEpoch   2 Batch 1703/3125   Loss: 0.914596 mae: 0.750498 (1863.7209508998 steps/sec)\n",
      "Step #7955\tEpoch   2 Batch 1704/3125   Loss: 0.841625 mae: 0.740187 (1329.3980424970841 steps/sec)\n",
      "Step #7956\tEpoch   2 Batch 1705/3125   Loss: 0.836552 mae: 0.732736 (1360.4001115745637 steps/sec)\n",
      "Step #7957\tEpoch   2 Batch 1706/3125   Loss: 0.788585 mae: 0.711539 (1331.1616511047778 steps/sec)\n",
      "Step #7958\tEpoch   2 Batch 1707/3125   Loss: 0.887320 mae: 0.755665 (1445.8829450576725 steps/sec)\n",
      "Step #7959\tEpoch   2 Batch 1708/3125   Loss: 0.838094 mae: 0.710481 (1441.3019573345064 steps/sec)\n",
      "Step #7960\tEpoch   2 Batch 1709/3125   Loss: 0.697510 mae: 0.668199 (1395.1714732395303 steps/sec)\n",
      "Step #7961\tEpoch   2 Batch 1710/3125   Loss: 0.773677 mae: 0.671457 (1261.4144622084282 steps/sec)\n",
      "Step #7962\tEpoch   2 Batch 1711/3125   Loss: 0.765840 mae: 0.685899 (1502.1287568403861 steps/sec)\n",
      "Step #7963\tEpoch   2 Batch 1712/3125   Loss: 0.690037 mae: 0.660894 (1517.2126403518926 steps/sec)\n",
      "Step #7964\tEpoch   2 Batch 1713/3125   Loss: 0.762682 mae: 0.708753 (1401.2681994641223 steps/sec)\n",
      "Step #7965\tEpoch   2 Batch 1714/3125   Loss: 0.846162 mae: 0.748537 (1683.6615579765412 steps/sec)\n",
      "Step #7966\tEpoch   2 Batch 1715/3125   Loss: 0.710287 mae: 0.643711 (1490.7674372316528 steps/sec)\n",
      "Step #7967\tEpoch   2 Batch 1716/3125   Loss: 1.008242 mae: 0.768498 (1332.1932906029056 steps/sec)\n",
      "Step #7968\tEpoch   2 Batch 1717/3125   Loss: 0.809137 mae: 0.694646 (1505.1150105859979 steps/sec)\n",
      "Step #7969\tEpoch   2 Batch 1718/3125   Loss: 0.804602 mae: 0.693197 (1483.2287769377117 steps/sec)\n",
      "Step #7970\tEpoch   2 Batch 1719/3125   Loss: 0.831286 mae: 0.732778 (1389.6522476675148 steps/sec)\n",
      "Step #7971\tEpoch   2 Batch 1720/3125   Loss: 0.838814 mae: 0.722218 (1591.5367043841875 steps/sec)\n",
      "Step #7972\tEpoch   2 Batch 1721/3125   Loss: 0.867247 mae: 0.722025 (1481.0395480225989 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #7973\tEpoch   2 Batch 1722/3125   Loss: 0.724506 mae: 0.691412 (1370.3472340203086 steps/sec)\n",
      "Step #7974\tEpoch   2 Batch 1723/3125   Loss: 0.889397 mae: 0.724737 (1242.3591858012844 steps/sec)\n",
      "Step #7975\tEpoch   2 Batch 1724/3125   Loss: 0.936848 mae: 0.783641 (1390.7120168173108 steps/sec)\n",
      "Step #7976\tEpoch   2 Batch 1725/3125   Loss: 0.778316 mae: 0.690020 (1308.7731998652005 steps/sec)\n",
      "Step #7977\tEpoch   2 Batch 1726/3125   Loss: 0.853131 mae: 0.749221 (1501.5479787493018 steps/sec)\n",
      "Step #7978\tEpoch   2 Batch 1727/3125   Loss: 0.742734 mae: 0.682374 (1424.5021056921614 steps/sec)\n",
      "Step #7979\tEpoch   2 Batch 1728/3125   Loss: 0.939678 mae: 0.761709 (1176.113778096819 steps/sec)\n",
      "Step #7980\tEpoch   2 Batch 1729/3125   Loss: 0.760444 mae: 0.695717 (1465.2078530007686 steps/sec)\n",
      "Step #7981\tEpoch   2 Batch 1730/3125   Loss: 0.833751 mae: 0.734505 (1513.1840221657815 steps/sec)\n",
      "Step #7982\tEpoch   2 Batch 1731/3125   Loss: 0.824434 mae: 0.715417 (1426.0422545746324 steps/sec)\n",
      "Step #7983\tEpoch   2 Batch 1732/3125   Loss: 0.891426 mae: 0.717030 (1512.780154224585 steps/sec)\n",
      "Step #7984\tEpoch   2 Batch 1733/3125   Loss: 0.811645 mae: 0.720312 (1508.2975525204795 steps/sec)\n",
      "Step #7985\tEpoch   2 Batch 1734/3125   Loss: 0.869467 mae: 0.749581 (1389.7811766888892 steps/sec)\n",
      "Step #7986\tEpoch   2 Batch 1735/3125   Loss: 0.866197 mae: 0.748552 (1211.2114124000116 steps/sec)\n",
      "Step #7987\tEpoch   2 Batch 1736/3125   Loss: 0.695161 mae: 0.657516 (1350.8051426068585 steps/sec)\n",
      "Step #7988\tEpoch   2 Batch 1737/3125   Loss: 0.752510 mae: 0.695172 (1401.4929462632906 steps/sec)\n",
      "Step #7989\tEpoch   2 Batch 1738/3125   Loss: 0.838638 mae: 0.711025 (1176.2325148350478 steps/sec)\n",
      "Step #7990\tEpoch   2 Batch 1739/3125   Loss: 0.824558 mae: 0.714646 (1206.008327007568 steps/sec)\n",
      "Step #7991\tEpoch   2 Batch 1740/3125   Loss: 0.923473 mae: 0.757434 (1207.7029392794618 steps/sec)\n",
      "Step #7992\tEpoch   2 Batch 1741/3125   Loss: 0.834651 mae: 0.707616 (1287.3308083753307 steps/sec)\n",
      "Step #7993\tEpoch   2 Batch 1742/3125   Loss: 0.884374 mae: 0.741755 (1372.526587911908 steps/sec)\n",
      "Step #7994\tEpoch   2 Batch 1743/3125   Loss: 0.828964 mae: 0.741509 (1515.3379818635067 steps/sec)\n",
      "Step #7995\tEpoch   2 Batch 1744/3125   Loss: 0.833692 mae: 0.730528 (1328.6568677141408 steps/sec)\n",
      "Step #7996\tEpoch   2 Batch 1745/3125   Loss: 0.676439 mae: 0.661739 (1204.3530445469216 steps/sec)\n",
      "Step #7997\tEpoch   2 Batch 1746/3125   Loss: 0.792199 mae: 0.710662 (1552.031852460351 steps/sec)\n",
      "Step #7998\tEpoch   2 Batch 1747/3125   Loss: 0.803432 mae: 0.732696 (1384.7066047764624 steps/sec)\n",
      "Step #7999\tEpoch   2 Batch 1748/3125   Loss: 0.709014 mae: 0.658548 (1592.6607734135303 steps/sec)\n",
      "Step #8000\tEpoch   2 Batch 1749/3125   Loss: 0.836252 mae: 0.734681 (1457.8944441354763 steps/sec)\n",
      "Step #8001\tEpoch   2 Batch 1750/3125   Loss: 0.722767 mae: 0.680470 (1528.9599160117234 steps/sec)\n",
      "Step #8002\tEpoch   2 Batch 1751/3125   Loss: 0.860142 mae: 0.735706 (1222.2803755748148 steps/sec)\n",
      "Step #8003\tEpoch   2 Batch 1752/3125   Loss: 0.791702 mae: 0.700366 (1376.571751145419 steps/sec)\n",
      "Step #8004\tEpoch   2 Batch 1753/3125   Loss: 0.644619 mae: 0.638601 (1569.8068012545568 steps/sec)\n",
      "Step #8005\tEpoch   2 Batch 1754/3125   Loss: 0.862787 mae: 0.734197 (1500.8172670931913 steps/sec)\n",
      "Step #8006\tEpoch   2 Batch 1755/3125   Loss: 0.758189 mae: 0.698429 (1370.1592196473255 steps/sec)\n",
      "Step #8007\tEpoch   2 Batch 1756/3125   Loss: 0.898014 mae: 0.766107 (1589.6667778417877 steps/sec)\n",
      "Step #8008\tEpoch   2 Batch 1757/3125   Loss: 0.761060 mae: 0.713120 (1710.7737488273442 steps/sec)\n",
      "Step #8009\tEpoch   2 Batch 1758/3125   Loss: 0.909068 mae: 0.778771 (1568.9259957506658 steps/sec)\n",
      "Step #8010\tEpoch   2 Batch 1759/3125   Loss: 0.990070 mae: 0.781813 (1714.7884675137777 steps/sec)\n",
      "Step #8011\tEpoch   2 Batch 1760/3125   Loss: 0.832047 mae: 0.742860 (1266.5338019833073 steps/sec)\n",
      "Step #8012\tEpoch   2 Batch 1761/3125   Loss: 0.755712 mae: 0.709203 (1443.1268923754474 steps/sec)\n",
      "Step #8013\tEpoch   2 Batch 1762/3125   Loss: 0.696891 mae: 0.672220 (1397.3840095417686 steps/sec)\n",
      "Step #8014\tEpoch   2 Batch 1763/3125   Loss: 0.737367 mae: 0.701628 (1467.5661301609516 steps/sec)\n",
      "Step #8015\tEpoch   2 Batch 1764/3125   Loss: 0.959389 mae: 0.786096 (1477.0548379372033 steps/sec)\n",
      "Step #8016\tEpoch   2 Batch 1765/3125   Loss: 0.807458 mae: 0.734361 (1359.650419470702 steps/sec)\n",
      "Step #8017\tEpoch   2 Batch 1766/3125   Loss: 0.774845 mae: 0.721903 (1560.4970607932137 steps/sec)\n",
      "Step #8018\tEpoch   2 Batch 1767/3125   Loss: 0.710679 mae: 0.661307 (1362.742702674603 steps/sec)\n",
      "Step #8019\tEpoch   2 Batch 1768/3125   Loss: 0.812940 mae: 0.684518 (1513.620878803626 steps/sec)\n",
      "Step #8020\tEpoch   2 Batch 1769/3125   Loss: 0.884211 mae: 0.732886 (1357.7666131454912 steps/sec)\n",
      "Step #8021\tEpoch   2 Batch 1770/3125   Loss: 0.950891 mae: 0.745567 (1550.620351063987 steps/sec)\n",
      "Step #8022\tEpoch   2 Batch 1771/3125   Loss: 0.852941 mae: 0.734596 (1709.1703341483292 steps/sec)\n",
      "Step #8023\tEpoch   2 Batch 1772/3125   Loss: 0.926538 mae: 0.751353 (1660.2557099315204 steps/sec)\n",
      "Step #8024\tEpoch   2 Batch 1773/3125   Loss: 0.753271 mae: 0.679614 (1633.028865995437 steps/sec)\n",
      "Step #8025\tEpoch   2 Batch 1774/3125   Loss: 0.763390 mae: 0.679784 (1606.3237231532828 steps/sec)\n",
      "Step #8026\tEpoch   2 Batch 1775/3125   Loss: 0.866940 mae: 0.736781 (1448.8096718480137 steps/sec)\n",
      "Step #8027\tEpoch   2 Batch 1776/3125   Loss: 0.861356 mae: 0.730960 (1179.3546355344108 steps/sec)\n",
      "Step #8028\tEpoch   2 Batch 1777/3125   Loss: 0.820885 mae: 0.739725 (1497.687572308 steps/sec)\n",
      "Step #8029\tEpoch   2 Batch 1778/3125   Loss: 0.860683 mae: 0.731578 (1606.372939518353 steps/sec)\n",
      "Step #8030\tEpoch   2 Batch 1779/3125   Loss: 0.754764 mae: 0.674333 (1497.890819744727 steps/sec)\n",
      "Step #8031\tEpoch   2 Batch 1780/3125   Loss: 0.875296 mae: 0.748438 (1315.067943387826 steps/sec)\n",
      "Step #8032\tEpoch   2 Batch 1781/3125   Loss: 0.760364 mae: 0.688100 (1589.7390803377855 steps/sec)\n",
      "Step #8033\tEpoch   2 Batch 1782/3125   Loss: 0.815993 mae: 0.744494 (1284.3270785360835 steps/sec)\n",
      "Step #8034\tEpoch   2 Batch 1783/3125   Loss: 0.917180 mae: 0.743654 (1334.7963898825058 steps/sec)\n",
      "Step #8035\tEpoch   2 Batch 1784/3125   Loss: 0.939650 mae: 0.757217 (1323.5251053946936 steps/sec)\n",
      "Step #8036\tEpoch   2 Batch 1785/3125   Loss: 0.921949 mae: 0.745169 (1604.3575384803696 steps/sec)\n",
      "Step #8037\tEpoch   2 Batch 1786/3125   Loss: 0.760646 mae: 0.685885 (1451.7985213081163 steps/sec)\n",
      "Step #8038\tEpoch   2 Batch 1787/3125   Loss: 0.744971 mae: 0.673000 (1526.3448255784333 steps/sec)\n",
      "Step #8039\tEpoch   2 Batch 1788/3125   Loss: 0.832253 mae: 0.722375 (1430.165647142263 steps/sec)\n",
      "Step #8040\tEpoch   2 Batch 1789/3125   Loss: 0.733048 mae: 0.688933 (1096.366621009818 steps/sec)\n",
      "Step #8041\tEpoch   2 Batch 1790/3125   Loss: 0.859228 mae: 0.735636 (1470.756715057157 steps/sec)\n",
      "Step #8042\tEpoch   2 Batch 1791/3125   Loss: 0.931049 mae: 0.755160 (1536.1837719844416 steps/sec)\n",
      "Step #8043\tEpoch   2 Batch 1792/3125   Loss: 0.810112 mae: 0.721300 (1566.078962893265 steps/sec)\n",
      "Step #8044\tEpoch   2 Batch 1793/3125   Loss: 0.735708 mae: 0.656827 (1482.9770533536046 steps/sec)\n",
      "Step #8045\tEpoch   2 Batch 1794/3125   Loss: 0.776936 mae: 0.676244 (1504.747827709175 steps/sec)\n",
      "Step #8046\tEpoch   2 Batch 1795/3125   Loss: 0.872705 mae: 0.726758 (1153.561645342633 steps/sec)\n",
      "Step #8047\tEpoch   2 Batch 1796/3125   Loss: 0.822752 mae: 0.720840 (1753.3396316330711 steps/sec)\n",
      "Step #8048\tEpoch   2 Batch 1797/3125   Loss: 0.802710 mae: 0.703903 (1264.426195902519 steps/sec)\n",
      "Step #8049\tEpoch   2 Batch 1798/3125   Loss: 1.025983 mae: 0.791251 (1278.3848531213616 steps/sec)\n",
      "Step #8050\tEpoch   2 Batch 1799/3125   Loss: 0.940516 mae: 0.760640 (1582.8637416881147 steps/sec)\n",
      "Step #8051\tEpoch   2 Batch 1800/3125   Loss: 0.776662 mae: 0.702606 (1137.65433438212 steps/sec)\n",
      "Step #8052\tEpoch   2 Batch 1801/3125   Loss: 0.777639 mae: 0.701377 (1318.9469314851385 steps/sec)\n",
      "Step #8053\tEpoch   2 Batch 1802/3125   Loss: 0.836681 mae: 0.733814 (1307.0439389217825 steps/sec)\n",
      "Step #8054\tEpoch   2 Batch 1803/3125   Loss: 0.759514 mae: 0.695046 (1309.9585865715553 steps/sec)\n",
      "Step #8055\tEpoch   2 Batch 1804/3125   Loss: 0.859370 mae: 0.743285 (1498.715071821625 steps/sec)\n",
      "Step #8056\tEpoch   2 Batch 1805/3125   Loss: 0.870520 mae: 0.734190 (1586.2399685346686 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8057\tEpoch   2 Batch 1806/3125   Loss: 0.791446 mae: 0.704242 (1276.1366955505794 steps/sec)\n",
      "Step #8058\tEpoch   2 Batch 1807/3125   Loss: 0.882302 mae: 0.751177 (1858.94658464375 steps/sec)\n",
      "Step #8059\tEpoch   2 Batch 1808/3125   Loss: 0.789976 mae: 0.683231 (1717.8505897771952 steps/sec)\n",
      "Step #8060\tEpoch   2 Batch 1809/3125   Loss: 0.801133 mae: 0.706445 (1872.2565439417206 steps/sec)\n",
      "Step #8061\tEpoch   2 Batch 1810/3125   Loss: 0.903358 mae: 0.754481 (1772.1562629395212 steps/sec)\n",
      "Step #8062\tEpoch   2 Batch 1811/3125   Loss: 0.860969 mae: 0.744381 (1771.0338304592362 steps/sec)\n",
      "Step #8063\tEpoch   2 Batch 1812/3125   Loss: 0.775835 mae: 0.711349 (1797.9235787831246 steps/sec)\n",
      "Step #8064\tEpoch   2 Batch 1813/3125   Loss: 0.792146 mae: 0.707663 (1851.2994350282486 steps/sec)\n",
      "Step #8065\tEpoch   2 Batch 1814/3125   Loss: 0.797043 mae: 0.702735 (1002.2662862440917 steps/sec)\n",
      "Step #8066\tEpoch   2 Batch 1815/3125   Loss: 0.791255 mae: 0.696248 (1419.7381426269683 steps/sec)\n",
      "Step #8067\tEpoch   2 Batch 1816/3125   Loss: 0.850437 mae: 0.721567 (1416.439503437842 steps/sec)\n",
      "Step #8068\tEpoch   2 Batch 1817/3125   Loss: 0.783704 mae: 0.702814 (1339.015062029511 steps/sec)\n",
      "Step #8069\tEpoch   2 Batch 1818/3125   Loss: 0.878962 mae: 0.771926 (1504.391615615271 steps/sec)\n",
      "Step #8070\tEpoch   2 Batch 1819/3125   Loss: 0.777297 mae: 0.711759 (1070.2539946618763 steps/sec)\n",
      "Step #8071\tEpoch   2 Batch 1820/3125   Loss: 0.784767 mae: 0.694223 (1174.5329091805188 steps/sec)\n",
      "Step #8072\tEpoch   2 Batch 1821/3125   Loss: 0.821280 mae: 0.718616 (1513.6427282569468 steps/sec)\n",
      "Step #8073\tEpoch   2 Batch 1822/3125   Loss: 0.805696 mae: 0.706517 (1698.882885218279 steps/sec)\n",
      "Step #8074\tEpoch   2 Batch 1823/3125   Loss: 0.843854 mae: 0.744250 (1640.964006259781 steps/sec)\n",
      "Step #8075\tEpoch   2 Batch 1824/3125   Loss: 0.735312 mae: 0.691781 (1747.087981205795 steps/sec)\n",
      "Step #8076\tEpoch   2 Batch 1825/3125   Loss: 0.930688 mae: 0.779722 (1669.7867732534994 steps/sec)\n",
      "Step #8077\tEpoch   2 Batch 1826/3125   Loss: 0.866923 mae: 0.742602 (1007.5583015441381 steps/sec)\n",
      "Step #8078\tEpoch   2 Batch 1827/3125   Loss: 0.846851 mae: 0.735829 (1393.8454585332784 steps/sec)\n",
      "Step #8079\tEpoch   2 Batch 1828/3125   Loss: 0.742877 mae: 0.671765 (1515.7432168721723 steps/sec)\n",
      "Step #8080\tEpoch   2 Batch 1829/3125   Loss: 0.832177 mae: 0.744398 (1411.871789520456 steps/sec)\n",
      "Step #8081\tEpoch   2 Batch 1830/3125   Loss: 0.851252 mae: 0.741590 (1540.0757865053022 steps/sec)\n",
      "Step #8082\tEpoch   2 Batch 1831/3125   Loss: 0.730878 mae: 0.678112 (1397.8963085414139 steps/sec)\n",
      "Step #8083\tEpoch   2 Batch 1832/3125   Loss: 0.718791 mae: 0.681326 (1248.438523175102 steps/sec)\n",
      "Step #8084\tEpoch   2 Batch 1833/3125   Loss: 0.763613 mae: 0.694514 (1285.9809416352912 steps/sec)\n",
      "Step #8085\tEpoch   2 Batch 1834/3125   Loss: 0.828905 mae: 0.730576 (1417.7992914897645 steps/sec)\n",
      "Step #8086\tEpoch   2 Batch 1835/3125   Loss: 0.920845 mae: 0.754944 (1270.7778632846348 steps/sec)\n",
      "Step #8087\tEpoch   2 Batch 1836/3125   Loss: 0.861130 mae: 0.750295 (1473.5779984119958 steps/sec)\n",
      "Step #8088\tEpoch   2 Batch 1837/3125   Loss: 0.791671 mae: 0.736851 (1543.692078938264 steps/sec)\n",
      "Step #8089\tEpoch   2 Batch 1838/3125   Loss: 0.830236 mae: 0.730972 (1649.3008477908677 steps/sec)\n",
      "Step #8090\tEpoch   2 Batch 1839/3125   Loss: 0.748181 mae: 0.675794 (1544.9202548896828 steps/sec)\n",
      "Step #8091\tEpoch   2 Batch 1840/3125   Loss: 0.851403 mae: 0.701346 (1189.5765028872224 steps/sec)\n",
      "Step #8092\tEpoch   2 Batch 1841/3125   Loss: 0.838425 mae: 0.700748 (1512.1693045390634 steps/sec)\n",
      "Step #8093\tEpoch   2 Batch 1842/3125   Loss: 0.764357 mae: 0.679270 (1354.13701814425 steps/sec)\n",
      "Step #8094\tEpoch   2 Batch 1843/3125   Loss: 0.596665 mae: 0.620430 (1346.6589610222823 steps/sec)\n",
      "Step #8095\tEpoch   2 Batch 1844/3125   Loss: 0.835511 mae: 0.725281 (1085.757775005048 steps/sec)\n",
      "Step #8096\tEpoch   2 Batch 1845/3125   Loss: 0.831178 mae: 0.744888 (1703.464353307178 steps/sec)\n",
      "Step #8097\tEpoch   2 Batch 1846/3125   Loss: 1.091677 mae: 0.814027 (1377.7202583120372 steps/sec)\n",
      "Step #8098\tEpoch   2 Batch 1847/3125   Loss: 0.832752 mae: 0.725422 (1453.0257049816394 steps/sec)\n",
      "Step #8099\tEpoch   2 Batch 1848/3125   Loss: 0.951828 mae: 0.782023 (1324.319101776366 steps/sec)\n",
      "Step #8100\tEpoch   2 Batch 1849/3125   Loss: 0.784686 mae: 0.692717 (1480.0361336946703 steps/sec)\n",
      "Step #8101\tEpoch   2 Batch 1850/3125   Loss: 0.784600 mae: 0.707355 (1300.3255228517041 steps/sec)\n",
      "Step #8102\tEpoch   2 Batch 1851/3125   Loss: 0.841279 mae: 0.728506 (1259.195302226999 steps/sec)\n",
      "Step #8103\tEpoch   2 Batch 1852/3125   Loss: 0.908722 mae: 0.769603 (1744.0513613758462 steps/sec)\n",
      "Step #8104\tEpoch   2 Batch 1853/3125   Loss: 0.803920 mae: 0.721648 (1895.7306214689265 steps/sec)\n",
      "Step #8105\tEpoch   2 Batch 1854/3125   Loss: 0.934443 mae: 0.772959 (1779.554846538308 steps/sec)\n",
      "Step #8106\tEpoch   2 Batch 1855/3125   Loss: 0.793535 mae: 0.689036 (1492.93239933937 steps/sec)\n",
      "Step #8107\tEpoch   2 Batch 1856/3125   Loss: 0.905652 mae: 0.738171 (1105.608831576894 steps/sec)\n",
      "Step #8108\tEpoch   2 Batch 1857/3125   Loss: 0.872532 mae: 0.718513 (1254.645201045761 steps/sec)\n",
      "Step #8109\tEpoch   2 Batch 1858/3125   Loss: 0.743183 mae: 0.659253 (1298.9965560813657 steps/sec)\n",
      "Step #8110\tEpoch   2 Batch 1859/3125   Loss: 0.909883 mae: 0.736021 (1341.1987413982758 steps/sec)\n",
      "Step #8111\tEpoch   2 Batch 1860/3125   Loss: 0.891519 mae: 0.753741 (1270.2929275797737 steps/sec)\n",
      "Step #8112\tEpoch   2 Batch 1861/3125   Loss: 0.847719 mae: 0.714432 (1397.0116642352298 steps/sec)\n",
      "Step #8113\tEpoch   2 Batch 1862/3125   Loss: 0.724446 mae: 0.662410 (1113.7291556027615 steps/sec)\n",
      "Step #8114\tEpoch   2 Batch 1863/3125   Loss: 0.767696 mae: 0.677652 (1503.9708550569776 steps/sec)\n",
      "Step #8115\tEpoch   2 Batch 1864/3125   Loss: 0.709223 mae: 0.666052 (1716.7396589690486 steps/sec)\n",
      "Step #8116\tEpoch   2 Batch 1865/3125   Loss: 0.681449 mae: 0.675304 (1322.3650774634123 steps/sec)\n",
      "Step #8117\tEpoch   2 Batch 1866/3125   Loss: 0.835011 mae: 0.738529 (1389.7443373845276 steps/sec)\n",
      "Step #8118\tEpoch   2 Batch 1867/3125   Loss: 0.695420 mae: 0.671829 (1571.0298226820187 steps/sec)\n",
      "Step #8119\tEpoch   2 Batch 1868/3125   Loss: 0.910847 mae: 0.766442 (1324.9633560778368 steps/sec)\n",
      "Step #8120\tEpoch   2 Batch 1869/3125   Loss: 0.870691 mae: 0.738837 (1309.4432927895302 steps/sec)\n",
      "Step #8121\tEpoch   2 Batch 1870/3125   Loss: 0.882568 mae: 0.743518 (1991.3136780135783 steps/sec)\n",
      "Step #8122\tEpoch   2 Batch 1871/3125   Loss: 0.715345 mae: 0.680522 (1969.3417222274393 steps/sec)\n",
      "Step #8123\tEpoch   2 Batch 1872/3125   Loss: 0.837906 mae: 0.732217 (1741.7916645902892 steps/sec)\n",
      "Step #8124\tEpoch   2 Batch 1873/3125   Loss: 0.704728 mae: 0.674414 (1257.6699110639345 steps/sec)\n",
      "Step #8125\tEpoch   2 Batch 1874/3125   Loss: 0.892598 mae: 0.746781 (1439.5507993492631 steps/sec)\n",
      "Step #8126\tEpoch   2 Batch 1875/3125   Loss: 0.824367 mae: 0.713112 (1289.9200393652357 steps/sec)\n",
      "Step #8127\tEpoch   2 Batch 1876/3125   Loss: 0.882595 mae: 0.749804 (1290.6265577785846 steps/sec)\n",
      "Step #8128\tEpoch   2 Batch 1877/3125   Loss: 0.846703 mae: 0.713636 (1267.14601636234 steps/sec)\n",
      "Step #8129\tEpoch   2 Batch 1878/3125   Loss: 0.809241 mae: 0.728903 (1333.7352692398197 steps/sec)\n",
      "Step #8130\tEpoch   2 Batch 1879/3125   Loss: 0.750726 mae: 0.676725 (1625.2960505921012 steps/sec)\n",
      "Step #8131\tEpoch   2 Batch 1880/3125   Loss: 0.839350 mae: 0.731442 (1437.9419105345437 steps/sec)\n",
      "Step #8132\tEpoch   2 Batch 1881/3125   Loss: 0.759569 mae: 0.690572 (1288.8894351914448 steps/sec)\n",
      "Step #8133\tEpoch   2 Batch 1882/3125   Loss: 0.757557 mae: 0.704131 (1537.3774842204807 steps/sec)\n",
      "Step #8134\tEpoch   2 Batch 1883/3125   Loss: 0.844359 mae: 0.726263 (1408.4487367190964 steps/sec)\n",
      "Step #8135\tEpoch   2 Batch 1884/3125   Loss: 0.909073 mae: 0.764926 (1651.1447737221679 steps/sec)\n",
      "Step #8136\tEpoch   2 Batch 1885/3125   Loss: 0.833641 mae: 0.709000 (1507.9830301287122 steps/sec)\n",
      "Step #8137\tEpoch   2 Batch 1886/3125   Loss: 0.806277 mae: 0.722557 (1422.5888290439432 steps/sec)\n",
      "Step #8138\tEpoch   2 Batch 1887/3125   Loss: 0.744330 mae: 0.669591 (1382.3244041341488 steps/sec)\n",
      "Step #8139\tEpoch   2 Batch 1888/3125   Loss: 0.796878 mae: 0.695209 (1485.30876176581 steps/sec)\n",
      "Step #8140\tEpoch   2 Batch 1889/3125   Loss: 0.905350 mae: 0.743544 (1578.1943514219276 steps/sec)\n",
      "Step #8141\tEpoch   2 Batch 1890/3125   Loss: 0.716423 mae: 0.646817 (1575.5384765639674 steps/sec)\n",
      "Step #8142\tEpoch   2 Batch 1891/3125   Loss: 0.823152 mae: 0.725609 (1195.6533141769005 steps/sec)\n",
      "Step #8143\tEpoch   2 Batch 1892/3125   Loss: 0.791977 mae: 0.686798 (1338.7244419194014 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8144\tEpoch   2 Batch 1893/3125   Loss: 0.808485 mae: 0.735494 (1769.7335887460865 steps/sec)\n",
      "Step #8145\tEpoch   2 Batch 1894/3125   Loss: 0.705326 mae: 0.662263 (1773.7300607270329 steps/sec)\n",
      "Step #8146\tEpoch   2 Batch 1895/3125   Loss: 0.750327 mae: 0.695494 (1890.4331365213864 steps/sec)\n",
      "Step #8147\tEpoch   2 Batch 1896/3125   Loss: 0.925148 mae: 0.771737 (2026.096785724637 steps/sec)\n",
      "Step #8148\tEpoch   2 Batch 1897/3125   Loss: 0.819085 mae: 0.735608 (1857.349593927961 steps/sec)\n",
      "Step #8149\tEpoch   2 Batch 1898/3125   Loss: 0.780872 mae: 0.729962 (2019.0549543651557 steps/sec)\n",
      "Step #8150\tEpoch   2 Batch 1899/3125   Loss: 0.876066 mae: 0.729069 (1990.6899039374264 steps/sec)\n",
      "Step #8151\tEpoch   2 Batch 1900/3125   Loss: 0.706816 mae: 0.659945 (1987.8218009478674 steps/sec)\n",
      "Step #8152\tEpoch   2 Batch 1901/3125   Loss: 0.884794 mae: 0.749290 (1907.0912827601258 steps/sec)\n",
      "Step #8153\tEpoch   2 Batch 1902/3125   Loss: 0.822694 mae: 0.720585 (1766.4094875509586 steps/sec)\n",
      "Step #8154\tEpoch   2 Batch 1903/3125   Loss: 0.840681 mae: 0.738804 (1890.075345181874 steps/sec)\n",
      "Step #8155\tEpoch   2 Batch 1904/3125   Loss: 0.855440 mae: 0.733702 (2025.3532280650925 steps/sec)\n",
      "Step #8156\tEpoch   2 Batch 1905/3125   Loss: 0.753782 mae: 0.693367 (1940.3162384463792 steps/sec)\n",
      "Step #8157\tEpoch   2 Batch 1906/3125   Loss: 0.916825 mae: 0.792510 (2028.5661775374585 steps/sec)\n",
      "Step #8158\tEpoch   2 Batch 1907/3125   Loss: 0.900340 mae: 0.748348 (2142.53080239472 steps/sec)\n",
      "Step #8159\tEpoch   2 Batch 1908/3125   Loss: 0.834630 mae: 0.738525 (1924.469363970892 steps/sec)\n",
      "Step #8160\tEpoch   2 Batch 1909/3125   Loss: 0.726634 mae: 0.651353 (1370.9115868605982 steps/sec)\n",
      "Step #8161\tEpoch   2 Batch 1910/3125   Loss: 0.733019 mae: 0.706074 (884.2996533897805 steps/sec)\n",
      "Step #8162\tEpoch   2 Batch 1911/3125   Loss: 0.808502 mae: 0.719622 (936.1700607997715 steps/sec)\n",
      "Step #8163\tEpoch   2 Batch 1912/3125   Loss: 0.880433 mae: 0.756731 (1108.0798900982775 steps/sec)\n",
      "Step #8164\tEpoch   2 Batch 1913/3125   Loss: 0.824520 mae: 0.708295 (1155.6148229783716 steps/sec)\n",
      "Step #8165\tEpoch   2 Batch 1914/3125   Loss: 0.852623 mae: 0.715996 (1125.3653013367104 steps/sec)\n",
      "Step #8166\tEpoch   2 Batch 1915/3125   Loss: 0.723092 mae: 0.677430 (1352.6347699333085 steps/sec)\n",
      "Step #8167\tEpoch   2 Batch 1916/3125   Loss: 0.777994 mae: 0.687485 (1289.6582684041252 steps/sec)\n",
      "Step #8168\tEpoch   2 Batch 1917/3125   Loss: 0.792615 mae: 0.709993 (1494.5496009122007 steps/sec)\n",
      "Step #8169\tEpoch   2 Batch 1918/3125   Loss: 0.724353 mae: 0.671641 (1731.0089804543054 steps/sec)\n",
      "Step #8170\tEpoch   2 Batch 1919/3125   Loss: 0.743747 mae: 0.681754 (1014.6511391621146 steps/sec)\n",
      "Step #8171\tEpoch   2 Batch 1920/3125   Loss: 0.820340 mae: 0.677765 (892.2646221658717 steps/sec)\n",
      "Step #8172\tEpoch   2 Batch 1921/3125   Loss: 0.694999 mae: 0.657391 (1387.7762778263057 steps/sec)\n",
      "Step #8173\tEpoch   2 Batch 1922/3125   Loss: 0.949083 mae: 0.768705 (984.5044503699253 steps/sec)\n",
      "Step #8174\tEpoch   2 Batch 1923/3125   Loss: 0.831984 mae: 0.719838 (1214.0933458381094 steps/sec)\n",
      "Step #8175\tEpoch   2 Batch 1924/3125   Loss: 0.995977 mae: 0.795516 (1101.1273004121708 steps/sec)\n",
      "Step #8176\tEpoch   2 Batch 1925/3125   Loss: 0.779016 mae: 0.692308 (1241.9913060987599 steps/sec)\n",
      "Step #8177\tEpoch   2 Batch 1926/3125   Loss: 0.805786 mae: 0.706911 (1594.2468356836064 steps/sec)\n",
      "Step #8178\tEpoch   2 Batch 1927/3125   Loss: 0.875068 mae: 0.741433 (1392.2168965837725 steps/sec)\n",
      "Step #8179\tEpoch   2 Batch 1928/3125   Loss: 0.836187 mae: 0.731203 (791.3512371277957 steps/sec)\n",
      "Step #8180\tEpoch   2 Batch 1929/3125   Loss: 0.793424 mae: 0.716283 (755.1263493709537 steps/sec)\n",
      "Step #8181\tEpoch   2 Batch 1930/3125   Loss: 0.932301 mae: 0.762832 (849.4035974515689 steps/sec)\n",
      "Step #8182\tEpoch   2 Batch 1931/3125   Loss: 0.661003 mae: 0.661719 (870.7475762419814 steps/sec)\n",
      "Step #8183\tEpoch   2 Batch 1932/3125   Loss: 0.891320 mae: 0.754073 (945.3912698520031 steps/sec)\n",
      "Step #8184\tEpoch   2 Batch 1933/3125   Loss: 0.651817 mae: 0.629501 (1390.398525502052 steps/sec)\n",
      "Step #8185\tEpoch   2 Batch 1934/3125   Loss: 0.731357 mae: 0.685386 (665.9845345273822 steps/sec)\n",
      "Step #8186\tEpoch   2 Batch 1935/3125   Loss: 0.742735 mae: 0.676865 (889.3623517836847 steps/sec)\n",
      "Step #8187\tEpoch   2 Batch 1936/3125   Loss: 0.773754 mae: 0.692694 (738.335011512544 steps/sec)\n",
      "Step #8188\tEpoch   2 Batch 1937/3125   Loss: 0.995867 mae: 0.764685 (1132.5060212336239 steps/sec)\n",
      "Step #8189\tEpoch   2 Batch 1938/3125   Loss: 0.865588 mae: 0.733028 (882.2869643871347 steps/sec)\n",
      "Step #8190\tEpoch   2 Batch 1939/3125   Loss: 0.808560 mae: 0.697911 (932.7142374268382 steps/sec)\n",
      "Step #8191\tEpoch   2 Batch 1940/3125   Loss: 0.764103 mae: 0.695881 (797.9969711037206 steps/sec)\n",
      "Step #8192\tEpoch   2 Batch 1941/3125   Loss: 0.711948 mae: 0.685581 (912.8887766784053 steps/sec)\n",
      "Step #8193\tEpoch   2 Batch 1942/3125   Loss: 0.680060 mae: 0.659850 (1060.5120632721278 steps/sec)\n",
      "Step #8194\tEpoch   2 Batch 1943/3125   Loss: 0.773784 mae: 0.701139 (999.3576364069573 steps/sec)\n",
      "Step #8195\tEpoch   2 Batch 1944/3125   Loss: 0.843734 mae: 0.739877 (1380.5953838659136 steps/sec)\n",
      "Step #8196\tEpoch   2 Batch 1945/3125   Loss: 0.852359 mae: 0.724008 (1691.9884465815758 steps/sec)\n",
      "Step #8197\tEpoch   2 Batch 1946/3125   Loss: 0.856022 mae: 0.759933 (1541.445487354007 steps/sec)\n",
      "Step #8198\tEpoch   2 Batch 1947/3125   Loss: 0.783974 mae: 0.722971 (1081.9431260060258 steps/sec)\n",
      "Step #8199\tEpoch   2 Batch 1948/3125   Loss: 0.773786 mae: 0.712572 (1065.4906642956942 steps/sec)\n",
      "Step #8200\tEpoch   2 Batch 1949/3125   Loss: 0.810311 mae: 0.701690 (705.9454000740566 steps/sec)\n",
      "Step #8201\tEpoch   2 Batch 1950/3125   Loss: 0.776802 mae: 0.695515 (1061.02715364806 steps/sec)\n",
      "Step #8202\tEpoch   2 Batch 1951/3125   Loss: 0.782413 mae: 0.695847 (1035.1756511952772 steps/sec)\n",
      "Step #8203\tEpoch   2 Batch 1952/3125   Loss: 0.814617 mae: 0.704541 (860.9846618714488 steps/sec)\n",
      "Step #8204\tEpoch   2 Batch 1953/3125   Loss: 1.088098 mae: 0.820326 (869.1705141701411 steps/sec)\n",
      "Step #8205\tEpoch   2 Batch 1954/3125   Loss: 0.693957 mae: 0.667994 (1550.3337744231949 steps/sec)\n",
      "Step #8206\tEpoch   2 Batch 1955/3125   Loss: 0.739713 mae: 0.682788 (1652.6801897646856 steps/sec)\n",
      "Step #8207\tEpoch   2 Batch 1956/3125   Loss: 0.820206 mae: 0.742539 (1121.4056927132629 steps/sec)\n",
      "Step #8208\tEpoch   2 Batch 1957/3125   Loss: 0.882930 mae: 0.734170 (1135.6705765127638 steps/sec)\n",
      "Step #8209\tEpoch   2 Batch 1958/3125   Loss: 0.874749 mae: 0.730042 (850.7301875770753 steps/sec)\n",
      "Step #8210\tEpoch   2 Batch 1959/3125   Loss: 0.908746 mae: 0.735032 (1266.7556613290003 steps/sec)\n",
      "Step #8211\tEpoch   2 Batch 1960/3125   Loss: 0.710821 mae: 0.671223 (1097.095027595407 steps/sec)\n",
      "Step #8212\tEpoch   2 Batch 1961/3125   Loss: 0.961833 mae: 0.773480 (1356.9934775856714 steps/sec)\n",
      "Step #8213\tEpoch   2 Batch 1962/3125   Loss: 0.793852 mae: 0.719510 (1149.3511048754822 steps/sec)\n",
      "Step #8214\tEpoch   2 Batch 1963/3125   Loss: 0.756827 mae: 0.701915 (1167.3737941629975 steps/sec)\n",
      "Step #8215\tEpoch   2 Batch 1964/3125   Loss: 0.963185 mae: 0.757512 (1108.2614187043212 steps/sec)\n",
      "Step #8216\tEpoch   2 Batch 1965/3125   Loss: 0.745462 mae: 0.684103 (1380.6499183651972 steps/sec)\n",
      "Step #8217\tEpoch   2 Batch 1966/3125   Loss: 0.825664 mae: 0.737306 (1369.255680334291 steps/sec)\n",
      "Step #8218\tEpoch   2 Batch 1967/3125   Loss: 0.840230 mae: 0.736723 (1309.0999887639048 steps/sec)\n",
      "Step #8219\tEpoch   2 Batch 1968/3125   Loss: 0.826178 mae: 0.712444 (1461.8068770344967 steps/sec)\n",
      "Step #8220\tEpoch   2 Batch 1969/3125   Loss: 0.696348 mae: 0.673464 (1323.307967036434 steps/sec)\n",
      "Step #8221\tEpoch   2 Batch 1970/3125   Loss: 0.869282 mae: 0.735065 (1209.3953461549552 steps/sec)\n",
      "Step #8222\tEpoch   2 Batch 1971/3125   Loss: 0.755943 mae: 0.683413 (1152.0977426673771 steps/sec)\n",
      "Step #8223\tEpoch   2 Batch 1972/3125   Loss: 0.763113 mae: 0.687698 (919.2874175901248 steps/sec)\n",
      "Step #8224\tEpoch   2 Batch 1973/3125   Loss: 0.795337 mae: 0.734840 (1078.4434925254936 steps/sec)\n",
      "Step #8225\tEpoch   2 Batch 1974/3125   Loss: 0.813294 mae: 0.707650 (1456.4162395655374 steps/sec)\n",
      "Step #8226\tEpoch   2 Batch 1975/3125   Loss: 0.889512 mae: 0.723088 (1344.1214172178638 steps/sec)\n",
      "Step #8227\tEpoch   2 Batch 1976/3125   Loss: 0.817465 mae: 0.717157 (1188.1881019830028 steps/sec)\n",
      "Step #8228\tEpoch   2 Batch 1977/3125   Loss: 0.896837 mae: 0.761416 (1244.9256779218313 steps/sec)\n",
      "Step #8229\tEpoch   2 Batch 1978/3125   Loss: 0.949552 mae: 0.764561 (1646.0257285706437 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8230\tEpoch   2 Batch 1979/3125   Loss: 0.769421 mae: 0.704575 (1432.3047712712919 steps/sec)\n",
      "Step #8231\tEpoch   2 Batch 1980/3125   Loss: 0.782010 mae: 0.710324 (1219.4516644861171 steps/sec)\n",
      "Step #8232\tEpoch   2 Batch 1981/3125   Loss: 0.901650 mae: 0.746840 (1467.997592014448 steps/sec)\n",
      "Step #8233\tEpoch   2 Batch 1982/3125   Loss: 0.930701 mae: 0.776815 (1546.4582257945578 steps/sec)\n",
      "Step #8234\tEpoch   2 Batch 1983/3125   Loss: 0.793047 mae: 0.696239 (1229.6477845076781 steps/sec)\n",
      "Step #8235\tEpoch   2 Batch 1984/3125   Loss: 0.722282 mae: 0.698835 (1501.0535959688502 steps/sec)\n",
      "Step #8236\tEpoch   2 Batch 1985/3125   Loss: 0.825727 mae: 0.731336 (1617.2868258901374 steps/sec)\n",
      "Step #8237\tEpoch   2 Batch 1986/3125   Loss: 0.844305 mae: 0.725253 (1502.4946624826262 steps/sec)\n",
      "Step #8238\tEpoch   2 Batch 1987/3125   Loss: 0.805178 mae: 0.745398 (1573.469785867559 steps/sec)\n",
      "Step #8239\tEpoch   2 Batch 1988/3125   Loss: 0.725592 mae: 0.686041 (1863.2904194542919 steps/sec)\n",
      "Step #8240\tEpoch   2 Batch 1989/3125   Loss: 0.782798 mae: 0.681654 (2076.408677313637 steps/sec)\n",
      "Step #8241\tEpoch   2 Batch 1990/3125   Loss: 0.760301 mae: 0.678250 (1835.4530973761136 steps/sec)\n",
      "Step #8242\tEpoch   2 Batch 1991/3125   Loss: 0.721676 mae: 0.676176 (1830.0394429124926 steps/sec)\n",
      "Step #8243\tEpoch   2 Batch 1992/3125   Loss: 0.905818 mae: 0.767225 (1976.6551048107374 steps/sec)\n",
      "Step #8244\tEpoch   2 Batch 1993/3125   Loss: 0.816202 mae: 0.709771 (1802.900594045787 steps/sec)\n",
      "Step #8245\tEpoch   2 Batch 1994/3125   Loss: 0.845387 mae: 0.724959 (1621.21261315584 steps/sec)\n",
      "Step #8246\tEpoch   2 Batch 1995/3125   Loss: 0.903433 mae: 0.757182 (1445.7533642179571 steps/sec)\n",
      "Step #8247\tEpoch   2 Batch 1996/3125   Loss: 0.829751 mae: 0.711833 (1539.9740051842768 steps/sec)\n",
      "Step #8248\tEpoch   2 Batch 1997/3125   Loss: 0.917055 mae: 0.730567 (1393.0307015795836 steps/sec)\n",
      "Step #8249\tEpoch   2 Batch 1998/3125   Loss: 0.884101 mae: 0.721671 (1240.69076086635 steps/sec)\n",
      "Step #8250\tEpoch   2 Batch 1999/3125   Loss: 0.898889 mae: 0.755148 (1078.0332587966175 steps/sec)\n",
      "Step #8251\tEpoch   2 Batch 2000/3125   Loss: 0.951011 mae: 0.781404 (1375.3710346998603 steps/sec)\n",
      "Step #8252\tEpoch   2 Batch 2001/3125   Loss: 0.745608 mae: 0.675543 (1371.745529231695 steps/sec)\n",
      "Step #8253\tEpoch   2 Batch 2002/3125   Loss: 0.778459 mae: 0.717663 (1603.1801363789255 steps/sec)\n",
      "Step #8254\tEpoch   2 Batch 2003/3125   Loss: 0.716015 mae: 0.672688 (1925.9008926275576 steps/sec)\n",
      "Step #8255\tEpoch   2 Batch 2004/3125   Loss: 0.886556 mae: 0.759387 (1547.8507321681625 steps/sec)\n",
      "Step #8256\tEpoch   2 Batch 2005/3125   Loss: 0.818055 mae: 0.722247 (1514.0033353306815 steps/sec)\n",
      "Step #8257\tEpoch   2 Batch 2006/3125   Loss: 0.897678 mae: 0.762292 (1917.3435242919052 steps/sec)\n",
      "Step #8258\tEpoch   2 Batch 2007/3125   Loss: 0.749754 mae: 0.684517 (1955.641762094819 steps/sec)\n",
      "Step #8259\tEpoch   2 Batch 2008/3125   Loss: 0.847166 mae: 0.745667 (2044.645503470868 steps/sec)\n",
      "Step #8260\tEpoch   2 Batch 2009/3125   Loss: 0.792991 mae: 0.714541 (1734.6744309158285 steps/sec)\n",
      "Step #8261\tEpoch   2 Batch 2010/3125   Loss: 0.734712 mae: 0.642330 (1876.377431419215 steps/sec)\n",
      "Step #8262\tEpoch   2 Batch 2011/3125   Loss: 0.738381 mae: 0.674486 (2022.3259402121505 steps/sec)\n",
      "Step #8263\tEpoch   2 Batch 2012/3125   Loss: 0.829654 mae: 0.739385 (1888.5094733808803 steps/sec)\n",
      "Step #8264\tEpoch   2 Batch 2013/3125   Loss: 0.735987 mae: 0.692377 (1559.974708967159 steps/sec)\n",
      "Step #8265\tEpoch   2 Batch 2014/3125   Loss: 0.870868 mae: 0.709583 (1869.719339538533 steps/sec)\n",
      "Step #8266\tEpoch   2 Batch 2015/3125   Loss: 0.938265 mae: 0.767223 (2019.3660208758618 steps/sec)\n",
      "Step #8267\tEpoch   2 Batch 2016/3125   Loss: 0.761493 mae: 0.706873 (1837.7531437584892 steps/sec)\n",
      "Step #8268\tEpoch   2 Batch 2017/3125   Loss: 0.792482 mae: 0.721614 (2006.901633539719 steps/sec)\n",
      "Step #8269\tEpoch   2 Batch 2018/3125   Loss: 0.729977 mae: 0.683709 (1847.1088719976747 steps/sec)\n",
      "Step #8270\tEpoch   2 Batch 2019/3125   Loss: 0.880673 mae: 0.715121 (1821.503826009919 steps/sec)\n",
      "Step #8271\tEpoch   2 Batch 2020/3125   Loss: 0.828510 mae: 0.726200 (1673.3708358268502 steps/sec)\n",
      "Step #8272\tEpoch   2 Batch 2021/3125   Loss: 0.733242 mae: 0.682428 (1774.8108528968703 steps/sec)\n",
      "Step #8273\tEpoch   2 Batch 2022/3125   Loss: 0.898762 mae: 0.777333 (2257.0407680055105 steps/sec)\n",
      "Step #8274\tEpoch   2 Batch 2023/3125   Loss: 1.052322 mae: 0.807641 (1881.1743705205372 steps/sec)\n",
      "Step #8275\tEpoch   2 Batch 2024/3125   Loss: 0.867690 mae: 0.716959 (1888.0674145164485 steps/sec)\n",
      "Step #8276\tEpoch   2 Batch 2025/3125   Loss: 0.843379 mae: 0.740684 (1900.28271112722 steps/sec)\n",
      "Step #8277\tEpoch   2 Batch 2026/3125   Loss: 0.866812 mae: 0.734235 (1996.9072557608074 steps/sec)\n",
      "Step #8278\tEpoch   2 Batch 2027/3125   Loss: 0.815457 mae: 0.704689 (1876.0920712451805 steps/sec)\n",
      "Step #8279\tEpoch   2 Batch 2028/3125   Loss: 0.791434 mae: 0.723772 (1936.1781487157707 steps/sec)\n",
      "Step #8280\tEpoch   2 Batch 2029/3125   Loss: 0.996540 mae: 0.798666 (1452.6432450404523 steps/sec)\n",
      "Step #8281\tEpoch   2 Batch 2030/3125   Loss: 0.781822 mae: 0.701414 (1799.0186322616066 steps/sec)\n",
      "Step #8282\tEpoch   2 Batch 2031/3125   Loss: 0.901992 mae: 0.746694 (1164.0368113142615 steps/sec)\n",
      "Step #8283\tEpoch   2 Batch 2032/3125   Loss: 0.790305 mae: 0.726282 (1439.264292087022 steps/sec)\n",
      "Step #8284\tEpoch   2 Batch 2033/3125   Loss: 0.893699 mae: 0.755686 (1657.0417193426042 steps/sec)\n",
      "Step #8285\tEpoch   2 Batch 2034/3125   Loss: 0.822160 mae: 0.703926 (1429.8438671848367 steps/sec)\n",
      "Step #8286\tEpoch   2 Batch 2035/3125   Loss: 0.994389 mae: 0.792407 (1062.1556601145646 steps/sec)\n",
      "Step #8287\tEpoch   2 Batch 2036/3125   Loss: 0.706070 mae: 0.667322 (1451.7583208727917 steps/sec)\n",
      "Step #8288\tEpoch   2 Batch 2037/3125   Loss: 0.843975 mae: 0.719342 (1464.9417418759954 steps/sec)\n",
      "Step #8289\tEpoch   2 Batch 2038/3125   Loss: 0.905164 mae: 0.756640 (1565.050485451384 steps/sec)\n",
      "Step #8290\tEpoch   2 Batch 2039/3125   Loss: 0.944031 mae: 0.772941 (1466.4065504534553 steps/sec)\n",
      "Step #8291\tEpoch   2 Batch 2040/3125   Loss: 0.787006 mae: 0.726482 (1078.1163890602509 steps/sec)\n",
      "Step #8292\tEpoch   2 Batch 2041/3125   Loss: 0.785192 mae: 0.698491 (1230.5569083985142 steps/sec)\n",
      "Step #8293\tEpoch   2 Batch 2042/3125   Loss: 0.771456 mae: 0.695228 (1361.5482999733813 steps/sec)\n",
      "Step #8294\tEpoch   2 Batch 2043/3125   Loss: 0.785851 mae: 0.693970 (1473.050123623286 steps/sec)\n",
      "Step #8295\tEpoch   2 Batch 2044/3125   Loss: 0.970996 mae: 0.783532 (1462.5408847138244 steps/sec)\n",
      "Step #8296\tEpoch   2 Batch 2045/3125   Loss: 0.666292 mae: 0.660204 (1778.287303592779 steps/sec)\n",
      "Step #8297\tEpoch   2 Batch 2046/3125   Loss: 0.750489 mae: 0.685144 (1774.1802307874523 steps/sec)\n",
      "Step #8298\tEpoch   2 Batch 2047/3125   Loss: 0.918274 mae: 0.754354 (1384.1125689696137 steps/sec)\n",
      "Step #8299\tEpoch   2 Batch 2048/3125   Loss: 0.892236 mae: 0.756106 (1513.959616231474 steps/sec)\n",
      "Step #8300\tEpoch   2 Batch 2049/3125   Loss: 0.791033 mae: 0.686570 (1318.1096522378584 steps/sec)\n",
      "Step #8301\tEpoch   2 Batch 2050/3125   Loss: 0.862289 mae: 0.738435 (1481.866295461451 steps/sec)\n",
      "Step #8302\tEpoch   2 Batch 2051/3125   Loss: 0.802958 mae: 0.732902 (1569.0668582031483 steps/sec)\n",
      "Step #8303\tEpoch   2 Batch 2052/3125   Loss: 0.829835 mae: 0.717984 (1257.8207750301387 steps/sec)\n",
      "Step #8304\tEpoch   2 Batch 2053/3125   Loss: 0.914582 mae: 0.736927 (1170.8476166017174 steps/sec)\n",
      "Step #8305\tEpoch   2 Batch 2054/3125   Loss: 0.675898 mae: 0.671101 (1490.407220524483 steps/sec)\n",
      "Step #8306\tEpoch   2 Batch 2055/3125   Loss: 0.762988 mae: 0.686480 (1552.698330433495 steps/sec)\n",
      "Step #8307\tEpoch   2 Batch 2056/3125   Loss: 0.815003 mae: 0.694237 (1566.5234961493356 steps/sec)\n",
      "Step #8308\tEpoch   2 Batch 2057/3125   Loss: 0.750956 mae: 0.699305 (1741.849532384259 steps/sec)\n",
      "Step #8309\tEpoch   2 Batch 2058/3125   Loss: 0.749589 mae: 0.677219 (1668.7769555184213 steps/sec)\n",
      "Step #8310\tEpoch   2 Batch 2059/3125   Loss: 0.913163 mae: 0.746170 (1335.7570970885536 steps/sec)\n",
      "Step #8311\tEpoch   2 Batch 2060/3125   Loss: 0.762887 mae: 0.680773 (1776.901112495022 steps/sec)\n",
      "Step #8312\tEpoch   2 Batch 2061/3125   Loss: 0.716644 mae: 0.675806 (1901.6439821909485 steps/sec)\n",
      "Step #8313\tEpoch   2 Batch 2062/3125   Loss: 0.784615 mae: 0.712312 (1722.7614760293102 steps/sec)\n",
      "Step #8314\tEpoch   2 Batch 2063/3125   Loss: 0.787435 mae: 0.707516 (2064.8372963127063 steps/sec)\n",
      "Step #8315\tEpoch   2 Batch 2064/3125   Loss: 0.695475 mae: 0.652109 (2036.0699029126213 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8316\tEpoch   2 Batch 2065/3125   Loss: 0.642786 mae: 0.645234 (1931.5416213826516 steps/sec)\n",
      "Step #8317\tEpoch   2 Batch 2066/3125   Loss: 0.725048 mae: 0.667214 (1826.6124326066317 steps/sec)\n",
      "Step #8318\tEpoch   2 Batch 2067/3125   Loss: 0.842328 mae: 0.712012 (1592.2496393591982 steps/sec)\n",
      "Step #8319\tEpoch   2 Batch 2068/3125   Loss: 0.745273 mae: 0.674530 (1461.6846140442585 steps/sec)\n",
      "Step #8320\tEpoch   2 Batch 2069/3125   Loss: 0.893436 mae: 0.732879 (1799.8523833227484 steps/sec)\n",
      "Step #8321\tEpoch   2 Batch 2070/3125   Loss: 0.804367 mae: 0.707332 (2110.4053455701805 steps/sec)\n",
      "Step #8322\tEpoch   2 Batch 2071/3125   Loss: 0.934855 mae: 0.762667 (1985.30018743965 steps/sec)\n",
      "Step #8323\tEpoch   2 Batch 2072/3125   Loss: 0.748280 mae: 0.684196 (1865.8765959339828 steps/sec)\n",
      "Step #8324\tEpoch   2 Batch 2073/3125   Loss: 0.721146 mae: 0.661552 (1576.7941594424103 steps/sec)\n",
      "Step #8325\tEpoch   2 Batch 2074/3125   Loss: 0.883954 mae: 0.715968 (1827.0102625755753 steps/sec)\n",
      "Step #8326\tEpoch   2 Batch 2075/3125   Loss: 0.902351 mae: 0.766802 (1731.7808716906968 steps/sec)\n",
      "Step #8327\tEpoch   2 Batch 2076/3125   Loss: 0.928765 mae: 0.759067 (1737.1314972043901 steps/sec)\n",
      "Step #8328\tEpoch   2 Batch 2077/3125   Loss: 0.809837 mae: 0.718432 (1856.8233533729403 steps/sec)\n",
      "Step #8329\tEpoch   2 Batch 2078/3125   Loss: 0.860441 mae: 0.727109 (1883.0492951423184 steps/sec)\n",
      "Step #8330\tEpoch   2 Batch 2079/3125   Loss: 0.827877 mae: 0.707919 (1955.4229449500224 steps/sec)\n",
      "Step #8331\tEpoch   2 Batch 2080/3125   Loss: 0.955524 mae: 0.769341 (1675.9384015407607 steps/sec)\n",
      "Step #8332\tEpoch   2 Batch 2081/3125   Loss: 0.811776 mae: 0.729527 (1715.0969936864144 steps/sec)\n",
      "Step #8333\tEpoch   2 Batch 2082/3125   Loss: 0.856359 mae: 0.732322 (1994.097063745626 steps/sec)\n",
      "Step #8334\tEpoch   2 Batch 2083/3125   Loss: 0.763692 mae: 0.711143 (1674.2525487190542 steps/sec)\n",
      "Step #8335\tEpoch   2 Batch 2084/3125   Loss: 0.886699 mae: 0.759075 (1656.2434351331929 steps/sec)\n",
      "Step #8336\tEpoch   2 Batch 2085/3125   Loss: 0.889840 mae: 0.756800 (1780.3101946569097 steps/sec)\n",
      "Step #8337\tEpoch   2 Batch 2086/3125   Loss: 0.927126 mae: 0.747913 (1888.0164210412597 steps/sec)\n",
      "Step #8338\tEpoch   2 Batch 2087/3125   Loss: 0.838207 mae: 0.731883 (1808.2481871405537 steps/sec)\n",
      "Step #8339\tEpoch   2 Batch 2088/3125   Loss: 0.815965 mae: 0.719276 (1586.3959575176254 steps/sec)\n",
      "Step #8340\tEpoch   2 Batch 2089/3125   Loss: 0.818149 mae: 0.702378 (1816.785640030494 steps/sec)\n",
      "Step #8341\tEpoch   2 Batch 2090/3125   Loss: 0.717170 mae: 0.658016 (1913.9313517016053 steps/sec)\n",
      "Step #8342\tEpoch   2 Batch 2091/3125   Loss: 0.701779 mae: 0.670778 (2104.919151669661 steps/sec)\n",
      "Step #8343\tEpoch   2 Batch 2092/3125   Loss: 0.968227 mae: 0.782052 (2051.3655215588074 steps/sec)\n",
      "Step #8344\tEpoch   2 Batch 2093/3125   Loss: 0.828539 mae: 0.721879 (2060.0909635654575 steps/sec)\n",
      "Step #8345\tEpoch   2 Batch 2094/3125   Loss: 0.823697 mae: 0.738381 (1917.255880712725 steps/sec)\n",
      "Step #8346\tEpoch   2 Batch 2095/3125   Loss: 0.831853 mae: 0.702737 (1654.0097167013691 steps/sec)\n",
      "Step #8347\tEpoch   2 Batch 2096/3125   Loss: 0.629477 mae: 0.608219 (1764.6555931404723 steps/sec)\n",
      "Step #8348\tEpoch   2 Batch 2097/3125   Loss: 0.714174 mae: 0.662722 (1950.2222552867 steps/sec)\n",
      "Step #8349\tEpoch   2 Batch 2098/3125   Loss: 0.788069 mae: 0.714643 (1925.282069642972 steps/sec)\n",
      "Step #8350\tEpoch   2 Batch 2099/3125   Loss: 0.862327 mae: 0.717140 (1759.88721425934 steps/sec)\n",
      "Step #8351\tEpoch   2 Batch 2100/3125   Loss: 0.792953 mae: 0.710574 (1986.0334296131446 steps/sec)\n",
      "Step #8352\tEpoch   2 Batch 2101/3125   Loss: 0.758845 mae: 0.680359 (1749.6533484619683 steps/sec)\n",
      "Step #8353\tEpoch   2 Batch 2102/3125   Loss: 0.871669 mae: 0.728301 (1993.926428781958 steps/sec)\n",
      "Step #8354\tEpoch   2 Batch 2103/3125   Loss: 0.963546 mae: 0.789507 (2003.297511582366 steps/sec)\n",
      "Step #8355\tEpoch   2 Batch 2104/3125   Loss: 0.785585 mae: 0.669034 (1752.2262606007437 steps/sec)\n",
      "Step #8356\tEpoch   2 Batch 2105/3125   Loss: 0.776519 mae: 0.708859 (2022.8916476160161 steps/sec)\n",
      "Step #8357\tEpoch   2 Batch 2106/3125   Loss: 0.758672 mae: 0.684152 (1989.2548186371225 steps/sec)\n",
      "Step #8358\tEpoch   2 Batch 2107/3125   Loss: 0.896808 mae: 0.752692 (2036.247827480071 steps/sec)\n",
      "Step #8359\tEpoch   2 Batch 2108/3125   Loss: 0.880068 mae: 0.732984 (2133.0078621629596 steps/sec)\n",
      "Step #8360\tEpoch   2 Batch 2109/3125   Loss: 0.795053 mae: 0.728188 (1975.3518075466723 steps/sec)\n",
      "Step #8361\tEpoch   2 Batch 2110/3125   Loss: 0.841349 mae: 0.722320 (2003.6803133807864 steps/sec)\n",
      "Step #8362\tEpoch   2 Batch 2111/3125   Loss: 0.831890 mae: 0.704530 (1853.2462597538022 steps/sec)\n",
      "Step #8363\tEpoch   2 Batch 2112/3125   Loss: 0.875402 mae: 0.725672 (1558.3634282996716 steps/sec)\n",
      "Step #8364\tEpoch   2 Batch 2113/3125   Loss: 0.886504 mae: 0.729932 (1964.0300436419486 steps/sec)\n",
      "Step #8365\tEpoch   2 Batch 2114/3125   Loss: 0.888258 mae: 0.734903 (2023.145343340601 steps/sec)\n",
      "Step #8366\tEpoch   2 Batch 2115/3125   Loss: 0.989111 mae: 0.781018 (2182.169316573712 steps/sec)\n",
      "Step #8367\tEpoch   2 Batch 2116/3125   Loss: 0.832832 mae: 0.711877 (2056.091845838603 steps/sec)\n",
      "Step #8368\tEpoch   2 Batch 2117/3125   Loss: 0.729718 mae: 0.670591 (2037.810945273632 steps/sec)\n",
      "Step #8369\tEpoch   2 Batch 2118/3125   Loss: 0.810084 mae: 0.707831 (1952.0011914069771 steps/sec)\n",
      "Step #8370\tEpoch   2 Batch 2119/3125   Loss: 0.864513 mae: 0.719090 (1771.34796820758 steps/sec)\n",
      "Step #8371\tEpoch   2 Batch 2120/3125   Loss: 0.768049 mae: 0.697497 (1652.2895591062368 steps/sec)\n",
      "Step #8372\tEpoch   2 Batch 2121/3125   Loss: 0.819286 mae: 0.703386 (1946.4211464211464 steps/sec)\n",
      "Step #8373\tEpoch   2 Batch 2122/3125   Loss: 0.859440 mae: 0.727158 (1971.1371988758658 steps/sec)\n",
      "Step #8374\tEpoch   2 Batch 2123/3125   Loss: 0.714826 mae: 0.689970 (2035.003008131659 steps/sec)\n",
      "Step #8375\tEpoch   2 Batch 2124/3125   Loss: 0.661582 mae: 0.617593 (1949.7145832171213 steps/sec)\n",
      "Step #8376\tEpoch   2 Batch 2125/3125   Loss: 0.920864 mae: 0.762243 (2024.1998378440987 steps/sec)\n",
      "Step #8377\tEpoch   2 Batch 2126/3125   Loss: 0.882103 mae: 0.742865 (2090.837670235888 steps/sec)\n",
      "Step #8378\tEpoch   2 Batch 2127/3125   Loss: 0.823296 mae: 0.709608 (1762.549586499025 steps/sec)\n",
      "Step #8379\tEpoch   2 Batch 2128/3125   Loss: 0.736498 mae: 0.668203 (1836.4817766257422 steps/sec)\n",
      "Step #8380\tEpoch   2 Batch 2129/3125   Loss: 0.727553 mae: 0.699865 (2219.443327336226 steps/sec)\n",
      "Step #8381\tEpoch   2 Batch 2130/3125   Loss: 0.807667 mae: 0.713513 (2208.0630047274603 steps/sec)\n",
      "Step #8382\tEpoch   2 Batch 2131/3125   Loss: 0.863711 mae: 0.723994 (2082.553301357484 steps/sec)\n",
      "Step #8383\tEpoch   2 Batch 2132/3125   Loss: 0.757552 mae: 0.692080 (2115.685404140269 steps/sec)\n",
      "Step #8384\tEpoch   2 Batch 2133/3125   Loss: 0.761823 mae: 0.686594 (2121.5283608663544 steps/sec)\n",
      "Step #8385\tEpoch   2 Batch 2134/3125   Loss: 0.732904 mae: 0.672815 (2058.9583231063766 steps/sec)\n",
      "Step #8386\tEpoch   2 Batch 2135/3125   Loss: 0.783091 mae: 0.686684 (1973.994484134828 steps/sec)\n",
      "Step #8387\tEpoch   2 Batch 2136/3125   Loss: 0.933004 mae: 0.754399 (1716.9785986802246 steps/sec)\n",
      "Step #8388\tEpoch   2 Batch 2137/3125   Loss: 0.816583 mae: 0.711695 (1869.1527478208168 steps/sec)\n",
      "Step #8389\tEpoch   2 Batch 2138/3125   Loss: 0.725164 mae: 0.684734 (1817.588684445446 steps/sec)\n",
      "Step #8390\tEpoch   2 Batch 2139/3125   Loss: 0.878349 mae: 0.737908 (1990.2931602273914 steps/sec)\n",
      "Step #8391\tEpoch   2 Batch 2140/3125   Loss: 0.801783 mae: 0.704862 (2019.4632488179725 steps/sec)\n",
      "Step #8392\tEpoch   2 Batch 2141/3125   Loss: 0.860835 mae: 0.747381 (1952.0556998315228 steps/sec)\n",
      "Step #8393\tEpoch   2 Batch 2142/3125   Loss: 0.667340 mae: 0.657031 (2148.5011781579756 steps/sec)\n",
      "Step #8394\tEpoch   2 Batch 2143/3125   Loss: 0.744310 mae: 0.678457 (2086.9052950015425 steps/sec)\n",
      "Step #8395\tEpoch   2 Batch 2144/3125   Loss: 0.677690 mae: 0.650415 (1982.5412881330296 steps/sec)\n",
      "Step #8396\tEpoch   2 Batch 2145/3125   Loss: 0.915857 mae: 0.756368 (1772.5457050366401 steps/sec)\n",
      "Step #8397\tEpoch   2 Batch 2146/3125   Loss: 0.643349 mae: 0.658328 (1945.9696202061818 steps/sec)\n",
      "Step #8398\tEpoch   2 Batch 2147/3125   Loss: 0.801714 mae: 0.718278 (2114.064516129032 steps/sec)\n",
      "Step #8399\tEpoch   2 Batch 2148/3125   Loss: 0.756007 mae: 0.682375 (2171.3694063075936 steps/sec)\n",
      "Step #8400\tEpoch   2 Batch 2149/3125   Loss: 0.755172 mae: 0.688676 (1928.0433203703194 steps/sec)\n",
      "Step #8401\tEpoch   2 Batch 2150/3125   Loss: 0.826703 mae: 0.711619 (2049.080569831747 steps/sec)\n",
      "Step #8402\tEpoch   2 Batch 2151/3125   Loss: 0.848953 mae: 0.733552 (2105.447463004237 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8403\tEpoch   2 Batch 2152/3125   Loss: 0.945638 mae: 0.757251 (2108.813739982101 steps/sec)\n",
      "Step #8404\tEpoch   2 Batch 2153/3125   Loss: 0.912153 mae: 0.784416 (1666.3901470003973 steps/sec)\n",
      "Step #8405\tEpoch   2 Batch 2154/3125   Loss: 0.769553 mae: 0.699559 (1849.1936266081175 steps/sec)\n",
      "Step #8406\tEpoch   2 Batch 2155/3125   Loss: 0.819513 mae: 0.719542 (2262.226680905688 steps/sec)\n",
      "Step #8407\tEpoch   2 Batch 2156/3125   Loss: 0.856018 mae: 0.719370 (2221.5357888157964 steps/sec)\n",
      "Step #8408\tEpoch   2 Batch 2157/3125   Loss: 0.768198 mae: 0.703210 (2075.894836870447 steps/sec)\n",
      "Step #8409\tEpoch   2 Batch 2158/3125   Loss: 0.722537 mae: 0.662220 (2051.9475944933124 steps/sec)\n",
      "Step #8410\tEpoch   2 Batch 2159/3125   Loss: 0.775742 mae: 0.707345 (2009.5747331301866 steps/sec)\n",
      "Step #8411\tEpoch   2 Batch 2160/3125   Loss: 0.775730 mae: 0.698631 (1906.3631735873755 steps/sec)\n",
      "Step #8412\tEpoch   2 Batch 2161/3125   Loss: 0.873752 mae: 0.756968 (1760.5816130359226 steps/sec)\n",
      "Step #8413\tEpoch   2 Batch 2162/3125   Loss: 0.889750 mae: 0.730840 (1967.254204853522 steps/sec)\n",
      "Step #8414\tEpoch   2 Batch 2163/3125   Loss: 0.947993 mae: 0.768581 (2081.065364730632 steps/sec)\n",
      "Step #8415\tEpoch   2 Batch 2164/3125   Loss: 0.729304 mae: 0.680175 (1998.467666622196 steps/sec)\n",
      "Step #8416\tEpoch   2 Batch 2165/3125   Loss: 0.766510 mae: 0.702385 (1959.918506196146 steps/sec)\n",
      "Step #8417\tEpoch   2 Batch 2166/3125   Loss: 0.729204 mae: 0.688899 (2030.9432500484215 steps/sec)\n",
      "Step #8418\tEpoch   2 Batch 2167/3125   Loss: 0.822396 mae: 0.748850 (2180.7397548015433 steps/sec)\n",
      "Step #8419\tEpoch   2 Batch 2168/3125   Loss: 0.874038 mae: 0.748800 (1979.3415886439143 steps/sec)\n",
      "Step #8420\tEpoch   2 Batch 2169/3125   Loss: 0.692985 mae: 0.654744 (1930.7769502011656 steps/sec)\n",
      "Step #8421\tEpoch   2 Batch 2170/3125   Loss: 0.790527 mae: 0.700393 (1813.2982862676604 steps/sec)\n",
      "Step #8422\tEpoch   2 Batch 2171/3125   Loss: 0.814202 mae: 0.716084 (2129.7369757286483 steps/sec)\n",
      "Step #8423\tEpoch   2 Batch 2172/3125   Loss: 0.811676 mae: 0.702040 (1921.4542256102031 steps/sec)\n",
      "Step #8424\tEpoch   2 Batch 2173/3125   Loss: 0.788217 mae: 0.686764 (1835.886930868153 steps/sec)\n",
      "Step #8425\tEpoch   2 Batch 2174/3125   Loss: 0.726489 mae: 0.652807 (1998.296282885647 steps/sec)\n",
      "Step #8426\tEpoch   2 Batch 2175/3125   Loss: 0.801923 mae: 0.702938 (2016.6086504990672 steps/sec)\n",
      "Step #8427\tEpoch   2 Batch 2176/3125   Loss: 0.756409 mae: 0.712674 (2006.1145229485928 steps/sec)\n",
      "Step #8428\tEpoch   2 Batch 2177/3125   Loss: 0.856615 mae: 0.746326 (1767.7048475602046 steps/sec)\n",
      "Step #8429\tEpoch   2 Batch 2178/3125   Loss: 0.761578 mae: 0.698849 (1858.4523767324802 steps/sec)\n",
      "Step #8430\tEpoch   2 Batch 2179/3125   Loss: 0.852961 mae: 0.729118 (2082.904930276906 steps/sec)\n",
      "Step #8431\tEpoch   2 Batch 2180/3125   Loss: 0.846260 mae: 0.711285 (2151.2781584670306 steps/sec)\n",
      "Step #8432\tEpoch   2 Batch 2181/3125   Loss: 0.937395 mae: 0.756232 (2090.650078256622 steps/sec)\n",
      "Step #8433\tEpoch   2 Batch 2182/3125   Loss: 0.691328 mae: 0.661130 (2228.9734923367982 steps/sec)\n",
      "Step #8434\tEpoch   2 Batch 2183/3125   Loss: 0.725427 mae: 0.654497 (1990.5954268032235 steps/sec)\n",
      "Step #8435\tEpoch   2 Batch 2184/3125   Loss: 0.852360 mae: 0.727144 (2253.2819030632527 steps/sec)\n",
      "Step #8436\tEpoch   2 Batch 2185/3125   Loss: 0.881146 mae: 0.741254 (1932.0754716981132 steps/sec)\n",
      "Step #8437\tEpoch   2 Batch 2186/3125   Loss: 0.824054 mae: 0.715024 (2041.0039804965402 steps/sec)\n",
      "Step #8438\tEpoch   2 Batch 2187/3125   Loss: 0.841645 mae: 0.718361 (2195.6947818075214 steps/sec)\n",
      "Step #8439\tEpoch   2 Batch 2188/3125   Loss: 0.788891 mae: 0.699804 (1927.6888713220762 steps/sec)\n",
      "Step #8440\tEpoch   2 Batch 2189/3125   Loss: 0.910492 mae: 0.718938 (2103.652285562388 steps/sec)\n",
      "Step #8441\tEpoch   2 Batch 2190/3125   Loss: 0.848565 mae: 0.710186 (2008.3430694681197 steps/sec)\n",
      "Step #8442\tEpoch   2 Batch 2191/3125   Loss: 0.856783 mae: 0.720629 (2149.44807158158 steps/sec)\n",
      "Step #8443\tEpoch   2 Batch 2192/3125   Loss: 0.832413 mae: 0.714317 (1840.6080499921009 steps/sec)\n",
      "Step #8444\tEpoch   2 Batch 2193/3125   Loss: 0.876058 mae: 0.752401 (1924.469363970892 steps/sec)\n",
      "Step #8445\tEpoch   2 Batch 2194/3125   Loss: 0.810399 mae: 0.711264 (2033.3456146134306 steps/sec)\n",
      "Step #8446\tEpoch   2 Batch 2195/3125   Loss: 0.870570 mae: 0.726682 (2213.680121601081 steps/sec)\n",
      "Step #8447\tEpoch   2 Batch 2196/3125   Loss: 0.728423 mae: 0.668725 (2261.1047019374873 steps/sec)\n",
      "Step #8448\tEpoch   2 Batch 2197/3125   Loss: 0.786887 mae: 0.676382 (2168.0023156763014 steps/sec)\n",
      "Step #8449\tEpoch   2 Batch 2198/3125   Loss: 0.823811 mae: 0.714172 (2287.2698717389408 steps/sec)\n",
      "Step #8450\tEpoch   2 Batch 2199/3125   Loss: 0.749866 mae: 0.676809 (2083.6913538541025 steps/sec)\n",
      "Step #8451\tEpoch   2 Batch 2200/3125   Loss: 0.768306 mae: 0.681539 (2054.1383430955784 steps/sec)\n",
      "Step #8452\tEpoch   2 Batch 2201/3125   Loss: 0.923867 mae: 0.776594 (1760.9364110401116 steps/sec)\n",
      "Step #8453\tEpoch   2 Batch 2202/3125   Loss: 0.881310 mae: 0.739045 (1902.3684902802095 steps/sec)\n",
      "Step #8454\tEpoch   2 Batch 2203/3125   Loss: 0.823509 mae: 0.731950 (2169.6845546623626 steps/sec)\n",
      "Step #8455\tEpoch   2 Batch 2204/3125   Loss: 0.803836 mae: 0.719266 (2265.2322315834954 steps/sec)\n",
      "Step #8456\tEpoch   2 Batch 2205/3125   Loss: 0.892644 mae: 0.724416 (2082.553301357484 steps/sec)\n",
      "Step #8457\tEpoch   2 Batch 2206/3125   Loss: 0.682264 mae: 0.653957 (2041.7391981618864 steps/sec)\n",
      "Step #8458\tEpoch   2 Batch 2207/3125   Loss: 0.823636 mae: 0.698013 (2213.0727506806525 steps/sec)\n",
      "Step #8459\tEpoch   2 Batch 2208/3125   Loss: 0.847847 mae: 0.750178 (2123.117729835893 steps/sec)\n",
      "Step #8460\tEpoch   2 Batch 2209/3125   Loss: 0.729967 mae: 0.665369 (2049.7815484161038 steps/sec)\n",
      "Step #8461\tEpoch   2 Batch 2210/3125   Loss: 0.862862 mae: 0.728104 (1854.1802235111047 steps/sec)\n",
      "Step #8462\tEpoch   2 Batch 2211/3125   Loss: 0.762715 mae: 0.696820 (1822.1527126125186 steps/sec)\n",
      "Step #8463\tEpoch   2 Batch 2212/3125   Loss: 0.784574 mae: 0.703762 (2228.3813794349226 steps/sec)\n",
      "Step #8464\tEpoch   2 Batch 2213/3125   Loss: 0.753692 mae: 0.686026 (2142.684035759898 steps/sec)\n",
      "Step #8465\tEpoch   2 Batch 2214/3125   Loss: 0.842477 mae: 0.722596 (2013.704102012598 steps/sec)\n",
      "Step #8466\tEpoch   2 Batch 2215/3125   Loss: 0.846615 mae: 0.729821 (1943.9313323816764 steps/sec)\n",
      "Step #8467\tEpoch   2 Batch 2216/3125   Loss: 0.837174 mae: 0.725536 (2223.490744078543 steps/sec)\n",
      "Step #8468\tEpoch   2 Batch 2217/3125   Loss: 0.816967 mae: 0.743566 (1998.8105223027069 steps/sec)\n",
      "Step #8469\tEpoch   2 Batch 2218/3125   Loss: 0.876525 mae: 0.762049 (1938.7377393201505 steps/sec)\n",
      "Step #8470\tEpoch   2 Batch 2219/3125   Loss: 0.962438 mae: 0.772464 (1742.1678739948163 steps/sec)\n",
      "Step #8471\tEpoch   2 Batch 2220/3125   Loss: 0.808027 mae: 0.691064 (2050.4429104988367 steps/sec)\n",
      "Step #8472\tEpoch   2 Batch 2221/3125   Loss: 0.822788 mae: 0.719692 (1849.4056227732901 steps/sec)\n",
      "Step #8473\tEpoch   2 Batch 2222/3125   Loss: 0.943916 mae: 0.775652 (1325.9099559326535 steps/sec)\n",
      "Step #8474\tEpoch   2 Batch 2223/3125   Loss: 0.800677 mae: 0.701579 (1539.1150546393947 steps/sec)\n",
      "Step #8475\tEpoch   2 Batch 2224/3125   Loss: 0.710520 mae: 0.649789 (1369.1126547239776 steps/sec)\n",
      "Step #8476\tEpoch   2 Batch 2225/3125   Loss: 0.733546 mae: 0.682433 (1208.8376516701733 steps/sec)\n",
      "Step #8477\tEpoch   2 Batch 2226/3125   Loss: 0.782504 mae: 0.724734 (1529.294402473529 steps/sec)\n",
      "Step #8478\tEpoch   2 Batch 2227/3125   Loss: 0.927125 mae: 0.724965 (1459.2537957331924 steps/sec)\n",
      "Step #8479\tEpoch   2 Batch 2228/3125   Loss: 0.797507 mae: 0.701837 (1534.8234019818792 steps/sec)\n",
      "Step #8480\tEpoch   2 Batch 2229/3125   Loss: 0.829140 mae: 0.728024 (1570.7356531899277 steps/sec)\n",
      "Step #8481\tEpoch   2 Batch 2230/3125   Loss: 0.722114 mae: 0.664770 (1447.1800321572255 steps/sec)\n",
      "Step #8482\tEpoch   2 Batch 2231/3125   Loss: 0.893020 mae: 0.737552 (1714.620227291309 steps/sec)\n",
      "Step #8483\tEpoch   2 Batch 2232/3125   Loss: 0.722477 mae: 0.673467 (1386.6292870319555 steps/sec)\n",
      "Step #8484\tEpoch   2 Batch 2233/3125   Loss: 0.760601 mae: 0.693901 (1480.328655730299 steps/sec)\n",
      "Step #8485\tEpoch   2 Batch 2234/3125   Loss: 0.809646 mae: 0.722442 (1528.21342427621 steps/sec)\n",
      "Step #8486\tEpoch   2 Batch 2235/3125   Loss: 0.951160 mae: 0.785888 (1618.9598338698595 steps/sec)\n",
      "Step #8487\tEpoch   2 Batch 2236/3125   Loss: 0.789537 mae: 0.708508 (1632.4822518370906 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8488\tEpoch   2 Batch 2237/3125   Loss: 0.731988 mae: 0.695908 (1531.0249165918365 steps/sec)\n",
      "Step #8489\tEpoch   2 Batch 2238/3125   Loss: 0.861436 mae: 0.738035 (1530.3099072540335 steps/sec)\n",
      "Step #8490\tEpoch   2 Batch 2239/3125   Loss: 0.601548 mae: 0.613465 (1511.308408520942 steps/sec)\n",
      "Step #8491\tEpoch   2 Batch 2240/3125   Loss: 0.844782 mae: 0.731153 (1609.306751385116 steps/sec)\n",
      "Step #8492\tEpoch   2 Batch 2241/3125   Loss: 0.896171 mae: 0.749829 (1547.2112376792775 steps/sec)\n",
      "Step #8493\tEpoch   2 Batch 2242/3125   Loss: 0.731855 mae: 0.707987 (1531.7741582061208 steps/sec)\n",
      "Step #8494\tEpoch   2 Batch 2243/3125   Loss: 0.685516 mae: 0.650243 (1707.9314922346464 steps/sec)\n",
      "Step #8495\tEpoch   2 Batch 2244/3125   Loss: 0.870695 mae: 0.745023 (1355.774066962756 steps/sec)\n",
      "Step #8496\tEpoch   2 Batch 2245/3125   Loss: 0.738070 mae: 0.670949 (1530.9690324276182 steps/sec)\n",
      "Step #8497\tEpoch   2 Batch 2246/3125   Loss: 0.764332 mae: 0.686550 (1634.0849943118951 steps/sec)\n",
      "Step #8498\tEpoch   2 Batch 2247/3125   Loss: 0.849205 mae: 0.722636 (1628.5017627234465 steps/sec)\n",
      "Step #8499\tEpoch   2 Batch 2248/3125   Loss: 0.858069 mae: 0.722821 (1716.9785986802246 steps/sec)\n",
      "Step #8500\tEpoch   2 Batch 2249/3125   Loss: 0.768972 mae: 0.703664 (1837.5599113268552 steps/sec)\n",
      "Step #8501\tEpoch   2 Batch 2250/3125   Loss: 0.914313 mae: 0.759980 (1980.8372374187697 steps/sec)\n",
      "Step #8502\tEpoch   2 Batch 2251/3125   Loss: 0.872418 mae: 0.724580 (2193.4671422146453 steps/sec)\n",
      "Step #8503\tEpoch   2 Batch 2252/3125   Loss: 0.892534 mae: 0.746301 (1858.040737492137 steps/sec)\n",
      "Step #8504\tEpoch   2 Batch 2253/3125   Loss: 0.738963 mae: 0.675310 (1761.9128433044602 steps/sec)\n",
      "Step #8505\tEpoch   2 Batch 2254/3125   Loss: 0.911820 mae: 0.757154 (1830.4227909087735 steps/sec)\n",
      "Step #8506\tEpoch   2 Batch 2255/3125   Loss: 0.921702 mae: 0.754858 (1962.4861971514663 steps/sec)\n",
      "Step #8507\tEpoch   2 Batch 2256/3125   Loss: 0.703841 mae: 0.678014 (1730.6375744772154 steps/sec)\n",
      "Step #8508\tEpoch   2 Batch 2257/3125   Loss: 0.819116 mae: 0.728318 (1807.6559065638064 steps/sec)\n",
      "Step #8509\tEpoch   2 Batch 2258/3125   Loss: 0.864922 mae: 0.755397 (1844.1364755539923 steps/sec)\n",
      "Step #8510\tEpoch   2 Batch 2259/3125   Loss: 0.813662 mae: 0.706346 (1552.5029241497757 steps/sec)\n",
      "Step #8511\tEpoch   2 Batch 2260/3125   Loss: 0.843175 mae: 0.695407 (1831.493821230514 steps/sec)\n",
      "Step #8512\tEpoch   2 Batch 2261/3125   Loss: 0.850780 mae: 0.702097 (1997.8394032637586 steps/sec)\n",
      "Step #8513\tEpoch   2 Batch 2262/3125   Loss: 0.903655 mae: 0.748382 (2190.8548624677455 steps/sec)\n",
      "Step #8514\tEpoch   2 Batch 2263/3125   Loss: 0.781554 mae: 0.711011 (2137.769622833843 steps/sec)\n",
      "Step #8515\tEpoch   2 Batch 2264/3125   Loss: 0.750462 mae: 0.685245 (2377.1034764177143 steps/sec)\n",
      "Step #8516\tEpoch   2 Batch 2265/3125   Loss: 0.847604 mae: 0.726131 (2280.8269981619846 steps/sec)\n",
      "Step #8517\tEpoch   2 Batch 2266/3125   Loss: 0.874758 mae: 0.749933 (2257.623907333247 steps/sec)\n",
      "Step #8518\tEpoch   2 Batch 2267/3125   Loss: 0.737533 mae: 0.682810 (2185.4894849830134 steps/sec)\n",
      "Step #8519\tEpoch   2 Batch 2268/3125   Loss: 0.872583 mae: 0.727944 (1812.5930215472908 steps/sec)\n",
      "Step #8520\tEpoch   2 Batch 2269/3125   Loss: 0.834579 mae: 0.713449 (2003.297511582366 steps/sec)\n",
      "Step #8521\tEpoch   2 Batch 2270/3125   Loss: 0.848465 mae: 0.693324 (2057.705780193687 steps/sec)\n",
      "Step #8522\tEpoch   2 Batch 2271/3125   Loss: 0.922998 mae: 0.749741 (2014.0135218192993 steps/sec)\n",
      "Step #8523\tEpoch   2 Batch 2272/3125   Loss: 0.856084 mae: 0.740768 (2006.1720954704167 steps/sec)\n",
      "Step #8524\tEpoch   2 Batch 2273/3125   Loss: 0.809594 mae: 0.710880 (2080.0539564777528 steps/sec)\n",
      "Step #8525\tEpoch   2 Batch 2274/3125   Loss: 0.855971 mae: 0.726805 (2141.5243852626418 steps/sec)\n",
      "Step #8526\tEpoch   2 Batch 2275/3125   Loss: 0.709877 mae: 0.663401 (2049.36090372513 steps/sec)\n",
      "Step #8527\tEpoch   2 Batch 2276/3125   Loss: 0.807575 mae: 0.702037 (1941.1423864046576 steps/sec)\n",
      "Step #8528\tEpoch   2 Batch 2277/3125   Loss: 0.818598 mae: 0.726934 (1997.9345692891031 steps/sec)\n",
      "Step #8529\tEpoch   2 Batch 2278/3125   Loss: 0.882196 mae: 0.748574 (1861.5702809462518 steps/sec)\n",
      "Step #8530\tEpoch   2 Batch 2279/3125   Loss: 0.784763 mae: 0.724817 (2279.8847638201883 steps/sec)\n",
      "Step #8531\tEpoch   2 Batch 2280/3125   Loss: 0.815820 mae: 0.720409 (2260.690339132872 steps/sec)\n",
      "Step #8532\tEpoch   2 Batch 2281/3125   Loss: 0.916484 mae: 0.757920 (1962.320928970441 steps/sec)\n",
      "Step #8533\tEpoch   2 Batch 2282/3125   Loss: 0.834308 mae: 0.746625 (2232.6516272583067 steps/sec)\n",
      "Step #8534\tEpoch   2 Batch 2283/3125   Loss: 0.749926 mae: 0.707657 (2025.6858048064291 steps/sec)\n",
      "Step #8535\tEpoch   2 Batch 2284/3125   Loss: 0.845472 mae: 0.745401 (1884.2336028751124 steps/sec)\n",
      "Step #8536\tEpoch   2 Batch 2285/3125   Loss: 0.975573 mae: 0.776862 (1822.6911644561874 steps/sec)\n",
      "Step #8537\tEpoch   2 Batch 2286/3125   Loss: 0.828800 mae: 0.745320 (2000.9655843598232 steps/sec)\n",
      "Step #8538\tEpoch   2 Batch 2287/3125   Loss: 0.772074 mae: 0.702462 (1926.6440055121727 steps/sec)\n",
      "Step #8539\tEpoch   2 Batch 2288/3125   Loss: 0.800393 mae: 0.694919 (2247.1010532857586 steps/sec)\n",
      "Step #8540\tEpoch   2 Batch 2289/3125   Loss: 1.006842 mae: 0.775762 (2230.633083730429 steps/sec)\n",
      "Step #8541\tEpoch   2 Batch 2290/3125   Loss: 0.736920 mae: 0.686937 (2203.9788551070383 steps/sec)\n",
      "Step #8542\tEpoch   2 Batch 2291/3125   Loss: 0.642461 mae: 0.641321 (2218.0114435595606 steps/sec)\n",
      "Step #8543\tEpoch   2 Batch 2292/3125   Loss: 0.798159 mae: 0.732105 (2083.25667795802 steps/sec)\n",
      "Step #8544\tEpoch   2 Batch 2293/3125   Loss: 0.797155 mae: 0.720191 (1634.6709069934213 steps/sec)\n",
      "Step #8545\tEpoch   2 Batch 2294/3125   Loss: 0.742781 mae: 0.663478 (1963.8829059989137 steps/sec)\n",
      "Step #8546\tEpoch   2 Batch 2295/3125   Loss: 1.032480 mae: 0.793858 (2003.201833986054 steps/sec)\n",
      "Step #8547\tEpoch   2 Batch 2296/3125   Loss: 0.858948 mae: 0.748633 (2119.5557037890503 steps/sec)\n",
      "Step #8548\tEpoch   2 Batch 2297/3125   Loss: 0.914457 mae: 0.788241 (2011.1164387502638 steps/sec)\n",
      "Step #8549\tEpoch   2 Batch 2298/3125   Loss: 0.853655 mae: 0.723817 (2068.421623647536 steps/sec)\n",
      "Step #8550\tEpoch   2 Batch 2299/3125   Loss: 0.861846 mae: 0.707546 (1976.1894441251025 steps/sec)\n",
      "Step #8551\tEpoch   2 Batch 2300/3125   Loss: 0.811569 mae: 0.711688 (2120.713122794244 steps/sec)\n",
      "Step #8552\tEpoch   2 Batch 2301/3125   Loss: 0.870304 mae: 0.726549 (2177.5922580109236 steps/sec)\n",
      "Step #8553\tEpoch   2 Batch 2302/3125   Loss: 0.832385 mae: 0.721729 (1819.0548886267434 steps/sec)\n",
      "Step #8554\tEpoch   2 Batch 2303/3125   Loss: 0.853953 mae: 0.727569 (1972.4905944319037 steps/sec)\n",
      "Step #8555\tEpoch   2 Batch 2304/3125   Loss: 0.768273 mae: 0.685159 (2289.9172326439693 steps/sec)\n",
      "Step #8556\tEpoch   2 Batch 2305/3125   Loss: 0.790693 mae: 0.702517 (2248.185073218842 steps/sec)\n",
      "Step #8557\tEpoch   2 Batch 2306/3125   Loss: 0.766410 mae: 0.696709 (2161.7895062364705 steps/sec)\n",
      "Step #8558\tEpoch   2 Batch 2307/3125   Loss: 0.965445 mae: 0.786650 (1872.5908993499536 steps/sec)\n",
      "Step #8559\tEpoch   2 Batch 2308/3125   Loss: 0.910933 mae: 0.760695 (2030.864580105361 steps/sec)\n",
      "Step #8560\tEpoch   2 Batch 2309/3125   Loss: 0.885373 mae: 0.742262 (1990.5387448270626 steps/sec)\n",
      "Step #8561\tEpoch   2 Batch 2310/3125   Loss: 0.821017 mae: 0.734084 (2162.815065385091 steps/sec)\n",
      "Step #8562\tEpoch   2 Batch 2311/3125   Loss: 0.850499 mae: 0.726956 (1792.6827600355605 steps/sec)\n",
      "Step #8563\tEpoch   2 Batch 2312/3125   Loss: 0.904747 mae: 0.718900 (2039.059203298039 steps/sec)\n",
      "Step #8564\tEpoch   2 Batch 2313/3125   Loss: 0.748366 mae: 0.687100 (2079.86829447293 steps/sec)\n",
      "Step #8565\tEpoch   2 Batch 2314/3125   Loss: 0.818116 mae: 0.704139 (2099.356324140347 steps/sec)\n",
      "Step #8566\tEpoch   2 Batch 2315/3125   Loss: 0.908201 mae: 0.750368 (2023.3014954172697 steps/sec)\n",
      "Step #8567\tEpoch   2 Batch 2316/3125   Loss: 0.965158 mae: 0.792552 (1797.8465125848707 steps/sec)\n",
      "Step #8568\tEpoch   2 Batch 2317/3125   Loss: 0.865145 mae: 0.747494 (2061.7922626947843 steps/sec)\n",
      "Step #8569\tEpoch   2 Batch 2318/3125   Loss: 0.847260 mae: 0.717536 (2144.020283395015 steps/sec)\n",
      "Step #8570\tEpoch   2 Batch 2319/3125   Loss: 0.853892 mae: 0.734546 (2298.52584969147 steps/sec)\n",
      "Step #8571\tEpoch   2 Batch 2320/3125   Loss: 0.845176 mae: 0.744311 (2058.0288711592625 steps/sec)\n",
      "Step #8572\tEpoch   2 Batch 2321/3125   Loss: 0.764636 mae: 0.683593 (1777.7747637010978 steps/sec)\n",
      "Step #8573\tEpoch   2 Batch 2322/3125   Loss: 0.787161 mae: 0.688144 (2199.079326797043 steps/sec)\n",
      "Step #8574\tEpoch   2 Batch 2323/3125   Loss: 0.792926 mae: 0.702290 (2069.0950708394175 steps/sec)\n",
      "Step #8575\tEpoch   2 Batch 2324/3125   Loss: 0.675820 mae: 0.635429 (2103.0405134376256 steps/sec)\n",
      "Step #8576\tEpoch   2 Batch 2325/3125   Loss: 0.761246 mae: 0.693797 (2170.919856731745 steps/sec)\n",
      "Step #8577\tEpoch   2 Batch 2326/3125   Loss: 0.742895 mae: 0.676569 (2090.0250146998733 steps/sec)\n",
      "Step #8578\tEpoch   2 Batch 2327/3125   Loss: 0.816848 mae: 0.696772 (2127.0152947381234 steps/sec)\n",
      "Step #8579\tEpoch   2 Batch 2328/3125   Loss: 0.863799 mae: 0.741707 (2040.805363902648 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8580\tEpoch   2 Batch 2329/3125   Loss: 0.815291 mae: 0.712589 (1780.1741846765021 steps/sec)\n",
      "Step #8581\tEpoch   2 Batch 2330/3125   Loss: 0.895418 mae: 0.765520 (1994.7419483706508 steps/sec)\n",
      "Step #8582\tEpoch   2 Batch 2331/3125   Loss: 0.754147 mae: 0.705084 (2040.5869302922974 steps/sec)\n",
      "Step #8583\tEpoch   2 Batch 2332/3125   Loss: 0.837241 mae: 0.724636 (2158.6964353724693 steps/sec)\n",
      "Step #8584\tEpoch   2 Batch 2333/3125   Loss: 0.764208 mae: 0.697339 (1908.8444909661857 steps/sec)\n",
      "Step #8585\tEpoch   2 Batch 2334/3125   Loss: 0.740395 mae: 0.678180 (2064.9186203365466 steps/sec)\n",
      "Step #8586\tEpoch   2 Batch 2335/3125   Loss: 0.865186 mae: 0.753812 (1947.8855317054142 steps/sec)\n",
      "Step #8587\tEpoch   2 Batch 2336/3125   Loss: 0.707906 mae: 0.659270 (1829.3530125001091 steps/sec)\n",
      "Step #8588\tEpoch   2 Batch 2337/3125   Loss: 0.892329 mae: 0.744070 (2014.8262014103723 steps/sec)\n",
      "Step #8589\tEpoch   2 Batch 2338/3125   Loss: 0.804049 mae: 0.706085 (1919.3791071003643 steps/sec)\n",
      "Step #8590\tEpoch   2 Batch 2339/3125   Loss: 0.714362 mae: 0.671371 (2057.786543424293 steps/sec)\n",
      "Step #8591\tEpoch   2 Batch 2340/3125   Loss: 0.813407 mae: 0.705702 (2173.056876703244 steps/sec)\n",
      "Step #8592\tEpoch   2 Batch 2341/3125   Loss: 0.788141 mae: 0.720793 (2031.120279706734 steps/sec)\n",
      "Step #8593\tEpoch   2 Batch 2342/3125   Loss: 0.802334 mae: 0.721526 (1911.4542223032402 steps/sec)\n",
      "Step #8594\tEpoch   2 Batch 2343/3125   Loss: 0.740011 mae: 0.681277 (2284.977119198082 steps/sec)\n",
      "Step #8595\tEpoch   2 Batch 2344/3125   Loss: 0.892089 mae: 0.753206 (2063.821286227427 steps/sec)\n",
      "Step #8596\tEpoch   2 Batch 2345/3125   Loss: 0.845900 mae: 0.696295 (2110.5964997031087 steps/sec)\n",
      "Step #8597\tEpoch   2 Batch 2346/3125   Loss: 0.720629 mae: 0.660685 (1741.7916645902892 steps/sec)\n",
      "Step #8598\tEpoch   2 Batch 2347/3125   Loss: 0.890667 mae: 0.747405 (1832.646177239083 steps/sec)\n",
      "Step #8599\tEpoch   2 Batch 2348/3125   Loss: 1.006363 mae: 0.780062 (1906.7791678789642 steps/sec)\n",
      "Step #8600\tEpoch   2 Batch 2349/3125   Loss: 0.809624 mae: 0.695570 (1792.8206881812353 steps/sec)\n",
      "Step #8601\tEpoch   2 Batch 2350/3125   Loss: 0.911157 mae: 0.762259 (2037.4941706824188 steps/sec)\n",
      "Step #8602\tEpoch   2 Batch 2351/3125   Loss: 0.802511 mae: 0.713749 (1877.855978796182 steps/sec)\n",
      "Step #8603\tEpoch   2 Batch 2352/3125   Loss: 0.801366 mae: 0.716140 (1978.5572768269901 steps/sec)\n",
      "Step #8604\tEpoch   2 Batch 2353/3125   Loss: 0.842270 mae: 0.713575 (2087.59083398037 steps/sec)\n",
      "Step #8605\tEpoch   2 Batch 2354/3125   Loss: 0.814448 mae: 0.711324 (1881.5625616824275 steps/sec)\n",
      "Step #8606\tEpoch   2 Batch 2355/3125   Loss: 0.833393 mae: 0.741274 (1730.4662100833402 steps/sec)\n",
      "Step #8607\tEpoch   2 Batch 2356/3125   Loss: 0.900855 mae: 0.776255 (2009.6517622707322 steps/sec)\n",
      "Step #8608\tEpoch   2 Batch 2357/3125   Loss: 0.791434 mae: 0.707705 (1993.301017013592 steps/sec)\n",
      "Step #8609\tEpoch   2 Batch 2358/3125   Loss: 0.770830 mae: 0.702523 (2053.997512267265 steps/sec)\n",
      "Step #8610\tEpoch   2 Batch 2359/3125   Loss: 0.856366 mae: 0.724618 (2084.3747825827677 steps/sec)\n",
      "Step #8611\tEpoch   2 Batch 2360/3125   Loss: 0.811188 mae: 0.710666 (2185.6944835277072 steps/sec)\n",
      "Step #8612\tEpoch   2 Batch 2361/3125   Loss: 0.713481 mae: 0.669412 (2191.1524396614773 steps/sec)\n",
      "Step #8613\tEpoch   2 Batch 2362/3125   Loss: 0.885079 mae: 0.744867 (2295.6826342061477 steps/sec)\n",
      "Step #8614\tEpoch   2 Batch 2363/3125   Loss: 0.757883 mae: 0.701680 (1520.4025113460061 steps/sec)\n",
      "Step #8615\tEpoch   2 Batch 2364/3125   Loss: 0.856635 mae: 0.724117 (1890.859255252006 steps/sec)\n",
      "Step #8616\tEpoch   2 Batch 2365/3125   Loss: 0.680677 mae: 0.629992 (2018.5108185108186 steps/sec)\n",
      "Step #8617\tEpoch   2 Batch 2366/3125   Loss: 0.798379 mae: 0.715914 (2166.837493800628 steps/sec)\n",
      "Step #8618\tEpoch   2 Batch 2367/3125   Loss: 0.886837 mae: 0.726230 (2001.442995934416 steps/sec)\n",
      "Step #8619\tEpoch   2 Batch 2368/3125   Loss: 0.905635 mae: 0.752362 (2230.5619076995076 steps/sec)\n",
      "Step #8620\tEpoch   2 Batch 2369/3125   Loss: 0.805613 mae: 0.719304 (2132.9427798458128 steps/sec)\n",
      "Step #8621\tEpoch   2 Batch 2370/3125   Loss: 0.823939 mae: 0.718953 (2151.0795646866954 steps/sec)\n",
      "Step #8622\tEpoch   2 Batch 2371/3125   Loss: 0.843375 mae: 0.719433 (2085.680755842864 steps/sec)\n",
      "Step #8623\tEpoch   2 Batch 2372/3125   Loss: 0.802118 mae: 0.724958 (1736.2398271337147 steps/sec)\n",
      "Step #8624\tEpoch   2 Batch 2373/3125   Loss: 0.743061 mae: 0.679425 (1777.0215650552896 steps/sec)\n",
      "Step #8625\tEpoch   2 Batch 2374/3125   Loss: 0.793784 mae: 0.709287 (1873.0926564370054 steps/sec)\n",
      "Step #8626\tEpoch   2 Batch 2375/3125   Loss: 0.882132 mae: 0.757291 (2022.3259402121505 steps/sec)\n",
      "Step #8627\tEpoch   2 Batch 2376/3125   Loss: 0.802960 mae: 0.702651 (2119.5342820181113 steps/sec)\n",
      "Step #8628\tEpoch   2 Batch 2377/3125   Loss: 0.753375 mae: 0.682897 (2022.579494054221 steps/sec)\n",
      "Step #8629\tEpoch   2 Batch 2378/3125   Loss: 0.934694 mae: 0.768553 (2038.0089794173098 steps/sec)\n",
      "Step #8630\tEpoch   2 Batch 2379/3125   Loss: 0.898522 mae: 0.768325 (1973.4000809251818 steps/sec)\n",
      "Step #8631\tEpoch   2 Batch 2380/3125   Loss: 0.811386 mae: 0.711107 (1972.6946918887395 steps/sec)\n",
      "Step #8632\tEpoch   2 Batch 2381/3125   Loss: 0.786037 mae: 0.706516 (1928.2737821585538 steps/sec)\n",
      "Step #8633\tEpoch   2 Batch 2382/3125   Loss: 0.813109 mae: 0.715771 (1869.1527478208168 steps/sec)\n",
      "Step #8634\tEpoch   2 Batch 2383/3125   Loss: 0.785802 mae: 0.706974 (2074.539519240281 steps/sec)\n",
      "Step #8635\tEpoch   2 Batch 2384/3125   Loss: 0.909994 mae: 0.775991 (1980.650157722748 steps/sec)\n",
      "Step #8636\tEpoch   2 Batch 2385/3125   Loss: 0.880047 mae: 0.744811 (1949.0079088484308 steps/sec)\n",
      "Step #8637\tEpoch   2 Batch 2386/3125   Loss: 0.781689 mae: 0.685527 (2056.837975676736 steps/sec)\n",
      "Step #8638\tEpoch   2 Batch 2387/3125   Loss: 0.815385 mae: 0.731996 (2166.4793388429753 steps/sec)\n",
      "Step #8639\tEpoch   2 Batch 2388/3125   Loss: 0.780809 mae: 0.684031 (2107.3092304909665 steps/sec)\n",
      "Step #8640\tEpoch   2 Batch 2389/3125   Loss: 0.837667 mae: 0.754466 (1873.0759268686977 steps/sec)\n",
      "Step #8641\tEpoch   2 Batch 2390/3125   Loss: 0.850201 mae: 0.737041 (1910.9491179472227 steps/sec)\n",
      "Step #8642\tEpoch   2 Batch 2391/3125   Loss: 0.845486 mae: 0.724267 (2027.9776812911587 steps/sec)\n",
      "Step #8643\tEpoch   2 Batch 2392/3125   Loss: 0.812583 mae: 0.710504 (2071.343065405053 steps/sec)\n",
      "Step #8644\tEpoch   2 Batch 2393/3125   Loss: 0.824298 mae: 0.719044 (1970.1927774228702 steps/sec)\n",
      "Step #8645\tEpoch   2 Batch 2394/3125   Loss: 0.773428 mae: 0.682570 (1976.0404790396592 steps/sec)\n",
      "Step #8646\tEpoch   2 Batch 2395/3125   Loss: 0.778196 mae: 0.722535 (1953.437596058012 steps/sec)\n",
      "Step #8647\tEpoch   2 Batch 2396/3125   Loss: 0.918838 mae: 0.773199 (2020.6891235643259 steps/sec)\n",
      "Step #8648\tEpoch   2 Batch 2397/3125   Loss: 0.726410 mae: 0.668700 (1965.5578986831622 steps/sec)\n",
      "Step #8649\tEpoch   2 Batch 2398/3125   Loss: 0.727844 mae: 0.680218 (1967.5125951083132 steps/sec)\n",
      "Step #8650\tEpoch   2 Batch 2399/3125   Loss: 0.736294 mae: 0.676992 (1999.2106693104797 steps/sec)\n",
      "Step #8651\tEpoch   2 Batch 2400/3125   Loss: 0.868605 mae: 0.726469 (2017.7534035695387 steps/sec)\n",
      "Step #8652\tEpoch   2 Batch 2401/3125   Loss: 0.823613 mae: 0.693128 (2078.6519972247 steps/sec)\n",
      "Step #8653\tEpoch   2 Batch 2402/3125   Loss: 0.755978 mae: 0.689475 (2129.563963524848 steps/sec)\n",
      "Step #8654\tEpoch   2 Batch 2403/3125   Loss: 0.788412 mae: 0.711946 (2063.7197402086204 steps/sec)\n",
      "Step #8655\tEpoch   2 Batch 2404/3125   Loss: 0.847477 mae: 0.733783 (2164.3999050499006 steps/sec)\n",
      "Step #8656\tEpoch   2 Batch 2405/3125   Loss: 0.828830 mae: 0.720115 (1728.5549437868847 steps/sec)\n",
      "Step #8657\tEpoch   2 Batch 2406/3125   Loss: 0.851125 mae: 0.724852 (2067.3205642577605 steps/sec)\n",
      "Step #8658\tEpoch   2 Batch 2407/3125   Loss: 0.737850 mae: 0.704700 (2013.6267619157352 steps/sec)\n",
      "Step #8659\tEpoch   2 Batch 2408/3125   Loss: 0.919639 mae: 0.766168 (2105.9760396059487 steps/sec)\n",
      "Step #8660\tEpoch   2 Batch 2409/3125   Loss: 0.690778 mae: 0.668388 (2074.683181148165 steps/sec)\n",
      "Step #8661\tEpoch   2 Batch 2410/3125   Loss: 0.881516 mae: 0.731912 (2165.897589490426 steps/sec)\n",
      "Step #8662\tEpoch   2 Batch 2411/3125   Loss: 0.897466 mae: 0.767661 (2166.2779287049757 steps/sec)\n",
      "Step #8663\tEpoch   2 Batch 2412/3125   Loss: 0.882300 mae: 0.777301 (1830.2949903997207 steps/sec)\n",
      "Step #8664\tEpoch   2 Batch 2413/3125   Loss: 0.846297 mae: 0.728746 (1810.668094144463 steps/sec)\n",
      "Step #8665\tEpoch   2 Batch 2414/3125   Loss: 0.850547 mae: 0.735327 (1810.0898505942569 steps/sec)\n",
      "Step #8666\tEpoch   2 Batch 2415/3125   Loss: 0.854008 mae: 0.744486 (2013.3561183541024 steps/sec)\n",
      "Step #8667\tEpoch   2 Batch 2416/3125   Loss: 0.790544 mae: 0.707915 (1869.4193364354353 steps/sec)\n",
      "Step #8668\tEpoch   2 Batch 2417/3125   Loss: 0.770910 mae: 0.689973 (1923.6749894512832 steps/sec)\n",
      "Step #8669\tEpoch   2 Batch 2418/3125   Loss: 0.715385 mae: 0.662347 (2114.490824763057 steps/sec)\n",
      "Step #8670\tEpoch   2 Batch 2419/3125   Loss: 0.767497 mae: 0.705231 (2068.5032302608865 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8671\tEpoch   2 Batch 2420/3125   Loss: 0.758744 mae: 0.678550 (2112.9994962216624 steps/sec)\n",
      "Step #8672\tEpoch   2 Batch 2421/3125   Loss: 0.734786 mae: 0.684956 (1837.9625247585493 steps/sec)\n",
      "Step #8673\tEpoch   2 Batch 2422/3125   Loss: 0.791764 mae: 0.698110 (2001.1374262867612 steps/sec)\n",
      "Step #8674\tEpoch   2 Batch 2423/3125   Loss: 0.671286 mae: 0.626171 (2051.445787846774 steps/sec)\n",
      "Step #8675\tEpoch   2 Batch 2424/3125   Loss: 0.777776 mae: 0.706032 (2001.6722344182494 steps/sec)\n",
      "Step #8676\tEpoch   2 Batch 2425/3125   Loss: 0.764384 mae: 0.693800 (2143.9764455712766 steps/sec)\n",
      "Step #8677\tEpoch   2 Batch 2426/3125   Loss: 0.949908 mae: 0.771877 (2230.8466390800686 steps/sec)\n",
      "Step #8678\tEpoch   2 Batch 2427/3125   Loss: 0.878789 mae: 0.726152 (2342.5845872010545 steps/sec)\n",
      "Step #8679\tEpoch   2 Batch 2428/3125   Loss: 0.770089 mae: 0.680270 (2147.1153747709195 steps/sec)\n",
      "Step #8680\tEpoch   2 Batch 2429/3125   Loss: 0.881385 mae: 0.715658 (1917.3259949350424 steps/sec)\n",
      "Step #8681\tEpoch   2 Batch 2430/3125   Loss: 0.792607 mae: 0.717845 (1761.6168404075702 steps/sec)\n",
      "Step #8682\tEpoch   2 Batch 2431/3125   Loss: 0.772580 mae: 0.716113 (1971.674626753413 steps/sec)\n",
      "Step #8683\tEpoch   2 Batch 2432/3125   Loss: 0.802266 mae: 0.706966 (2013.9554983626394 steps/sec)\n",
      "Step #8684\tEpoch   2 Batch 2433/3125   Loss: 0.697327 mae: 0.667760 (2065.7729095046247 steps/sec)\n",
      "Step #8685\tEpoch   2 Batch 2434/3125   Loss: 0.938040 mae: 0.750207 (2113.361482571322 steps/sec)\n",
      "Step #8686\tEpoch   2 Batch 2435/3125   Loss: 0.764265 mae: 0.668937 (2171.8191421055903 steps/sec)\n",
      "Step #8687\tEpoch   2 Batch 2436/3125   Loss: 0.768260 mae: 0.695189 (2139.121565107407 steps/sec)\n",
      "Step #8688\tEpoch   2 Batch 2437/3125   Loss: 0.766954 mae: 0.685282 (2228.357701462088 steps/sec)\n",
      "Step #8689\tEpoch   2 Batch 2438/3125   Loss: 0.808293 mae: 0.731003 (1857.2837975468274 steps/sec)\n",
      "Step #8690\tEpoch   2 Batch 2439/3125   Loss: 0.772727 mae: 0.688249 (1950.9842592937148 steps/sec)\n",
      "Step #8691\tEpoch   2 Batch 2440/3125   Loss: 0.772635 mae: 0.717846 (2017.015956065517 steps/sec)\n",
      "Step #8692\tEpoch   2 Batch 2441/3125   Loss: 0.748302 mae: 0.676829 (1923.533835965733 steps/sec)\n",
      "Step #8693\tEpoch   2 Batch 2442/3125   Loss: 0.738752 mae: 0.657802 (1949.460846285417 steps/sec)\n",
      "Step #8694\tEpoch   2 Batch 2443/3125   Loss: 0.779034 mae: 0.704869 (2126.6054859808346 steps/sec)\n",
      "Step #8695\tEpoch   2 Batch 2444/3125   Loss: 0.782989 mae: 0.711082 (2090.0250146998733 steps/sec)\n",
      "Step #8696\tEpoch   2 Batch 2445/3125   Loss: 0.884532 mae: 0.750952 (2050.122196805287 steps/sec)\n",
      "Step #8697\tEpoch   2 Batch 2446/3125   Loss: 0.744641 mae: 0.681386 (1718.6812106112882 steps/sec)\n",
      "Step #8698\tEpoch   2 Batch 2447/3125   Loss: 0.759704 mae: 0.691649 (2099.9659543788675 steps/sec)\n",
      "Step #8699\tEpoch   2 Batch 2448/3125   Loss: 0.765577 mae: 0.697937 (2104.8135212172306 steps/sec)\n",
      "Step #8700\tEpoch   2 Batch 2449/3125   Loss: 0.812988 mae: 0.726519 (2112.0844369693737 steps/sec)\n",
      "Step #8701\tEpoch   2 Batch 2450/3125   Loss: 0.889484 mae: 0.755444 (2125.958740939733 steps/sec)\n",
      "Step #8702\tEpoch   2 Batch 2451/3125   Loss: 0.830980 mae: 0.729366 (1854.9510423945444 steps/sec)\n",
      "Step #8703\tEpoch   2 Batch 2452/3125   Loss: 0.862832 mae: 0.741397 (2146.478065955661 steps/sec)\n",
      "Step #8704\tEpoch   2 Batch 2453/3125   Loss: 0.867897 mae: 0.726730 (1574.1666228804336 steps/sec)\n",
      "Step #8705\tEpoch   2 Batch 2454/3125   Loss: 0.745551 mae: 0.671212 (1640.0400400400401 steps/sec)\n",
      "Step #8706\tEpoch   2 Batch 2455/3125   Loss: 0.883870 mae: 0.738440 (2157.5859833948907 steps/sec)\n",
      "Step #8707\tEpoch   2 Batch 2456/3125   Loss: 0.746206 mae: 0.712365 (1939.1321232743715 steps/sec)\n",
      "Step #8708\tEpoch   2 Batch 2457/3125   Loss: 0.750246 mae: 0.672940 (2017.7534035695387 steps/sec)\n",
      "Step #8709\tEpoch   2 Batch 2458/3125   Loss: 0.814371 mae: 0.725586 (2208.109502500658 steps/sec)\n",
      "Step #8710\tEpoch   2 Batch 2459/3125   Loss: 0.773144 mae: 0.704238 (1956.4080079108905 steps/sec)\n",
      "Step #8711\tEpoch   2 Batch 2460/3125   Loss: 0.832607 mae: 0.722947 (1991.162424161864 steps/sec)\n",
      "Step #8712\tEpoch   2 Batch 2461/3125   Loss: 0.712860 mae: 0.665721 (1934.320869228357 steps/sec)\n",
      "Step #8713\tEpoch   2 Batch 2462/3125   Loss: 0.857739 mae: 0.734260 (1657.8276679841897 steps/sec)\n",
      "Step #8714\tEpoch   2 Batch 2463/3125   Loss: 0.811447 mae: 0.701491 (2043.2510376273895 steps/sec)\n",
      "Step #8715\tEpoch   2 Batch 2464/3125   Loss: 0.861551 mae: 0.725634 (2274.5683297180044 steps/sec)\n",
      "Step #8716\tEpoch   2 Batch 2465/3125   Loss: 0.713775 mae: 0.687427 (2102.808555012985 steps/sec)\n",
      "Step #8717\tEpoch   2 Batch 2466/3125   Loss: 0.741179 mae: 0.659280 (2055.829820605823 steps/sec)\n",
      "Step #8718\tEpoch   2 Batch 2467/3125   Loss: 0.743098 mae: 0.686757 (2117.308779581617 steps/sec)\n",
      "Step #8719\tEpoch   2 Batch 2468/3125   Loss: 1.016191 mae: 0.809789 (2198.6873833637374 steps/sec)\n",
      "Step #8720\tEpoch   2 Batch 2469/3125   Loss: 0.839532 mae: 0.698954 (1995.9949746830623 steps/sec)\n",
      "Step #8721\tEpoch   2 Batch 2470/3125   Loss: 0.875231 mae: 0.755494 (1955.1130378035707 steps/sec)\n",
      "Step #8722\tEpoch   2 Batch 2471/3125   Loss: 0.824598 mae: 0.721578 (1917.255880712725 steps/sec)\n",
      "Step #8723\tEpoch   2 Batch 2472/3125   Loss: 0.750883 mae: 0.663438 (2256.3122673380244 steps/sec)\n",
      "Step #8724\tEpoch   2 Batch 2473/3125   Loss: 0.831789 mae: 0.723998 (2044.7252447252447 steps/sec)\n",
      "Step #8725\tEpoch   2 Batch 2474/3125   Loss: 0.888924 mae: 0.734939 (2169.8416968442834 steps/sec)\n",
      "Step #8726\tEpoch   2 Batch 2475/3125   Loss: 0.870222 mae: 0.748664 (2195.3729874589117 steps/sec)\n",
      "Step #8727\tEpoch   2 Batch 2476/3125   Loss: 0.807227 mae: 0.709148 (2098.180108253044 steps/sec)\n",
      "Step #8728\tEpoch   2 Batch 2477/3125   Loss: 0.847016 mae: 0.728270 (2393.4080482070713 steps/sec)\n",
      "Step #8729\tEpoch   2 Batch 2478/3125   Loss: 0.793103 mae: 0.719820 (2155.567889813958 steps/sec)\n",
      "Step #8730\tEpoch   2 Batch 2479/3125   Loss: 0.819159 mae: 0.725949 (1881.3262523324242 steps/sec)\n",
      "Step #8731\tEpoch   2 Batch 2480/3125   Loss: 0.904225 mae: 0.735011 (1983.8165599311344 steps/sec)\n",
      "Step #8732\tEpoch   2 Batch 2481/3125   Loss: 0.798819 mae: 0.709639 (2191.450097704212 steps/sec)\n",
      "Step #8733\tEpoch   2 Batch 2482/3125   Loss: 0.882914 mae: 0.735886 (2103.314712106472 steps/sec)\n",
      "Step #8734\tEpoch   2 Batch 2483/3125   Loss: 0.893690 mae: 0.769049 (1907.8893740902474 steps/sec)\n",
      "Step #8735\tEpoch   2 Batch 2484/3125   Loss: 0.736160 mae: 0.696760 (1799.3427769817504 steps/sec)\n",
      "Step #8736\tEpoch   2 Batch 2485/3125   Loss: 0.762492 mae: 0.690755 (1906.5018181818182 steps/sec)\n",
      "Step #8737\tEpoch   2 Batch 2486/3125   Loss: 0.848684 mae: 0.697490 (2024.1412259789397 steps/sec)\n",
      "Step #8738\tEpoch   2 Batch 2487/3125   Loss: 0.734437 mae: 0.657231 (2114.341597185115 steps/sec)\n",
      "Step #8739\tEpoch   2 Batch 2488/3125   Loss: 0.819998 mae: 0.723004 (1900.782191768406 steps/sec)\n",
      "Step #8740\tEpoch   2 Batch 2489/3125   Loss: 0.905476 mae: 0.765593 (1760.0792271991004 steps/sec)\n",
      "Step #8741\tEpoch   2 Batch 2490/3125   Loss: 0.911694 mae: 0.749804 (1774.3753754515994 steps/sec)\n",
      "Step #8742\tEpoch   2 Batch 2491/3125   Loss: 0.839816 mae: 0.716968 (2096.6278430392404 steps/sec)\n",
      "Step #8743\tEpoch   2 Batch 2492/3125   Loss: 0.939948 mae: 0.755702 (1803.226139294927 steps/sec)\n",
      "Step #8744\tEpoch   2 Batch 2493/3125   Loss: 0.872774 mae: 0.731942 (2127.360519375127 steps/sec)\n",
      "Step #8745\tEpoch   2 Batch 2494/3125   Loss: 0.790575 mae: 0.694574 (2087.341494973624 steps/sec)\n",
      "Step #8746\tEpoch   2 Batch 2495/3125   Loss: 0.850749 mae: 0.715576 (1988.9152330191005 steps/sec)\n",
      "Step #8747\tEpoch   2 Batch 2496/3125   Loss: 0.837728 mae: 0.739509 (1557.4722801910123 steps/sec)\n",
      "Step #8748\tEpoch   2 Batch 2497/3125   Loss: 0.711470 mae: 0.681068 (2128.850585213834 steps/sec)\n",
      "Step #8749\tEpoch   2 Batch 2498/3125   Loss: 0.866839 mae: 0.723745 (1996.736139542412 steps/sec)\n",
      "Step #8750\tEpoch   2 Batch 2499/3125   Loss: 0.898142 mae: 0.762576 (2159.5411436397526 steps/sec)\n",
      "Step #8751\tEpoch   2 Batch 2500/3125   Loss: 0.763964 mae: 0.705354 (1898.0297037767782 steps/sec)\n",
      "Step #8752\tEpoch   2 Batch 2501/3125   Loss: 0.876828 mae: 0.726210 (1978.79997358017 steps/sec)\n",
      "Step #8753\tEpoch   2 Batch 2502/3125   Loss: 0.740792 mae: 0.687381 (1961.3115612666704 steps/sec)\n",
      "Step #8754\tEpoch   2 Batch 2503/3125   Loss: 0.832094 mae: 0.724813 (1961.7151837162314 steps/sec)\n",
      "Step #8755\tEpoch   2 Batch 2504/3125   Loss: 0.718267 mae: 0.675950 (1763.246088265216 steps/sec)\n",
      "Step #8756\tEpoch   2 Batch 2505/3125   Loss: 0.785894 mae: 0.687609 (2079.9508068275363 steps/sec)\n",
      "Step #8757\tEpoch   2 Batch 2506/3125   Loss: 0.955426 mae: 0.766363 (2112.1907984852146 steps/sec)\n",
      "Step #8758\tEpoch   2 Batch 2507/3125   Loss: 0.662755 mae: 0.640396 (1902.8690681426367 steps/sec)\n",
      "Step #8759\tEpoch   2 Batch 2508/3125   Loss: 0.858900 mae: 0.730529 (2112.914340983739 steps/sec)\n",
      "Step #8760\tEpoch   2 Batch 2509/3125   Loss: 0.772171 mae: 0.708119 (2079.2496604237517 steps/sec)\n",
      "Step #8761\tEpoch   2 Batch 2510/3125   Loss: 0.893919 mae: 0.752767 (2038.999727764166 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8762\tEpoch   2 Batch 2511/3125   Loss: 0.776650 mae: 0.694169 (1910.3050618959564 steps/sec)\n",
      "Step #8763\tEpoch   2 Batch 2512/3125   Loss: 0.864151 mae: 0.737645 (1813.0944867594042 steps/sec)\n",
      "Step #8764\tEpoch   2 Batch 2513/3125   Loss: 0.902053 mae: 0.745601 (2040.3288417570657 steps/sec)\n",
      "Step #8765\tEpoch   2 Batch 2514/3125   Loss: 0.819810 mae: 0.721196 (2171.077177907759 steps/sec)\n",
      "Step #8766\tEpoch   2 Batch 2515/3125   Loss: 0.881420 mae: 0.740166 (2022.2479364344674 steps/sec)\n",
      "Step #8767\tEpoch   2 Batch 2516/3125   Loss: 0.711410 mae: 0.675886 (1890.7740161384845 steps/sec)\n",
      "Step #8768\tEpoch   2 Batch 2517/3125   Loss: 0.739982 mae: 0.693050 (2055.1649794694395 steps/sec)\n",
      "Step #8769\tEpoch   2 Batch 2518/3125   Loss: 0.872132 mae: 0.745734 (2135.1795477453447 steps/sec)\n",
      "Step #8770\tEpoch   2 Batch 2519/3125   Loss: 0.798914 mae: 0.712955 (1876.159207006683 steps/sec)\n",
      "Step #8771\tEpoch   2 Batch 2520/3125   Loss: 0.889398 mae: 0.755235 (1727.6008929821814 steps/sec)\n",
      "Step #8772\tEpoch   2 Batch 2521/3125   Loss: 0.950846 mae: 0.778675 (1788.9056648838618 steps/sec)\n",
      "Step #8773\tEpoch   2 Batch 2522/3125   Loss: 0.735751 mae: 0.676873 (2087.7778773307846 steps/sec)\n",
      "Step #8774\tEpoch   2 Batch 2523/3125   Loss: 0.861423 mae: 0.742410 (2054.5609514759044 steps/sec)\n",
      "Step #8775\tEpoch   2 Batch 2524/3125   Loss: 0.778497 mae: 0.709586 (2118.1000090898992 steps/sec)\n",
      "Step #8776\tEpoch   2 Batch 2525/3125   Loss: 0.789277 mae: 0.702820 (2055.8499740219 steps/sec)\n",
      "Step #8777\tEpoch   2 Batch 2526/3125   Loss: 0.931130 mae: 0.734699 (2129.1964059089296 steps/sec)\n",
      "Step #8778\tEpoch   2 Batch 2527/3125   Loss: 0.753336 mae: 0.687227 (2014.5165318629806 steps/sec)\n",
      "Step #8779\tEpoch   2 Batch 2528/3125   Loss: 0.775825 mae: 0.698354 (1746.0552169713924 steps/sec)\n",
      "Step #8780\tEpoch   2 Batch 2529/3125   Loss: 0.860065 mae: 0.741925 (1562.3105919513685 steps/sec)\n",
      "Step #8781\tEpoch   2 Batch 2530/3125   Loss: 0.707580 mae: 0.665709 (1956.6453009395323 steps/sec)\n",
      "Step #8782\tEpoch   2 Batch 2531/3125   Loss: 0.911039 mae: 0.760597 (1821.962746733389 steps/sec)\n",
      "Step #8783\tEpoch   2 Batch 2532/3125   Loss: 0.805154 mae: 0.715393 (2064.9186203365466 steps/sec)\n",
      "Step #8784\tEpoch   2 Batch 2533/3125   Loss: 0.865181 mae: 0.736609 (1912.6222092514226 steps/sec)\n",
      "Step #8785\tEpoch   2 Batch 2534/3125   Loss: 0.936114 mae: 0.761244 (2057.342424093785 steps/sec)\n",
      "Step #8786\tEpoch   2 Batch 2535/3125   Loss: 0.968770 mae: 0.770840 (1765.725351519744 steps/sec)\n",
      "Step #8787\tEpoch   2 Batch 2536/3125   Loss: 0.758723 mae: 0.701773 (1576.6400529267596 steps/sec)\n",
      "Step #8788\tEpoch   2 Batch 2537/3125   Loss: 0.824529 mae: 0.723536 (1915.2597788067253 steps/sec)\n",
      "Step #8789\tEpoch   2 Batch 2538/3125   Loss: 0.802886 mae: 0.732549 (2114.874649563341 steps/sec)\n",
      "Step #8790\tEpoch   2 Batch 2539/3125   Loss: 0.870741 mae: 0.751919 (2060.839999213852 steps/sec)\n",
      "Step #8791\tEpoch   2 Batch 2540/3125   Loss: 0.774558 mae: 0.720084 (2050.783778762187 steps/sec)\n",
      "Step #8792\tEpoch   2 Batch 2541/3125   Loss: 0.774205 mae: 0.701860 (1960.5597988164575 steps/sec)\n",
      "Step #8793\tEpoch   2 Batch 2542/3125   Loss: 0.973711 mae: 0.777462 (1954.0200326112276 steps/sec)\n",
      "Step #8794\tEpoch   2 Batch 2543/3125   Loss: 0.789795 mae: 0.705561 (1903.4562881208249 steps/sec)\n",
      "Step #8795\tEpoch   2 Batch 2544/3125   Loss: 0.913264 mae: 0.758584 (1792.3610102132388 steps/sec)\n",
      "Step #8796\tEpoch   2 Batch 2545/3125   Loss: 0.882097 mae: 0.757177 (1794.3085952873937 steps/sec)\n",
      "Step #8797\tEpoch   2 Batch 2546/3125   Loss: 0.775880 mae: 0.719219 (2085.411135308215 steps/sec)\n",
      "Step #8798\tEpoch   2 Batch 2547/3125   Loss: 1.030386 mae: 0.798323 (1817.2736804707065 steps/sec)\n",
      "Step #8799\tEpoch   2 Batch 2548/3125   Loss: 0.855904 mae: 0.726352 (1818.5658911367598 steps/sec)\n",
      "Step #8800\tEpoch   2 Batch 2549/3125   Loss: 0.651707 mae: 0.652212 (2059.90884802766 steps/sec)\n",
      "Step #8801\tEpoch   2 Batch 2550/3125   Loss: 0.904148 mae: 0.747168 (1965.944841292161 steps/sec)\n",
      "Step #8802\tEpoch   2 Batch 2551/3125   Loss: 0.786543 mae: 0.706955 (1915.5571793934964 steps/sec)\n",
      "Step #8803\tEpoch   2 Batch 2552/3125   Loss: 0.808505 mae: 0.699779 (1863.3731985143852 steps/sec)\n",
      "Step #8804\tEpoch   2 Batch 2553/3125   Loss: 0.855599 mae: 0.738505 (1640.4120679270668 steps/sec)\n",
      "Step #8805\tEpoch   2 Batch 2554/3125   Loss: 0.849524 mae: 0.717306 (2048.94043164343 steps/sec)\n",
      "Step #8806\tEpoch   2 Batch 2555/3125   Loss: 0.701047 mae: 0.651351 (1964.2691893410763 steps/sec)\n",
      "Step #8807\tEpoch   2 Batch 2556/3125   Loss: 1.000914 mae: 0.795212 (2215.317009274713 steps/sec)\n",
      "Step #8808\tEpoch   2 Batch 2557/3125   Loss: 0.932161 mae: 0.755890 (1931.0436271891863 steps/sec)\n",
      "Step #8809\tEpoch   2 Batch 2558/3125   Loss: 0.851890 mae: 0.746857 (2051.0244598969184 steps/sec)\n",
      "Step #8810\tEpoch   2 Batch 2559/3125   Loss: 0.922940 mae: 0.766106 (2025.6858048064291 steps/sec)\n",
      "Step #8811\tEpoch   2 Batch 2560/3125   Loss: 0.617933 mae: 0.623452 (1759.2081201241506 steps/sec)\n",
      "Step #8812\tEpoch   2 Batch 2561/3125   Loss: 0.645776 mae: 0.630578 (2190.946416072044 steps/sec)\n",
      "Step #8813\tEpoch   2 Batch 2562/3125   Loss: 0.880456 mae: 0.742718 (2062.6439664414347 steps/sec)\n",
      "Step #8814\tEpoch   2 Batch 2563/3125   Loss: 0.788484 mae: 0.689762 (2069.544279313952 steps/sec)\n",
      "Step #8815\tEpoch   2 Batch 2564/3125   Loss: 0.830893 mae: 0.725577 (2044.5059712405557 steps/sec)\n",
      "Step #8816\tEpoch   2 Batch 2565/3125   Loss: 0.812897 mae: 0.713786 (1927.2282823455894 steps/sec)\n",
      "Step #8817\tEpoch   2 Batch 2566/3125   Loss: 0.761587 mae: 0.673625 (2063.212159968518 steps/sec)\n",
      "Step #8818\tEpoch   2 Batch 2567/3125   Loss: 0.826391 mae: 0.719321 (2094.7639690752544 steps/sec)\n",
      "Step #8819\tEpoch   2 Batch 2568/3125   Loss: 0.879567 mae: 0.724265 (1594.7923954372623 steps/sec)\n",
      "Step #8820\tEpoch   2 Batch 2569/3125   Loss: 0.812923 mae: 0.707026 (1439.0075204479333 steps/sec)\n",
      "Step #8821\tEpoch   2 Batch 2570/3125   Loss: 0.913336 mae: 0.775794 (1926.6263056839166 steps/sec)\n",
      "Step #8822\tEpoch   2 Batch 2571/3125   Loss: 0.733546 mae: 0.677585 (1971.8600146679955 steps/sec)\n",
      "Step #8823\tEpoch   2 Batch 2572/3125   Loss: 0.894949 mae: 0.748046 (2008.1892176577612 steps/sec)\n",
      "Step #8824\tEpoch   2 Batch 2573/3125   Loss: 0.816788 mae: 0.735176 (1854.672161593293 steps/sec)\n",
      "Step #8825\tEpoch   2 Batch 2574/3125   Loss: 0.897583 mae: 0.726522 (1991.067902172262 steps/sec)\n",
      "Step #8826\tEpoch   2 Batch 2575/3125   Loss: 0.770562 mae: 0.677171 (1781.9439369864642 steps/sec)\n",
      "Step #8827\tEpoch   2 Batch 2576/3125   Loss: 0.887249 mae: 0.738831 (1961.0914735641213 steps/sec)\n",
      "Step #8828\tEpoch   2 Batch 2577/3125   Loss: 0.774437 mae: 0.693867 (2039.1583368986037 steps/sec)\n",
      "Step #8829\tEpoch   2 Batch 2578/3125   Loss: 0.836573 mae: 0.736587 (2199.7251853949674 steps/sec)\n",
      "Step #8830\tEpoch   2 Batch 2579/3125   Loss: 0.715964 mae: 0.663040 (2142.881082296201 steps/sec)\n",
      "Step #8831\tEpoch   2 Batch 2580/3125   Loss: 0.828542 mae: 0.712991 (1629.210236012492 steps/sec)\n",
      "Step #8832\tEpoch   2 Batch 2581/3125   Loss: 0.789932 mae: 0.714567 (1991.4649548463065 steps/sec)\n",
      "Step #8833\tEpoch   2 Batch 2582/3125   Loss: 0.913336 mae: 0.760494 (2000.3929910241613 steps/sec)\n",
      "Step #8834\tEpoch   2 Batch 2583/3125   Loss: 0.810645 mae: 0.722355 (1937.842008482642 steps/sec)\n",
      "Step #8835\tEpoch   2 Batch 2584/3125   Loss: 0.984258 mae: 0.780810 (1722.16729351093 steps/sec)\n",
      "Step #8836\tEpoch   2 Batch 2585/3125   Loss: 0.739853 mae: 0.701599 (1954.8032288734364 steps/sec)\n",
      "Step #8837\tEpoch   2 Batch 2586/3125   Loss: 0.908630 mae: 0.733554 (2091.254661853573 steps/sec)\n",
      "Step #8838\tEpoch   2 Batch 2587/3125   Loss: 0.790746 mae: 0.729272 (2150.616321758927 steps/sec)\n",
      "Step #8839\tEpoch   2 Batch 2588/3125   Loss: 0.817756 mae: 0.715268 (1907.6117013535147 steps/sec)\n",
      "Step #8840\tEpoch   2 Batch 2589/3125   Loss: 0.840329 mae: 0.721579 (1854.3277775321633 steps/sec)\n",
      "Step #8841\tEpoch   2 Batch 2590/3125   Loss: 0.877695 mae: 0.736845 (1799.2655890730637 steps/sec)\n",
      "Step #8842\tEpoch   2 Batch 2591/3125   Loss: 1.003477 mae: 0.777203 (1857.349593927961 steps/sec)\n",
      "Step #8843\tEpoch   2 Batch 2592/3125   Loss: 0.787133 mae: 0.707301 (1942.3469482263592 steps/sec)\n",
      "Step #8844\tEpoch   2 Batch 2593/3125   Loss: 0.805640 mae: 0.704392 (1820.6340938292183 steps/sec)\n",
      "Step #8845\tEpoch   2 Batch 2594/3125   Loss: 0.742989 mae: 0.706681 (2253.0398255283035 steps/sec)\n",
      "Step #8846\tEpoch   2 Batch 2595/3125   Loss: 0.779135 mae: 0.695837 (1686.0032479539498 steps/sec)\n",
      "Step #8847\tEpoch   2 Batch 2596/3125   Loss: 0.842802 mae: 0.721485 (2053.2132367338945 steps/sec)\n",
      "Step #8848\tEpoch   2 Batch 2597/3125   Loss: 0.835913 mae: 0.703242 (2131.8153170553196 steps/sec)\n",
      "Step #8849\tEpoch   2 Batch 2598/3125   Loss: 0.840782 mae: 0.734915 (2073.9035413020047 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8850\tEpoch   2 Batch 2599/3125   Loss: 0.970147 mae: 0.742416 (1926.1308424948797 steps/sec)\n",
      "Step #8851\tEpoch   2 Batch 2600/3125   Loss: 0.803766 mae: 0.694808 (1787.5638217168575 steps/sec)\n",
      "Step #8852\tEpoch   2 Batch 2601/3125   Loss: 0.850185 mae: 0.720926 (1830.2310968372547 steps/sec)\n",
      "Step #8853\tEpoch   2 Batch 2602/3125   Loss: 0.799036 mae: 0.715533 (1698.9517004488082 steps/sec)\n",
      "Step #8854\tEpoch   2 Batch 2603/3125   Loss: 0.965972 mae: 0.774330 (1875.4042066103877 steps/sec)\n",
      "Step #8855\tEpoch   2 Batch 2604/3125   Loss: 0.924523 mae: 0.738007 (1787.5638217168575 steps/sec)\n",
      "Step #8856\tEpoch   2 Batch 2605/3125   Loss: 0.743784 mae: 0.692757 (1937.1618064087052 steps/sec)\n",
      "Step #8857\tEpoch   2 Batch 2606/3125   Loss: 0.806827 mae: 0.715461 (2045.922110356669 steps/sec)\n",
      "Step #8858\tEpoch   2 Batch 2607/3125   Loss: 0.772696 mae: 0.722155 (1670.6780214615183 steps/sec)\n",
      "Step #8859\tEpoch   2 Batch 2608/3125   Loss: 0.768775 mae: 0.679569 (1631.7582340629158 steps/sec)\n",
      "Step #8860\tEpoch   2 Batch 2609/3125   Loss: 0.872660 mae: 0.724331 (1810.668094144463 steps/sec)\n",
      "Step #8861\tEpoch   2 Batch 2610/3125   Loss: 0.874255 mae: 0.745469 (2047.0004880429478 steps/sec)\n",
      "Step #8862\tEpoch   2 Batch 2611/3125   Loss: 0.783166 mae: 0.698196 (2047.9800001953106 steps/sec)\n",
      "Step #8863\tEpoch   2 Batch 2612/3125   Loss: 0.687240 mae: 0.662521 (2039.8128604915817 steps/sec)\n",
      "Step #8864\tEpoch   2 Batch 2613/3125   Loss: 0.969922 mae: 0.776253 (1954.5663824036535 steps/sec)\n",
      "Step #8865\tEpoch   2 Batch 2614/3125   Loss: 0.899454 mae: 0.761979 (2082.4705823941213 steps/sec)\n",
      "Step #8866\tEpoch   2 Batch 2615/3125   Loss: 0.966036 mae: 0.780034 (1737.4913007456503 steps/sec)\n",
      "Step #8867\tEpoch   2 Batch 2616/3125   Loss: 0.779215 mae: 0.694834 (1554.2979114477566 steps/sec)\n",
      "Step #8868\tEpoch   2 Batch 2617/3125   Loss: 0.748935 mae: 0.691543 (1768.4799932537842 steps/sec)\n",
      "Step #8869\tEpoch   2 Batch 2618/3125   Loss: 0.778795 mae: 0.717678 (2030.215785550403 steps/sec)\n",
      "Step #8870\tEpoch   2 Batch 2619/3125   Loss: 0.942562 mae: 0.770338 (1979.9582699987727 steps/sec)\n",
      "Step #8871\tEpoch   2 Batch 2620/3125   Loss: 0.738830 mae: 0.687524 (1774.3753754515994 steps/sec)\n",
      "Step #8872\tEpoch   2 Batch 2621/3125   Loss: 0.769638 mae: 0.690297 (1826.8670238250795 steps/sec)\n",
      "Step #8873\tEpoch   2 Batch 2622/3125   Loss: 0.742262 mae: 0.669022 (1867.3552615176393 steps/sec)\n",
      "Step #8874\tEpoch   2 Batch 2623/3125   Loss: 0.867547 mae: 0.737742 (1693.5324186606156 steps/sec)\n",
      "Step #8875\tEpoch   2 Batch 2624/3125   Loss: 0.952118 mae: 0.764685 (1859.721372387312 steps/sec)\n",
      "Step #8876\tEpoch   2 Batch 2625/3125   Loss: 0.910569 mae: 0.769505 (2188.659869128252 steps/sec)\n",
      "Step #8877\tEpoch   2 Batch 2626/3125   Loss: 0.773809 mae: 0.693161 (2106.9704824482087 steps/sec)\n",
      "Step #8878\tEpoch   2 Batch 2627/3125   Loss: 0.747722 mae: 0.699971 (1917.0981424601434 steps/sec)\n",
      "Step #8879\tEpoch   2 Batch 2628/3125   Loss: 0.981918 mae: 0.798025 (2252.047851205945 steps/sec)\n",
      "Step #8880\tEpoch   2 Batch 2629/3125   Loss: 0.813087 mae: 0.718073 (2101.818036040009 steps/sec)\n",
      "Step #8881\tEpoch   2 Batch 2630/3125   Loss: 0.600881 mae: 0.612989 (1947.4875795143241 steps/sec)\n",
      "Step #8882\tEpoch   2 Batch 2631/3125   Loss: 0.630740 mae: 0.633725 (2138.4017701461185 steps/sec)\n",
      "Step #8883\tEpoch   2 Batch 2632/3125   Loss: 0.906425 mae: 0.743940 (1638.8097024256065 steps/sec)\n",
      "Step #8884\tEpoch   2 Batch 2633/3125   Loss: 0.681432 mae: 0.648356 (1795.9220025177053 steps/sec)\n",
      "Step #8885\tEpoch   2 Batch 2634/3125   Loss: 0.859180 mae: 0.735836 (2117.757783230836 steps/sec)\n",
      "Step #8886\tEpoch   2 Batch 2635/3125   Loss: 0.795502 mae: 0.709658 (2123.4831915755367 steps/sec)\n",
      "Step #8887\tEpoch   2 Batch 2636/3125   Loss: 0.967508 mae: 0.764448 (1917.0280449010934 steps/sec)\n",
      "Step #8888\tEpoch   2 Batch 2637/3125   Loss: 0.721167 mae: 0.669768 (1938.2897704166512 steps/sec)\n",
      "Step #8889\tEpoch   2 Batch 2638/3125   Loss: 0.862694 mae: 0.713091 (1743.8628293932263 steps/sec)\n",
      "Step #8890\tEpoch   2 Batch 2639/3125   Loss: 0.839610 mae: 0.738987 (1697.85132531291 steps/sec)\n",
      "Step #8891\tEpoch   2 Batch 2640/3125   Loss: 0.764898 mae: 0.700823 (1868.4699614216092 steps/sec)\n",
      "Step #8892\tEpoch   2 Batch 2641/3125   Loss: 0.725549 mae: 0.691372 (1663.7725310992637 steps/sec)\n",
      "Step #8893\tEpoch   2 Batch 2642/3125   Loss: 0.803931 mae: 0.730057 (2100.470743775165 steps/sec)\n",
      "Step #8894\tEpoch   2 Batch 2643/3125   Loss: 0.751614 mae: 0.678008 (2009.555476767696 steps/sec)\n",
      "Step #8895\tEpoch   2 Batch 2644/3125   Loss: 0.953994 mae: 0.762278 (2107.6692696555815 steps/sec)\n",
      "Step #8896\tEpoch   2 Batch 2645/3125   Loss: 0.804785 mae: 0.727173 (1869.7860199714694 steps/sec)\n",
      "Step #8897\tEpoch   2 Batch 2646/3125   Loss: 0.734288 mae: 0.664933 (1805.3683648697508 steps/sec)\n",
      "Step #8898\tEpoch   2 Batch 2647/3125   Loss: 0.758485 mae: 0.696388 (1789.302504159379 steps/sec)\n",
      "Step #8899\tEpoch   2 Batch 2648/3125   Loss: 0.816985 mae: 0.713444 (1913.9488190413608 steps/sec)\n",
      "Step #8900\tEpoch   2 Batch 2649/3125   Loss: 0.878671 mae: 0.746998 (1646.5814515875757 steps/sec)\n",
      "Step #8901\tEpoch   2 Batch 2650/3125   Loss: 0.742392 mae: 0.701272 (1811.4657383973533 steps/sec)\n",
      "Step #8902\tEpoch   2 Batch 2651/3125   Loss: 0.919425 mae: 0.744668 (1517.5859324118967 steps/sec)\n",
      "Step #8903\tEpoch   2 Batch 2652/3125   Loss: 0.831456 mae: 0.732105 (1599.5850685704697 steps/sec)\n",
      "Step #8904\tEpoch   2 Batch 2653/3125   Loss: 0.832850 mae: 0.734472 (1527.145093755689 steps/sec)\n",
      "Step #8905\tEpoch   2 Batch 2654/3125   Loss: 0.880255 mae: 0.759561 (1429.9706116994075 steps/sec)\n",
      "Step #8906\tEpoch   2 Batch 2655/3125   Loss: 0.885603 mae: 0.722715 (1327.1014080050625 steps/sec)\n",
      "Step #8907\tEpoch   2 Batch 2656/3125   Loss: 0.958485 mae: 0.787889 (1697.4527912454369 steps/sec)\n",
      "Step #8908\tEpoch   2 Batch 2657/3125   Loss: 0.863066 mae: 0.746894 (2008.5161809353242 steps/sec)\n",
      "Step #8909\tEpoch   2 Batch 2658/3125   Loss: 0.882731 mae: 0.728406 (1802.4512247529008 steps/sec)\n",
      "Step #8910\tEpoch   2 Batch 2659/3125   Loss: 0.713621 mae: 0.674170 (2039.1583368986037 steps/sec)\n",
      "Step #8911\tEpoch   2 Batch 2660/3125   Loss: 0.888600 mae: 0.740106 (1959.4606968335092 steps/sec)\n",
      "Step #8912\tEpoch   2 Batch 2661/3125   Loss: 0.799099 mae: 0.700963 (1863.605019016813 steps/sec)\n",
      "Step #8913\tEpoch   2 Batch 2662/3125   Loss: 0.696216 mae: 0.682506 (1814.0825576969655 steps/sec)\n",
      "Step #8914\tEpoch   2 Batch 2663/3125   Loss: 0.755153 mae: 0.693198 (2023.3210161217185 steps/sec)\n",
      "Step #8915\tEpoch   2 Batch 2664/3125   Loss: 0.757036 mae: 0.706701 (1664.895247016981 steps/sec)\n",
      "Step #8916\tEpoch   2 Batch 2665/3125   Loss: 0.846007 mae: 0.740116 (2128.6561104344296 steps/sec)\n",
      "Step #8917\tEpoch   2 Batch 2666/3125   Loss: 0.896880 mae: 0.753019 (2101.6495299941876 steps/sec)\n",
      "Step #8918\tEpoch   2 Batch 2667/3125   Loss: 0.678100 mae: 0.668231 (2032.084649522296 steps/sec)\n",
      "Step #8919\tEpoch   2 Batch 2668/3125   Loss: 0.848058 mae: 0.749598 (2092.5692732914918 steps/sec)\n",
      "Step #8920\tEpoch   2 Batch 2669/3125   Loss: 0.790267 mae: 0.704104 (2012.2356553444636 steps/sec)\n",
      "Step #8921\tEpoch   2 Batch 2670/3125   Loss: 0.826294 mae: 0.719465 (1668.2194221713123 steps/sec)\n",
      "Step #8922\tEpoch   2 Batch 2671/3125   Loss: 0.749373 mae: 0.655070 (1875.8235762395013 steps/sec)\n",
      "Step #8923\tEpoch   2 Batch 2672/3125   Loss: 0.864722 mae: 0.742283 (1980.3509036997866 steps/sec)\n",
      "Step #8924\tEpoch   2 Batch 2673/3125   Loss: 0.820170 mae: 0.722700 (1461.7763093694673 steps/sec)\n",
      "Step #8925\tEpoch   2 Batch 2674/3125   Loss: 0.817151 mae: 0.716376 (1812.4520344315 steps/sec)\n",
      "Step #8926\tEpoch   2 Batch 2675/3125   Loss: 0.806026 mae: 0.710723 (2003.6803133807864 steps/sec)\n",
      "Step #8927\tEpoch   2 Batch 2676/3125   Loss: 0.773179 mae: 0.687249 (1862.7608075819617 steps/sec)\n",
      "Step #8928\tEpoch   2 Batch 2677/3125   Loss: 0.800401 mae: 0.731853 (1913.721768490213 steps/sec)\n",
      "Step #8929\tEpoch   2 Batch 2678/3125   Loss: 0.841484 mae: 0.715575 (2062.197748168543 steps/sec)\n",
      "Step #8930\tEpoch   2 Batch 2679/3125   Loss: 0.735398 mae: 0.648651 (1536.0487515472903 steps/sec)\n",
      "Step #8931\tEpoch   2 Batch 2680/3125   Loss: 0.744696 mae: 0.686664 (1592.3947212562073 steps/sec)\n",
      "Step #8932\tEpoch   2 Batch 2681/3125   Loss: 0.732557 mae: 0.667706 (2074.498476635144 steps/sec)\n",
      "Step #8933\tEpoch   2 Batch 2682/3125   Loss: 0.917204 mae: 0.765630 (2021.5072005552236 steps/sec)\n",
      "Step #8934\tEpoch   2 Batch 2683/3125   Loss: 0.859241 mae: 0.704884 (2226.8670029200957 steps/sec)\n",
      "Step #8935\tEpoch   2 Batch 2684/3125   Loss: 0.881991 mae: 0.733856 (2095.4756195043965 steps/sec)\n",
      "Step #8936\tEpoch   2 Batch 2685/3125   Loss: 0.805774 mae: 0.703318 (2111.4890103804837 steps/sec)\n",
      "Step #8937\tEpoch   2 Batch 2686/3125   Loss: 0.753804 mae: 0.699760 (2130.2994595912396 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #8938\tEpoch   2 Batch 2687/3125   Loss: 0.789446 mae: 0.705645 (2157.4084170893043 steps/sec)\n",
      "Step #8939\tEpoch   2 Batch 2688/3125   Loss: 0.677989 mae: 0.653458 (1857.695101426167 steps/sec)\n",
      "Step #8940\tEpoch   2 Batch 2689/3125   Loss: 0.823241 mae: 0.727053 (1875.3371248703365 steps/sec)\n",
      "Step #8941\tEpoch   2 Batch 2690/3125   Loss: 0.887001 mae: 0.747119 (2143.340998518064 steps/sec)\n",
      "Step #8942\tEpoch   2 Batch 2691/3125   Loss: 0.802889 mae: 0.714871 (1867.0560165236282 steps/sec)\n",
      "Step #8943\tEpoch   2 Batch 2692/3125   Loss: 0.939066 mae: 0.770120 (1850.7276177028637 steps/sec)\n",
      "Step #8944\tEpoch   2 Batch 2693/3125   Loss: 0.750429 mae: 0.691083 (2044.9046852908195 steps/sec)\n",
      "Step #8945\tEpoch   2 Batch 2694/3125   Loss: 0.811155 mae: 0.722935 (2064.491740662716 steps/sec)\n",
      "Step #8946\tEpoch   2 Batch 2695/3125   Loss: 0.835895 mae: 0.726682 (1829.3530125001091 steps/sec)\n",
      "Step #8947\tEpoch   2 Batch 2696/3125   Loss: 0.676964 mae: 0.666041 (1829.0658224094473 steps/sec)\n",
      "Step #8948\tEpoch   2 Batch 2697/3125   Loss: 0.693300 mae: 0.660424 (1739.9130521355326 steps/sec)\n",
      "Step #8949\tEpoch   2 Batch 2698/3125   Loss: 0.809511 mae: 0.702953 (1993.8506003936072 steps/sec)\n",
      "Step #8950\tEpoch   2 Batch 2699/3125   Loss: 0.779167 mae: 0.702312 (1995.8430088697705 steps/sec)\n",
      "Step #8951\tEpoch   2 Batch 2700/3125   Loss: 0.825761 mae: 0.714969 (1931.096971426993 steps/sec)\n",
      "Step #8952\tEpoch   2 Batch 2701/3125   Loss: 0.854307 mae: 0.725153 (1994.2487637885126 steps/sec)\n",
      "Step #8953\tEpoch   2 Batch 2702/3125   Loss: 0.833844 mae: 0.740957 (1800.6405247838443 steps/sec)\n",
      "Step #8954\tEpoch   2 Batch 2703/3125   Loss: 0.815574 mae: 0.731191 (1551.7217906030337 steps/sec)\n",
      "Step #8955\tEpoch   2 Batch 2704/3125   Loss: 0.790672 mae: 0.690211 (1954.5663824036535 steps/sec)\n",
      "Step #8956\tEpoch   2 Batch 2705/3125   Loss: 0.767780 mae: 0.687140 (2030.5499612703331 steps/sec)\n",
      "Step #8957\tEpoch   2 Batch 2706/3125   Loss: 0.717573 mae: 0.688833 (1886.657610406902 steps/sec)\n",
      "Step #8958\tEpoch   2 Batch 2707/3125   Loss: 0.854350 mae: 0.725500 (1834.9873563922406 steps/sec)\n",
      "Step #8959\tEpoch   2 Batch 2708/3125   Loss: 0.724618 mae: 0.693641 (1681.9196715000642 steps/sec)\n",
      "Step #8960\tEpoch   2 Batch 2709/3125   Loss: 0.724779 mae: 0.676330 (1754.2049351735675 steps/sec)\n",
      "Step #8961\tEpoch   2 Batch 2710/3125   Loss: 0.744766 mae: 0.699607 (1601.1849589616338 steps/sec)\n",
      "Step #8962\tEpoch   2 Batch 2711/3125   Loss: 0.823869 mae: 0.731144 (1569.3486590038315 steps/sec)\n",
      "Step #8963\tEpoch   2 Batch 2712/3125   Loss: 0.860483 mae: 0.722947 (1745.7499854323269 steps/sec)\n",
      "Step #8964\tEpoch   2 Batch 2713/3125   Loss: 0.845533 mae: 0.752809 (2075.7099165619156 steps/sec)\n",
      "Step #8965\tEpoch   2 Batch 2714/3125   Loss: 0.850074 mae: 0.741740 (1925.4411575680788 steps/sec)\n",
      "Step #8966\tEpoch   2 Batch 2715/3125   Loss: 0.799352 mae: 0.692998 (2230.0638026371757 steps/sec)\n",
      "Step #8967\tEpoch   2 Batch 2716/3125   Loss: 0.759110 mae: 0.694647 (2211.019504480759 steps/sec)\n",
      "Step #8968\tEpoch   2 Batch 2717/3125   Loss: 0.820151 mae: 0.731790 (2029.9603136192043 steps/sec)\n",
      "Step #8969\tEpoch   2 Batch 2718/3125   Loss: 0.749282 mae: 0.696561 (2242.9673044631495 steps/sec)\n",
      "Step #8970\tEpoch   2 Batch 2719/3125   Loss: 0.785139 mae: 0.705574 (2097.298810916764 steps/sec)\n",
      "Step #8971\tEpoch   2 Batch 2720/3125   Loss: 0.711155 mae: 0.665777 (1670.9043104135128 steps/sec)\n",
      "Step #8972\tEpoch   2 Batch 2721/3125   Loss: 0.814484 mae: 0.726415 (2050.7035642693004 steps/sec)\n",
      "Step #8973\tEpoch   2 Batch 2722/3125   Loss: 0.806743 mae: 0.717961 (2115.1519430352296 steps/sec)\n",
      "Step #8974\tEpoch   2 Batch 2723/3125   Loss: 0.762139 mae: 0.693578 (2121.2708495594916 steps/sec)\n",
      "Step #8975\tEpoch   2 Batch 2724/3125   Loss: 0.786759 mae: 0.705818 (2026.1946629050647 steps/sec)\n",
      "Step #8976\tEpoch   2 Batch 2725/3125   Loss: 0.873243 mae: 0.717933 (2093.1122932739813 steps/sec)\n",
      "Step #8977\tEpoch   2 Batch 2726/3125   Loss: 0.775528 mae: 0.697603 (2010.6729561557415 steps/sec)\n",
      "Step #8978\tEpoch   2 Batch 2727/3125   Loss: 0.795063 mae: 0.699766 (2059.564939847778 steps/sec)\n",
      "Step #8979\tEpoch   2 Batch 2728/3125   Loss: 0.906912 mae: 0.750231 (1962.1556886227545 steps/sec)\n",
      "Step #8980\tEpoch   2 Batch 2729/3125   Loss: 0.911770 mae: 0.756858 (1919.8359515178147 steps/sec)\n",
      "Step #8981\tEpoch   2 Batch 2730/3125   Loss: 0.942029 mae: 0.752619 (1961.4766594648186 steps/sec)\n",
      "Step #8982\tEpoch   2 Batch 2731/3125   Loss: 0.717888 mae: 0.662788 (2101.986569108951 steps/sec)\n",
      "Step #8983\tEpoch   2 Batch 2732/3125   Loss: 0.783949 mae: 0.716947 (1992.6002641405455 steps/sec)\n",
      "Step #8984\tEpoch   2 Batch 2733/3125   Loss: 0.823435 mae: 0.727079 (2282.266647803328 steps/sec)\n",
      "Step #8985\tEpoch   2 Batch 2734/3125   Loss: 0.731716 mae: 0.696271 (2038.1476262209048 steps/sec)\n",
      "Step #8986\tEpoch   2 Batch 2735/3125   Loss: 0.696682 mae: 0.648676 (1859.968781041578 steps/sec)\n",
      "Step #8987\tEpoch   2 Batch 2736/3125   Loss: 0.844263 mae: 0.736439 (2122.88132161801 steps/sec)\n",
      "Step #8988\tEpoch   2 Batch 2737/3125   Loss: 0.792365 mae: 0.716277 (1749.4781977592952 steps/sec)\n",
      "Step #8989\tEpoch   2 Batch 2738/3125   Loss: 0.818344 mae: 0.745293 (1722.8322393552787 steps/sec)\n",
      "Step #8990\tEpoch   2 Batch 2739/3125   Loss: 0.862421 mae: 0.743326 (2068.768496231701 steps/sec)\n",
      "Step #8991\tEpoch   2 Batch 2740/3125   Loss: 0.834180 mae: 0.716340 (2131.273691805811 steps/sec)\n",
      "Step #8992\tEpoch   2 Batch 2741/3125   Loss: 0.779096 mae: 0.703042 (2218.691944732443 steps/sec)\n",
      "Step #8993\tEpoch   2 Batch 2742/3125   Loss: 0.792139 mae: 0.693479 (2145.467937962925 steps/sec)\n",
      "Step #8994\tEpoch   2 Batch 2743/3125   Loss: 0.711535 mae: 0.657990 (1995.6910661946633 steps/sec)\n",
      "Step #8995\tEpoch   2 Batch 2744/3125   Loss: 0.854298 mae: 0.726030 (2151.2781584670306 steps/sec)\n",
      "Step #8996\tEpoch   2 Batch 2745/3125   Loss: 0.844760 mae: 0.715126 (2019.9689850800899 steps/sec)\n",
      "Step #8997\tEpoch   2 Batch 2746/3125   Loss: 0.675967 mae: 0.661218 (1967.0327815035407 steps/sec)\n",
      "Step #8998\tEpoch   2 Batch 2747/3125   Loss: 0.897626 mae: 0.741188 (2171.3244429719207 steps/sec)\n",
      "Step #8999\tEpoch   2 Batch 2748/3125   Loss: 0.813882 mae: 0.713754 (1892.0534103211837 steps/sec)\n",
      "Step #9000\tEpoch   2 Batch 2749/3125   Loss: 0.968378 mae: 0.768004 (2174.2960228922157 steps/sec)\n",
      "Step #9001\tEpoch   2 Batch 2750/3125   Loss: 0.927734 mae: 0.762282 (2189.916878994194 steps/sec)\n",
      "Step #9002\tEpoch   2 Batch 2751/3125   Loss: 0.951842 mae: 0.766508 (2242.8473648186173 steps/sec)\n",
      "Step #9003\tEpoch   2 Batch 2752/3125   Loss: 0.816672 mae: 0.727849 (2046.6608761845278 steps/sec)\n",
      "Step #9004\tEpoch   2 Batch 2753/3125   Loss: 0.804107 mae: 0.711781 (1640.4762277257153 steps/sec)\n",
      "Step #9005\tEpoch   2 Batch 2754/3125   Loss: 0.757781 mae: 0.700034 (2044.645503470868 steps/sec)\n",
      "Step #9006\tEpoch   2 Batch 2755/3125   Loss: 0.877404 mae: 0.727537 (2240.858238857961 steps/sec)\n",
      "Step #9007\tEpoch   2 Batch 2756/3125   Loss: 0.952192 mae: 0.762161 (2140.518913181048 steps/sec)\n",
      "Step #9008\tEpoch   2 Batch 2757/3125   Loss: 0.778498 mae: 0.729162 (2355.1862000808587 steps/sec)\n",
      "Step #9009\tEpoch   2 Batch 2758/3125   Loss: 0.809512 mae: 0.726778 (2101.3547094188375 steps/sec)\n",
      "Step #9010\tEpoch   2 Batch 2759/3125   Loss: 0.804942 mae: 0.700436 (2011.830277913681 steps/sec)\n",
      "Step #9011\tEpoch   2 Batch 2760/3125   Loss: 0.725755 mae: 0.671242 (2055.1649794694395 steps/sec)\n",
      "Step #9012\tEpoch   2 Batch 2761/3125   Loss: 0.869819 mae: 0.741277 (2180.1503227885605 steps/sec)\n",
      "Step #9013\tEpoch   2 Batch 2762/3125   Loss: 0.698433 mae: 0.645354 (2005.1171240080314 steps/sec)\n",
      "Step #9014\tEpoch   2 Batch 2763/3125   Loss: 0.709642 mae: 0.661389 (1999.1153816823 steps/sec)\n",
      "Step #9015\tEpoch   2 Batch 2764/3125   Loss: 0.752470 mae: 0.694805 (2128.2024740970764 steps/sec)\n",
      "Step #9016\tEpoch   2 Batch 2765/3125   Loss: 0.772899 mae: 0.687980 (2134.093152469243 steps/sec)\n",
      "Step #9017\tEpoch   2 Batch 2766/3125   Loss: 0.706332 mae: 0.665947 (2124.7525354353047 steps/sec)\n",
      "Step #9018\tEpoch   2 Batch 2767/3125   Loss: 0.743311 mae: 0.708960 (2040.2295943185134 steps/sec)\n",
      "Step #9019\tEpoch   2 Batch 2768/3125   Loss: 0.864863 mae: 0.732484 (2062.278864402946 steps/sec)\n",
      "Step #9020\tEpoch   2 Batch 2769/3125   Loss: 0.776895 mae: 0.697829 (2072.3664966994743 steps/sec)\n",
      "Step #9021\tEpoch   2 Batch 2770/3125   Loss: 0.800569 mae: 0.700630 (2027.3307296701596 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9022\tEpoch   2 Batch 2771/3125   Loss: 0.886989 mae: 0.737209 (1642.7378546474283 steps/sec)\n",
      "Step #9023\tEpoch   2 Batch 2772/3125   Loss: 0.912474 mae: 0.763666 (2065.0812876035175 steps/sec)\n",
      "Step #9024\tEpoch   2 Batch 2773/3125   Loss: 0.951911 mae: 0.764495 (2093.530192766514 steps/sec)\n",
      "Step #9025\tEpoch   2 Batch 2774/3125   Loss: 0.769315 mae: 0.696297 (2107.3304058603053 steps/sec)\n",
      "Step #9026\tEpoch   2 Batch 2775/3125   Loss: 0.853917 mae: 0.728876 (2132.0753949696023 steps/sec)\n",
      "Step #9027\tEpoch   2 Batch 2776/3125   Loss: 0.758433 mae: 0.692212 (2078.0546775136495 steps/sec)\n",
      "Step #9028\tEpoch   2 Batch 2777/3125   Loss: 0.927046 mae: 0.749908 (2216.019273849275 steps/sec)\n",
      "Step #9029\tEpoch   2 Batch 2778/3125   Loss: 0.737355 mae: 0.665509 (2146.5000358235843 steps/sec)\n",
      "Step #9030\tEpoch   2 Batch 2779/3125   Loss: 0.797540 mae: 0.684546 (2119.791371851373 steps/sec)\n",
      "Step #9031\tEpoch   2 Batch 2780/3125   Loss: 0.810886 mae: 0.695944 (1969.9891973134188 steps/sec)\n",
      "Step #9032\tEpoch   2 Batch 2781/3125   Loss: 0.925453 mae: 0.758016 (1843.5366614801726 steps/sec)\n",
      "Step #9033\tEpoch   2 Batch 2782/3125   Loss: 0.942163 mae: 0.765037 (2077.107908681226 steps/sec)\n",
      "Step #9034\tEpoch   2 Batch 2783/3125   Loss: 0.798376 mae: 0.707852 (2028.644669510626 steps/sec)\n",
      "Step #9035\tEpoch   2 Batch 2784/3125   Loss: 0.794619 mae: 0.714758 (2054.23894836858 steps/sec)\n",
      "Step #9036\tEpoch   2 Batch 2785/3125   Loss: 0.875615 mae: 0.761665 (1969.0273878712198 steps/sec)\n",
      "Step #9037\tEpoch   2 Batch 2786/3125   Loss: 0.925329 mae: 0.756131 (2269.104758607258 steps/sec)\n",
      "Step #9038\tEpoch   2 Batch 2787/3125   Loss: 0.752734 mae: 0.684707 (2088.547185595347 steps/sec)\n",
      "Step #9039\tEpoch   2 Batch 2788/3125   Loss: 0.912845 mae: 0.743972 (2084.395499542798 steps/sec)\n",
      "Step #9040\tEpoch   2 Batch 2789/3125   Loss: 0.797016 mae: 0.705646 (1813.2512515455182 steps/sec)\n",
      "Step #9041\tEpoch   2 Batch 2790/3125   Loss: 0.792792 mae: 0.721339 (1861.8512402563965 steps/sec)\n",
      "Step #9042\tEpoch   2 Batch 2791/3125   Loss: 0.796530 mae: 0.708441 (2123.956328870344 steps/sec)\n",
      "Step #9043\tEpoch   2 Batch 2792/3125   Loss: 0.688227 mae: 0.665958 (2204.2800084086607 steps/sec)\n",
      "Step #9044\tEpoch   2 Batch 2793/3125   Loss: 0.954250 mae: 0.789581 (2096.7955447573913 steps/sec)\n",
      "Step #9045\tEpoch   2 Batch 2794/3125   Loss: 0.765505 mae: 0.707407 (1899.7146558205684 steps/sec)\n",
      "Step #9046\tEpoch   2 Batch 2795/3125   Loss: 0.874141 mae: 0.740873 (2046.0019512195122 steps/sec)\n",
      "Step #9047\tEpoch   2 Batch 2796/3125   Loss: 0.822188 mae: 0.709259 (1831.7017782901862 steps/sec)\n",
      "Step #9048\tEpoch   2 Batch 2797/3125   Loss: 0.954069 mae: 0.770787 (1893.3003511876282 steps/sec)\n",
      "Step #9049\tEpoch   2 Batch 2798/3125   Loss: 0.760737 mae: 0.695018 (1821.6936961979136 steps/sec)\n",
      "Step #9050\tEpoch   2 Batch 2799/3125   Loss: 0.841416 mae: 0.727161 (2095.8316260755723 steps/sec)\n",
      "Step #9051\tEpoch   2 Batch 2800/3125   Loss: 1.000615 mae: 0.770845 (2319.422232544765 steps/sec)\n",
      "Step #9052\tEpoch   2 Batch 2801/3125   Loss: 0.739742 mae: 0.695662 (2056.091845838603 steps/sec)\n",
      "Step #9053\tEpoch   2 Batch 2802/3125   Loss: 0.807873 mae: 0.717723 (2077.1902021572687 steps/sec)\n",
      "Step #9054\tEpoch   2 Batch 2803/3125   Loss: 0.870401 mae: 0.745829 (2190.3743315508023 steps/sec)\n",
      "Step #9055\tEpoch   2 Batch 2804/3125   Loss: 0.714044 mae: 0.655178 (2137.5735151719005 steps/sec)\n",
      "Step #9056\tEpoch   2 Batch 2805/3125   Loss: 0.796496 mae: 0.697539 (2255.0748948890823 steps/sec)\n",
      "Step #9057\tEpoch   2 Batch 2806/3125   Loss: 0.807950 mae: 0.707412 (1934.5528342788616 steps/sec)\n",
      "Step #9058\tEpoch   2 Batch 2807/3125   Loss: 0.904418 mae: 0.762774 (1957.0287420679358 steps/sec)\n",
      "Step #9059\tEpoch   2 Batch 2808/3125   Loss: 0.909065 mae: 0.750907 (2016.0464512655856 steps/sec)\n",
      "Step #9060\tEpoch   2 Batch 2809/3125   Loss: 0.852456 mae: 0.745588 (2195.5798444255997 steps/sec)\n",
      "Step #9061\tEpoch   2 Batch 2810/3125   Loss: 0.771573 mae: 0.699409 (2053.6555749231284 steps/sec)\n",
      "Step #9062\tEpoch   2 Batch 2811/3125   Loss: 0.755008 mae: 0.681804 (2317.909721915204 steps/sec)\n",
      "Step #9063\tEpoch   2 Batch 2812/3125   Loss: 0.870806 mae: 0.761332 (2143.735369580995 steps/sec)\n",
      "Step #9064\tEpoch   2 Batch 2813/3125   Loss: 0.923915 mae: 0.774649 (1947.415242039577 steps/sec)\n",
      "Step #9065\tEpoch   2 Batch 2814/3125   Loss: 0.810170 mae: 0.713637 (2192.8020242999646 steps/sec)\n",
      "Step #9066\tEpoch   2 Batch 2815/3125   Loss: 0.763985 mae: 0.684954 (1840.866557820263 steps/sec)\n",
      "Step #9067\tEpoch   2 Batch 2816/3125   Loss: 0.849520 mae: 0.709632 (1895.5250051971764 steps/sec)\n",
      "Step #9068\tEpoch   2 Batch 2817/3125   Loss: 0.745521 mae: 0.672597 (2132.032044243829 steps/sec)\n",
      "Step #9069\tEpoch   2 Batch 2818/3125   Loss: 0.923943 mae: 0.735692 (2099.50344385712 steps/sec)\n",
      "Step #9070\tEpoch   2 Batch 2819/3125   Loss: 0.904593 mae: 0.764012 (2078.6932043454126 steps/sec)\n",
      "Step #9071\tEpoch   2 Batch 2820/3125   Loss: 1.007796 mae: 0.788576 (2042.5545177407887 steps/sec)\n",
      "Step #9072\tEpoch   2 Batch 2821/3125   Loss: 0.801079 mae: 0.715695 (2222.6188331301996 steps/sec)\n",
      "Step #9073\tEpoch   2 Batch 2822/3125   Loss: 0.810153 mae: 0.721308 (2014.9036336734498 steps/sec)\n",
      "Step #9074\tEpoch   2 Batch 2823/3125   Loss: 0.755453 mae: 0.685480 (1486.1402837422227 steps/sec)\n",
      "Step #9075\tEpoch   2 Batch 2824/3125   Loss: 0.851455 mae: 0.760043 (1933.7144543208055 steps/sec)\n",
      "Step #9076\tEpoch   2 Batch 2825/3125   Loss: 0.797872 mae: 0.680768 (2030.3533739955465 steps/sec)\n",
      "Step #9077\tEpoch   2 Batch 2826/3125   Loss: 0.850952 mae: 0.732962 (2344.2604992231077 steps/sec)\n",
      "Step #9078\tEpoch   2 Batch 2827/3125   Loss: 0.802379 mae: 0.739408 (1971.470740305523 steps/sec)\n",
      "Step #9079\tEpoch   2 Batch 2828/3125   Loss: 0.968366 mae: 0.762288 (2214.5450321545104 steps/sec)\n",
      "Step #9080\tEpoch   2 Batch 2829/3125   Loss: 0.705755 mae: 0.658903 (2217.8707023276966 steps/sec)\n",
      "Step #9081\tEpoch   2 Batch 2830/3125   Loss: 0.709596 mae: 0.650636 (2230.3484068575317 steps/sec)\n",
      "Step #9082\tEpoch   2 Batch 2831/3125   Loss: 0.872232 mae: 0.739191 (1956.8826515377725 steps/sec)\n",
      "Step #9083\tEpoch   2 Batch 2832/3125   Loss: 0.730514 mae: 0.679650 (1565.3892662536389 steps/sec)\n",
      "Step #9084\tEpoch   2 Batch 2833/3125   Loss: 0.705813 mae: 0.660501 (2156.299289511295 steps/sec)\n",
      "Step #9085\tEpoch   2 Batch 2834/3125   Loss: 0.748037 mae: 0.672171 (2041.7391981618864 steps/sec)\n",
      "Step #9086\tEpoch   2 Batch 2835/3125   Loss: 0.888856 mae: 0.724488 (2007.785468784406 steps/sec)\n",
      "Step #9087\tEpoch   2 Batch 2836/3125   Loss: 0.805462 mae: 0.693737 (2194.1326637371835 steps/sec)\n",
      "Step #9088\tEpoch   2 Batch 2837/3125   Loss: 0.733576 mae: 0.682324 (2288.0933937046534 steps/sec)\n",
      "Step #9089\tEpoch   2 Batch 2838/3125   Loss: 0.620345 mae: 0.603445 (2091.6926820997196 steps/sec)\n",
      "Step #9090\tEpoch   2 Batch 2839/3125   Loss: 0.816988 mae: 0.724045 (2070.1367158580524 steps/sec)\n",
      "Step #9091\tEpoch   2 Batch 2840/3125   Loss: 0.816130 mae: 0.710236 (2001.9206353752018 steps/sec)\n",
      "Step #9092\tEpoch   2 Batch 2841/3125   Loss: 0.862813 mae: 0.719880 (1857.2837975468274 steps/sec)\n",
      "Step #9093\tEpoch   2 Batch 2842/3125   Loss: 0.799039 mae: 0.717606 (2031.2776653138712 steps/sec)\n",
      "Step #9094\tEpoch   2 Batch 2843/3125   Loss: 0.859181 mae: 0.736456 (2162.4359410606203 steps/sec)\n",
      "Step #9095\tEpoch   2 Batch 2844/3125   Loss: 0.847757 mae: 0.722086 (2200.8101584636374 steps/sec)\n",
      "Step #9096\tEpoch   2 Batch 2845/3125   Loss: 0.815972 mae: 0.718369 (2084.6441351888666 steps/sec)\n",
      "Step #9097\tEpoch   2 Batch 2846/3125   Loss: 0.754089 mae: 0.685166 (1964.563602469344 steps/sec)\n",
      "Step #9098\tEpoch   2 Batch 2847/3125   Loss: 0.797616 mae: 0.703710 (2077.8899600701498 steps/sec)\n",
      "Step #9099\tEpoch   2 Batch 2848/3125   Loss: 0.736112 mae: 0.716705 (2054.3194396826175 steps/sec)\n",
      "Step #9100\tEpoch   2 Batch 2849/3125   Loss: 0.810082 mae: 0.725204 (1985.5257427429892 steps/sec)\n",
      "Step #9101\tEpoch   2 Batch 2850/3125   Loss: 0.830632 mae: 0.723570 (1837.3667195261917 steps/sec)\n",
      "Step #9102\tEpoch   2 Batch 2851/3125   Loss: 0.814872 mae: 0.696197 (1857.2837975468274 steps/sec)\n",
      "Step #9103\tEpoch   2 Batch 2852/3125   Loss: 0.853683 mae: 0.734370 (2152.4925843434708 steps/sec)\n",
      "Step #9104\tEpoch   2 Batch 2853/3125   Loss: 0.830657 mae: 0.735943 (2160.2084856975102 steps/sec)\n",
      "Step #9105\tEpoch   2 Batch 2854/3125   Loss: 0.884995 mae: 0.730927 (2063.536982554192 steps/sec)\n",
      "Step #9106\tEpoch   2 Batch 2855/3125   Loss: 0.898051 mae: 0.760866 (1996.26097054848 steps/sec)\n",
      "Step #9107\tEpoch   2 Batch 2856/3125   Loss: 0.808804 mae: 0.711639 (1948.1207617278217 steps/sec)\n",
      "Step #9108\tEpoch   2 Batch 2857/3125   Loss: 0.681875 mae: 0.646057 (2201.3646001721495 steps/sec)\n",
      "Step #9109\tEpoch   2 Batch 2858/3125   Loss: 1.040365 mae: 0.796817 (2025.607541629641 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9110\tEpoch   2 Batch 2859/3125   Loss: 0.776377 mae: 0.696951 (1715.9249531570895 steps/sec)\n",
      "Step #9111\tEpoch   2 Batch 2860/3125   Loss: 0.774957 mae: 0.703092 (1969.9891973134188 steps/sec)\n",
      "Step #9112\tEpoch   2 Batch 2861/3125   Loss: 0.735157 mae: 0.686939 (2232.532788281383 steps/sec)\n",
      "Step #9113\tEpoch   2 Batch 2862/3125   Loss: 0.783882 mae: 0.699461 (2023.8091561800354 steps/sec)\n",
      "Step #9114\tEpoch   2 Batch 2863/3125   Loss: 0.745617 mae: 0.675601 (2073.9035413020047 steps/sec)\n",
      "Step #9115\tEpoch   2 Batch 2864/3125   Loss: 0.854762 mae: 0.732135 (2010.692233940556 steps/sec)\n",
      "Step #9116\tEpoch   2 Batch 2865/3125   Loss: 0.756250 mae: 0.707937 (1984.4360333081 steps/sec)\n",
      "Step #9117\tEpoch   2 Batch 2866/3125   Loss: 0.869258 mae: 0.708386 (1874.2812201160057 steps/sec)\n",
      "Step #9118\tEpoch   2 Batch 2867/3125   Loss: 0.915633 mae: 0.785941 (1824.4830136152073 steps/sec)\n",
      "Step #9119\tEpoch   2 Batch 2868/3125   Loss: 0.758073 mae: 0.673906 (1850.8256184416066 steps/sec)\n",
      "Step #9120\tEpoch   2 Batch 2869/3125   Loss: 0.713508 mae: 0.690599 (2151.101628851598 steps/sec)\n",
      "Step #9121\tEpoch   2 Batch 2870/3125   Loss: 0.674646 mae: 0.654234 (2136.2235283332143 steps/sec)\n",
      "Step #9122\tEpoch   2 Batch 2871/3125   Loss: 0.783204 mae: 0.707396 (1984.4548112680855 steps/sec)\n",
      "Step #9123\tEpoch   2 Batch 2872/3125   Loss: 0.845687 mae: 0.725541 (2260.1543303013323 steps/sec)\n",
      "Step #9124\tEpoch   2 Batch 2873/3125   Loss: 0.771815 mae: 0.710219 (2092.5692732914918 steps/sec)\n",
      "Step #9125\tEpoch   2 Batch 2874/3125   Loss: 0.861601 mae: 0.715148 (2018.8994570449381 steps/sec)\n",
      "Step #9126\tEpoch   2 Batch 2875/3125   Loss: 0.868957 mae: 0.738990 (2162.9042904290427 steps/sec)\n",
      "Step #9127\tEpoch   2 Batch 2876/3125   Loss: 0.740940 mae: 0.697860 (2037.5535584163226 steps/sec)\n",
      "Step #9128\tEpoch   2 Batch 2877/3125   Loss: 0.778820 mae: 0.699397 (2043.3107614361572 steps/sec)\n",
      "Step #9129\tEpoch   2 Batch 2878/3125   Loss: 0.830567 mae: 0.710973 (2074.4163962965895 steps/sec)\n",
      "Step #9130\tEpoch   2 Batch 2879/3125   Loss: 0.707086 mae: 0.683287 (2105.9760396059487 steps/sec)\n",
      "Step #9131\tEpoch   2 Batch 2880/3125   Loss: 0.809032 mae: 0.735298 (2016.0464512655856 steps/sec)\n",
      "Step #9132\tEpoch   2 Batch 2881/3125   Loss: 0.749080 mae: 0.688829 (2104.201073596548 steps/sec)\n",
      "Step #9133\tEpoch   2 Batch 2882/3125   Loss: 0.818321 mae: 0.730081 (2005.7691571980565 steps/sec)\n",
      "Step #9134\tEpoch   2 Batch 2883/3125   Loss: 0.951194 mae: 0.784254 (2026.9976126269803 steps/sec)\n",
      "Step #9135\tEpoch   2 Batch 2884/3125   Loss: 0.752033 mae: 0.677250 (1978.3146395992717 steps/sec)\n",
      "Step #9136\tEpoch   2 Batch 2885/3125   Loss: 0.836773 mae: 0.744839 (1812.123149772313 steps/sec)\n",
      "Step #9137\tEpoch   2 Batch 2886/3125   Loss: 0.733886 mae: 0.674277 (1962.8719311874654 steps/sec)\n",
      "Step #9138\tEpoch   2 Batch 2887/3125   Loss: 0.896394 mae: 0.757126 (2165.718652539397 steps/sec)\n",
      "Step #9139\tEpoch   2 Batch 2888/3125   Loss: 0.858993 mae: 0.746611 (2130.191266544099 steps/sec)\n",
      "Step #9140\tEpoch   2 Batch 2889/3125   Loss: 0.775647 mae: 0.704991 (2053.0323351183074 steps/sec)\n",
      "Step #9141\tEpoch   2 Batch 2890/3125   Loss: 0.766820 mae: 0.686981 (2003.8526219232533 steps/sec)\n",
      "Step #9142\tEpoch   2 Batch 2891/3125   Loss: 0.892298 mae: 0.750324 (2017.4234261967063 steps/sec)\n",
      "Step #9143\tEpoch   2 Batch 2892/3125   Loss: 0.761349 mae: 0.694113 (2039.0790292470442 steps/sec)\n",
      "Step #9144\tEpoch   2 Batch 2893/3125   Loss: 0.733194 mae: 0.686222 (1995.1974122348017 steps/sec)\n",
      "Step #9145\tEpoch   2 Batch 2894/3125   Loss: 0.839223 mae: 0.727814 (2109.3652246507277 steps/sec)\n",
      "Step #9146\tEpoch   2 Batch 2895/3125   Loss: 0.790533 mae: 0.693906 (2231.036500388302 steps/sec)\n",
      "Step #9147\tEpoch   2 Batch 2896/3125   Loss: 0.748395 mae: 0.693953 (1984.9243757926818 steps/sec)\n",
      "Step #9148\tEpoch   2 Batch 2897/3125   Loss: 0.752195 mae: 0.691965 (2008.2853722767536 steps/sec)\n",
      "Step #9149\tEpoch   2 Batch 2898/3125   Loss: 0.824253 mae: 0.703878 (2024.9621010959302 steps/sec)\n",
      "Step #9150\tEpoch   2 Batch 2899/3125   Loss: 0.851402 mae: 0.740006 (2007.8623608145758 steps/sec)\n",
      "Step #9151\tEpoch   2 Batch 2900/3125   Loss: 0.757377 mae: 0.695891 (2001.385694517345 steps/sec)\n",
      "Step #9152\tEpoch   2 Batch 2901/3125   Loss: 0.713561 mae: 0.679257 (1940.7831052129895 steps/sec)\n",
      "Step #9153\tEpoch   2 Batch 2902/3125   Loss: 0.784367 mae: 0.685770 (1919.168329154236 steps/sec)\n",
      "Step #9154\tEpoch   2 Batch 2903/3125   Loss: 0.716009 mae: 0.666891 (2015.7170319108036 steps/sec)\n",
      "Step #9155\tEpoch   2 Batch 2904/3125   Loss: 0.778810 mae: 0.704127 (2033.523063347846 steps/sec)\n",
      "Step #9156\tEpoch   2 Batch 2905/3125   Loss: 0.904249 mae: 0.743977 (1926.8564288208163 steps/sec)\n",
      "Step #9157\tEpoch   2 Batch 2906/3125   Loss: 0.971276 mae: 0.773188 (1900.506583776632 steps/sec)\n",
      "Step #9158\tEpoch   2 Batch 2907/3125   Loss: 0.667686 mae: 0.653792 (1968.4914019674102 steps/sec)\n",
      "Step #9159\tEpoch   2 Batch 2908/3125   Loss: 0.868860 mae: 0.731493 (1905.2028162616398 steps/sec)\n",
      "Step #9160\tEpoch   2 Batch 2909/3125   Loss: 0.827857 mae: 0.729106 (1583.9277352305855 steps/sec)\n",
      "Step #9161\tEpoch   2 Batch 2910/3125   Loss: 0.873988 mae: 0.743536 (1748.9092000800588 steps/sec)\n",
      "Step #9162\tEpoch   2 Batch 2911/3125   Loss: 0.855064 mae: 0.751383 (1756.6588207701263 steps/sec)\n",
      "Step #9163\tEpoch   2 Batch 2912/3125   Loss: 0.734772 mae: 0.681946 (1738.5860193659637 steps/sec)\n",
      "Step #9164\tEpoch   2 Batch 2913/3125   Loss: 0.707894 mae: 0.664355 (1756.5999648202903 steps/sec)\n",
      "Step #9165\tEpoch   2 Batch 2914/3125   Loss: 0.774205 mae: 0.698892 (1725.4403791250834 steps/sec)\n",
      "Step #9166\tEpoch   2 Batch 2915/3125   Loss: 0.853758 mae: 0.733262 (1677.5873930085593 steps/sec)\n",
      "Step #9167\tEpoch   2 Batch 2916/3125   Loss: 1.081082 mae: 0.812180 (1829.4328035312385 steps/sec)\n",
      "Step #9168\tEpoch   2 Batch 2917/3125   Loss: 0.902343 mae: 0.756251 (2137.9221758943045 steps/sec)\n",
      "Step #9169\tEpoch   2 Batch 2918/3125   Loss: 0.748845 mae: 0.693065 (2169.931502597107 steps/sec)\n",
      "Step #9170\tEpoch   2 Batch 2919/3125   Loss: 0.764292 mae: 0.685850 (2190.580247558364 steps/sec)\n",
      "Step #9171\tEpoch   2 Batch 2920/3125   Loss: 0.790424 mae: 0.719826 (2100.554898936277 steps/sec)\n",
      "Step #9172\tEpoch   2 Batch 2921/3125   Loss: 0.766610 mae: 0.702935 (1958.1799676928392 steps/sec)\n",
      "Step #9173\tEpoch   2 Batch 2922/3125   Loss: 1.020167 mae: 0.805891 (2014.264995437737 steps/sec)\n",
      "Step #9174\tEpoch   2 Batch 2923/3125   Loss: 0.762779 mae: 0.705369 (1986.1839052156042 steps/sec)\n",
      "Step #9175\tEpoch   2 Batch 2924/3125   Loss: 0.857262 mae: 0.749508 (1666.7212398172064 steps/sec)\n",
      "Step #9176\tEpoch   2 Batch 2925/3125   Loss: 0.850018 mae: 0.732039 (1836.819563294299 steps/sec)\n",
      "Step #9177\tEpoch   2 Batch 2926/3125   Loss: 0.703721 mae: 0.664277 (1914.1759234750225 steps/sec)\n",
      "Step #9178\tEpoch   2 Batch 2927/3125   Loss: 0.753961 mae: 0.689644 (1845.987007728465 steps/sec)\n",
      "Step #9179\tEpoch   2 Batch 2928/3125   Loss: 0.861534 mae: 0.738928 (2073.370441041257 steps/sec)\n",
      "Step #9180\tEpoch   2 Batch 2929/3125   Loss: 0.846674 mae: 0.731663 (1943.3548937116593 steps/sec)\n",
      "Step #9181\tEpoch   2 Batch 2930/3125   Loss: 0.643122 mae: 0.627440 (2128.6561104344296 steps/sec)\n",
      "Step #9182\tEpoch   2 Batch 2931/3125   Loss: 0.750243 mae: 0.685057 (1668.7238410490634 steps/sec)\n",
      "Step #9183\tEpoch   2 Batch 2932/3125   Loss: 0.649673 mae: 0.659467 (1725.7527505534024 steps/sec)\n",
      "Step #9184\tEpoch   2 Batch 2933/3125   Loss: 0.786245 mae: 0.666729 (1616.7756260022204 steps/sec)\n",
      "Step #9185\tEpoch   2 Batch 2934/3125   Loss: 0.696814 mae: 0.679692 (1824.8799164636268 steps/sec)\n",
      "Step #9186\tEpoch   2 Batch 2935/3125   Loss: 0.887805 mae: 0.768433 (1833.431249125752 steps/sec)\n",
      "Step #9187\tEpoch   2 Batch 2936/3125   Loss: 0.750029 mae: 0.691641 (1916.4324225532305 steps/sec)\n",
      "Step #9188\tEpoch   2 Batch 2937/3125   Loss: 0.838845 mae: 0.723530 (1586.1439905609718 steps/sec)\n",
      "Step #9189\tEpoch   2 Batch 2938/3125   Loss: 0.816063 mae: 0.732424 (1842.7752978805665 steps/sec)\n",
      "Step #9190\tEpoch   2 Batch 2939/3125   Loss: 0.794053 mae: 0.697604 (1498.9400253021606 steps/sec)\n",
      "Step #9191\tEpoch   2 Batch 2940/3125   Loss: 0.848876 mae: 0.736990 (1541.4568173465639 steps/sec)\n",
      "Step #9192\tEpoch   2 Batch 2941/3125   Loss: 0.805470 mae: 0.732296 (1781.5806240602142 steps/sec)\n",
      "Step #9193\tEpoch   2 Batch 2942/3125   Loss: 0.933748 mae: 0.767431 (1913.512231174211 steps/sec)\n",
      "Step #9194\tEpoch   2 Batch 2943/3125   Loss: 1.030108 mae: 0.818157 (1900.7132822767028 steps/sec)\n",
      "Step #9195\tEpoch   2 Batch 2944/3125   Loss: 0.858561 mae: 0.732398 (1942.4908764194809 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9196\tEpoch   2 Batch 2945/3125   Loss: 0.816627 mae: 0.731704 (1538.4601841323406 steps/sec)\n",
      "Step #9197\tEpoch   2 Batch 2946/3125   Loss: 0.747151 mae: 0.679148 (1614.8335232697816 steps/sec)\n",
      "Step #9198\tEpoch   2 Batch 2947/3125   Loss: 0.899449 mae: 0.745215 (1898.0984188185034 steps/sec)\n",
      "Step #9199\tEpoch   2 Batch 2948/3125   Loss: 0.777521 mae: 0.702955 (1888.237412662969 steps/sec)\n",
      "Step #9200\tEpoch   2 Batch 2949/3125   Loss: 0.876183 mae: 0.743931 (1879.9770510613885 steps/sec)\n",
      "Step #9201\tEpoch   2 Batch 2950/3125   Loss: 0.920086 mae: 0.760438 (1948.7724645492221 steps/sec)\n",
      "Step #9202\tEpoch   2 Batch 2951/3125   Loss: 0.828715 mae: 0.729440 (2122.322747788775 steps/sec)\n",
      "Step #9203\tEpoch   2 Batch 2952/3125   Loss: 0.852056 mae: 0.735328 (1659.4149344432224 steps/sec)\n",
      "Step #9204\tEpoch   2 Batch 2953/3125   Loss: 0.861201 mae: 0.706603 (1413.0799811333468 steps/sec)\n",
      "Step #9205\tEpoch   2 Batch 2954/3125   Loss: 0.779763 mae: 0.679568 (1598.9752660953368 steps/sec)\n",
      "Step #9206\tEpoch   2 Batch 2955/3125   Loss: 0.803182 mae: 0.693425 (1781.2023305984474 steps/sec)\n",
      "Step #9207\tEpoch   2 Batch 2956/3125   Loss: 0.880170 mae: 0.742420 (1777.593937801436 steps/sec)\n",
      "Step #9208\tEpoch   2 Batch 2957/3125   Loss: 0.880630 mae: 0.728446 (1993.4525959582518 steps/sec)\n",
      "Step #9209\tEpoch   2 Batch 2958/3125   Loss: 1.000032 mae: 0.811481 (1706.1390520509608 steps/sec)\n",
      "Step #9210\tEpoch   2 Batch 2959/3125   Loss: 0.758137 mae: 0.696923 (1880.8200749762336 steps/sec)\n",
      "Step #9211\tEpoch   2 Batch 2960/3125   Loss: 0.784977 mae: 0.709493 (1874.4152373461563 steps/sec)\n",
      "Step #9212\tEpoch   2 Batch 2961/3125   Loss: 0.784761 mae: 0.711665 (1475.143142523529 steps/sec)\n",
      "Step #9213\tEpoch   2 Batch 2962/3125   Loss: 0.828500 mae: 0.708153 (1594.8409077082192 steps/sec)\n",
      "Step #9214\tEpoch   2 Batch 2963/3125   Loss: 0.780744 mae: 0.705565 (1946.2043876907087 steps/sec)\n",
      "Step #9215\tEpoch   2 Batch 2964/3125   Loss: 0.997775 mae: 0.780112 (1703.0631801201885 steps/sec)\n",
      "Step #9216\tEpoch   2 Batch 2965/3125   Loss: 0.864360 mae: 0.756753 (1738.2113551595523 steps/sec)\n",
      "Step #9217\tEpoch   2 Batch 2966/3125   Loss: 0.678033 mae: 0.667598 (2003.3549225272732 steps/sec)\n",
      "Step #9218\tEpoch   2 Batch 2967/3125   Loss: 0.956011 mae: 0.791738 (1490.4919617347302 steps/sec)\n",
      "Step #9219\tEpoch   2 Batch 2968/3125   Loss: 0.708986 mae: 0.676140 (1508.0155608447726 steps/sec)\n",
      "Step #9220\tEpoch   2 Batch 2969/3125   Loss: 0.689884 mae: 0.648396 (1914.2458126055406 steps/sec)\n",
      "Step #9221\tEpoch   2 Batch 2970/3125   Loss: 0.868342 mae: 0.736726 (1919.080518672389 steps/sec)\n",
      "Step #9222\tEpoch   2 Batch 2971/3125   Loss: 0.726129 mae: 0.687853 (1777.7144843136757 steps/sec)\n",
      "Step #9223\tEpoch   2 Batch 2972/3125   Loss: 0.716253 mae: 0.676934 (1881.1743705205372 steps/sec)\n",
      "Step #9224\tEpoch   2 Batch 2973/3125   Loss: 0.870864 mae: 0.725188 (1841.2221246707638 steps/sec)\n",
      "Step #9225\tEpoch   2 Batch 2974/3125   Loss: 0.730714 mae: 0.677410 (1721.3474292468318 steps/sec)\n",
      "Step #9226\tEpoch   2 Batch 2975/3125   Loss: 0.717964 mae: 0.672152 (1673.1438783488377 steps/sec)\n",
      "Step #9227\tEpoch   2 Batch 2976/3125   Loss: 0.926942 mae: 0.769707 (1876.6796721194116 steps/sec)\n",
      "Step #9228\tEpoch   2 Batch 2977/3125   Loss: 0.898226 mae: 0.755778 (1891.5755673413428 steps/sec)\n",
      "Step #9229\tEpoch   2 Batch 2978/3125   Loss: 0.733052 mae: 0.656277 (2127.0152947381234 steps/sec)\n",
      "Step #9230\tEpoch   2 Batch 2979/3125   Loss: 0.903228 mae: 0.744969 (2033.6808215591393 steps/sec)\n",
      "Step #9231\tEpoch   2 Batch 2980/3125   Loss: 0.770683 mae: 0.694242 (2253.7662142266067 steps/sec)\n",
      "Step #9232\tEpoch   2 Batch 2981/3125   Loss: 0.668010 mae: 0.680810 (2075.7099165619156 steps/sec)\n",
      "Step #9233\tEpoch   2 Batch 2982/3125   Loss: 0.828791 mae: 0.722214 (2047.9800001953106 steps/sec)\n",
      "Step #9234\tEpoch   2 Batch 2983/3125   Loss: 0.766418 mae: 0.689119 (1687.8758611808641 steps/sec)\n",
      "Step #9235\tEpoch   2 Batch 2984/3125   Loss: 0.748310 mae: 0.693838 (1617.4988816388234 steps/sec)\n",
      "Step #9236\tEpoch   2 Batch 2985/3125   Loss: 0.801326 mae: 0.710294 (1499.7868840735177 steps/sec)\n",
      "Step #9237\tEpoch   2 Batch 2986/3125   Loss: 0.830212 mae: 0.726996 (2076.6554110925167 steps/sec)\n",
      "Step #9238\tEpoch   2 Batch 2987/3125   Loss: 0.771695 mae: 0.690802 (2063.4963741378124 steps/sec)\n",
      "Step #9239\tEpoch   2 Batch 2988/3125   Loss: 0.756956 mae: 0.691581 (2162.9042904290427 steps/sec)\n",
      "Step #9240\tEpoch   2 Batch 2989/3125   Loss: 0.838504 mae: 0.710442 (1993.528394075933 steps/sec)\n",
      "Step #9241\tEpoch   2 Batch 2990/3125   Loss: 0.769666 mae: 0.691447 (1291.0874021904417 steps/sec)\n",
      "Step #9242\tEpoch   2 Batch 2991/3125   Loss: 0.867271 mae: 0.745648 (720.2524994075605 steps/sec)\n",
      "Step #9243\tEpoch   2 Batch 2992/3125   Loss: 0.824227 mae: 0.724793 (1078.3714018326357 steps/sec)\n",
      "Step #9244\tEpoch   2 Batch 2993/3125   Loss: 0.675083 mae: 0.641667 (1417.080768425107 steps/sec)\n",
      "Step #9245\tEpoch   2 Batch 2994/3125   Loss: 0.780061 mae: 0.677957 (1519.025923699288 steps/sec)\n",
      "Step #9246\tEpoch   2 Batch 2995/3125   Loss: 0.749022 mae: 0.701725 (1314.400320898516 steps/sec)\n",
      "Step #9247\tEpoch   2 Batch 2996/3125   Loss: 0.788807 mae: 0.701252 (1246.4721510636148 steps/sec)\n",
      "Step #9248\tEpoch   2 Batch 2997/3125   Loss: 0.972517 mae: 0.769804 (1403.0306476755 steps/sec)\n",
      "Step #9249\tEpoch   2 Batch 2998/3125   Loss: 0.817831 mae: 0.733923 (1504.0895072796386 steps/sec)\n",
      "Step #9250\tEpoch   2 Batch 2999/3125   Loss: 0.757853 mae: 0.705120 (1422.7528985556407 steps/sec)\n",
      "Step #9251\tEpoch   2 Batch 3000/3125   Loss: 0.902529 mae: 0.730893 (1708.5854882599274 steps/sec)\n",
      "Step #9252\tEpoch   2 Batch 3001/3125   Loss: 0.794105 mae: 0.719305 (2142.3338202694836 steps/sec)\n",
      "Step #9253\tEpoch   2 Batch 3002/3125   Loss: 0.723892 mae: 0.670492 (1844.493306830375 steps/sec)\n",
      "Step #9254\tEpoch   2 Batch 3003/3125   Loss: 0.765817 mae: 0.683858 (1856.9384430119715 steps/sec)\n",
      "Step #9255\tEpoch   2 Batch 3004/3125   Loss: 0.794439 mae: 0.709799 (2013.2981327701243 steps/sec)\n",
      "Step #9256\tEpoch   2 Batch 3005/3125   Loss: 0.719959 mae: 0.670103 (1831.7657745789952 steps/sec)\n",
      "Step #9257\tEpoch   2 Batch 3006/3125   Loss: 0.693445 mae: 0.667097 (1962.0088316742758 steps/sec)\n",
      "Step #9258\tEpoch   2 Batch 3007/3125   Loss: 0.808044 mae: 0.693592 (1840.2042768266895 steps/sec)\n",
      "Step #9259\tEpoch   2 Batch 3008/3125   Loss: 0.812860 mae: 0.689883 (1878.7812547593237 steps/sec)\n",
      "Step #9260\tEpoch   2 Batch 3009/3125   Loss: 0.815040 mae: 0.698817 (978.318086610096 steps/sec)\n",
      "Step #9261\tEpoch   2 Batch 3010/3125   Loss: 0.822289 mae: 0.702584 (1773.9251063685808 steps/sec)\n",
      "Step #9262\tEpoch   2 Batch 3011/3125   Loss: 0.802244 mae: 0.692919 (1310.228664250906 steps/sec)\n",
      "Step #9263\tEpoch   2 Batch 3012/3125   Loss: 0.825433 mae: 0.733854 (1412.7943950417678 steps/sec)\n",
      "Step #9264\tEpoch   2 Batch 3013/3125   Loss: 0.918145 mae: 0.753351 (1372.715252595337 steps/sec)\n",
      "Step #9265\tEpoch   2 Batch 3014/3125   Loss: 0.877208 mae: 0.731812 (1424.947341242339 steps/sec)\n",
      "Step #9266\tEpoch   2 Batch 3015/3125   Loss: 0.886285 mae: 0.737416 (1443.9714944744724 steps/sec)\n",
      "Step #9267\tEpoch   2 Batch 3016/3125   Loss: 0.816700 mae: 0.727810 (1408.6095606558258 steps/sec)\n",
      "Step #9268\tEpoch   2 Batch 3017/3125   Loss: 0.839802 mae: 0.716446 (1291.2543408122554 steps/sec)\n",
      "Step #9269\tEpoch   2 Batch 3018/3125   Loss: 0.837139 mae: 0.724719 (1974.4033440974608 steps/sec)\n",
      "Step #9270\tEpoch   2 Batch 3019/3125   Loss: 0.902309 mae: 0.726725 (2073.8420157430483 steps/sec)\n",
      "Step #9271\tEpoch   2 Batch 3020/3125   Loss: 0.931537 mae: 0.748971 (1744.5301256935606 steps/sec)\n",
      "Step #9272\tEpoch   2 Batch 3021/3125   Loss: 0.746842 mae: 0.690308 (1191.6042592361102 steps/sec)\n",
      "Step #9273\tEpoch   2 Batch 3022/3125   Loss: 0.620370 mae: 0.628146 (837.63450308748 steps/sec)\n",
      "Step #9274\tEpoch   2 Batch 3023/3125   Loss: 0.878336 mae: 0.728484 (1004.5804013240148 steps/sec)\n",
      "Step #9275\tEpoch   2 Batch 3024/3125   Loss: 0.700204 mae: 0.684268 (970.8902191173271 steps/sec)\n",
      "Step #9276\tEpoch   2 Batch 3025/3125   Loss: 0.811909 mae: 0.725953 (943.3542652536121 steps/sec)\n",
      "Step #9277\tEpoch   2 Batch 3026/3125   Loss: 0.706428 mae: 0.654307 (746.4715136932178 steps/sec)\n",
      "Step #9278\tEpoch   2 Batch 3027/3125   Loss: 0.826443 mae: 0.736913 (1577.6363499586248 steps/sec)\n",
      "Step #9279\tEpoch   2 Batch 3028/3125   Loss: 0.750306 mae: 0.677171 (1898.4076980872462 steps/sec)\n",
      "Step #9280\tEpoch   2 Batch 3029/3125   Loss: 0.758147 mae: 0.702549 (1190.5016547738664 steps/sec)\n",
      "Step #9281\tEpoch   2 Batch 3030/3125   Loss: 0.835365 mae: 0.701868 (837.550870438634 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9282\tEpoch   2 Batch 3031/3125   Loss: 0.811073 mae: 0.713549 (833.0130484002303 steps/sec)\n",
      "Step #9283\tEpoch   2 Batch 3032/3125   Loss: 0.782418 mae: 0.689045 (1021.2822323406949 steps/sec)\n",
      "Step #9284\tEpoch   2 Batch 3033/3125   Loss: 0.781786 mae: 0.720325 (933.287050635052 steps/sec)\n",
      "Step #9285\tEpoch   2 Batch 3034/3125   Loss: 0.758218 mae: 0.699147 (823.9246931623529 steps/sec)\n",
      "Step #9286\tEpoch   2 Batch 3035/3125   Loss: 0.874948 mae: 0.753323 (1749.361450104687 steps/sec)\n",
      "Step #9287\tEpoch   2 Batch 3036/3125   Loss: 0.583953 mae: 0.607075 (1394.6982695556176 steps/sec)\n",
      "Step #9288\tEpoch   2 Batch 3037/3125   Loss: 0.835325 mae: 0.732931 (1451.9593453155724 steps/sec)\n",
      "Step #9289\tEpoch   2 Batch 3038/3125   Loss: 0.807564 mae: 0.744663 (1999.1725531691786 steps/sec)\n",
      "Step #9290\tEpoch   2 Batch 3039/3125   Loss: 0.734751 mae: 0.690876 (1989.1604776674349 steps/sec)\n",
      "Step #9291\tEpoch   2 Batch 3040/3125   Loss: 0.882708 mae: 0.728952 (1804.3121397229631 steps/sec)\n",
      "Step #9292\tEpoch   2 Batch 3041/3125   Loss: 0.817476 mae: 0.721030 (2102.8718113268087 steps/sec)\n",
      "Step #9293\tEpoch   2 Batch 3042/3125   Loss: 0.812663 mae: 0.709738 (1947.144024363069 steps/sec)\n",
      "Step #9294\tEpoch   2 Batch 3043/3125   Loss: 0.915093 mae: 0.760711 (1950.004649173377 steps/sec)\n",
      "Step #9295\tEpoch   2 Batch 3044/3125   Loss: 0.833722 mae: 0.710948 (1479.910802495272 steps/sec)\n",
      "Step #9296\tEpoch   2 Batch 3045/3125   Loss: 0.876675 mae: 0.723743 (1745.3867536661285 steps/sec)\n",
      "Step #9297\tEpoch   2 Batch 3046/3125   Loss: 0.626194 mae: 0.629327 (1425.0248019243575 steps/sec)\n",
      "Step #9298\tEpoch   2 Batch 3047/3125   Loss: 0.657194 mae: 0.645998 (1739.4368183137726 steps/sec)\n",
      "Step #9299\tEpoch   2 Batch 3048/3125   Loss: 0.724299 mae: 0.679792 (1871.187408544202 steps/sec)\n",
      "Step #9300\tEpoch   2 Batch 3049/3125   Loss: 0.769156 mae: 0.717401 (1879.623206331281 steps/sec)\n",
      "Step #9301\tEpoch   2 Batch 3050/3125   Loss: 0.793012 mae: 0.716889 (1747.7140523692851 steps/sec)\n",
      "Step #9302\tEpoch   2 Batch 3051/3125   Loss: 0.802872 mae: 0.692495 (1426.6146038829404 steps/sec)\n",
      "Step #9303\tEpoch   2 Batch 3052/3125   Loss: 0.723295 mae: 0.674155 (1584.538084336348 steps/sec)\n",
      "Step #9304\tEpoch   2 Batch 3053/3125   Loss: 0.834237 mae: 0.703931 (1497.6020109402002 steps/sec)\n",
      "Step #9305\tEpoch   2 Batch 3054/3125   Loss: 0.755844 mae: 0.677590 (1420.5363372191478 steps/sec)\n",
      "Step #9306\tEpoch   2 Batch 3055/3125   Loss: 0.884547 mae: 0.741604 (1564.3500249889973 steps/sec)\n",
      "Step #9307\tEpoch   2 Batch 3056/3125   Loss: 0.837690 mae: 0.725555 (1550.780880265026 steps/sec)\n",
      "Step #9308\tEpoch   2 Batch 3057/3125   Loss: 0.951051 mae: 0.760002 (1563.3937424053795 steps/sec)\n",
      "Step #9309\tEpoch   2 Batch 3058/3125   Loss: 0.901481 mae: 0.757480 (1186.7962219946012 steps/sec)\n",
      "Step #9310\tEpoch   2 Batch 3059/3125   Loss: 0.682940 mae: 0.642341 (1265.754086091596 steps/sec)\n",
      "Step #9311\tEpoch   2 Batch 3060/3125   Loss: 0.704878 mae: 0.686593 (1654.975615145441 steps/sec)\n",
      "Step #9312\tEpoch   2 Batch 3061/3125   Loss: 0.882335 mae: 0.730639 (1391.2471225097686 steps/sec)\n",
      "Step #9313\tEpoch   2 Batch 3062/3125   Loss: 0.729924 mae: 0.664296 (1656.5050828982394 steps/sec)\n",
      "Step #9314\tEpoch   2 Batch 3063/3125   Loss: 0.885550 mae: 0.780060 (1699.585062240664 steps/sec)\n",
      "Step #9315\tEpoch   2 Batch 3064/3125   Loss: 0.844838 mae: 0.717538 (1469.1596903569302 steps/sec)\n",
      "Step #9316\tEpoch   2 Batch 3065/3125   Loss: 0.776462 mae: 0.697598 (1974.9611534368614 steps/sec)\n",
      "Step #9317\tEpoch   2 Batch 3066/3125   Loss: 0.831829 mae: 0.707893 (2008.8432506992606 steps/sec)\n",
      "Step #9318\tEpoch   2 Batch 3067/3125   Loss: 0.724493 mae: 0.664308 (1828.7468280474725 steps/sec)\n",
      "Step #9319\tEpoch   2 Batch 3068/3125   Loss: 0.930527 mae: 0.752920 (1952.1283824665593 steps/sec)\n",
      "Step #9320\tEpoch   2 Batch 3069/3125   Loss: 0.694393 mae: 0.666894 (2104.5811715355203 steps/sec)\n",
      "Step #9321\tEpoch   2 Batch 3070/3125   Loss: 0.787959 mae: 0.699590 (2060.49578007251 steps/sec)\n",
      "Step #9322\tEpoch   2 Batch 3071/3125   Loss: 0.866369 mae: 0.753059 (1498.8543207758885 steps/sec)\n",
      "Step #9323\tEpoch   2 Batch 3072/3125   Loss: 0.835529 mae: 0.712195 (1653.2143504686526 steps/sec)\n",
      "Step #9324\tEpoch   2 Batch 3073/3125   Loss: 1.013119 mae: 0.785914 (1916.9579524680073 steps/sec)\n",
      "Step #9325\tEpoch   2 Batch 3074/3125   Loss: 0.816713 mae: 0.728833 (1916.5199908613206 steps/sec)\n",
      "Step #9326\tEpoch   2 Batch 3075/3125   Loss: 0.769083 mae: 0.696845 (1918.0625040013902 steps/sec)\n",
      "Step #9327\tEpoch   2 Batch 3076/3125   Loss: 0.904637 mae: 0.762621 (1946.8186628543845 steps/sec)\n",
      "Step #9328\tEpoch   2 Batch 3077/3125   Loss: 0.704136 mae: 0.668974 (1794.7539131699887 steps/sec)\n",
      "Step #9329\tEpoch   2 Batch 3078/3125   Loss: 0.658837 mae: 0.641011 (1896.1077005144525 steps/sec)\n",
      "Step #9330\tEpoch   2 Batch 3079/3125   Loss: 0.803948 mae: 0.701905 (1568.9259957506658 steps/sec)\n",
      "Step #9331\tEpoch   2 Batch 3080/3125   Loss: 0.956228 mae: 0.779214 (1602.616576747314 steps/sec)\n",
      "Step #9332\tEpoch   2 Batch 3081/3125   Loss: 0.741113 mae: 0.654192 (1936.9471049496171 steps/sec)\n",
      "Step #9333\tEpoch   2 Batch 3082/3125   Loss: 0.792490 mae: 0.695217 (1964.4715888865992 steps/sec)\n",
      "Step #9334\tEpoch   2 Batch 3083/3125   Loss: 0.774412 mae: 0.713798 (1866.0924347315406 steps/sec)\n",
      "Step #9335\tEpoch   2 Batch 3084/3125   Loss: 0.834872 mae: 0.722600 (1963.864515343628 steps/sec)\n",
      "Step #9336\tEpoch   2 Batch 3085/3125   Loss: 0.811314 mae: 0.716650 (2095.1825284232823 steps/sec)\n",
      "Step #9337\tEpoch   2 Batch 3086/3125   Loss: 0.817193 mae: 0.711485 (1934.320869228357 steps/sec)\n",
      "Step #9338\tEpoch   2 Batch 3087/3125   Loss: 0.912503 mae: 0.738093 (1332.8028776795531 steps/sec)\n",
      "Step #9339\tEpoch   2 Batch 3088/3125   Loss: 0.660822 mae: 0.653194 (1897.600347461001 steps/sec)\n",
      "Step #9340\tEpoch   2 Batch 3089/3125   Loss: 0.805404 mae: 0.721995 (1955.477644645438 steps/sec)\n",
      "Step #9341\tEpoch   2 Batch 3090/3125   Loss: 0.666320 mae: 0.660658 (1835.2764091748418 steps/sec)\n",
      "Step #9342\tEpoch   2 Batch 3091/3125   Loss: 0.900320 mae: 0.763970 (2007.689362028031 steps/sec)\n",
      "Step #9343\tEpoch   2 Batch 3092/3125   Loss: 0.653883 mae: 0.647600 (1780.3706502084165 steps/sec)\n",
      "Step #9344\tEpoch   2 Batch 3093/3125   Loss: 0.805904 mae: 0.720683 (1758.1020245630214 steps/sec)\n",
      "Step #9345\tEpoch   2 Batch 3094/3125   Loss: 0.870337 mae: 0.758792 (1908.5491709288146 steps/sec)\n",
      "Step #9346\tEpoch   2 Batch 3095/3125   Loss: 0.755353 mae: 0.682690 (1367.4700052164842 steps/sec)\n",
      "Step #9347\tEpoch   2 Batch 3096/3125   Loss: 0.750759 mae: 0.689522 (1802.6991249333814 steps/sec)\n",
      "Step #9348\tEpoch   2 Batch 3097/3125   Loss: 0.702695 mae: 0.642072 (2101.712716595011 steps/sec)\n",
      "Step #9349\tEpoch   2 Batch 3098/3125   Loss: 0.942223 mae: 0.749767 (2088.463989802422 steps/sec)\n",
      "Step #9350\tEpoch   2 Batch 3099/3125   Loss: 0.879547 mae: 0.759471 (1989.8210524318272 steps/sec)\n",
      "Step #9351\tEpoch   2 Batch 3100/3125   Loss: 0.987634 mae: 0.768176 (2145.66549688456 steps/sec)\n",
      "Step #9352\tEpoch   2 Batch 3101/3125   Loss: 0.850971 mae: 0.748238 (1915.7846656983384 steps/sec)\n",
      "Step #9353\tEpoch   2 Batch 3102/3125   Loss: 0.829695 mae: 0.697458 (2145.7313579438487 steps/sec)\n",
      "Step #9354\tEpoch   2 Batch 3103/3125   Loss: 0.875793 mae: 0.733321 (2017.5204671611493 steps/sec)\n",
      "Step #9355\tEpoch   2 Batch 3104/3125   Loss: 0.769356 mae: 0.668303 (1401.2962888719615 steps/sec)\n",
      "Step #9356\tEpoch   2 Batch 3105/3125   Loss: 0.809547 mae: 0.738673 (1999.5919106780195 steps/sec)\n",
      "Step #9357\tEpoch   2 Batch 3106/3125   Loss: 0.803325 mae: 0.716980 (2137.224968152866 steps/sec)\n",
      "Step #9358\tEpoch   2 Batch 3107/3125   Loss: 0.690255 mae: 0.662488 (2136.9200827397876 steps/sec)\n",
      "Step #9359\tEpoch   2 Batch 3108/3125   Loss: 0.801227 mae: 0.686523 (2014.2456490837144 steps/sec)\n",
      "Step #9360\tEpoch   2 Batch 3109/3125   Loss: 0.689498 mae: 0.657337 (1955.0948110305224 steps/sec)\n",
      "Step #9361\tEpoch   2 Batch 3110/3125   Loss: 0.716269 mae: 0.673244 (1983.9854688564292 steps/sec)\n",
      "Step #9362\tEpoch   2 Batch 3111/3125   Loss: 0.909224 mae: 0.756174 (1909.3310997204949 steps/sec)\n",
      "Step #9363\tEpoch   2 Batch 3112/3125   Loss: 0.846438 mae: 0.723578 (1325.063815806101 steps/sec)\n",
      "Step #9364\tEpoch   2 Batch 3113/3125   Loss: 0.847161 mae: 0.722329 (1909.2963337248154 steps/sec)\n",
      "Step #9365\tEpoch   2 Batch 3114/3125   Loss: 0.856244 mae: 0.725162 (1926.0600828412148 steps/sec)\n",
      "Step #9366\tEpoch   2 Batch 3115/3125   Loss: 0.731743 mae: 0.658139 (1990.0665205302664 steps/sec)\n",
      "Step #9367\tEpoch   2 Batch 3116/3125   Loss: 0.753600 mae: 0.699998 (2132.639114871461 steps/sec)\n",
      "Step #9368\tEpoch   2 Batch 3117/3125   Loss: 0.736657 mae: 0.687780 (1906.7965049143959 steps/sec)\n",
      "Step #9369\tEpoch   2 Batch 3118/3125   Loss: 0.828989 mae: 0.718130 (2110.7027114072343 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9370\tEpoch   2 Batch 3119/3125   Loss: 0.864253 mae: 0.752161 (1901.799187463726 steps/sec)\n",
      "Step #9371\tEpoch   2 Batch 3120/3125   Loss: 0.759845 mae: 0.689421 (1397.4212549892386 steps/sec)\n",
      "Step #9372\tEpoch   2 Batch 3121/3125   Loss: 0.830525 mae: 0.721161 (1503.7551716967469 steps/sec)\n",
      "Step #9373\tEpoch   2 Batch 3122/3125   Loss: 0.952302 mae: 0.782882 (1169.4569139058815 steps/sec)\n",
      "Step #9374\tEpoch   2 Batch 3123/3125   Loss: 0.803484 mae: 0.722075 (1260.845673334175 steps/sec)\n",
      "Step #9375\tEpoch   2 Batch 3124/3125   Loss: 0.851314 mae: 0.731402 (1181.0615830822517 steps/sec)\n",
      "\n",
      "Train time for epoch #3 (9375 total steps): 94.0121009349823\n",
      "Model test set loss: 0.828507 mae: 0.720266\n",
      "best loss = 0.8285074830055237\n",
      "Step #9376\tEpoch   3 Batch    0/3125   Loss: 0.902885 mae: 0.720298 (686.1891938761154 steps/sec)\n",
      "Step #9377\tEpoch   3 Batch    1/3125   Loss: 0.923603 mae: 0.749962 (2274.691686100114 steps/sec)\n",
      "Step #9378\tEpoch   3 Batch    2/3125   Loss: 0.762494 mae: 0.693287 (1986.5790121819525 steps/sec)\n",
      "Step #9379\tEpoch   3 Batch    3/3125   Loss: 0.869292 mae: 0.736622 (1921.8944455136136 steps/sec)\n",
      "Step #9380\tEpoch   3 Batch    4/3125   Loss: 0.850043 mae: 0.742291 (2431.1987015998147 steps/sec)\n",
      "Step #9381\tEpoch   3 Batch    5/3125   Loss: 0.768854 mae: 0.709769 (2228.3813794349226 steps/sec)\n",
      "Step #9382\tEpoch   3 Batch    6/3125   Loss: 0.713356 mae: 0.676529 (2073.2269608715424 steps/sec)\n",
      "Step #9383\tEpoch   3 Batch    7/3125   Loss: 0.825119 mae: 0.723321 (2031.5334689528238 steps/sec)\n",
      "Step #9384\tEpoch   3 Batch    8/3125   Loss: 0.808005 mae: 0.709423 (2131.273691805811 steps/sec)\n",
      "Step #9385\tEpoch   3 Batch    9/3125   Loss: 0.695280 mae: 0.644231 (1950.458050055338 steps/sec)\n",
      "Step #9386\tEpoch   3 Batch   10/3125   Loss: 0.797786 mae: 0.706687 (1943.8772767298512 steps/sec)\n",
      "Step #9387\tEpoch   3 Batch   11/3125   Loss: 0.866783 mae: 0.727798 (2109.153081031067 steps/sec)\n",
      "Step #9388\tEpoch   3 Batch   12/3125   Loss: 0.842232 mae: 0.731899 (2218.433774449134 steps/sec)\n",
      "Step #9389\tEpoch   3 Batch   13/3125   Loss: 0.708116 mae: 0.674553 (2397.621987469703 steps/sec)\n",
      "Step #9390\tEpoch   3 Batch   14/3125   Loss: 0.759153 mae: 0.696488 (2045.0642144577607 steps/sec)\n",
      "Step #9391\tEpoch   3 Batch   15/3125   Loss: 0.881698 mae: 0.724708 (2183.6689643682707 steps/sec)\n",
      "Step #9392\tEpoch   3 Batch   16/3125   Loss: 0.904027 mae: 0.758963 (2317.064601310367 steps/sec)\n",
      "Step #9393\tEpoch   3 Batch   17/3125   Loss: 0.802212 mae: 0.707973 (2304.9172400149473 steps/sec)\n",
      "Step #9394\tEpoch   3 Batch   18/3125   Loss: 0.756512 mae: 0.679967 (1628.8306201068722 steps/sec)\n",
      "Step #9395\tEpoch   3 Batch   19/3125   Loss: 0.874260 mae: 0.760163 (1514.9657947395415 steps/sec)\n",
      "Step #9396\tEpoch   3 Batch   20/3125   Loss: 0.785244 mae: 0.705589 (1637.3382884535808 steps/sec)\n",
      "Step #9397\tEpoch   3 Batch   21/3125   Loss: 0.814723 mae: 0.709523 (1741.2565697157897 steps/sec)\n",
      "Step #9398\tEpoch   3 Batch   22/3125   Loss: 0.824692 mae: 0.722616 (1605.352317525931 steps/sec)\n",
      "Step #9399\tEpoch   3 Batch   23/3125   Loss: 0.822958 mae: 0.711592 (1865.893197145755 steps/sec)\n",
      "Step #9400\tEpoch   3 Batch   24/3125   Loss: 0.724916 mae: 0.672854 (1650.0664856996734 steps/sec)\n",
      "Step #9401\tEpoch   3 Batch   25/3125   Loss: 0.862037 mae: 0.732116 (1663.5085826696704 steps/sec)\n",
      "Step #9402\tEpoch   3 Batch   26/3125   Loss: 0.750295 mae: 0.685546 (1416.7935630755096 steps/sec)\n",
      "Step #9403\tEpoch   3 Batch   27/3125   Loss: 0.823902 mae: 0.717578 (1755.07109322041 steps/sec)\n",
      "Step #9404\tEpoch   3 Batch   28/3125   Loss: 0.902416 mae: 0.736864 (1856.116687023171 steps/sec)\n",
      "Step #9405\tEpoch   3 Batch   29/3125   Loss: 0.750515 mae: 0.690325 (2152.4925843434708 steps/sec)\n",
      "Step #9406\tEpoch   3 Batch   30/3125   Loss: 0.833603 mae: 0.725218 (2261.25098390175 steps/sec)\n",
      "Step #9407\tEpoch   3 Batch   31/3125   Loss: 0.759238 mae: 0.700460 (2359.797456959604 steps/sec)\n",
      "Step #9408\tEpoch   3 Batch   32/3125   Loss: 0.741755 mae: 0.688121 (2284.7779666187303 steps/sec)\n",
      "Step #9409\tEpoch   3 Batch   33/3125   Loss: 0.735803 mae: 0.681520 (1576.533381445314 steps/sec)\n",
      "Step #9410\tEpoch   3 Batch   34/3125   Loss: 0.822452 mae: 0.737327 (1361.1771349200683 steps/sec)\n",
      "Step #9411\tEpoch   3 Batch   35/3125   Loss: 0.867778 mae: 0.732392 (1638.2720099992189 steps/sec)\n",
      "Step #9412\tEpoch   3 Batch   36/3125   Loss: 0.967500 mae: 0.786844 (1639.552810569932 steps/sec)\n",
      "Step #9413\tEpoch   3 Batch   37/3125   Loss: 0.799436 mae: 0.692905 (1608.911665873905 steps/sec)\n",
      "Step #9414\tEpoch   3 Batch   38/3125   Loss: 0.867065 mae: 0.739994 (1601.7475120103263 steps/sec)\n",
      "Step #9415\tEpoch   3 Batch   39/3125   Loss: 0.865806 mae: 0.738149 (1513.0857641719756 steps/sec)\n",
      "Step #9416\tEpoch   3 Batch   40/3125   Loss: 0.832290 mae: 0.728232 (1004.0561502958807 steps/sec)\n",
      "Step #9417\tEpoch   3 Batch   41/3125   Loss: 0.760853 mae: 0.703728 (1645.9869711953536 steps/sec)\n",
      "Step #9418\tEpoch   3 Batch   42/3125   Loss: 0.724271 mae: 0.652865 (1439.9659431882944 steps/sec)\n",
      "Step #9419\tEpoch   3 Batch   43/3125   Loss: 0.819468 mae: 0.706726 (1574.7458212564013 steps/sec)\n",
      "Step #9420\tEpoch   3 Batch   44/3125   Loss: 0.678808 mae: 0.665625 (1511.2103939527142 steps/sec)\n",
      "Step #9421\tEpoch   3 Batch   45/3125   Loss: 0.741118 mae: 0.661858 (1599.5362672565022 steps/sec)\n",
      "Step #9422\tEpoch   3 Batch   46/3125   Loss: 0.845659 mae: 0.718480 (1549.9212901032467 steps/sec)\n",
      "Step #9423\tEpoch   3 Batch   47/3125   Loss: 0.921100 mae: 0.757191 (1126.404950021753 steps/sec)\n",
      "Step #9424\tEpoch   3 Batch   48/3125   Loss: 0.952963 mae: 0.794363 (1881.9678015686416 steps/sec)\n",
      "Step #9425\tEpoch   3 Batch   49/3125   Loss: 0.921800 mae: 0.745559 (1955.5688176053711 steps/sec)\n",
      "Step #9426\tEpoch   3 Batch   50/3125   Loss: 0.877804 mae: 0.730363 (1739.119473906806 steps/sec)\n",
      "Step #9427\tEpoch   3 Batch   51/3125   Loss: 0.828270 mae: 0.721443 (1973.9573234438683 steps/sec)\n",
      "Step #9428\tEpoch   3 Batch   52/3125   Loss: 0.792488 mae: 0.715137 (2140.977815891295 steps/sec)\n",
      "Step #9429\tEpoch   3 Batch   53/3125   Loss: 0.793176 mae: 0.701398 (1959.387467182405 steps/sec)\n",
      "Step #9430\tEpoch   3 Batch   54/3125   Loss: 0.770004 mae: 0.686116 (1959.552241595186 steps/sec)\n",
      "Step #9431\tEpoch   3 Batch   55/3125   Loss: 0.861169 mae: 0.730481 (2134.288622023204 steps/sec)\n",
      "Step #9432\tEpoch   3 Batch   56/3125   Loss: 0.826579 mae: 0.720757 (1674.8542495248134 steps/sec)\n",
      "Step #9433\tEpoch   3 Batch   57/3125   Loss: 0.661907 mae: 0.647096 (1667.662261240199 steps/sec)\n",
      "Step #9434\tEpoch   3 Batch   58/3125   Loss: 0.720411 mae: 0.692091 (2119.5557037890503 steps/sec)\n",
      "Step #9435\tEpoch   3 Batch   59/3125   Loss: 0.782667 mae: 0.709485 (2039.6343123905856 steps/sec)\n",
      "Step #9436\tEpoch   3 Batch   60/3125   Loss: 0.691315 mae: 0.653371 (2105.9760396059487 steps/sec)\n",
      "Step #9437\tEpoch   3 Batch   61/3125   Loss: 0.751841 mae: 0.698644 (2190.6717782118644 steps/sec)\n",
      "Step #9438\tEpoch   3 Batch   62/3125   Loss: 0.775151 mae: 0.704440 (2119.512860680176 steps/sec)\n",
      "Step #9439\tEpoch   3 Batch   63/3125   Loss: 0.638157 mae: 0.657184 (1981.5484626868492 steps/sec)\n",
      "Step #9440\tEpoch   3 Batch   64/3125   Loss: 0.733843 mae: 0.675122 (1507.6144467448814 steps/sec)\n",
      "Step #9441\tEpoch   3 Batch   65/3125   Loss: 0.883674 mae: 0.741713 (2173.4397346875326 steps/sec)\n",
      "Step #9442\tEpoch   3 Batch   66/3125   Loss: 0.841952 mae: 0.739158 (2027.3307296701596 steps/sec)\n",
      "Step #9443\tEpoch   3 Batch   67/3125   Loss: 0.815341 mae: 0.684689 (2282.7635002013735 steps/sec)\n",
      "Step #9444\tEpoch   3 Batch   68/3125   Loss: 0.774387 mae: 0.684244 (2224.1982012557273 steps/sec)\n",
      "Step #9445\tEpoch   3 Batch   69/3125   Loss: 0.747819 mae: 0.679431 (2149.514165060883 steps/sec)\n",
      "Step #9446\tEpoch   3 Batch   70/3125   Loss: 0.740056 mae: 0.688234 (2020.6891235643259 steps/sec)\n",
      "Step #9447\tEpoch   3 Batch   71/3125   Loss: 0.835261 mae: 0.735426 (2050.8639995305944 steps/sec)\n",
      "Step #9448\tEpoch   3 Batch   72/3125   Loss: 0.838810 mae: 0.703683 (1482.5367426143634 steps/sec)\n",
      "Step #9449\tEpoch   3 Batch   73/3125   Loss: 0.809679 mae: 0.702029 (2079.930178125124 steps/sec)\n",
      "Step #9450\tEpoch   3 Batch   74/3125   Loss: 0.868634 mae: 0.739000 (2067.4020840110807 steps/sec)\n",
      "Step #9451\tEpoch   3 Batch   75/3125   Loss: 0.783805 mae: 0.711378 (2017.1129578331795 steps/sec)\n",
      "Step #9452\tEpoch   3 Batch   76/3125   Loss: 0.735308 mae: 0.673523 (1886.301246649517 steps/sec)\n",
      "Step #9453\tEpoch   3 Batch   77/3125   Loss: 0.748863 mae: 0.660959 (2123.3111939089586 steps/sec)\n",
      "Step #9454\tEpoch   3 Batch   78/3125   Loss: 0.634319 mae: 0.617863 (1945.1393590873256 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9455\tEpoch   3 Batch   79/3125   Loss: 0.777150 mae: 0.700684 (1772.7404902789517 steps/sec)\n",
      "Step #9456\tEpoch   3 Batch   80/3125   Loss: 0.773107 mae: 0.711479 (1743.2540045386156 steps/sec)\n",
      "Step #9457\tEpoch   3 Batch   81/3125   Loss: 0.870145 mae: 0.726746 (2046.2215457268585 steps/sec)\n",
      "Step #9458\tEpoch   3 Batch   82/3125   Loss: 0.839759 mae: 0.743375 (2001.8250892499188 steps/sec)\n",
      "Step #9459\tEpoch   3 Batch   83/3125   Loss: 0.727201 mae: 0.668820 (2339.083395607705 steps/sec)\n",
      "Step #9460\tEpoch   3 Batch   84/3125   Loss: 0.815879 mae: 0.720447 (2220.641895826936 steps/sec)\n",
      "Step #9461\tEpoch   3 Batch   85/3125   Loss: 0.772113 mae: 0.685242 (2140.4096796252256 steps/sec)\n",
      "Step #9462\tEpoch   3 Batch   86/3125   Loss: 0.960349 mae: 0.779798 (2037.4941706824188 steps/sec)\n",
      "Step #9463\tEpoch   3 Batch   87/3125   Loss: 0.723050 mae: 0.653883 (2205.856613934702 steps/sec)\n",
      "Step #9464\tEpoch   3 Batch   88/3125   Loss: 1.088207 mae: 0.792168 (1766.9154941444099 steps/sec)\n",
      "Step #9465\tEpoch   3 Batch   89/3125   Loss: 0.847119 mae: 0.732787 (1944.5441732809138 steps/sec)\n",
      "Step #9466\tEpoch   3 Batch   90/3125   Loss: 0.879764 mae: 0.746318 (2354.0758368318257 steps/sec)\n",
      "Step #9467\tEpoch   3 Batch   91/3125   Loss: 0.851859 mae: 0.731247 (2248.7877585596793 steps/sec)\n",
      "Step #9468\tEpoch   3 Batch   92/3125   Loss: 0.938666 mae: 0.767371 (2243.471191082394 steps/sec)\n",
      "Step #9469\tEpoch   3 Batch   93/3125   Loss: 0.826627 mae: 0.728692 (2023.1258260257189 steps/sec)\n",
      "Step #9470\tEpoch   3 Batch   94/3125   Loss: 0.838665 mae: 0.728474 (2164.8914535825998 steps/sec)\n",
      "Step #9471\tEpoch   3 Batch   95/3125   Loss: 0.695231 mae: 0.643290 (2206.3207507469597 steps/sec)\n",
      "Step #9472\tEpoch   3 Batch   96/3125   Loss: 0.606822 mae: 0.621161 (2153.7747378582944 steps/sec)\n",
      "Step #9473\tEpoch   3 Batch   97/3125   Loss: 0.850138 mae: 0.744370 (1850.7439504386043 steps/sec)\n",
      "Step #9474\tEpoch   3 Batch   98/3125   Loss: 0.771206 mae: 0.715835 (2041.9777609004693 steps/sec)\n",
      "Step #9475\tEpoch   3 Batch   99/3125   Loss: 1.028542 mae: 0.814331 (2094.324661706696 steps/sec)\n",
      "Step #9476\tEpoch   3 Batch  100/3125   Loss: 0.908894 mae: 0.767984 (2069.013417521705 steps/sec)\n",
      "Step #9477\tEpoch   3 Batch  101/3125   Loss: 0.791198 mae: 0.693123 (2097.508576457998 steps/sec)\n",
      "Step #9478\tEpoch   3 Batch  102/3125   Loss: 0.872205 mae: 0.738938 (2121.421056890831 steps/sec)\n",
      "Step #9479\tEpoch   3 Batch  103/3125   Loss: 0.810383 mae: 0.699672 (2107.7540026332454 steps/sec)\n",
      "Step #9480\tEpoch   3 Batch  104/3125   Loss: 0.822479 mae: 0.725443 (1889.7176892509258 steps/sec)\n",
      "Step #9481\tEpoch   3 Batch  105/3125   Loss: 0.683805 mae: 0.645618 (1955.331785591079 steps/sec)\n",
      "Step #9482\tEpoch   3 Batch  106/3125   Loss: 0.740438 mae: 0.692512 (1935.8743111390093 steps/sec)\n",
      "Step #9483\tEpoch   3 Batch  107/3125   Loss: 0.921215 mae: 0.751837 (1933.1262386505048 steps/sec)\n",
      "Step #9484\tEpoch   3 Batch  108/3125   Loss: 0.773202 mae: 0.694627 (1867.0726387293787 steps/sec)\n",
      "Step #9485\tEpoch   3 Batch  109/3125   Loss: 0.885543 mae: 0.713998 (2077.02562172548 steps/sec)\n",
      "Step #9486\tEpoch   3 Batch  110/3125   Loss: 0.695978 mae: 0.675863 (2162.5251348257834 steps/sec)\n",
      "Step #9487\tEpoch   3 Batch  111/3125   Loss: 0.769095 mae: 0.700402 (2006.844019138756 steps/sec)\n",
      "Step #9488\tEpoch   3 Batch  112/3125   Loss: 0.784650 mae: 0.697701 (2020.7864789600978 steps/sec)\n",
      "Step #9489\tEpoch   3 Batch  113/3125   Loss: 0.856176 mae: 0.724390 (1785.5852327393166 steps/sec)\n",
      "Step #9490\tEpoch   3 Batch  114/3125   Loss: 0.900333 mae: 0.750834 (1861.9834857498001 steps/sec)\n",
      "Step #9491\tEpoch   3 Batch  115/3125   Loss: 0.803442 mae: 0.715917 (1805.5859765127252 steps/sec)\n",
      "Step #9492\tEpoch   3 Batch  116/3125   Loss: 0.926465 mae: 0.760566 (2280.7277789257323 steps/sec)\n",
      "Step #9493\tEpoch   3 Batch  117/3125   Loss: 0.846657 mae: 0.713811 (2129.1315559706795 steps/sec)\n",
      "Step #9494\tEpoch   3 Batch  118/3125   Loss: 0.751290 mae: 0.698670 (1999.4393966840505 steps/sec)\n",
      "Step #9495\tEpoch   3 Batch  119/3125   Loss: 0.804553 mae: 0.721598 (2157.319644896154 steps/sec)\n",
      "Step #9496\tEpoch   3 Batch  120/3125   Loss: 0.894676 mae: 0.761527 (1940.9268017288452 steps/sec)\n",
      "Step #9497\tEpoch   3 Batch  121/3125   Loss: 0.977085 mae: 0.811213 (1854.8033891709267 steps/sec)\n",
      "Step #9498\tEpoch   3 Batch  122/3125   Loss: 1.024606 mae: 0.827313 (1726.5199601537865 steps/sec)\n",
      "Step #9499\tEpoch   3 Batch  123/3125   Loss: 0.799301 mae: 0.711612 (2060.1516759008214 steps/sec)\n",
      "Step #9500\tEpoch   3 Batch  124/3125   Loss: 0.691856 mae: 0.679017 (2112.7440510970964 steps/sec)\n",
      "Step #9501\tEpoch   3 Batch  125/3125   Loss: 0.814389 mae: 0.740047 (2144.173729896633 steps/sec)\n",
      "Step #9502\tEpoch   3 Batch  126/3125   Loss: 0.901681 mae: 0.748779 (2023.88728044779 steps/sec)\n",
      "Step #9503\tEpoch   3 Batch  127/3125   Loss: 0.916382 mae: 0.761439 (2141.699346405229 steps/sec)\n",
      "Step #9504\tEpoch   3 Batch  128/3125   Loss: 0.862805 mae: 0.743326 (2140.4970655779534 steps/sec)\n",
      "Step #9505\tEpoch   3 Batch  129/3125   Loss: 0.945496 mae: 0.789222 (2012.7184605787227 steps/sec)\n",
      "Step #9506\tEpoch   3 Batch  130/3125   Loss: 0.760950 mae: 0.692738 (1845.4996655989298 steps/sec)\n",
      "Step #9507\tEpoch   3 Batch  131/3125   Loss: 0.798781 mae: 0.707149 (1931.470463629833 steps/sec)\n",
      "Step #9508\tEpoch   3 Batch  132/3125   Loss: 0.831411 mae: 0.721670 (2011.8881789750378 steps/sec)\n",
      "Step #9509\tEpoch   3 Batch  133/3125   Loss: 0.826506 mae: 0.724404 (2208.597847378731 steps/sec)\n",
      "Step #9510\tEpoch   3 Batch  134/3125   Loss: 0.788381 mae: 0.684906 (2173.7100686166796 steps/sec)\n",
      "Step #9511\tEpoch   3 Batch  135/3125   Loss: 0.767971 mae: 0.732427 (2075.7099165619156 steps/sec)\n",
      "Step #9512\tEpoch   3 Batch  136/3125   Loss: 0.710920 mae: 0.683357 (2031.120279706734 steps/sec)\n",
      "Step #9513\tEpoch   3 Batch  137/3125   Loss: 0.702481 mae: 0.660604 (1970.211286791992 steps/sec)\n",
      "Step #9514\tEpoch   3 Batch  138/3125   Loss: 0.774279 mae: 0.706271 (1753.0171945398768 steps/sec)\n",
      "Step #9515\tEpoch   3 Batch  139/3125   Loss: 0.895253 mae: 0.758598 (1999.7635167350052 steps/sec)\n",
      "Step #9516\tEpoch   3 Batch  140/3125   Loss: 0.846330 mae: 0.727503 (2080.7143565829942 steps/sec)\n",
      "Step #9517\tEpoch   3 Batch  141/3125   Loss: 0.806974 mae: 0.731004 (2116.0483114209896 steps/sec)\n",
      "Step #9518\tEpoch   3 Batch  142/3125   Loss: 0.783214 mae: 0.708282 (2111.9142808229526 steps/sec)\n",
      "Step #9519\tEpoch   3 Batch  143/3125   Loss: 0.789801 mae: 0.695610 (1948.79057362958 steps/sec)\n",
      "Step #9520\tEpoch   3 Batch  144/3125   Loss: 0.673535 mae: 0.640991 (2146.565948125857 steps/sec)\n",
      "Step #9521\tEpoch   3 Batch  145/3125   Loss: 0.826306 mae: 0.709647 (2132.2704950535317 steps/sec)\n",
      "Step #9522\tEpoch   3 Batch  146/3125   Loss: 0.703411 mae: 0.676352 (1685.556064588206 steps/sec)\n",
      "Step #9523\tEpoch   3 Batch  147/3125   Loss: 0.894304 mae: 0.748665 (2080.735000843346 steps/sec)\n",
      "Step #9524\tEpoch   3 Batch  148/3125   Loss: 0.679741 mae: 0.663418 (2267.9759484362157 steps/sec)\n",
      "Step #9525\tEpoch   3 Batch  149/3125   Loss: 0.775579 mae: 0.701305 (2043.0718871472134 steps/sec)\n",
      "Step #9526\tEpoch   3 Batch  150/3125   Loss: 0.830394 mae: 0.715707 (2140.846680754193 steps/sec)\n",
      "Step #9527\tEpoch   3 Batch  151/3125   Loss: 0.830928 mae: 0.711407 (2145.182639293788 steps/sec)\n",
      "Step #9528\tEpoch   3 Batch  152/3125   Loss: 0.714735 mae: 0.675232 (2027.7423783877862 steps/sec)\n",
      "Step #9529\tEpoch   3 Batch  153/3125   Loss: 0.956236 mae: 0.773816 (1976.599213941696 steps/sec)\n",
      "Step #9530\tEpoch   3 Batch  154/3125   Loss: 0.865941 mae: 0.743669 (1833.110730394042 steps/sec)\n",
      "Step #9531\tEpoch   3 Batch  155/3125   Loss: 0.766861 mae: 0.698250 (1887.5236260868 steps/sec)\n",
      "Step #9532\tEpoch   3 Batch  156/3125   Loss: 0.917136 mae: 0.759697 (2130.2778201025953 steps/sec)\n",
      "Step #9533\tEpoch   3 Batch  157/3125   Loss: 0.839798 mae: 0.729676 (1999.687243740107 steps/sec)\n",
      "Step #9534\tEpoch   3 Batch  158/3125   Loss: 0.784605 mae: 0.706002 (2068.584843313836 steps/sec)\n",
      "Step #9535\tEpoch   3 Batch  159/3125   Loss: 0.745721 mae: 0.672439 (2191.6333120839386 steps/sec)\n",
      "Step #9536\tEpoch   3 Batch  160/3125   Loss: 0.715772 mae: 0.637648 (2035.8129556463748 steps/sec)\n",
      "Step #9537\tEpoch   3 Batch  161/3125   Loss: 0.755280 mae: 0.664440 (2080.9208176225443 steps/sec)\n",
      "Step #9538\tEpoch   3 Batch  162/3125   Loss: 0.860313 mae: 0.759755 (1818.439740910628 steps/sec)\n",
      "Step #9539\tEpoch   3 Batch  163/3125   Loss: 0.783596 mae: 0.701421 (1951.0750137225898 steps/sec)\n",
      "Step #9540\tEpoch   3 Batch  164/3125   Loss: 0.802943 mae: 0.702082 (2209.435512758381 steps/sec)\n",
      "Step #9541\tEpoch   3 Batch  165/3125   Loss: 0.784721 mae: 0.708292 (2220.054200542005 steps/sec)\n",
      "Step #9542\tEpoch   3 Batch  166/3125   Loss: 0.954641 mae: 0.768192 (2213.5399294927274 steps/sec)\n",
      "Step #9543\tEpoch   3 Batch  167/3125   Loss: 0.815641 mae: 0.715546 (2107.3304058603053 steps/sec)\n",
      "Step #9544\tEpoch   3 Batch  168/3125   Loss: 0.973676 mae: 0.794736 (2224.9061087653035 steps/sec)\n",
      "Step #9545\tEpoch   3 Batch  169/3125   Loss: 0.933242 mae: 0.747689 (2001.366594774111 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9546\tEpoch   3 Batch  170/3125   Loss: 0.834494 mae: 0.735194 (1695.833097481098 steps/sec)\n",
      "Step #9547\tEpoch   3 Batch  171/3125   Loss: 0.875225 mae: 0.750821 (1784.126930111872 steps/sec)\n",
      "Step #9548\tEpoch   3 Batch  172/3125   Loss: 0.686493 mae: 0.663897 (2047.5800861151522 steps/sec)\n",
      "Step #9549\tEpoch   3 Batch  173/3125   Loss: 0.792873 mae: 0.696208 (1989.9532200366268 steps/sec)\n",
      "Step #9550\tEpoch   3 Batch  174/3125   Loss: 0.739317 mae: 0.669671 (2226.394182281437 steps/sec)\n",
      "Step #9551\tEpoch   3 Batch  175/3125   Loss: 0.809862 mae: 0.726460 (2117.543948221372 steps/sec)\n",
      "Step #9552\tEpoch   3 Batch  176/3125   Loss: 0.863037 mae: 0.733622 (2133.0078621629596 steps/sec)\n",
      "Step #9553\tEpoch   3 Batch  177/3125   Loss: 0.661493 mae: 0.639379 (2128.4832737901916 steps/sec)\n",
      "Step #9554\tEpoch   3 Batch  178/3125   Loss: 0.696951 mae: 0.678769 (2074.950034629465 steps/sec)\n",
      "Step #9555\tEpoch   3 Batch  179/3125   Loss: 0.837953 mae: 0.725048 (1592.0562379484688 steps/sec)\n",
      "Step #9556\tEpoch   3 Batch  180/3125   Loss: 0.799693 mae: 0.701552 (2025.509721162483 steps/sec)\n",
      "Step #9557\tEpoch   3 Batch  181/3125   Loss: 0.760682 mae: 0.690552 (2097.592494423829 steps/sec)\n",
      "Step #9558\tEpoch   3 Batch  182/3125   Loss: 0.826990 mae: 0.730212 (1879.4884433729758 steps/sec)\n",
      "Step #9559\tEpoch   3 Batch  183/3125   Loss: 0.778197 mae: 0.696901 (2246.4511429612016 steps/sec)\n",
      "Step #9560\tEpoch   3 Batch  184/3125   Loss: 0.949292 mae: 0.781764 (2217.4954796823617 steps/sec)\n",
      "Step #9561\tEpoch   3 Batch  185/3125   Loss: 0.695489 mae: 0.652242 (2021.0006938555239 steps/sec)\n",
      "Step #9562\tEpoch   3 Batch  186/3125   Loss: 0.792447 mae: 0.677722 (2046.2215457268585 steps/sec)\n",
      "Step #9563\tEpoch   3 Batch  187/3125   Loss: 0.824477 mae: 0.724159 (1984.360925021763 steps/sec)\n",
      "Step #9564\tEpoch   3 Batch  188/3125   Loss: 0.792725 mae: 0.699520 (1920.9966107905102 steps/sec)\n",
      "Step #9565\tEpoch   3 Batch  189/3125   Loss: 0.880213 mae: 0.736572 (1714.3819434793627 steps/sec)\n",
      "Step #9566\tEpoch   3 Batch  190/3125   Loss: 0.749056 mae: 0.700963 (1768.0476166388453 steps/sec)\n",
      "Step #9567\tEpoch   3 Batch  191/3125   Loss: 0.837324 mae: 0.731949 (1895.7477581718254 steps/sec)\n",
      "Step #9568\tEpoch   3 Batch  192/3125   Loss: 0.924824 mae: 0.740882 (1956.4080079108905 steps/sec)\n",
      "Step #9569\tEpoch   3 Batch  193/3125   Loss: 0.858585 mae: 0.719535 (1767.5409614995617 steps/sec)\n",
      "Step #9570\tEpoch   3 Batch  194/3125   Loss: 0.798951 mae: 0.688820 (1880.4153291609132 steps/sec)\n",
      "Step #9571\tEpoch   3 Batch  195/3125   Loss: 0.848113 mae: 0.725451 (1860.7444212767846 steps/sec)\n",
      "Step #9572\tEpoch   3 Batch  196/3125   Loss: 0.879821 mae: 0.762559 (1737.1458864848746 steps/sec)\n",
      "Step #9573\tEpoch   3 Batch  197/3125   Loss: 0.785587 mae: 0.708571 (1975.8915364104882 steps/sec)\n",
      "Step #9574\tEpoch   3 Batch  198/3125   Loss: 0.918130 mae: 0.784928 (2073.4729390362063 steps/sec)\n",
      "Step #9575\tEpoch   3 Batch  199/3125   Loss: 0.873219 mae: 0.731120 (2081.0860159568133 steps/sec)\n",
      "Step #9576\tEpoch   3 Batch  200/3125   Loss: 0.968125 mae: 0.748900 (2165.2267283389774 steps/sec)\n",
      "Step #9577\tEpoch   3 Batch  201/3125   Loss: 0.849603 mae: 0.722582 (2016.4535297398127 steps/sec)\n",
      "Step #9578\tEpoch   3 Batch  202/3125   Loss: 0.904605 mae: 0.758839 (2216.3237267894697 steps/sec)\n",
      "Step #9579\tEpoch   3 Batch  203/3125   Loss: 0.891842 mae: 0.730573 (2103.377998876675 steps/sec)\n",
      "Step #9580\tEpoch   3 Batch  204/3125   Loss: 0.755110 mae: 0.678193 (2116.689039837702 steps/sec)\n",
      "Step #9581\tEpoch   3 Batch  205/3125   Loss: 0.705773 mae: 0.656147 (1629.8306560039791 steps/sec)\n",
      "Step #9582\tEpoch   3 Batch  206/3125   Loss: 0.847192 mae: 0.724273 (2069.625974538636 steps/sec)\n",
      "Step #9583\tEpoch   3 Batch  207/3125   Loss: 0.846643 mae: 0.707481 (1911.8201542472698 steps/sec)\n",
      "Step #9584\tEpoch   3 Batch  208/3125   Loss: 0.829704 mae: 0.705952 (2072.0184166065624 steps/sec)\n",
      "Step #9585\tEpoch   3 Batch  209/3125   Loss: 0.708894 mae: 0.679994 (2231.15518011788 steps/sec)\n",
      "Step #9586\tEpoch   3 Batch  210/3125   Loss: 0.764015 mae: 0.684281 (2107.478645362275 steps/sec)\n",
      "Step #9587\tEpoch   3 Batch  211/3125   Loss: 0.801113 mae: 0.694718 (1981.3050913110433 steps/sec)\n",
      "Step #9588\tEpoch   3 Batch  212/3125   Loss: 0.868480 mae: 0.741680 (2173.056876703244 steps/sec)\n",
      "Step #9589\tEpoch   3 Batch  213/3125   Loss: 0.706257 mae: 0.667263 (2177.6827064858467 steps/sec)\n",
      "Step #9590\tEpoch   3 Batch  214/3125   Loss: 0.883267 mae: 0.731536 (1631.529730276414 steps/sec)\n",
      "Step #9591\tEpoch   3 Batch  215/3125   Loss: 0.846539 mae: 0.726423 (1832.1658527209665 steps/sec)\n",
      "Step #9592\tEpoch   3 Batch  216/3125   Loss: 0.865026 mae: 0.747062 (2069.442169352372 steps/sec)\n",
      "Step #9593\tEpoch   3 Batch  217/3125   Loss: 0.760886 mae: 0.684405 (1992.2784617723057 steps/sec)\n",
      "Step #9594\tEpoch   3 Batch  218/3125   Loss: 0.818449 mae: 0.732258 (2043.4103088765469 steps/sec)\n",
      "Step #9595\tEpoch   3 Batch  219/3125   Loss: 0.990694 mae: 0.779382 (2027.2327427041344 steps/sec)\n",
      "Step #9596\tEpoch   3 Batch  220/3125   Loss: 0.804066 mae: 0.697966 (1988.9340958450698 steps/sec)\n",
      "Step #9597\tEpoch   3 Batch  221/3125   Loss: 0.727279 mae: 0.656790 (1842.2249161088564 steps/sec)\n",
      "Step #9598\tEpoch   3 Batch  222/3125   Loss: 0.865706 mae: 0.754813 (2041.4212012070475 steps/sec)\n",
      "Step #9599\tEpoch   3 Batch  223/3125   Loss: 0.884022 mae: 0.729517 (1790.3106565703993 steps/sec)\n",
      "Step #9600\tEpoch   3 Batch  224/3125   Loss: 0.600832 mae: 0.621797 (2108.6441104016894 steps/sec)\n",
      "Step #9601\tEpoch   3 Batch  225/3125   Loss: 0.720905 mae: 0.668475 (1903.9055832955062 steps/sec)\n",
      "Step #9602\tEpoch   3 Batch  226/3125   Loss: 0.764113 mae: 0.685907 (1821.282360808358 steps/sec)\n",
      "Step #9603\tEpoch   3 Batch  227/3125   Loss: 0.833275 mae: 0.741607 (1794.3853584660271 steps/sec)\n",
      "Step #9604\tEpoch   3 Batch  228/3125   Loss: 0.916608 mae: 0.757880 (1890.859255252006 steps/sec)\n",
      "Step #9605\tEpoch   3 Batch  229/3125   Loss: 0.789971 mae: 0.681493 (2185.6944835277072 steps/sec)\n",
      "Step #9606\tEpoch   3 Batch  230/3125   Loss: 0.862863 mae: 0.746198 (1717.963169277148 steps/sec)\n",
      "Step #9607\tEpoch   3 Batch  231/3125   Loss: 0.735183 mae: 0.660833 (1913.3027397385251 steps/sec)\n",
      "Step #9608\tEpoch   3 Batch  232/3125   Loss: 0.769130 mae: 0.711164 (2068.07486736485 steps/sec)\n",
      "Step #9609\tEpoch   3 Batch  233/3125   Loss: 0.717626 mae: 0.684215 (1990.3687182650785 steps/sec)\n",
      "Step #9610\tEpoch   3 Batch  234/3125   Loss: 0.760762 mae: 0.687456 (2008.6123668684393 steps/sec)\n",
      "Step #9611\tEpoch   3 Batch  235/3125   Loss: 0.797130 mae: 0.711304 (2161.4330179539506 steps/sec)\n",
      "Step #9612\tEpoch   3 Batch  236/3125   Loss: 0.755780 mae: 0.700207 (2143.056265200597 steps/sec)\n",
      "Step #9613\tEpoch   3 Batch  237/3125   Loss: 0.859827 mae: 0.716169 (2144.173729896633 steps/sec)\n",
      "Step #9614\tEpoch   3 Batch  238/3125   Loss: 0.670317 mae: 0.633664 (2047.6600564359433 steps/sec)\n",
      "Step #9615\tEpoch   3 Batch  239/3125   Loss: 0.779019 mae: 0.714175 (1655.4197846610464 steps/sec)\n",
      "Step #9616\tEpoch   3 Batch  240/3125   Loss: 0.883573 mae: 0.729610 (1862.26456980988 steps/sec)\n",
      "Step #9617\tEpoch   3 Batch  241/3125   Loss: 0.734123 mae: 0.685180 (2094.7430454976775 steps/sec)\n",
      "Step #9618\tEpoch   3 Batch  242/3125   Loss: 0.772396 mae: 0.684013 (2043.4103088765469 steps/sec)\n",
      "Step #9619\tEpoch   3 Batch  243/3125   Loss: 0.836897 mae: 0.728170 (2125.570882701722 steps/sec)\n",
      "Step #9620\tEpoch   3 Batch  244/3125   Loss: 0.778846 mae: 0.693696 (2125.6786069046607 steps/sec)\n",
      "Step #9621\tEpoch   3 Batch  245/3125   Loss: 0.947705 mae: 0.778760 (1969.8411654753295 steps/sec)\n",
      "Step #9622\tEpoch   3 Batch  246/3125   Loss: 0.966455 mae: 0.790735 (1901.799187463726 steps/sec)\n",
      "Step #9623\tEpoch   3 Batch  247/3125   Loss: 0.803153 mae: 0.713206 (2067.4632280452697 steps/sec)\n",
      "Step #9624\tEpoch   3 Batch  248/3125   Loss: 0.742085 mae: 0.684306 (1720.7542215731 steps/sec)\n",
      "Step #9625\tEpoch   3 Batch  249/3125   Loss: 0.817475 mae: 0.691934 (1895.7306214689265 steps/sec)\n",
      "Step #9626\tEpoch   3 Batch  250/3125   Loss: 0.773395 mae: 0.691382 (2134.1800234060956 steps/sec)\n",
      "Step #9627\tEpoch   3 Batch  251/3125   Loss: 0.891645 mae: 0.746229 (2145.182639293788 steps/sec)\n",
      "Step #9628\tEpoch   3 Batch  252/3125   Loss: 0.818840 mae: 0.704768 (2225.6853276731226 steps/sec)\n",
      "Step #9629\tEpoch   3 Batch  253/3125   Loss: 0.896251 mae: 0.721941 (2318.6529127555364 steps/sec)\n",
      "Step #9630\tEpoch   3 Batch  254/3125   Loss: 0.783565 mae: 0.723167 (2241.2174582139955 steps/sec)\n",
      "Step #9631\tEpoch   3 Batch  255/3125   Loss: 0.956679 mae: 0.790097 (2062.9889037538364 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9632\tEpoch   3 Batch  256/3125   Loss: 0.803617 mae: 0.710101 (1939.2576427290044 steps/sec)\n",
      "Step #9633\tEpoch   3 Batch  257/3125   Loss: 0.847160 mae: 0.734006 (1866.0592255125284 steps/sec)\n",
      "Step #9634\tEpoch   3 Batch  258/3125   Loss: 0.946279 mae: 0.774826 (2192.4123150906903 steps/sec)\n",
      "Step #9635\tEpoch   3 Batch  259/3125   Loss: 0.753309 mae: 0.673387 (2011.7530816825747 steps/sec)\n",
      "Step #9636\tEpoch   3 Batch  260/3125   Loss: 0.911131 mae: 0.745518 (2076.593722150708 steps/sec)\n",
      "Step #9637\tEpoch   3 Batch  261/3125   Loss: 1.004031 mae: 0.775501 (2102.59772811582 steps/sec)\n",
      "Step #9638\tEpoch   3 Batch  262/3125   Loss: 0.721557 mae: 0.661870 (2079.930178125124 steps/sec)\n",
      "Step #9639\tEpoch   3 Batch  263/3125   Loss: 0.716713 mae: 0.676375 (2220.830024038716 steps/sec)\n",
      "Step #9640\tEpoch   3 Batch  264/3125   Loss: 0.838713 mae: 0.744672 (2046.9005905031477 steps/sec)\n",
      "Step #9641\tEpoch   3 Batch  265/3125   Loss: 0.760889 mae: 0.684879 (1994.0212223785798 steps/sec)\n",
      "Step #9642\tEpoch   3 Batch  266/3125   Loss: 0.749498 mae: 0.687914 (1890.859255252006 steps/sec)\n",
      "Step #9643\tEpoch   3 Batch  267/3125   Loss: 0.855219 mae: 0.734146 (2178.813946723185 steps/sec)\n",
      "Step #9644\tEpoch   3 Batch  268/3125   Loss: 0.905451 mae: 0.727930 (2323.894374078876 steps/sec)\n",
      "Step #9645\tEpoch   3 Batch  269/3125   Loss: 0.839457 mae: 0.726419 (2162.235281987834 steps/sec)\n",
      "Step #9646\tEpoch   3 Batch  270/3125   Loss: 0.659799 mae: 0.648380 (2239.0399624184574 steps/sec)\n",
      "Step #9647\tEpoch   3 Batch  271/3125   Loss: 0.808308 mae: 0.709071 (2265.4279911852395 steps/sec)\n",
      "Step #9648\tEpoch   3 Batch  272/3125   Loss: 0.838226 mae: 0.736830 (1969.1937876767638 steps/sec)\n",
      "Step #9649\tEpoch   3 Batch  273/3125   Loss: 0.690749 mae: 0.642827 (2261.909487035679 steps/sec)\n",
      "Step #9650\tEpoch   3 Batch  274/3125   Loss: 0.823346 mae: 0.728275 (1595.5932254972076 steps/sec)\n",
      "Step #9651\tEpoch   3 Batch  275/3125   Loss: 0.722131 mae: 0.680535 (1800.2386409483831 steps/sec)\n",
      "Step #9652\tEpoch   3 Batch  276/3125   Loss: 0.820416 mae: 0.699689 (2062.278864402946 steps/sec)\n",
      "Step #9653\tEpoch   3 Batch  277/3125   Loss: 0.775895 mae: 0.692421 (1851.3648080793814 steps/sec)\n",
      "Step #9654\tEpoch   3 Batch  278/3125   Loss: 0.746441 mae: 0.681213 (2075.5455706099506 steps/sec)\n",
      "Step #9655\tEpoch   3 Batch  279/3125   Loss: 0.807872 mae: 0.693669 (2101.986569108951 steps/sec)\n",
      "Step #9656\tEpoch   3 Batch  280/3125   Loss: 0.875490 mae: 0.737394 (1921.0845967114 steps/sec)\n",
      "Step #9657\tEpoch   3 Batch  281/3125   Loss: 0.845971 mae: 0.719862 (2029.9799630235507 steps/sec)\n",
      "Step #9658\tEpoch   3 Batch  282/3125   Loss: 0.925166 mae: 0.759939 (1664.2214357134922 steps/sec)\n",
      "Step #9659\tEpoch   3 Batch  283/3125   Loss: 0.751792 mae: 0.690406 (1960.303231414924 steps/sec)\n",
      "Step #9660\tEpoch   3 Batch  284/3125   Loss: 0.868723 mae: 0.727417 (2032.124031007752 steps/sec)\n",
      "Step #9661\tEpoch   3 Batch  285/3125   Loss: 0.810719 mae: 0.702639 (2185.216213400021 steps/sec)\n",
      "Step #9662\tEpoch   3 Batch  286/3125   Loss: 0.859961 mae: 0.707275 (2122.1724127462785 steps/sec)\n",
      "Step #9663\tEpoch   3 Batch  287/3125   Loss: 0.763572 mae: 0.704465 (2016.763795126267 steps/sec)\n",
      "Step #9664\tEpoch   3 Batch  288/3125   Loss: 0.881686 mae: 0.737683 (2061.771992606866 steps/sec)\n",
      "Step #9665\tEpoch   3 Batch  289/3125   Loss: 0.767682 mae: 0.689591 (2006.2872504281108 steps/sec)\n",
      "Step #9666\tEpoch   3 Batch  290/3125   Loss: 0.940724 mae: 0.793691 (2248.9324511265295 steps/sec)\n",
      "Step #9667\tEpoch   3 Batch  291/3125   Loss: 0.725913 mae: 0.668353 (1993.4715449472915 steps/sec)\n",
      "Step #9668\tEpoch   3 Batch  292/3125   Loss: 0.903037 mae: 0.745203 (1865.6110167154459 steps/sec)\n",
      "Step #9669\tEpoch   3 Batch  293/3125   Loss: 0.959129 mae: 0.767859 (1913.2852841893987 steps/sec)\n",
      "Step #9670\tEpoch   3 Batch  294/3125   Loss: 0.789386 mae: 0.704497 (2164.690338563171 steps/sec)\n",
      "Step #9671\tEpoch   3 Batch  295/3125   Loss: 0.737623 mae: 0.658930 (2156.254948128194 steps/sec)\n",
      "Step #9672\tEpoch   3 Batch  296/3125   Loss: 0.902349 mae: 0.744949 (2209.1562203729063 steps/sec)\n",
      "Step #9673\tEpoch   3 Batch  297/3125   Loss: 0.795831 mae: 0.714084 (2166.3003057598544 steps/sec)\n",
      "Step #9674\tEpoch   3 Batch  298/3125   Loss: 0.815956 mae: 0.702387 (2061.609846250639 steps/sec)\n",
      "Step #9675\tEpoch   3 Batch  299/3125   Loss: 0.783951 mae: 0.710847 (2065.528754764555 steps/sec)\n",
      "Step #9676\tEpoch   3 Batch  300/3125   Loss: 0.978065 mae: 0.792883 (1736.4123369902711 steps/sec)\n",
      "Step #9677\tEpoch   3 Batch  301/3125   Loss: 0.841278 mae: 0.732762 (1974.328993325237 steps/sec)\n",
      "Step #9678\tEpoch   3 Batch  302/3125   Loss: 0.766340 mae: 0.705978 (2131.8369877913656 steps/sec)\n",
      "Step #9679\tEpoch   3 Batch  303/3125   Loss: 0.813216 mae: 0.725886 (2120.370052070168 steps/sec)\n",
      "Step #9680\tEpoch   3 Batch  304/3125   Loss: 0.867251 mae: 0.741403 (2147.643089022929 steps/sec)\n",
      "Step #9681\tEpoch   3 Batch  305/3125   Loss: 0.724900 mae: 0.661924 (2111.4890103804837 steps/sec)\n",
      "Step #9682\tEpoch   3 Batch  306/3125   Loss: 0.807027 mae: 0.707920 (2157.874590990472 steps/sec)\n",
      "Step #9683\tEpoch   3 Batch  307/3125   Loss: 0.770428 mae: 0.703029 (2044.326600639476 steps/sec)\n",
      "Step #9684\tEpoch   3 Batch  308/3125   Loss: 0.712341 mae: 0.682724 (2105.4686009738466 steps/sec)\n",
      "Step #9685\tEpoch   3 Batch  309/3125   Loss: 0.740657 mae: 0.688482 (1919.8359515178147 steps/sec)\n",
      "Step #9686\tEpoch   3 Batch  310/3125   Loss: 0.895819 mae: 0.748664 (2090.837670235888 steps/sec)\n",
      "Step #9687\tEpoch   3 Batch  311/3125   Loss: 0.906130 mae: 0.739705 (2197.0289354034403 steps/sec)\n",
      "Step #9688\tEpoch   3 Batch  312/3125   Loss: 0.763889 mae: 0.697501 (2240.331592047773 steps/sec)\n",
      "Step #9689\tEpoch   3 Batch  313/3125   Loss: 0.826740 mae: 0.697254 (2164.3999050499006 steps/sec)\n",
      "Step #9690\tEpoch   3 Batch  314/3125   Loss: 0.949475 mae: 0.776909 (2200.0482569789033 steps/sec)\n",
      "Step #9691\tEpoch   3 Batch  315/3125   Loss: 0.797498 mae: 0.699236 (2051.8672889332433 steps/sec)\n",
      "Step #9692\tEpoch   3 Batch  316/3125   Loss: 0.745109 mae: 0.696501 (2267.7797482590076 steps/sec)\n",
      "Step #9693\tEpoch   3 Batch  317/3125   Loss: 0.944569 mae: 0.764914 (2262.9345879103093 steps/sec)\n",
      "Step #9694\tEpoch   3 Batch  318/3125   Loss: 0.788422 mae: 0.700744 (1742.703529196686 steps/sec)\n",
      "Step #9695\tEpoch   3 Batch  319/3125   Loss: 0.904670 mae: 0.760835 (2078.322399064476 steps/sec)\n",
      "Step #9696\tEpoch   3 Batch  320/3125   Loss: 0.942615 mae: 0.766361 (2116.4113432233326 steps/sec)\n",
      "Step #9697\tEpoch   3 Batch  321/3125   Loss: 0.846002 mae: 0.719585 (2273.26157414935 steps/sec)\n",
      "Step #9698\tEpoch   3 Batch  322/3125   Loss: 0.672457 mae: 0.650507 (2267.4609953616105 steps/sec)\n",
      "Step #9699\tEpoch   3 Batch  323/3125   Loss: 0.790553 mae: 0.686009 (2055.245543370672 steps/sec)\n",
      "Step #9700\tEpoch   3 Batch  324/3125   Loss: 0.765681 mae: 0.699495 (2125.6570611905654 steps/sec)\n",
      "Step #9701\tEpoch   3 Batch  325/3125   Loss: 0.805362 mae: 0.711324 (2174.9499600717672 steps/sec)\n",
      "Step #9702\tEpoch   3 Batch  326/3125   Loss: 0.728492 mae: 0.647313 (2146.01680259509 steps/sec)\n",
      "Step #9703\tEpoch   3 Batch  327/3125   Loss: 0.945724 mae: 0.787076 (1892.4290277752712 steps/sec)\n",
      "Step #9704\tEpoch   3 Batch  328/3125   Loss: 0.801311 mae: 0.704374 (2066.8926910037057 steps/sec)\n",
      "Step #9705\tEpoch   3 Batch  329/3125   Loss: 0.916709 mae: 0.761343 (2269.227523074759 steps/sec)\n",
      "Step #9706\tEpoch   3 Batch  330/3125   Loss: 0.874206 mae: 0.742827 (2101.3757652882296 steps/sec)\n",
      "Step #9707\tEpoch   3 Batch  331/3125   Loss: 0.807607 mae: 0.710806 (2227.552949673911 steps/sec)\n",
      "Step #9708\tEpoch   3 Batch  332/3125   Loss: 0.833234 mae: 0.715355 (2162.324459200297 steps/sec)\n",
      "Step #9709\tEpoch   3 Batch  333/3125   Loss: 0.868933 mae: 0.749608 (2103.842218254048 steps/sec)\n",
      "Step #9710\tEpoch   3 Batch  334/3125   Loss: 0.823508 mae: 0.707093 (2166.7255576563452 steps/sec)\n",
      "Step #9711\tEpoch   3 Batch  335/3125   Loss: 0.731524 mae: 0.679262 (2003.3549225272732 steps/sec)\n",
      "Step #9712\tEpoch   3 Batch  336/3125   Loss: 0.678810 mae: 0.662450 (1850.1398311439686 steps/sec)\n",
      "Step #9713\tEpoch   3 Batch  337/3125   Loss: 0.711101 mae: 0.650159 (2006.9976648930062 steps/sec)\n",
      "Step #9714\tEpoch   3 Batch  338/3125   Loss: 0.903045 mae: 0.765688 (2217.9645278309517 steps/sec)\n",
      "Step #9715\tEpoch   3 Batch  339/3125   Loss: 0.786765 mae: 0.681617 (2058.6349402675933 steps/sec)\n",
      "Step #9716\tEpoch   3 Batch  340/3125   Loss: 0.655071 mae: 0.648569 (2199.079326797043 steps/sec)\n",
      "Step #9717\tEpoch   3 Batch  341/3125   Loss: 0.810524 mae: 0.676603 (2143.450531479967 steps/sec)\n",
      "Step #9718\tEpoch   3 Batch  342/3125   Loss: 0.883830 mae: 0.746682 (2349.4079293772334 steps/sec)\n",
      "Step #9719\tEpoch   3 Batch  343/3125   Loss: 0.890169 mae: 0.742543 (2091.4215050760913 steps/sec)\n",
      "Step #9720\tEpoch   3 Batch  344/3125   Loss: 0.932040 mae: 0.749087 (2149.9769332499513 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9721\tEpoch   3 Batch  345/3125   Loss: 0.784087 mae: 0.716373 (2018.491390511757 steps/sec)\n",
      "Step #9722\tEpoch   3 Batch  346/3125   Loss: 0.891571 mae: 0.735997 (1722.365308804205 steps/sec)\n",
      "Step #9723\tEpoch   3 Batch  347/3125   Loss: 0.728688 mae: 0.680877 (1746.2296829203303 steps/sec)\n",
      "Step #9724\tEpoch   3 Batch  348/3125   Loss: 0.941511 mae: 0.771359 (2072.284584980237 steps/sec)\n",
      "Step #9725\tEpoch   3 Batch  349/3125   Loss: 0.904153 mae: 0.767587 (2027.8796317784481 steps/sec)\n",
      "Step #9726\tEpoch   3 Batch  350/3125   Loss: 0.778762 mae: 0.673894 (2076.573159984553 steps/sec)\n",
      "Step #9727\tEpoch   3 Batch  351/3125   Loss: 1.018128 mae: 0.804741 (2231.677520963691 steps/sec)\n",
      "Step #9728\tEpoch   3 Batch  352/3125   Loss: 0.729358 mae: 0.697410 (2103.1459660031087 steps/sec)\n",
      "Step #9729\tEpoch   3 Batch  353/3125   Loss: 0.854219 mae: 0.744272 (2112.4673885671114 steps/sec)\n",
      "Step #9730\tEpoch   3 Batch  354/3125   Loss: 0.780848 mae: 0.711475 (1935.6598949632187 steps/sec)\n",
      "Step #9731\tEpoch   3 Batch  355/3125   Loss: 0.761613 mae: 0.714984 (2041.918114989533 steps/sec)\n",
      "Step #9732\tEpoch   3 Batch  356/3125   Loss: 0.833385 mae: 0.708836 (2176.484873644336 steps/sec)\n",
      "Step #9733\tEpoch   3 Batch  357/3125   Loss: 0.855920 mae: 0.715986 (2214.3579672041137 steps/sec)\n",
      "Step #9734\tEpoch   3 Batch  358/3125   Loss: 0.876784 mae: 0.739930 (1983.1037058751217 steps/sec)\n",
      "Step #9735\tEpoch   3 Batch  359/3125   Loss: 0.738399 mae: 0.673681 (2103.293617362699 steps/sec)\n",
      "Step #9736\tEpoch   3 Batch  360/3125   Loss: 0.773757 mae: 0.680356 (2127.5547574844527 steps/sec)\n",
      "Step #9737\tEpoch   3 Batch  361/3125   Loss: 0.831209 mae: 0.733213 (2078.6519972247 steps/sec)\n",
      "Step #9738\tEpoch   3 Batch  362/3125   Loss: 0.716207 mae: 0.667693 (1976.8227963840998 steps/sec)\n",
      "Step #9739\tEpoch   3 Batch  363/3125   Loss: 0.866924 mae: 0.734620 (1845.4996655989298 steps/sec)\n",
      "Step #9740\tEpoch   3 Batch  364/3125   Loss: 0.707360 mae: 0.684740 (2157.4971965885825 steps/sec)\n",
      "Step #9741\tEpoch   3 Batch  365/3125   Loss: 0.785269 mae: 0.718403 (1960.0833699400896 steps/sec)\n",
      "Step #9742\tEpoch   3 Batch  366/3125   Loss: 0.904293 mae: 0.744177 (1975.5006688144088 steps/sec)\n",
      "Step #9743\tEpoch   3 Batch  367/3125   Loss: 0.852902 mae: 0.722189 (1907.299416119468 steps/sec)\n",
      "Step #9744\tEpoch   3 Batch  368/3125   Loss: 0.848899 mae: 0.721223 (1905.1162790697674 steps/sec)\n",
      "Step #9745\tEpoch   3 Batch  369/3125   Loss: 0.875611 mae: 0.733168 (1701.3775535039185 steps/sec)\n",
      "Step #9746\tEpoch   3 Batch  370/3125   Loss: 0.959239 mae: 0.793406 (1943.174826729921 steps/sec)\n",
      "Step #9747\tEpoch   3 Batch  371/3125   Loss: 0.768050 mae: 0.707120 (1867.4882900853088 steps/sec)\n",
      "Step #9748\tEpoch   3 Batch  372/3125   Loss: 0.771371 mae: 0.717058 (2050.783778762187 steps/sec)\n",
      "Step #9749\tEpoch   3 Batch  373/3125   Loss: 0.849692 mae: 0.729007 (2012.3128886159516 steps/sec)\n",
      "Step #9750\tEpoch   3 Batch  374/3125   Loss: 0.789646 mae: 0.702497 (2068.8501302186096 steps/sec)\n",
      "Step #9751\tEpoch   3 Batch  375/3125   Loss: 0.802780 mae: 0.728809 (2098.894082088133 steps/sec)\n",
      "Step #9752\tEpoch   3 Batch  376/3125   Loss: 0.773949 mae: 0.694259 (2135.7449105333376 steps/sec)\n",
      "Step #9753\tEpoch   3 Batch  377/3125   Loss: 0.765175 mae: 0.706163 (2316.194515313166 steps/sec)\n",
      "Step #9754\tEpoch   3 Batch  378/3125   Loss: 0.861556 mae: 0.747064 (2112.3822762114846 steps/sec)\n",
      "Step #9755\tEpoch   3 Batch  379/3125   Loss: 0.798823 mae: 0.708833 (1828.2207305378781 steps/sec)\n",
      "Step #9756\tEpoch   3 Batch  380/3125   Loss: 0.798529 mae: 0.711042 (1926.0423937401272 steps/sec)\n",
      "Step #9757\tEpoch   3 Batch  381/3125   Loss: 0.768915 mae: 0.705741 (2064.2275702544416 steps/sec)\n",
      "Step #9758\tEpoch   3 Batch  382/3125   Loss: 0.768371 mae: 0.695951 (2061.6909162406605 steps/sec)\n",
      "Step #9759\tEpoch   3 Batch  383/3125   Loss: 0.841693 mae: 0.708369 (2147.1373576869523 steps/sec)\n",
      "Step #9760\tEpoch   3 Batch  384/3125   Loss: 0.797199 mae: 0.713079 (2169.190827377197 steps/sec)\n",
      "Step #9761\tEpoch   3 Batch  385/3125   Loss: 0.800382 mae: 0.717070 (2116.2191344009525 steps/sec)\n",
      "Step #9762\tEpoch   3 Batch  386/3125   Loss: 0.728305 mae: 0.688764 (2172.4040772354356 steps/sec)\n",
      "Step #9763\tEpoch   3 Batch  387/3125   Loss: 0.818955 mae: 0.734769 (2004.2547904620824 steps/sec)\n",
      "Step #9764\tEpoch   3 Batch  388/3125   Loss: 0.823623 mae: 0.719736 (1863.091779714471 steps/sec)\n",
      "Step #9765\tEpoch   3 Batch  389/3125   Loss: 0.653377 mae: 0.650317 (2021.8385153048928 steps/sec)\n",
      "Step #9766\tEpoch   3 Batch  390/3125   Loss: 0.906620 mae: 0.752503 (2041.8982337935465 steps/sec)\n",
      "Step #9767\tEpoch   3 Batch  391/3125   Loss: 0.871630 mae: 0.713257 (1851.920665477473 steps/sec)\n",
      "Step #9768\tEpoch   3 Batch  392/3125   Loss: 0.685969 mae: 0.656891 (2174.45383379128 steps/sec)\n",
      "Step #9769\tEpoch   3 Batch  393/3125   Loss: 0.883963 mae: 0.731034 (2115.322621317114 steps/sec)\n",
      "Step #9770\tEpoch   3 Batch  394/3125   Loss: 0.828102 mae: 0.723820 (2137.769622833843 steps/sec)\n",
      "Step #9771\tEpoch   3 Batch  395/3125   Loss: 0.818604 mae: 0.712350 (2131.1004298474704 steps/sec)\n",
      "Step #9772\tEpoch   3 Batch  396/3125   Loss: 0.807715 mae: 0.697559 (2094.0737116438836 steps/sec)\n",
      "Step #9773\tEpoch   3 Batch  397/3125   Loss: 0.772557 mae: 0.694607 (1446.3815494541116 steps/sec)\n",
      "Step #9774\tEpoch   3 Batch  398/3125   Loss: 0.862164 mae: 0.750571 (1881.9678015686416 steps/sec)\n",
      "Step #9775\tEpoch   3 Batch  399/3125   Loss: 0.904001 mae: 0.749460 (1971.0075187969924 steps/sec)\n",
      "Step #9776\tEpoch   3 Batch  400/3125   Loss: 0.834732 mae: 0.715070 (2028.8801818797465 steps/sec)\n",
      "Step #9777\tEpoch   3 Batch  401/3125   Loss: 0.888701 mae: 0.759765 (2104.8135212172306 steps/sec)\n",
      "Step #9778\tEpoch   3 Batch  402/3125   Loss: 0.853753 mae: 0.721261 (2015.8914169814766 steps/sec)\n",
      "Step #9779\tEpoch   3 Batch  403/3125   Loss: 0.932704 mae: 0.772541 (2199.9328633769724 steps/sec)\n",
      "Step #9780\tEpoch   3 Batch  404/3125   Loss: 0.781654 mae: 0.692088 (2137.83500005097 steps/sec)\n",
      "Step #9781\tEpoch   3 Batch  405/3125   Loss: 0.788766 mae: 0.689973 (2090.4833580877003 steps/sec)\n",
      "Step #9782\tEpoch   3 Batch  406/3125   Loss: 0.966777 mae: 0.775076 (1724.8017896502945 steps/sec)\n",
      "Step #9783\tEpoch   3 Batch  407/3125   Loss: 0.760451 mae: 0.679709 (1983.2537378360742 steps/sec)\n",
      "Step #9784\tEpoch   3 Batch  408/3125   Loss: 0.819861 mae: 0.705519 (2062.1774701069853 steps/sec)\n",
      "Step #9785\tEpoch   3 Batch  409/3125   Loss: 0.745171 mae: 0.673678 (2006.575195667566 steps/sec)\n",
      "Step #9786\tEpoch   3 Batch  410/3125   Loss: 0.825325 mae: 0.732926 (2068.768496231701 steps/sec)\n",
      "Step #9787\tEpoch   3 Batch  411/3125   Loss: 0.887419 mae: 0.768635 (2128.0081177067477 steps/sec)\n",
      "Step #9788\tEpoch   3 Batch  412/3125   Loss: 0.657921 mae: 0.647143 (2157.896794772856 steps/sec)\n",
      "Step #9789\tEpoch   3 Batch  413/3125   Loss: 0.765936 mae: 0.693013 (2305.0185751027675 steps/sec)\n",
      "Step #9790\tEpoch   3 Batch  414/3125   Loss: 0.766281 mae: 0.677834 (2216.0426903365565 steps/sec)\n",
      "Step #9791\tEpoch   3 Batch  415/3125   Loss: 0.956824 mae: 0.761611 (1675.9517949045808 steps/sec)\n",
      "Step #9792\tEpoch   3 Batch  416/3125   Loss: 0.864576 mae: 0.715233 (2059.888614955456 steps/sec)\n",
      "Step #9793\tEpoch   3 Batch  417/3125   Loss: 0.938847 mae: 0.754884 (2199.171569090089 steps/sec)\n",
      "Step #9794\tEpoch   3 Batch  418/3125   Loss: 0.800962 mae: 0.705073 (2019.1327120080105 steps/sec)\n",
      "Step #9795\tEpoch   3 Batch  419/3125   Loss: 0.865311 mae: 0.743188 (2195.9706806282725 steps/sec)\n",
      "Step #9796\tEpoch   3 Batch  420/3125   Loss: 0.736202 mae: 0.682426 (2055.245543370672 steps/sec)\n",
      "Step #9797\tEpoch   3 Batch  421/3125   Loss: 0.817690 mae: 0.720509 (2165.8081173190126 steps/sec)\n",
      "Step #9798\tEpoch   3 Batch  422/3125   Loss: 0.917179 mae: 0.771946 (2184.7153929494125 steps/sec)\n",
      "Step #9799\tEpoch   3 Batch  423/3125   Loss: 0.851495 mae: 0.735899 (2029.6459748756363 steps/sec)\n",
      "Step #9800\tEpoch   3 Batch  424/3125   Loss: 0.888271 mae: 0.771688 (1640.2581048844395 steps/sec)\n",
      "Step #9801\tEpoch   3 Batch  425/3125   Loss: 0.854113 mae: 0.738729 (1880.1287395892168 steps/sec)\n",
      "Step #9802\tEpoch   3 Batch  426/3125   Loss: 0.731397 mae: 0.677715 (2093.1122932739813 steps/sec)\n",
      "Step #9803\tEpoch   3 Batch  427/3125   Loss: 0.715669 mae: 0.689906 (1726.0510288065843 steps/sec)\n",
      "Step #9804\tEpoch   3 Batch  428/3125   Loss: 0.745526 mae: 0.686788 (1820.902831441943 steps/sec)\n",
      "Step #9805\tEpoch   3 Batch  429/3125   Loss: 0.751251 mae: 0.700546 (1782.474034031992 steps/sec)\n",
      "Step #9806\tEpoch   3 Batch  430/3125   Loss: 0.941645 mae: 0.771024 (1883.6074261027331 steps/sec)\n",
      "Step #9807\tEpoch   3 Batch  431/3125   Loss: 0.767587 mae: 0.705799 (2040.3089914968964 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9808\tEpoch   3 Batch  432/3125   Loss: 0.856380 mae: 0.723525 (2008.5161809353242 steps/sec)\n",
      "Step #9809\tEpoch   3 Batch  433/3125   Loss: 0.856251 mae: 0.736734 (1722.591667761861 steps/sec)\n",
      "Step #9810\tEpoch   3 Batch  434/3125   Loss: 0.821579 mae: 0.734419 (2074.950034629465 steps/sec)\n",
      "Step #9811\tEpoch   3 Batch  435/3125   Loss: 0.845385 mae: 0.717961 (2101.9022991961833 steps/sec)\n",
      "Step #9812\tEpoch   3 Batch  436/3125   Loss: 0.761978 mae: 0.694546 (2219.0675724292637 steps/sec)\n",
      "Step #9813\tEpoch   3 Batch  437/3125   Loss: 0.808512 mae: 0.721883 (2088.921648704106 steps/sec)\n",
      "Step #9814\tEpoch   3 Batch  438/3125   Loss: 0.902138 mae: 0.769673 (2026.5862662105487 steps/sec)\n",
      "Step #9815\tEpoch   3 Batch  439/3125   Loss: 0.923957 mae: 0.727858 (2011.830277913681 steps/sec)\n",
      "Step #9816\tEpoch   3 Batch  440/3125   Loss: 0.768707 mae: 0.694969 (2108.7289217805755 steps/sec)\n",
      "Step #9817\tEpoch   3 Batch  441/3125   Loss: 0.788362 mae: 0.701011 (1896.519230595321 steps/sec)\n",
      "Step #9818\tEpoch   3 Batch  442/3125   Loss: 0.922475 mae: 0.754595 (1918.5011709601874 steps/sec)\n",
      "Step #9819\tEpoch   3 Batch  443/3125   Loss: 0.751404 mae: 0.704583 (2203.099032471557 steps/sec)\n",
      "Step #9820\tEpoch   3 Batch  444/3125   Loss: 0.822840 mae: 0.710871 (2056.2732870533787 steps/sec)\n",
      "Step #9821\tEpoch   3 Batch  445/3125   Loss: 0.895230 mae: 0.746536 (2117.0309203420115 steps/sec)\n",
      "Step #9822\tEpoch   3 Batch  446/3125   Loss: 0.874781 mae: 0.729794 (2061.184333382476 steps/sec)\n",
      "Step #9823\tEpoch   3 Batch  447/3125   Loss: 0.798886 mae: 0.687497 (1844.6230978977921 steps/sec)\n",
      "Step #9824\tEpoch   3 Batch  448/3125   Loss: 0.790148 mae: 0.697138 (2232.1525885558585 steps/sec)\n",
      "Step #9825\tEpoch   3 Batch  449/3125   Loss: 0.796127 mae: 0.713979 (1822.6911644561874 steps/sec)\n",
      "Step #9826\tEpoch   3 Batch  450/3125   Loss: 0.755031 mae: 0.675326 (1724.021933033549 steps/sec)\n",
      "Step #9827\tEpoch   3 Batch  451/3125   Loss: 0.751464 mae: 0.685647 (1785.7068655750547 steps/sec)\n",
      "Step #9828\tEpoch   3 Batch  452/3125   Loss: 0.789949 mae: 0.715862 (1758.8982731001165 steps/sec)\n",
      "Step #9829\tEpoch   3 Batch  453/3125   Loss: 0.747225 mae: 0.688077 (1946.5837471573768 steps/sec)\n",
      "Step #9830\tEpoch   3 Batch  454/3125   Loss: 0.846902 mae: 0.731750 (1863.0255758792541 steps/sec)\n",
      "Step #9831\tEpoch   3 Batch  455/3125   Loss: 0.733294 mae: 0.671338 (1987.3508647239992 steps/sec)\n",
      "Step #9832\tEpoch   3 Batch  456/3125   Loss: 0.886819 mae: 0.756742 (1939.4011134333327 steps/sec)\n",
      "Step #9833\tEpoch   3 Batch  457/3125   Loss: 0.891009 mae: 0.770450 (1718.4981234737859 steps/sec)\n",
      "Step #9834\tEpoch   3 Batch  458/3125   Loss: 0.886130 mae: 0.739360 (1803.2416443821528 steps/sec)\n",
      "Step #9835\tEpoch   3 Batch  459/3125   Loss: 0.838308 mae: 0.722458 (2031.4547532789584 steps/sec)\n",
      "Step #9836\tEpoch   3 Batch  460/3125   Loss: 0.796335 mae: 0.710938 (1973.6229401744793 steps/sec)\n",
      "Step #9837\tEpoch   3 Batch  461/3125   Loss: 0.765392 mae: 0.695968 (1965.1895234971653 steps/sec)\n",
      "Step #9838\tEpoch   3 Batch  462/3125   Loss: 0.751293 mae: 0.708226 (1924.9286350243697 steps/sec)\n",
      "Step #9839\tEpoch   3 Batch  463/3125   Loss: 0.751081 mae: 0.659976 (2063.63851058805 steps/sec)\n",
      "Step #9840\tEpoch   3 Batch  464/3125   Loss: 0.728522 mae: 0.672013 (1987.6710770747243 steps/sec)\n",
      "Step #9841\tEpoch   3 Batch  465/3125   Loss: 0.811579 mae: 0.727578 (1867.7710387331783 steps/sec)\n",
      "Step #9842\tEpoch   3 Batch  466/3125   Loss: 0.860181 mae: 0.741493 (1991.559514539135 steps/sec)\n",
      "Step #9843\tEpoch   3 Batch  467/3125   Loss: 0.863843 mae: 0.741339 (2072.9810410612263 steps/sec)\n",
      "Step #9844\tEpoch   3 Batch  468/3125   Loss: 0.673877 mae: 0.678084 (2111.829213030562 steps/sec)\n",
      "Step #9845\tEpoch   3 Batch  469/3125   Loss: 0.769955 mae: 0.687266 (2160.4532811373238 steps/sec)\n",
      "Step #9846\tEpoch   3 Batch  470/3125   Loss: 0.768670 mae: 0.695719 (2236.5565710751116 steps/sec)\n",
      "Step #9847\tEpoch   3 Batch  471/3125   Loss: 0.832610 mae: 0.723169 (1991.6351687591407 steps/sec)\n",
      "Step #9848\tEpoch   3 Batch  472/3125   Loss: 0.862214 mae: 0.743122 (1934.0711229157444 steps/sec)\n",
      "Step #9849\tEpoch   3 Batch  473/3125   Loss: 0.812981 mae: 0.727193 (1731.7236709550627 steps/sec)\n",
      "Step #9850\tEpoch   3 Batch  474/3125   Loss: 0.789515 mae: 0.689722 (1789.7911634932962 steps/sec)\n",
      "Step #9851\tEpoch   3 Batch  475/3125   Loss: 0.757250 mae: 0.676738 (1961.4032790564997 steps/sec)\n",
      "Step #9852\tEpoch   3 Batch  476/3125   Loss: 0.699236 mae: 0.668491 (2043.3306701482938 steps/sec)\n",
      "Step #9853\tEpoch   3 Batch  477/3125   Loss: 0.891907 mae: 0.751713 (2156.1884394727645 steps/sec)\n",
      "Step #9854\tEpoch   3 Batch  478/3125   Loss: 0.877005 mae: 0.730654 (2177.9767159280914 steps/sec)\n",
      "Step #9855\tEpoch   3 Batch  479/3125   Loss: 0.887075 mae: 0.727727 (2146.104646998025 steps/sec)\n",
      "Step #9856\tEpoch   3 Batch  480/3125   Loss: 0.917806 mae: 0.751065 (2030.392688405236 steps/sec)\n",
      "Step #9857\tEpoch   3 Batch  481/3125   Loss: 0.796653 mae: 0.680723 (2162.547434416763 steps/sec)\n",
      "Step #9858\tEpoch   3 Batch  482/3125   Loss: 0.889091 mae: 0.719586 (1708.0566867568007 steps/sec)\n",
      "Step #9859\tEpoch   3 Batch  483/3125   Loss: 0.822689 mae: 0.721050 (1842.3220184131 steps/sec)\n",
      "Step #9860\tEpoch   3 Batch  484/3125   Loss: 0.707112 mae: 0.680162 (1956.5722815692495 steps/sec)\n",
      "Step #9861\tEpoch   3 Batch  485/3125   Loss: 0.843236 mae: 0.747494 (2133.8108707596507 steps/sec)\n",
      "Step #9862\tEpoch   3 Batch  486/3125   Loss: 0.845942 mae: 0.739489 (1939.7778250534163 steps/sec)\n",
      "Step #9863\tEpoch   3 Batch  487/3125   Loss: 0.762854 mae: 0.700333 (2114.8959772491203 steps/sec)\n",
      "Step #9864\tEpoch   3 Batch  488/3125   Loss: 0.742169 mae: 0.672402 (2003.0487688399014 steps/sec)\n",
      "Step #9865\tEpoch   3 Batch  489/3125   Loss: 0.732822 mae: 0.691587 (1986.0334296131446 steps/sec)\n",
      "Step #9866\tEpoch   3 Batch  490/3125   Loss: 0.807207 mae: 0.707983 (1692.794239912178 steps/sec)\n",
      "Step #9867\tEpoch   3 Batch  491/3125   Loss: 0.771475 mae: 0.709981 (1932.164475442007 steps/sec)\n",
      "Step #9868\tEpoch   3 Batch  492/3125   Loss: 0.699727 mae: 0.665400 (2022.2479364344674 steps/sec)\n",
      "Step #9869\tEpoch   3 Batch  493/3125   Loss: 0.703098 mae: 0.670395 (2005.9993878175696 steps/sec)\n",
      "Step #9870\tEpoch   3 Batch  494/3125   Loss: 0.779032 mae: 0.723941 (2078.5901896068112 steps/sec)\n",
      "Step #9871\tEpoch   3 Batch  495/3125   Loss: 0.759272 mae: 0.692144 (2148.677281203254 steps/sec)\n",
      "Step #9872\tEpoch   3 Batch  496/3125   Loss: 0.693799 mae: 0.679017 (2194.431132085343 steps/sec)\n",
      "Step #9873\tEpoch   3 Batch  497/3125   Loss: 0.953688 mae: 0.763951 (2221.347541018335 steps/sec)\n",
      "Step #9874\tEpoch   3 Batch  498/3125   Loss: 0.740209 mae: 0.682905 (2182.8279989591465 steps/sec)\n",
      "Step #9875\tEpoch   3 Batch  499/3125   Loss: 0.800943 mae: 0.699180 (1668.4317718940938 steps/sec)\n",
      "Step #9876\tEpoch   3 Batch  500/3125   Loss: 0.667571 mae: 0.666967 (1901.4370812290897 steps/sec)\n",
      "Step #9877\tEpoch   3 Batch  501/3125   Loss: 0.862344 mae: 0.729233 (1963.3313361294188 steps/sec)\n",
      "Step #9878\tEpoch   3 Batch  502/3125   Loss: 0.896405 mae: 0.743082 (2143.691543407374 steps/sec)\n",
      "Step #9879\tEpoch   3 Batch  503/3125   Loss: 0.765947 mae: 0.669795 (2056.333774574692 steps/sec)\n",
      "Step #9880\tEpoch   3 Batch  504/3125   Loss: 0.933114 mae: 0.725395 (2052.2889632630695 steps/sec)\n",
      "Step #9881\tEpoch   3 Batch  505/3125   Loss: 0.725496 mae: 0.679304 (2242.9433155080214 steps/sec)\n",
      "Step #9882\tEpoch   3 Batch  506/3125   Loss: 0.811799 mae: 0.678345 (2183.6917021564605 steps/sec)\n",
      "Step #9883\tEpoch   3 Batch  507/3125   Loss: 0.771834 mae: 0.708013 (2086.801464734915 steps/sec)\n",
      "Step #9884\tEpoch   3 Batch  508/3125   Loss: 0.839116 mae: 0.723472 (1678.258642765685 steps/sec)\n",
      "Step #9885\tEpoch   3 Batch  509/3125   Loss: 0.720690 mae: 0.667568 (1915.644667732359 steps/sec)\n",
      "Step #9886\tEpoch   3 Batch  510/3125   Loss: 0.880143 mae: 0.732706 (2223.608622353228 steps/sec)\n",
      "Step #9887\tEpoch   3 Batch  511/3125   Loss: 0.932128 mae: 0.772538 (2235.1501715942277 steps/sec)\n",
      "Step #9888\tEpoch   3 Batch  512/3125   Loss: 0.762462 mae: 0.689016 (1964.4715888865992 steps/sec)\n",
      "Step #9889\tEpoch   3 Batch  513/3125   Loss: 0.764834 mae: 0.692295 (2193.2606831349744 steps/sec)\n",
      "Step #9890\tEpoch   3 Batch  514/3125   Loss: 0.732674 mae: 0.688092 (2002.169097991293 steps/sec)\n",
      "Step #9891\tEpoch   3 Batch  515/3125   Loss: 0.875688 mae: 0.746211 (2145.072929239204 steps/sec)\n",
      "Step #9892\tEpoch   3 Batch  516/3125   Loss: 0.939129 mae: 0.793357 (2045.9021511145797 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9893\tEpoch   3 Batch  517/3125   Loss: 0.787164 mae: 0.708380 (1701.3637506794414 steps/sec)\n",
      "Step #9894\tEpoch   3 Batch  518/3125   Loss: 0.795397 mae: 0.697753 (1948.8086830465004 steps/sec)\n",
      "Step #9895\tEpoch   3 Batch  519/3125   Loss: 0.795295 mae: 0.727339 (2217.284472732655 steps/sec)\n",
      "Step #9896\tEpoch   3 Batch  520/3125   Loss: 0.883642 mae: 0.729741 (2004.4846735421465 steps/sec)\n",
      "Step #9897\tEpoch   3 Batch  521/3125   Loss: 0.863912 mae: 0.720981 (1971.5078074323371 steps/sec)\n",
      "Step #9898\tEpoch   3 Batch  522/3125   Loss: 0.821187 mae: 0.720272 (2257.3565977417306 steps/sec)\n",
      "Step #9899\tEpoch   3 Batch  523/3125   Loss: 0.667897 mae: 0.618400 (2120.8846997906576 steps/sec)\n",
      "Step #9900\tEpoch   3 Batch  524/3125   Loss: 0.836746 mae: 0.707651 (1961.4032790564997 steps/sec)\n",
      "Step #9901\tEpoch   3 Batch  525/3125   Loss: 0.903593 mae: 0.727940 (1962.0822574005465 steps/sec)\n",
      "Step #9902\tEpoch   3 Batch  526/3125   Loss: 0.723594 mae: 0.685153 (1841.4969749655347 steps/sec)\n",
      "Step #9903\tEpoch   3 Batch  527/3125   Loss: 0.820347 mae: 0.699618 (2094.0737116438836 steps/sec)\n",
      "Step #9904\tEpoch   3 Batch  528/3125   Loss: 0.840879 mae: 0.719272 (2022.1504401739483 steps/sec)\n",
      "Step #9905\tEpoch   3 Batch  529/3125   Loss: 0.724451 mae: 0.689872 (2121.3566797155545 steps/sec)\n",
      "Step #9906\tEpoch   3 Batch  530/3125   Loss: 0.721270 mae: 0.676518 (2286.3223077426246 steps/sec)\n",
      "Step #9907\tEpoch   3 Batch  531/3125   Loss: 0.885110 mae: 0.759262 (2060.6779994104354 steps/sec)\n",
      "Step #9908\tEpoch   3 Batch  532/3125   Loss: 0.782333 mae: 0.702558 (2105.701146655421 steps/sec)\n",
      "Step #9909\tEpoch   3 Batch  533/3125   Loss: 0.791582 mae: 0.701140 (2102.9772469741183 steps/sec)\n",
      "Step #9910\tEpoch   3 Batch  534/3125   Loss: 0.764811 mae: 0.695631 (1840.0105286246985 steps/sec)\n",
      "Step #9911\tEpoch   3 Batch  535/3125   Loss: 0.874176 mae: 0.729996 (1925.2290461764435 steps/sec)\n",
      "Step #9912\tEpoch   3 Batch  536/3125   Loss: 0.918622 mae: 0.751200 (1938.7377393201505 steps/sec)\n",
      "Step #9913\tEpoch   3 Batch  537/3125   Loss: 0.771732 mae: 0.696894 (1689.7662538574962 steps/sec)\n",
      "Step #9914\tEpoch   3 Batch  538/3125   Loss: 0.776358 mae: 0.707815 (2067.728227325163 steps/sec)\n",
      "Step #9915\tEpoch   3 Batch  539/3125   Loss: 0.892454 mae: 0.724856 (1695.6137159304985 steps/sec)\n",
      "Step #9916\tEpoch   3 Batch  540/3125   Loss: 0.730841 mae: 0.693835 (1815.9361307864156 steps/sec)\n",
      "Step #9917\tEpoch   3 Batch  541/3125   Loss: 0.608045 mae: 0.617073 (1728.3127714457603 steps/sec)\n",
      "Step #9918\tEpoch   3 Batch  542/3125   Loss: 0.722040 mae: 0.661626 (1689.875182311182 steps/sec)\n",
      "Step #9919\tEpoch   3 Batch  543/3125   Loss: 0.894976 mae: 0.761046 (1974.8867606482659 steps/sec)\n",
      "Step #9920\tEpoch   3 Batch  544/3125   Loss: 0.823205 mae: 0.737718 (2173.732599479669 steps/sec)\n",
      "Step #9921\tEpoch   3 Batch  545/3125   Loss: 0.701113 mae: 0.662032 (2216.3237267894697 steps/sec)\n",
      "Step #9922\tEpoch   3 Batch  546/3125   Loss: 0.948013 mae: 0.781808 (2087.424601357673 steps/sec)\n",
      "Step #9923\tEpoch   3 Batch  547/3125   Loss: 0.905827 mae: 0.769507 (1959.1495086132804 steps/sec)\n",
      "Step #9924\tEpoch   3 Batch  548/3125   Loss: 0.735328 mae: 0.682645 (2090.816825020189 steps/sec)\n",
      "Step #9925\tEpoch   3 Batch  549/3125   Loss: 0.783872 mae: 0.681606 (2324.3064714553293 steps/sec)\n",
      "Step #9926\tEpoch   3 Batch  550/3125   Loss: 0.854546 mae: 0.721678 (2203.2379051321113 steps/sec)\n",
      "Step #9927\tEpoch   3 Batch  551/3125   Loss: 0.810322 mae: 0.726175 (1849.9439852509197 steps/sec)\n",
      "Step #9928\tEpoch   3 Batch  552/3125   Loss: 0.850164 mae: 0.734602 (1890.5183449021906 steps/sec)\n",
      "Step #9929\tEpoch   3 Batch  553/3125   Loss: 0.803977 mae: 0.696491 (2078.0546775136495 steps/sec)\n",
      "Step #9930\tEpoch   3 Batch  554/3125   Loss: 0.826720 mae: 0.725728 (2068.829721117896 steps/sec)\n",
      "Step #9931\tEpoch   3 Batch  555/3125   Loss: 0.910149 mae: 0.756566 (1983.7227340661004 steps/sec)\n",
      "Step #9932\tEpoch   3 Batch  556/3125   Loss: 0.762440 mae: 0.676572 (1800.4395604395604 steps/sec)\n",
      "Step #9933\tEpoch   3 Batch  557/3125   Loss: 0.930672 mae: 0.754475 (1447.8494400949974 steps/sec)\n",
      "Step #9934\tEpoch   3 Batch  558/3125   Loss: 0.742624 mae: 0.691598 (2127.5763416861114 steps/sec)\n",
      "Step #9935\tEpoch   3 Batch  559/3125   Loss: 0.931740 mae: 0.771904 (1794.1243904525622 steps/sec)\n",
      "Step #9936\tEpoch   3 Batch  560/3125   Loss: 0.965324 mae: 0.777707 (1880.617680291264 steps/sec)\n",
      "Step #9937\tEpoch   3 Batch  561/3125   Loss: 0.897305 mae: 0.769577 (2179.04033582012 steps/sec)\n",
      "Step #9938\tEpoch   3 Batch  562/3125   Loss: 0.634040 mae: 0.650294 (1831.7017782901862 steps/sec)\n",
      "Step #9939\tEpoch   3 Batch  563/3125   Loss: 0.769929 mae: 0.728519 (1222.9147228960626 steps/sec)\n",
      "Step #9940\tEpoch   3 Batch  564/3125   Loss: 0.925044 mae: 0.734354 (1247.19119833482 steps/sec)\n",
      "Step #9941\tEpoch   3 Batch  565/3125   Loss: 0.912069 mae: 0.745849 (1404.13913159921 steps/sec)\n",
      "Step #9942\tEpoch   3 Batch  566/3125   Loss: 0.702890 mae: 0.655471 (1321.7483376926227 steps/sec)\n",
      "Step #9943\tEpoch   3 Batch  567/3125   Loss: 0.776423 mae: 0.708078 (2185.034070307779 steps/sec)\n",
      "Step #9944\tEpoch   3 Batch  568/3125   Loss: 1.107410 mae: 0.852876 (2081.8710663728234 steps/sec)\n",
      "Step #9945\tEpoch   3 Batch  569/3125   Loss: 0.800887 mae: 0.709576 (2088.8176177053556 steps/sec)\n",
      "Step #9946\tEpoch   3 Batch  570/3125   Loss: 0.884168 mae: 0.737126 (2056.0112155763177 steps/sec)\n",
      "Step #9947\tEpoch   3 Batch  571/3125   Loss: 0.862336 mae: 0.711885 (1637.5044897321777 steps/sec)\n",
      "Step #9948\tEpoch   3 Batch  572/3125   Loss: 0.873132 mae: 0.741969 (2003.8334750661686 steps/sec)\n",
      "Step #9949\tEpoch   3 Batch  573/3125   Loss: 0.764012 mae: 0.689813 (2147.5991029277734 steps/sec)\n",
      "Step #9950\tEpoch   3 Batch  574/3125   Loss: 0.879295 mae: 0.719225 (1398.5208896002134 steps/sec)\n",
      "Step #9951\tEpoch   3 Batch  575/3125   Loss: 0.892200 mae: 0.768843 (1758.3968473567265 steps/sec)\n",
      "Step #9952\tEpoch   3 Batch  576/3125   Loss: 0.791185 mae: 0.718788 (1987.2943673716927 steps/sec)\n",
      "Step #9953\tEpoch   3 Batch  577/3125   Loss: 0.811855 mae: 0.743872 (2004.082412751806 steps/sec)\n",
      "Step #9954\tEpoch   3 Batch  578/3125   Loss: 0.839587 mae: 0.726502 (2185.056836533753 steps/sec)\n",
      "Step #9955\tEpoch   3 Batch  579/3125   Loss: 0.849076 mae: 0.712894 (2097.928233446375 steps/sec)\n",
      "Step #9956\tEpoch   3 Batch  580/3125   Loss: 0.926482 mae: 0.756765 (1710.2156982670745 steps/sec)\n",
      "Step #9957\tEpoch   3 Batch  581/3125   Loss: 0.772196 mae: 0.704232 (1850.0745445723612 steps/sec)\n",
      "Step #9958\tEpoch   3 Batch  582/3125   Loss: 0.855072 mae: 0.722373 (1769.6738534239062 steps/sec)\n",
      "Step #9959\tEpoch   3 Batch  583/3125   Loss: 0.833739 mae: 0.720653 (1826.6124326066317 steps/sec)\n",
      "Step #9960\tEpoch   3 Batch  584/3125   Loss: 0.677706 mae: 0.650605 (2059.90884802766 steps/sec)\n",
      "Step #9961\tEpoch   3 Batch  585/3125   Loss: 0.732280 mae: 0.667185 (2009.0934347547015 steps/sec)\n",
      "Step #9962\tEpoch   3 Batch  586/3125   Loss: 0.899610 mae: 0.729616 (2253.3787486434503 steps/sec)\n",
      "Step #9963\tEpoch   3 Batch  587/3125   Loss: 0.800335 mae: 0.703402 (1921.3133978305482 steps/sec)\n",
      "Step #9964\tEpoch   3 Batch  588/3125   Loss: 0.848541 mae: 0.728347 (1994.4953255917906 steps/sec)\n",
      "Step #9965\tEpoch   3 Batch  589/3125   Loss: 0.756932 mae: 0.688534 (1924.7166365330079 steps/sec)\n",
      "Step #9966\tEpoch   3 Batch  590/3125   Loss: 0.946804 mae: 0.732768 (1775.1263320947005 steps/sec)\n",
      "Step #9967\tEpoch   3 Batch  591/3125   Loss: 0.786183 mae: 0.684112 (2004.9829345009896 steps/sec)\n",
      "Step #9968\tEpoch   3 Batch  592/3125   Loss: 0.773693 mae: 0.703572 (2089.2546175456773 steps/sec)\n",
      "Step #9969\tEpoch   3 Batch  593/3125   Loss: 0.786546 mae: 0.675886 (2020.0468131423563 steps/sec)\n",
      "Step #9970\tEpoch   3 Batch  594/3125   Loss: 0.829276 mae: 0.689700 (2156.6540862393435 steps/sec)\n",
      "Step #9971\tEpoch   3 Batch  595/3125   Loss: 0.949844 mae: 0.765721 (2005.8650802000936 steps/sec)\n",
      "Step #9972\tEpoch   3 Batch  596/3125   Loss: 0.777317 mae: 0.707826 (1723.0587210687613 steps/sec)\n",
      "Step #9973\tEpoch   3 Batch  597/3125   Loss: 0.832358 mae: 0.742124 (1620.749030094131 steps/sec)\n",
      "Step #9974\tEpoch   3 Batch  598/3125   Loss: 0.869342 mae: 0.747137 (1733.2264436308337 steps/sec)\n",
      "Step #9975\tEpoch   3 Batch  599/3125   Loss: 0.726948 mae: 0.683909 (1911.7504421229194 steps/sec)\n",
      "Step #9976\tEpoch   3 Batch  600/3125   Loss: 0.780010 mae: 0.700638 (1876.9484122722229 steps/sec)\n",
      "Step #9977\tEpoch   3 Batch  601/3125   Loss: 0.760712 mae: 0.681118 (1737.2610093111105 steps/sec)\n",
      "Step #9978\tEpoch   3 Batch  602/3125   Loss: 0.929884 mae: 0.764154 (2055.084421884034 steps/sec)\n",
      "Step #9979\tEpoch   3 Batch  603/3125   Loss: 0.794415 mae: 0.699287 (2298.5510423288542 steps/sec)\n",
      "Step #9980\tEpoch   3 Batch  604/3125   Loss: 0.859935 mae: 0.725912 (2076.758234140738 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #9981\tEpoch   3 Batch  605/3125   Loss: 0.838524 mae: 0.729483 (1771.213324099255 steps/sec)\n",
      "Step #9982\tEpoch   3 Batch  606/3125   Loss: 0.726067 mae: 0.679240 (1871.187408544202 steps/sec)\n",
      "Step #9983\tEpoch   3 Batch  607/3125   Loss: 0.904789 mae: 0.754172 (2150.6383764216052 steps/sec)\n",
      "Step #9984\tEpoch   3 Batch  608/3125   Loss: 0.818878 mae: 0.728693 (2215.8085477309946 steps/sec)\n",
      "Step #9985\tEpoch   3 Batch  609/3125   Loss: 0.755550 mae: 0.678314 (2106.4202490960224 steps/sec)\n",
      "Step #9986\tEpoch   3 Batch  610/3125   Loss: 0.777425 mae: 0.681507 (2028.72316755826 steps/sec)\n",
      "Step #9987\tEpoch   3 Batch  611/3125   Loss: 0.823178 mae: 0.722587 (2005.9418252078012 steps/sec)\n",
      "Step #9988\tEpoch   3 Batch  612/3125   Loss: 0.876135 mae: 0.734738 (1861.4380941391585 steps/sec)\n",
      "Step #9989\tEpoch   3 Batch  613/3125   Loss: 0.880797 mae: 0.728360 (1375.136552899905 steps/sec)\n",
      "Step #9990\tEpoch   3 Batch  614/3125   Loss: 0.898716 mae: 0.749867 (1273.7494229974977 steps/sec)\n",
      "Step #9991\tEpoch   3 Batch  615/3125   Loss: 0.839652 mae: 0.715018 (1357.8809010443983 steps/sec)\n",
      "Step #9992\tEpoch   3 Batch  616/3125   Loss: 0.808983 mae: 0.710676 (1062.5646637989128 steps/sec)\n",
      "Step #9993\tEpoch   3 Batch  617/3125   Loss: 0.910851 mae: 0.752425 (1185.146338293219 steps/sec)\n",
      "Step #9994\tEpoch   3 Batch  618/3125   Loss: 0.700440 mae: 0.703982 (1585.592342529657 steps/sec)\n",
      "Step #9995\tEpoch   3 Batch  619/3125   Loss: 0.806597 mae: 0.708362 (1364.4450227716331 steps/sec)\n",
      "Step #9996\tEpoch   3 Batch  620/3125   Loss: 0.856058 mae: 0.724931 (1418.2019827691142 steps/sec)\n",
      "Step #9997\tEpoch   3 Batch  621/3125   Loss: 0.632321 mae: 0.631043 (1129.0003391600674 steps/sec)\n",
      "Step #9998\tEpoch   3 Batch  622/3125   Loss: 0.757711 mae: 0.703411 (1326.5389773043544 steps/sec)\n",
      "Step #9999\tEpoch   3 Batch  623/3125   Loss: 0.781566 mae: 0.700091 (1432.9996515131845 steps/sec)\n",
      "Step #10000\tEpoch   3 Batch  624/3125   Loss: 0.661344 mae: 0.630446 (1274.3686346953161 steps/sec)\n",
      "Step #10001\tEpoch   3 Batch  625/3125   Loss: 0.796962 mae: 0.709484 (1273.8422663880656 steps/sec)\n",
      "Step #10002\tEpoch   3 Batch  626/3125   Loss: 0.758392 mae: 0.693325 (1538.5053297239401 steps/sec)\n",
      "Step #10003\tEpoch   3 Batch  627/3125   Loss: 0.886882 mae: 0.761914 (1419.3345786296327 steps/sec)\n",
      "Step #10004\tEpoch   3 Batch  628/3125   Loss: 0.785954 mae: 0.700405 (1448.3994170908413 steps/sec)\n",
      "Step #10005\tEpoch   3 Batch  629/3125   Loss: 0.929967 mae: 0.758256 (1818.8340184905726 steps/sec)\n",
      "Step #10006\tEpoch   3 Batch  630/3125   Loss: 0.845019 mae: 0.746914 (1421.181317936624 steps/sec)\n",
      "Step #10007\tEpoch   3 Batch  631/3125   Loss: 0.829128 mae: 0.718774 (1498.3510045440255 steps/sec)\n",
      "Step #10008\tEpoch   3 Batch  632/3125   Loss: 0.813746 mae: 0.675331 (1611.5451115397325 steps/sec)\n",
      "Step #10009\tEpoch   3 Batch  633/3125   Loss: 0.782748 mae: 0.702269 (1756.9678792245438 steps/sec)\n",
      "Step #10010\tEpoch   3 Batch  634/3125   Loss: 0.712871 mae: 0.673209 (1693.36832330736 steps/sec)\n",
      "Step #10011\tEpoch   3 Batch  635/3125   Loss: 0.795512 mae: 0.729248 (1793.1272711726733 steps/sec)\n",
      "Step #10012\tEpoch   3 Batch  636/3125   Loss: 0.856572 mae: 0.743090 (1939.3293754276942 steps/sec)\n",
      "Step #10013\tEpoch   3 Batch  637/3125   Loss: 0.825388 mae: 0.721555 (1753.2809978848452 steps/sec)\n",
      "Step #10014\tEpoch   3 Batch  638/3125   Loss: 0.858976 mae: 0.726765 (1774.3753754515994 steps/sec)\n",
      "Step #10015\tEpoch   3 Batch  639/3125   Loss: 0.852389 mae: 0.737996 (1991.483866066511 steps/sec)\n",
      "Step #10016\tEpoch   3 Batch  640/3125   Loss: 0.780263 mae: 0.711286 (1827.089849365313 steps/sec)\n",
      "Step #10017\tEpoch   3 Batch  641/3125   Loss: 0.953889 mae: 0.778120 (1932.0754716981132 steps/sec)\n",
      "Step #10018\tEpoch   3 Batch  642/3125   Loss: 0.799315 mae: 0.698813 (1783.8689372416256 steps/sec)\n",
      "Step #10019\tEpoch   3 Batch  643/3125   Loss: 0.714507 mae: 0.674542 (1505.8282891382864 steps/sec)\n",
      "Step #10020\tEpoch   3 Batch  644/3125   Loss: 0.944398 mae: 0.770764 (1253.4753477140825 steps/sec)\n",
      "Step #10021\tEpoch   3 Batch  645/3125   Loss: 0.846820 mae: 0.727288 (1496.565356701943 steps/sec)\n",
      "Step #10022\tEpoch   3 Batch  646/3125   Loss: 0.900363 mae: 0.762257 (1715.6722706262526 steps/sec)\n",
      "Step #10023\tEpoch   3 Batch  647/3125   Loss: 0.782644 mae: 0.698147 (1218.148339616285 steps/sec)\n",
      "Step #10024\tEpoch   3 Batch  648/3125   Loss: 0.867993 mae: 0.748675 (1609.8873851397514 steps/sec)\n",
      "Step #10025\tEpoch   3 Batch  649/3125   Loss: 0.785114 mae: 0.698302 (1648.1472458209882 steps/sec)\n",
      "Step #10026\tEpoch   3 Batch  650/3125   Loss: 0.810911 mae: 0.733462 (1531.528057722081 steps/sec)\n",
      "Step #10027\tEpoch   3 Batch  651/3125   Loss: 0.786518 mae: 0.681588 (1351.170671992784 steps/sec)\n",
      "Step #10028\tEpoch   3 Batch  652/3125   Loss: 0.848768 mae: 0.734152 (1353.1846250137116 steps/sec)\n",
      "Step #10029\tEpoch   3 Batch  653/3125   Loss: 0.776162 mae: 0.706473 (1541.7287871436342 steps/sec)\n",
      "Step #10030\tEpoch   3 Batch  654/3125   Loss: 0.782416 mae: 0.697587 (1660.518627024031 steps/sec)\n",
      "Step #10031\tEpoch   3 Batch  655/3125   Loss: 0.729804 mae: 0.678503 (1541.1736174903547 steps/sec)\n",
      "Step #10032\tEpoch   3 Batch  656/3125   Loss: 0.847161 mae: 0.713106 (1431.9722502936115 steps/sec)\n",
      "Step #10033\tEpoch   3 Batch  657/3125   Loss: 0.847180 mae: 0.732283 (1624.5909766980665 steps/sec)\n",
      "Step #10034\tEpoch   3 Batch  658/3125   Loss: 0.794517 mae: 0.704863 (1672.756857646505 steps/sec)\n",
      "Step #10035\tEpoch   3 Batch  659/3125   Loss: 0.889129 mae: 0.743509 (1659.6907200177275 steps/sec)\n",
      "Step #10036\tEpoch   3 Batch  660/3125   Loss: 0.797572 mae: 0.690004 (1552.8018007345102 steps/sec)\n",
      "Step #10037\tEpoch   3 Batch  661/3125   Loss: 0.823677 mae: 0.730615 (1522.7650304966598 steps/sec)\n",
      "Step #10038\tEpoch   3 Batch  662/3125   Loss: 0.833585 mae: 0.718364 (1583.9875525880495 steps/sec)\n",
      "Step #10039\tEpoch   3 Batch  663/3125   Loss: 0.769872 mae: 0.698071 (1291.7952003153796 steps/sec)\n",
      "Step #10040\tEpoch   3 Batch  664/3125   Loss: 0.766192 mae: 0.695519 (1588.558962549994 steps/sec)\n",
      "Step #10041\tEpoch   3 Batch  665/3125   Loss: 0.926350 mae: 0.735640 (1753.295655954252 steps/sec)\n",
      "Step #10042\tEpoch   3 Batch  666/3125   Loss: 0.731679 mae: 0.693373 (1731.5949831146634 steps/sec)\n",
      "Step #10043\tEpoch   3 Batch  667/3125   Loss: 0.744299 mae: 0.664549 (1706.5833374021452 steps/sec)\n",
      "Step #10044\tEpoch   3 Batch  668/3125   Loss: 0.783178 mae: 0.715971 (1968.6577111905901 steps/sec)\n",
      "Step #10045\tEpoch   3 Batch  669/3125   Loss: 0.827747 mae: 0.712317 (1933.80360913626 steps/sec)\n",
      "Step #10046\tEpoch   3 Batch  670/3125   Loss: 0.872219 mae: 0.728667 (1510.6225734187153 steps/sec)\n",
      "Step #10047\tEpoch   3 Batch  671/3125   Loss: 0.763964 mae: 0.685010 (1746.8551389802838 steps/sec)\n",
      "Step #10048\tEpoch   3 Batch  672/3125   Loss: 0.859400 mae: 0.725212 (2013.6074278197582 steps/sec)\n",
      "Step #10049\tEpoch   3 Batch  673/3125   Loss: 0.869032 mae: 0.741512 (2080.466657407591 steps/sec)\n",
      "Step #10050\tEpoch   3 Batch  674/3125   Loss: 0.835833 mae: 0.736740 (1978.874661483152 steps/sec)\n",
      "Step #10051\tEpoch   3 Batch  675/3125   Loss: 0.693348 mae: 0.640997 (1962.9454215299943 steps/sec)\n",
      "Step #10052\tEpoch   3 Batch  676/3125   Loss: 0.971606 mae: 0.774784 (2235.936583753585 steps/sec)\n",
      "Step #10053\tEpoch   3 Batch  677/3125   Loss: 0.828826 mae: 0.710656 (1967.2726590495488 steps/sec)\n",
      "Step #10054\tEpoch   3 Batch  678/3125   Loss: 0.781718 mae: 0.668048 (1880.7526052409735 steps/sec)\n",
      "Step #10055\tEpoch   3 Batch  679/3125   Loss: 0.872263 mae: 0.738069 (1739.667686998648 steps/sec)\n",
      "Step #10056\tEpoch   3 Batch  680/3125   Loss: 0.886937 mae: 0.755397 (1978.6319464100386 steps/sec)\n",
      "Step #10057\tEpoch   3 Batch  681/3125   Loss: 0.762577 mae: 0.673153 (1981.2863729121004 steps/sec)\n",
      "Step #10058\tEpoch   3 Batch  682/3125   Loss: 0.836195 mae: 0.698477 (1942.3469482263592 steps/sec)\n",
      "Step #10059\tEpoch   3 Batch  683/3125   Loss: 0.824703 mae: 0.724166 (2010.9428787864258 steps/sec)\n",
      "Step #10060\tEpoch   3 Batch  684/3125   Loss: 0.857868 mae: 0.745814 (1894.874181161057 steps/sec)\n",
      "Step #10061\tEpoch   3 Batch  685/3125   Loss: 0.831170 mae: 0.732982 (1894.0014088831892 steps/sec)\n",
      "Step #10062\tEpoch   3 Batch  686/3125   Loss: 0.886464 mae: 0.753379 (1523.9268975039058 steps/sec)\n",
      "Step #10063\tEpoch   3 Batch  687/3125   Loss: 0.854465 mae: 0.741106 (1915.4172146719275 steps/sec)\n",
      "Step #10064\tEpoch   3 Batch  688/3125   Loss: 0.865869 mae: 0.743030 (1852.8696635566864 steps/sec)\n",
      "Step #10065\tEpoch   3 Batch  689/3125   Loss: 0.873910 mae: 0.739618 (1877.519740729467 steps/sec)\n",
      "Step #10066\tEpoch   3 Batch  690/3125   Loss: 0.691821 mae: 0.666911 (1846.6371977528486 steps/sec)\n",
      "Step #10067\tEpoch   3 Batch  691/3125   Loss: 0.833424 mae: 0.755624 (1560.3577327718338 steps/sec)\n",
      "Step #10068\tEpoch   3 Batch  692/3125   Loss: 0.926027 mae: 0.778091 (1065.707243946439 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10069\tEpoch   3 Batch  693/3125   Loss: 0.850399 mae: 0.751896 (1231.8579912243088 steps/sec)\n",
      "Step #10070\tEpoch   3 Batch  694/3125   Loss: 0.776028 mae: 0.690066 (1523.1742480916307 steps/sec)\n",
      "Step #10071\tEpoch   3 Batch  695/3125   Loss: 0.744388 mae: 0.699009 (1399.8558192935145 steps/sec)\n",
      "Step #10072\tEpoch   3 Batch  696/3125   Loss: 0.714362 mae: 0.692195 (1383.582936387507 steps/sec)\n",
      "Step #10073\tEpoch   3 Batch  697/3125   Loss: 0.889412 mae: 0.734064 (1528.0241318508372 steps/sec)\n",
      "Step #10074\tEpoch   3 Batch  698/3125   Loss: 0.853955 mae: 0.734454 (1439.718804920914 steps/sec)\n",
      "Step #10075\tEpoch   3 Batch  699/3125   Loss: 0.747169 mae: 0.685187 (1266.1744018257673 steps/sec)\n",
      "Step #10076\tEpoch   3 Batch  700/3125   Loss: 0.846616 mae: 0.706589 (1500.9246800835933 steps/sec)\n",
      "Step #10077\tEpoch   3 Batch  701/3125   Loss: 0.779826 mae: 0.690416 (1645.9223796256329 steps/sec)\n",
      "Step #10078\tEpoch   3 Batch  702/3125   Loss: 0.764632 mae: 0.704125 (1516.1705911696874 steps/sec)\n",
      "Step #10079\tEpoch   3 Batch  703/3125   Loss: 0.796881 mae: 0.683946 (1488.5453487216614 steps/sec)\n",
      "Step #10080\tEpoch   3 Batch  704/3125   Loss: 0.771170 mae: 0.684963 (1707.2363011747084 steps/sec)\n",
      "Step #10081\tEpoch   3 Batch  705/3125   Loss: 0.917915 mae: 0.781220 (1544.4537728484527 steps/sec)\n",
      "Step #10082\tEpoch   3 Batch  706/3125   Loss: 0.817587 mae: 0.732536 (1206.5356468909652 steps/sec)\n",
      "Step #10083\tEpoch   3 Batch  707/3125   Loss: 0.866861 mae: 0.736093 (1432.2460798776158 steps/sec)\n",
      "Step #10084\tEpoch   3 Batch  708/3125   Loss: 0.773522 mae: 0.674516 (1364.4095143912975 steps/sec)\n",
      "Step #10085\tEpoch   3 Batch  709/3125   Loss: 0.759907 mae: 0.691425 (1486.5932757264072 steps/sec)\n",
      "Step #10086\tEpoch   3 Batch  710/3125   Loss: 0.758484 mae: 0.676136 (1615.6048256629124 steps/sec)\n",
      "Step #10087\tEpoch   3 Batch  711/3125   Loss: 0.857204 mae: 0.740469 (1610.3942377098276 steps/sec)\n",
      "Step #10088\tEpoch   3 Batch  712/3125   Loss: 0.692389 mae: 0.661183 (1482.5472231648005 steps/sec)\n",
      "Step #10089\tEpoch   3 Batch  713/3125   Loss: 0.765820 mae: 0.698695 (1491.17023848462 steps/sec)\n",
      "Step #10090\tEpoch   3 Batch  714/3125   Loss: 0.770262 mae: 0.710248 (1305.3517409652802 steps/sec)\n",
      "Step #10091\tEpoch   3 Batch  715/3125   Loss: 0.760169 mae: 0.689150 (1509.437438820752 steps/sec)\n",
      "Step #10092\tEpoch   3 Batch  716/3125   Loss: 0.849948 mae: 0.739461 (1498.265367359186 steps/sec)\n",
      "Step #10093\tEpoch   3 Batch  717/3125   Loss: 0.720406 mae: 0.694845 (1378.6984504736672 steps/sec)\n",
      "Step #10094\tEpoch   3 Batch  718/3125   Loss: 0.794801 mae: 0.685782 (1352.7830995000807 steps/sec)\n",
      "Step #10095\tEpoch   3 Batch  719/3125   Loss: 0.841620 mae: 0.716609 (1624.1631944982264 steps/sec)\n",
      "Step #10096\tEpoch   3 Batch  720/3125   Loss: 0.740736 mae: 0.681198 (1470.9836709500028 steps/sec)\n",
      "Step #10097\tEpoch   3 Batch  721/3125   Loss: 0.690636 mae: 0.646403 (1468.984743839397 steps/sec)\n",
      "Step #10098\tEpoch   3 Batch  722/3125   Loss: 0.824420 mae: 0.730568 (1523.9268975039058 steps/sec)\n",
      "Step #10099\tEpoch   3 Batch  723/3125   Loss: 1.005687 mae: 0.792002 (1227.3880243235808 steps/sec)\n",
      "Step #10100\tEpoch   3 Batch  724/3125   Loss: 0.657028 mae: 0.645063 (1397.1605785437805 steps/sec)\n",
      "Step #10101\tEpoch   3 Batch  725/3125   Loss: 0.756234 mae: 0.701233 (1560.450615354852 steps/sec)\n",
      "Step #10102\tEpoch   3 Batch  726/3125   Loss: 0.716041 mae: 0.686089 (1401.9051693595288 steps/sec)\n",
      "Step #10103\tEpoch   3 Batch  727/3125   Loss: 0.720100 mae: 0.689012 (1501.6447439082897 steps/sec)\n",
      "Step #10104\tEpoch   3 Batch  728/3125   Loss: 0.835594 mae: 0.695115 (1541.6834521796663 steps/sec)\n",
      "Step #10105\tEpoch   3 Batch  729/3125   Loss: 0.724918 mae: 0.672779 (1474.9252744625037 steps/sec)\n",
      "Step #10106\tEpoch   3 Batch  730/3125   Loss: 0.696402 mae: 0.667793 (1217.1020329992398 steps/sec)\n",
      "Step #10107\tEpoch   3 Batch  731/3125   Loss: 0.859747 mae: 0.737139 (1451.9191359734145 steps/sec)\n",
      "Step #10108\tEpoch   3 Batch  732/3125   Loss: 0.893606 mae: 0.753974 (1535.9137548428678 steps/sec)\n",
      "Step #10109\tEpoch   3 Batch  733/3125   Loss: 0.795256 mae: 0.702090 (1643.2656067575085 steps/sec)\n",
      "Step #10110\tEpoch   3 Batch  734/3125   Loss: 0.886926 mae: 0.747636 (1633.4104415418526 steps/sec)\n",
      "Step #10111\tEpoch   3 Batch  735/3125   Loss: 0.775498 mae: 0.687995 (1636.8271114475933 steps/sec)\n",
      "Step #10112\tEpoch   3 Batch  736/3125   Loss: 0.846384 mae: 0.711005 (1506.888647778632 steps/sec)\n",
      "Step #10113\tEpoch   3 Batch  737/3125   Loss: 0.660739 mae: 0.651388 (2097.4875980156826 steps/sec)\n",
      "Step #10114\tEpoch   3 Batch  738/3125   Loss: 0.876473 mae: 0.754997 (1964.563602469344 steps/sec)\n",
      "Step #10115\tEpoch   3 Batch  739/3125   Loss: 0.787812 mae: 0.708012 (1947.8674394412246 steps/sec)\n",
      "Step #10116\tEpoch   3 Batch  740/3125   Loss: 0.865305 mae: 0.738934 (1768.718636405806 steps/sec)\n",
      "Step #10117\tEpoch   3 Batch  741/3125   Loss: 0.882798 mae: 0.721313 (1987.5203760567117 steps/sec)\n",
      "Step #10118\tEpoch   3 Batch  742/3125   Loss: 0.788215 mae: 0.709798 (1797.8465125848707 steps/sec)\n",
      "Step #10119\tEpoch   3 Batch  743/3125   Loss: 0.709010 mae: 0.672508 (1816.8643384996578 steps/sec)\n",
      "Step #10120\tEpoch   3 Batch  744/3125   Loss: 0.800295 mae: 0.711465 (1778.5436843801415 steps/sec)\n",
      "Step #10121\tEpoch   3 Batch  745/3125   Loss: 0.763311 mae: 0.702328 (2070.9953289947957 steps/sec)\n",
      "Step #10122\tEpoch   3 Batch  746/3125   Loss: 0.748641 mae: 0.683809 (2134.4624028009607 steps/sec)\n",
      "Step #10123\tEpoch   3 Batch  747/3125   Loss: 0.726365 mae: 0.686201 (1902.9381340398888 steps/sec)\n",
      "Step #10124\tEpoch   3 Batch  748/3125   Loss: 0.733549 mae: 0.685121 (1789.5620712018294 steps/sec)\n",
      "Step #10125\tEpoch   3 Batch  749/3125   Loss: 0.772398 mae: 0.690797 (1981.3799684438272 steps/sec)\n",
      "Step #10126\tEpoch   3 Batch  750/3125   Loss: 0.784474 mae: 0.703484 (1827.8064426160927 steps/sec)\n",
      "Step #10127\tEpoch   3 Batch  751/3125   Loss: 0.751055 mae: 0.681180 (1631.6439741694546 steps/sec)\n",
      "Step #10128\tEpoch   3 Batch  752/3125   Loss: 0.828591 mae: 0.740849 (1753.4569108953938 steps/sec)\n",
      "Step #10129\tEpoch   3 Batch  753/3125   Loss: 0.883199 mae: 0.764998 (2009.151178386664 steps/sec)\n",
      "Step #10130\tEpoch   3 Batch  754/3125   Loss: 0.814697 mae: 0.707909 (2037.1577055709358 steps/sec)\n",
      "Step #10131\tEpoch   3 Batch  755/3125   Loss: 0.905091 mae: 0.753636 (2066.363188491477 steps/sec)\n",
      "Step #10132\tEpoch   3 Batch  756/3125   Loss: 0.810525 mae: 0.712107 (2014.6520005763966 steps/sec)\n",
      "Step #10133\tEpoch   3 Batch  757/3125   Loss: 0.815255 mae: 0.705701 (1871.9557261447826 steps/sec)\n",
      "Step #10134\tEpoch   3 Batch  758/3125   Loss: 0.733594 mae: 0.670659 (1840.5434343788945 steps/sec)\n",
      "Step #10135\tEpoch   3 Batch  759/3125   Loss: 0.880856 mae: 0.735076 (1833.7999842603685 steps/sec)\n",
      "Step #10136\tEpoch   3 Batch  760/3125   Loss: 0.740938 mae: 0.696762 (1893.2832587028745 steps/sec)\n",
      "Step #10137\tEpoch   3 Batch  761/3125   Loss: 0.776900 mae: 0.689247 (1858.880675069581 steps/sec)\n",
      "Step #10138\tEpoch   3 Batch  762/3125   Loss: 0.826781 mae: 0.711082 (2189.2539120812585 steps/sec)\n",
      "Step #10139\tEpoch   3 Batch  763/3125   Loss: 0.869428 mae: 0.743668 (2021.078601441733 steps/sec)\n",
      "Step #10140\tEpoch   3 Batch  764/3125   Loss: 0.839753 mae: 0.736879 (2036.1489766592877 steps/sec)\n",
      "Step #10141\tEpoch   3 Batch  765/3125   Loss: 0.773772 mae: 0.712928 (1918.5187218120775 steps/sec)\n",
      "Step #10142\tEpoch   3 Batch  766/3125   Loss: 0.819067 mae: 0.712680 (1855.771766350757 steps/sec)\n",
      "Step #10143\tEpoch   3 Batch  767/3125   Loss: 0.846391 mae: 0.713835 (1825.880877961291 steps/sec)\n",
      "Step #10144\tEpoch   3 Batch  768/3125   Loss: 0.707467 mae: 0.679929 (1878.1586960415548 steps/sec)\n",
      "Step #10145\tEpoch   3 Batch  769/3125   Loss: 0.883505 mae: 0.716851 (1525.3123863553712 steps/sec)\n",
      "Step #10146\tEpoch   3 Batch  770/3125   Loss: 0.743953 mae: 0.666630 (1443.1666162019324 steps/sec)\n",
      "Step #10147\tEpoch   3 Batch  771/3125   Loss: 0.754224 mae: 0.696891 (1843.795992649968 steps/sec)\n",
      "Step #10148\tEpoch   3 Batch  772/3125   Loss: 0.807473 mae: 0.707416 (1894.155369094177 steps/sec)\n",
      "Step #10149\tEpoch   3 Batch  773/3125   Loss: 0.704166 mae: 0.680840 (2134.9187120155552 steps/sec)\n",
      "Step #10150\tEpoch   3 Batch  774/3125   Loss: 0.766904 mae: 0.700535 (2159.5411436397526 steps/sec)\n",
      "Step #10151\tEpoch   3 Batch  775/3125   Loss: 0.773743 mae: 0.693580 (1522.0577135226151 steps/sec)\n",
      "Step #10152\tEpoch   3 Batch  776/3125   Loss: 0.805981 mae: 0.687407 (2116.5608631147625 steps/sec)\n",
      "Step #10153\tEpoch   3 Batch  777/3125   Loss: 0.718136 mae: 0.686876 (1879.4884433729758 steps/sec)\n",
      "Step #10154\tEpoch   3 Batch  778/3125   Loss: 0.695221 mae: 0.642910 (1929.6044459575094 steps/sec)\n",
      "Step #10155\tEpoch   3 Batch  779/3125   Loss: 0.760569 mae: 0.665412 (2076.6554110925167 steps/sec)\n",
      "Step #10156\tEpoch   3 Batch  780/3125   Loss: 0.825991 mae: 0.738968 (1848.851273913427 steps/sec)\n",
      "Step #10157\tEpoch   3 Batch  781/3125   Loss: 0.774871 mae: 0.688979 (2091.6926820997196 steps/sec)\n",
      "Step #10158\tEpoch   3 Batch  782/3125   Loss: 0.911338 mae: 0.775649 (1716.7256057629338 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10159\tEpoch   3 Batch  783/3125   Loss: 0.761192 mae: 0.680452 (1818.3766723604235 steps/sec)\n",
      "Step #10160\tEpoch   3 Batch  784/3125   Loss: 0.909309 mae: 0.752457 (1854.7377730609358 steps/sec)\n",
      "Step #10161\tEpoch   3 Batch  785/3125   Loss: 0.952879 mae: 0.773073 (2129.563963524848 steps/sec)\n",
      "Step #10162\tEpoch   3 Batch  786/3125   Loss: 0.885660 mae: 0.726374 (1987.2943673716927 steps/sec)\n",
      "Step #10163\tEpoch   3 Batch  787/3125   Loss: 0.853209 mae: 0.730175 (1713.0935557388968 steps/sec)\n",
      "Step #10164\tEpoch   3 Batch  788/3125   Loss: 0.786927 mae: 0.706405 (1697.2192547991324 steps/sec)\n",
      "Step #10165\tEpoch   3 Batch  789/3125   Loss: 0.856801 mae: 0.726100 (1403.5470960660696 steps/sec)\n",
      "Step #10166\tEpoch   3 Batch  790/3125   Loss: 0.738448 mae: 0.689226 (1691.1425069350364 steps/sec)\n",
      "Step #10167\tEpoch   3 Batch  791/3125   Loss: 0.846634 mae: 0.734887 (1346.4428108246925 steps/sec)\n",
      "Step #10168\tEpoch   3 Batch  792/3125   Loss: 0.903466 mae: 0.741403 (1793.6026820840889 steps/sec)\n",
      "Step #10169\tEpoch   3 Batch  793/3125   Loss: 0.790531 mae: 0.727920 (2043.808595653445 steps/sec)\n",
      "Step #10170\tEpoch   3 Batch  794/3125   Loss: 0.855507 mae: 0.733856 (1946.9632545444417 steps/sec)\n",
      "Step #10171\tEpoch   3 Batch  795/3125   Loss: 0.795485 mae: 0.708195 (2174.0030062717046 steps/sec)\n",
      "Step #10172\tEpoch   3 Batch  796/3125   Loss: 0.928627 mae: 0.746822 (2031.5531488244583 steps/sec)\n",
      "Step #10173\tEpoch   3 Batch  797/3125   Loss: 0.796039 mae: 0.705024 (2147.687076919926 steps/sec)\n",
      "Step #10174\tEpoch   3 Batch  798/3125   Loss: 0.717951 mae: 0.654702 (1861.3059261034339 steps/sec)\n",
      "Step #10175\tEpoch   3 Batch  799/3125   Loss: 0.889188 mae: 0.754398 (1975.6681645611357 steps/sec)\n",
      "Step #10176\tEpoch   3 Batch  800/3125   Loss: 0.697825 mae: 0.662695 (1833.1748251748252 steps/sec)\n",
      "Step #10177\tEpoch   3 Batch  801/3125   Loss: 0.957848 mae: 0.764025 (1890.2797807903087 steps/sec)\n",
      "Step #10178\tEpoch   3 Batch  802/3125   Loss: 0.724256 mae: 0.682221 (1745.7499854323269 steps/sec)\n",
      "Step #10179\tEpoch   3 Batch  803/3125   Loss: 0.832005 mae: 0.720158 (1761.380109689828 steps/sec)\n",
      "Step #10180\tEpoch   3 Batch  804/3125   Loss: 0.805735 mae: 0.718799 (1901.4888022486173 steps/sec)\n",
      "Step #10181\tEpoch   3 Batch  805/3125   Loss: 0.727545 mae: 0.661709 (2045.1639328275244 steps/sec)\n",
      "Step #10182\tEpoch   3 Batch  806/3125   Loss: 0.855612 mae: 0.720655 (1994.4953255917906 steps/sec)\n",
      "Step #10183\tEpoch   3 Batch  807/3125   Loss: 0.747578 mae: 0.708218 (1906.571148041747 steps/sec)\n",
      "Step #10184\tEpoch   3 Batch  808/3125   Loss: 0.819421 mae: 0.723826 (1996.5840608547464 steps/sec)\n",
      "Step #10185\tEpoch   3 Batch  809/3125   Loss: 0.848386 mae: 0.718131 (2230.372127154966 steps/sec)\n",
      "Step #10186\tEpoch   3 Batch  810/3125   Loss: 0.766266 mae: 0.685627 (2190.0998370859265 steps/sec)\n",
      "Step #10187\tEpoch   3 Batch  811/3125   Loss: 0.808229 mae: 0.725428 (1978.3892908691264 steps/sec)\n",
      "Step #10188\tEpoch   3 Batch  812/3125   Loss: 0.861485 mae: 0.726884 (1946.5295438935195 steps/sec)\n",
      "Step #10189\tEpoch   3 Batch  813/3125   Loss: 0.938062 mae: 0.767239 (1700.3843222469068 steps/sec)\n",
      "Step #10190\tEpoch   3 Batch  814/3125   Loss: 0.908081 mae: 0.746363 (1607.049970497406 steps/sec)\n",
      "Step #10191\tEpoch   3 Batch  815/3125   Loss: 0.763299 mae: 0.679089 (1808.061109243118 steps/sec)\n",
      "Step #10192\tEpoch   3 Batch  816/3125   Loss: 0.873498 mae: 0.735844 (1985.0746833765595 steps/sec)\n",
      "Step #10193\tEpoch   3 Batch  817/3125   Loss: 0.696690 mae: 0.645861 (2077.293078172669 steps/sec)\n",
      "Step #10194\tEpoch   3 Batch  818/3125   Loss: 0.655536 mae: 0.648190 (2202.8444780571836 steps/sec)\n",
      "Step #10195\tEpoch   3 Batch  819/3125   Loss: 0.709757 mae: 0.665419 (1911.8201542472698 steps/sec)\n",
      "Step #10196\tEpoch   3 Batch  820/3125   Loss: 0.789153 mae: 0.702799 (2151.653380118399 steps/sec)\n",
      "Step #10197\tEpoch   3 Batch  821/3125   Loss: 0.970283 mae: 0.787609 (2034.9240233654834 steps/sec)\n",
      "Step #10198\tEpoch   3 Batch  822/3125   Loss: 0.751753 mae: 0.691781 (1892.0022013117652 steps/sec)\n",
      "Step #10199\tEpoch   3 Batch  823/3125   Loss: 0.768735 mae: 0.690157 (1811.6691718930872 steps/sec)\n",
      "Step #10200\tEpoch   3 Batch  824/3125   Loss: 0.821164 mae: 0.708300 (1928.5752384105351 steps/sec)\n",
      "Step #10201\tEpoch   3 Batch  825/3125   Loss: 0.837652 mae: 0.738968 (2109.895770453539 steps/sec)\n",
      "Step #10202\tEpoch   3 Batch  826/3125   Loss: 0.865089 mae: 0.735337 (2023.2234163659868 steps/sec)\n",
      "Step #10203\tEpoch   3 Batch  827/3125   Loss: 0.682703 mae: 0.653388 (2066.8723205046076 steps/sec)\n",
      "Step #10204\tEpoch   3 Batch  828/3125   Loss: 0.953778 mae: 0.777703 (2022.3259402121505 steps/sec)\n",
      "Step #10205\tEpoch   3 Batch  829/3125   Loss: 0.957724 mae: 0.765252 (2122.3012700500935 steps/sec)\n",
      "Step #10206\tEpoch   3 Batch  830/3125   Loss: 0.705833 mae: 0.662777 (1905.9300391700672 steps/sec)\n",
      "Step #10207\tEpoch   3 Batch  831/3125   Loss: 0.913415 mae: 0.771988 (1556.1316939606877 steps/sec)\n",
      "Step #10208\tEpoch   3 Batch  832/3125   Loss: 0.868030 mae: 0.738178 (1879.0000895977064 steps/sec)\n",
      "Step #10209\tEpoch   3 Batch  833/3125   Loss: 0.952788 mae: 0.770183 (2040.725928088357 steps/sec)\n",
      "Step #10210\tEpoch   3 Batch  834/3125   Loss: 0.835462 mae: 0.723815 (2021.8385153048928 steps/sec)\n",
      "Step #10211\tEpoch   3 Batch  835/3125   Loss: 0.742550 mae: 0.690103 (1986.3344036219323 steps/sec)\n",
      "Step #10212\tEpoch   3 Batch  836/3125   Loss: 0.786358 mae: 0.671496 (2072.960550377099 steps/sec)\n",
      "Step #10213\tEpoch   3 Batch  837/3125   Loss: 0.834530 mae: 0.736282 (2196.9368727607953 steps/sec)\n",
      "Step #10214\tEpoch   3 Batch  838/3125   Loss: 0.880689 mae: 0.727051 (1836.4656946451246 steps/sec)\n",
      "Step #10215\tEpoch   3 Batch  839/3125   Loss: 0.814891 mae: 0.726847 (1594.0287162804132 steps/sec)\n",
      "Step #10216\tEpoch   3 Batch  840/3125   Loss: 0.746129 mae: 0.697443 (2056.9388455691237 steps/sec)\n",
      "Step #10217\tEpoch   3 Batch  841/3125   Loss: 0.903684 mae: 0.752841 (2195.2810635402493 steps/sec)\n",
      "Step #10218\tEpoch   3 Batch  842/3125   Loss: 0.703296 mae: 0.659888 (1949.3158834027365 steps/sec)\n",
      "Step #10219\tEpoch   3 Batch  843/3125   Loss: 0.742747 mae: 0.702002 (2289.4922433651022 steps/sec)\n",
      "Step #10220\tEpoch   3 Batch  844/3125   Loss: 0.910382 mae: 0.722196 (2114.256333739956 steps/sec)\n",
      "Step #10221\tEpoch   3 Batch  845/3125   Loss: 0.736855 mae: 0.678753 (2253.2576929688844 steps/sec)\n",
      "Step #10222\tEpoch   3 Batch  846/3125   Loss: 0.651923 mae: 0.643957 (2275.3333550326033 steps/sec)\n",
      "Step #10223\tEpoch   3 Batch  847/3125   Loss: 0.837795 mae: 0.708096 (1728.3697470680831 steps/sec)\n",
      "Step #10224\tEpoch   3 Batch  848/3125   Loss: 0.962536 mae: 0.759388 (1877.3684728799449 steps/sec)\n",
      "Step #10225\tEpoch   3 Batch  849/3125   Loss: 0.930842 mae: 0.778631 (2022.403950007715 steps/sec)\n",
      "Step #10226\tEpoch   3 Batch  850/3125   Loss: 0.885410 mae: 0.747950 (2136.092974933029 steps/sec)\n",
      "Step #10227\tEpoch   3 Batch  851/3125   Loss: 0.818897 mae: 0.712906 (2183.9645925540226 steps/sec)\n",
      "Step #10228\tEpoch   3 Batch  852/3125   Loss: 0.822649 mae: 0.732133 (2088.713597067846 steps/sec)\n",
      "Step #10229\tEpoch   3 Batch  853/3125   Loss: 0.849631 mae: 0.724809 (2040.7457864621852 steps/sec)\n",
      "Step #10230\tEpoch   3 Batch  854/3125   Loss: 0.746107 mae: 0.665697 (2075.5455706099506 steps/sec)\n",
      "Step #10231\tEpoch   3 Batch  855/3125   Loss: 0.973364 mae: 0.797511 (2002.2646769589169 steps/sec)\n",
      "Step #10232\tEpoch   3 Batch  856/3125   Loss: 0.706569 mae: 0.656930 (1868.1204347051487 steps/sec)\n",
      "Step #10233\tEpoch   3 Batch  857/3125   Loss: 0.705960 mae: 0.676012 (2041.6398134717044 steps/sec)\n",
      "Step #10234\tEpoch   3 Batch  858/3125   Loss: 0.862687 mae: 0.732173 (2044.5857016115667 steps/sec)\n",
      "Step #10235\tEpoch   3 Batch  859/3125   Loss: 0.935260 mae: 0.726143 (2183.3279543585313 steps/sec)\n",
      "Step #10236\tEpoch   3 Batch  860/3125   Loss: 0.807399 mae: 0.695502 (2222.1242688819193 steps/sec)\n",
      "Step #10237\tEpoch   3 Batch  861/3125   Loss: 0.666226 mae: 0.661911 (2125.4847112003 steps/sec)\n",
      "Step #10238\tEpoch   3 Batch  862/3125   Loss: 0.798664 mae: 0.710515 (2148.853413119659 steps/sec)\n",
      "Step #10239\tEpoch   3 Batch  863/3125   Loss: 0.703245 mae: 0.658325 (2095.4546817078167 steps/sec)\n",
      "Step #10240\tEpoch   3 Batch  864/3125   Loss: 0.802605 mae: 0.710468 (2091.1503983567163 steps/sec)\n",
      "Step #10241\tEpoch   3 Batch  865/3125   Loss: 0.897938 mae: 0.717007 (1284.515876127009 steps/sec)\n",
      "Step #10242\tEpoch   3 Batch  866/3125   Loss: 0.837336 mae: 0.719985 (2111.574050766737 steps/sec)\n",
      "Step #10243\tEpoch   3 Batch  867/3125   Loss: 0.686047 mae: 0.659260 (2124.3866366822667 steps/sec)\n",
      "Step #10244\tEpoch   3 Batch  868/3125   Loss: 0.859040 mae: 0.732579 (2085.411135308215 steps/sec)\n",
      "Step #10245\tEpoch   3 Batch  869/3125   Loss: 0.910863 mae: 0.765143 (2121.3566797155545 steps/sec)\n",
      "Step #10246\tEpoch   3 Batch  870/3125   Loss: 0.740993 mae: 0.669098 (2170.0213157840276 steps/sec)\n",
      "Step #10247\tEpoch   3 Batch  871/3125   Loss: 0.714009 mae: 0.671185 (2216.1129427677743 steps/sec)\n",
      "Step #10248\tEpoch   3 Batch  872/3125   Loss: 0.686782 mae: 0.674035 (2007.535610353806 steps/sec)\n",
      "Step #10249\tEpoch   3 Batch  873/3125   Loss: 0.816588 mae: 0.732928 (2027.0955768635943 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10250\tEpoch   3 Batch  874/3125   Loss: 0.757314 mae: 0.700110 (2078.9404813830843 steps/sec)\n",
      "Step #10251\tEpoch   3 Batch  875/3125   Loss: 0.873252 mae: 0.761420 (2010.846469527864 steps/sec)\n",
      "Step #10252\tEpoch   3 Batch  876/3125   Loss: 0.908736 mae: 0.779351 (2053.0524337236166 steps/sec)\n",
      "Step #10253\tEpoch   3 Batch  877/3125   Loss: 0.758074 mae: 0.695084 (2034.016139044072 steps/sec)\n",
      "Step #10254\tEpoch   3 Batch  878/3125   Loss: 0.841556 mae: 0.712546 (1860.4472911473258 steps/sec)\n",
      "Step #10255\tEpoch   3 Batch  879/3125   Loss: 0.719738 mae: 0.687996 (1957.7412458808265 steps/sec)\n",
      "Step #10256\tEpoch   3 Batch  880/3125   Loss: 0.762139 mae: 0.709252 (1623.0570389288755 steps/sec)\n",
      "Step #10257\tEpoch   3 Batch  881/3125   Loss: 0.751295 mae: 0.704150 (2254.783945639669 steps/sec)\n",
      "Step #10258\tEpoch   3 Batch  882/3125   Loss: 0.761547 mae: 0.692315 (1885.2838058936695 steps/sec)\n",
      "Step #10259\tEpoch   3 Batch  883/3125   Loss: 0.853901 mae: 0.709923 (1917.0981424601434 steps/sec)\n",
      "Step #10260\tEpoch   3 Batch  884/3125   Loss: 0.964865 mae: 0.792759 (2210.134052778012 steps/sec)\n",
      "Step #10261\tEpoch   3 Batch  885/3125   Loss: 0.838851 mae: 0.734376 (1988.1421650882132 steps/sec)\n",
      "Step #10262\tEpoch   3 Batch  886/3125   Loss: 0.840299 mae: 0.731321 (2308.4184572032405 steps/sec)\n",
      "Step #10263\tEpoch   3 Batch  887/3125   Loss: 0.808238 mae: 0.710773 (2028.487691638052 steps/sec)\n",
      "Step #10264\tEpoch   3 Batch  888/3125   Loss: 0.830832 mae: 0.733117 (2105.0036636654722 steps/sec)\n",
      "Step #10265\tEpoch   3 Batch  889/3125   Loss: 0.960545 mae: 0.766896 (2181.0345903446555 steps/sec)\n",
      "Step #10266\tEpoch   3 Batch  890/3125   Loss: 0.761148 mae: 0.661422 (2335.8007640644664 steps/sec)\n",
      "Step #10267\tEpoch   3 Batch  891/3125   Loss: 0.764401 mae: 0.694998 (2011.907480069457 steps/sec)\n",
      "Step #10268\tEpoch   3 Batch  892/3125   Loss: 0.783318 mae: 0.696919 (1980.8933682192144 steps/sec)\n",
      "Step #10269\tEpoch   3 Batch  893/3125   Loss: 0.785913 mae: 0.704749 (1939.042476468739 steps/sec)\n",
      "Step #10270\tEpoch   3 Batch  894/3125   Loss: 0.988893 mae: 0.777844 (1943.8772767298512 steps/sec)\n",
      "Step #10271\tEpoch   3 Batch  895/3125   Loss: 0.736896 mae: 0.687458 (2034.2726329165494 steps/sec)\n",
      "Step #10272\tEpoch   3 Batch  896/3125   Loss: 0.830898 mae: 0.728705 (1814.2394933993114 steps/sec)\n",
      "Step #10273\tEpoch   3 Batch  897/3125   Loss: 0.840249 mae: 0.703799 (1943.8052072037 steps/sec)\n",
      "Step #10274\tEpoch   3 Batch  898/3125   Loss: 0.685856 mae: 0.655316 (1958.728646548423 steps/sec)\n",
      "Step #10275\tEpoch   3 Batch  899/3125   Loss: 0.896982 mae: 0.755011 (1918.6415867671815 steps/sec)\n",
      "Step #10276\tEpoch   3 Batch  900/3125   Loss: 0.814547 mae: 0.715081 (1947.415242039577 steps/sec)\n",
      "Step #10277\tEpoch   3 Batch  901/3125   Loss: 0.717705 mae: 0.677215 (2180.3543208849705 steps/sec)\n",
      "Step #10278\tEpoch   3 Batch  902/3125   Loss: 0.767275 mae: 0.712745 (2111.063911174641 steps/sec)\n",
      "Step #10279\tEpoch   3 Batch  903/3125   Loss: 0.856692 mae: 0.730100 (2162.7258477023347 steps/sec)\n",
      "Step #10280\tEpoch   3 Batch  904/3125   Loss: 0.910701 mae: 0.745188 (2263.5940721232205 steps/sec)\n",
      "Step #10281\tEpoch   3 Batch  905/3125   Loss: 0.850335 mae: 0.711015 (2343.553181503252 steps/sec)\n",
      "Step #10282\tEpoch   3 Batch  906/3125   Loss: 0.863701 mae: 0.743680 (2262.861335606461 steps/sec)\n",
      "Step #10283\tEpoch   3 Batch  907/3125   Loss: 0.776877 mae: 0.703721 (2195.0972387950346 steps/sec)\n",
      "Step #10284\tEpoch   3 Batch  908/3125   Loss: 0.805434 mae: 0.731636 (2031.8680786335055 steps/sec)\n",
      "Step #10285\tEpoch   3 Batch  909/3125   Loss: 0.803390 mae: 0.703297 (2110.9364147885694 steps/sec)\n",
      "Step #10286\tEpoch   3 Batch  910/3125   Loss: 0.884955 mae: 0.746148 (2108.1141938078003 steps/sec)\n",
      "Step #10287\tEpoch   3 Batch  911/3125   Loss: 0.769634 mae: 0.690106 (2146.587919792829 steps/sec)\n",
      "Step #10288\tEpoch   3 Batch  912/3125   Loss: 0.929958 mae: 0.753366 (2248.908334405696 steps/sec)\n",
      "Step #10289\tEpoch   3 Batch  913/3125   Loss: 0.796214 mae: 0.712884 (2278.720445062587 steps/sec)\n",
      "Step #10290\tEpoch   3 Batch  914/3125   Loss: 0.870870 mae: 0.748063 (2085.6600133265706 steps/sec)\n",
      "Step #10291\tEpoch   3 Batch  915/3125   Loss: 0.857883 mae: 0.724309 (2027.1739550709508 steps/sec)\n",
      "Step #10292\tEpoch   3 Batch  916/3125   Loss: 0.882991 mae: 0.743720 (2116.689039837702 steps/sec)\n",
      "Step #10293\tEpoch   3 Batch  917/3125   Loss: 0.645097 mae: 0.649210 (1864.9971542401822 steps/sec)\n",
      "Step #10294\tEpoch   3 Batch  918/3125   Loss: 0.823472 mae: 0.708944 (1922.7754907443912 steps/sec)\n",
      "Step #10295\tEpoch   3 Batch  919/3125   Loss: 0.796055 mae: 0.700820 (2031.7106015248835 steps/sec)\n",
      "Step #10296\tEpoch   3 Batch  920/3125   Loss: 0.805911 mae: 0.686858 (2118.035833316501 steps/sec)\n",
      "Step #10297\tEpoch   3 Batch  921/3125   Loss: 0.879777 mae: 0.732304 (2181.420264830398 steps/sec)\n",
      "Step #10298\tEpoch   3 Batch  922/3125   Loss: 0.777145 mae: 0.697540 (2084.2919188606297 steps/sec)\n",
      "Step #10299\tEpoch   3 Batch  923/3125   Loss: 0.781426 mae: 0.687966 (2205.647816072612 steps/sec)\n",
      "Step #10300\tEpoch   3 Batch  924/3125   Loss: 0.931090 mae: 0.732114 (2231.3688354524656 steps/sec)\n",
      "Step #10301\tEpoch   3 Batch  925/3125   Loss: 0.825775 mae: 0.742629 (2140.58445866634 steps/sec)\n",
      "Step #10302\tEpoch   3 Batch  926/3125   Loss: 0.783386 mae: 0.715507 (1922.5639662269323 steps/sec)\n",
      "Step #10303\tEpoch   3 Batch  927/3125   Loss: 0.703545 mae: 0.652269 (1803.102108195481 steps/sec)\n",
      "Step #10304\tEpoch   3 Batch  928/3125   Loss: 0.844478 mae: 0.729631 (2119.44859926426 steps/sec)\n",
      "Step #10305\tEpoch   3 Batch  929/3125   Loss: 0.795972 mae: 0.710888 (2040.5670750099735 steps/sec)\n",
      "Step #10306\tEpoch   3 Batch  930/3125   Loss: 0.786690 mae: 0.685257 (2317.2694224373213 steps/sec)\n",
      "Step #10307\tEpoch   3 Batch  931/3125   Loss: 0.910748 mae: 0.763069 (2151.653380118399 steps/sec)\n",
      "Step #10308\tEpoch   3 Batch  932/3125   Loss: 0.845057 mae: 0.729434 (2108.3685205292154 steps/sec)\n",
      "Step #10309\tEpoch   3 Batch  933/3125   Loss: 0.754048 mae: 0.700921 (2006.0953328422886 steps/sec)\n",
      "Step #10310\tEpoch   3 Batch  934/3125   Loss: 0.809858 mae: 0.731294 (2130.9271960575115 steps/sec)\n",
      "Step #10311\tEpoch   3 Batch  935/3125   Loss: 0.900526 mae: 0.759455 (1656.4004138726316 steps/sec)\n",
      "Step #10312\tEpoch   3 Batch  936/3125   Loss: 0.798652 mae: 0.730489 (1931.257021825214 steps/sec)\n",
      "Step #10313\tEpoch   3 Batch  937/3125   Loss: 0.751112 mae: 0.669728 (2160.542311416974 steps/sec)\n",
      "Step #10314\tEpoch   3 Batch  938/3125   Loss: 0.837107 mae: 0.745730 (2022.0139612017433 steps/sec)\n",
      "Step #10315\tEpoch   3 Batch  939/3125   Loss: 0.898575 mae: 0.731673 (2267.2648842665167 steps/sec)\n",
      "Step #10316\tEpoch   3 Batch  940/3125   Loss: 0.810120 mae: 0.706967 (2179.312064844643 steps/sec)\n",
      "Step #10317\tEpoch   3 Batch  941/3125   Loss: 0.778427 mae: 0.706003 (2133.658903844784 steps/sec)\n",
      "Step #10318\tEpoch   3 Batch  942/3125   Loss: 0.770285 mae: 0.702351 (2066.118894208981 steps/sec)\n",
      "Step #10319\tEpoch   3 Batch  943/3125   Loss: 0.800416 mae: 0.701959 (1915.207305936073 steps/sec)\n",
      "Step #10320\tEpoch   3 Batch  944/3125   Loss: 0.784407 mae: 0.695330 (1974.8123734639107 steps/sec)\n",
      "Step #10321\tEpoch   3 Batch  945/3125   Loss: 0.820645 mae: 0.723624 (1950.6939018491646 steps/sec)\n",
      "Step #10322\tEpoch   3 Batch  946/3125   Loss: 0.899193 mae: 0.738282 (2321.142224681793 steps/sec)\n",
      "Step #10323\tEpoch   3 Batch  947/3125   Loss: 0.702918 mae: 0.655233 (2201.9655606887864 steps/sec)\n",
      "Step #10324\tEpoch   3 Batch  948/3125   Loss: 0.646974 mae: 0.648222 (2035.003008131659 steps/sec)\n",
      "Step #10325\tEpoch   3 Batch  949/3125   Loss: 0.707012 mae: 0.670562 (2138.031155696925 steps/sec)\n",
      "Step #10326\tEpoch   3 Batch  950/3125   Loss: 0.838376 mae: 0.715919 (2051.706696668786 steps/sec)\n",
      "Step #10327\tEpoch   3 Batch  951/3125   Loss: 0.889882 mae: 0.729662 (2145.9948426179853 steps/sec)\n",
      "Step #10328\tEpoch   3 Batch  952/3125   Loss: 0.915529 mae: 0.737430 (2141.5899923410775 steps/sec)\n",
      "Step #10329\tEpoch   3 Batch  953/3125   Loss: 0.839485 mae: 0.731575 (1678.0303575857959 steps/sec)\n",
      "Step #10330\tEpoch   3 Batch  954/3125   Loss: 0.879209 mae: 0.748290 (2155.6343601920094 steps/sec)\n",
      "Step #10331\tEpoch   3 Batch  955/3125   Loss: 0.879599 mae: 0.736901 (2050.2023658226612 steps/sec)\n",
      "Step #10332\tEpoch   3 Batch  956/3125   Loss: 0.760781 mae: 0.685994 (2233.8407132433613 steps/sec)\n",
      "Step #10333\tEpoch   3 Batch  957/3125   Loss: 0.880568 mae: 0.752091 (2132.378899418392 steps/sec)\n",
      "Step #10334\tEpoch   3 Batch  958/3125   Loss: 0.911258 mae: 0.731903 (2324.6156404145654 steps/sec)\n",
      "Step #10335\tEpoch   3 Batch  959/3125   Loss: 0.922312 mae: 0.746463 (2032.8528639143879 steps/sec)\n",
      "Step #10336\tEpoch   3 Batch  960/3125   Loss: 0.906941 mae: 0.728074 (2018.957765733155 steps/sec)\n",
      "Step #10337\tEpoch   3 Batch  961/3125   Loss: 0.837120 mae: 0.725113 (2051.3655215588074 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10338\tEpoch   3 Batch  962/3125   Loss: 0.723479 mae: 0.673977 (1950.6213259915173 steps/sec)\n",
      "Step #10339\tEpoch   3 Batch  963/3125   Loss: 0.789253 mae: 0.693208 (1906.4844865046682 steps/sec)\n",
      "Step #10340\tEpoch   3 Batch  964/3125   Loss: 0.755012 mae: 0.691095 (2023.145343340601 steps/sec)\n",
      "Step #10341\tEpoch   3 Batch  965/3125   Loss: 0.696969 mae: 0.674868 (2209.5519054291826 steps/sec)\n",
      "Step #10342\tEpoch   3 Batch  966/3125   Loss: 0.812459 mae: 0.714971 (2332.8386931710734 steps/sec)\n",
      "Step #10343\tEpoch   3 Batch  967/3125   Loss: 0.788530 mae: 0.697991 (2177.5922580109236 steps/sec)\n",
      "Step #10344\tEpoch   3 Batch  968/3125   Loss: 0.964945 mae: 0.758899 (2011.3286082845004 steps/sec)\n",
      "Step #10345\tEpoch   3 Batch  969/3125   Loss: 0.789609 mae: 0.706710 (2066.09853895944 steps/sec)\n",
      "Step #10346\tEpoch   3 Batch  970/3125   Loss: 0.901028 mae: 0.758821 (2122.945791365086 steps/sec)\n",
      "Step #10347\tEpoch   3 Batch  971/3125   Loss: 0.810279 mae: 0.710608 (1768.8081441933825 steps/sec)\n",
      "Step #10348\tEpoch   3 Batch  972/3125   Loss: 0.798016 mae: 0.731909 (1859.6389174617814 steps/sec)\n",
      "Step #10349\tEpoch   3 Batch  973/3125   Loss: 0.882542 mae: 0.739759 (2209.4587903115353 steps/sec)\n",
      "Step #10350\tEpoch   3 Batch  974/3125   Loss: 0.746867 mae: 0.669106 (2047.0804130956797 steps/sec)\n",
      "Step #10351\tEpoch   3 Batch  975/3125   Loss: 0.758586 mae: 0.678276 (2060.597009059288 steps/sec)\n",
      "Step #10352\tEpoch   3 Batch  976/3125   Loss: 0.815258 mae: 0.708216 (2111.9142808229526 steps/sec)\n",
      "Step #10353\tEpoch   3 Batch  977/3125   Loss: 0.701117 mae: 0.679799 (2321.9131975199293 steps/sec)\n",
      "Step #10354\tEpoch   3 Batch  978/3125   Loss: 0.644874 mae: 0.639044 (2082.284488750323 steps/sec)\n",
      "Step #10355\tEpoch   3 Batch  979/3125   Loss: 0.874767 mae: 0.735265 (2259.1804197009524 steps/sec)\n",
      "Step #10356\tEpoch   3 Batch  980/3125   Loss: 0.852049 mae: 0.723155 (1860.612351725178 steps/sec)\n",
      "Step #10357\tEpoch   3 Batch  981/3125   Loss: 0.830128 mae: 0.758088 (1868.386728912013 steps/sec)\n",
      "Step #10358\tEpoch   3 Batch  982/3125   Loss: 0.899258 mae: 0.724799 (2089.44195917066 steps/sec)\n",
      "Step #10359\tEpoch   3 Batch  983/3125   Loss: 0.825770 mae: 0.711663 (2010.2104001917087 steps/sec)\n",
      "Step #10360\tEpoch   3 Batch  984/3125   Loss: 0.838346 mae: 0.734395 (2039.3962968725689 steps/sec)\n",
      "Step #10361\tEpoch   3 Batch  985/3125   Loss: 0.874087 mae: 0.740025 (2136.83298859827 steps/sec)\n",
      "Step #10362\tEpoch   3 Batch  986/3125   Loss: 0.924307 mae: 0.774259 (2057.6048350699552 steps/sec)\n",
      "Step #10363\tEpoch   3 Batch  987/3125   Loss: 0.717550 mae: 0.672191 (2088.1312729010674 steps/sec)\n",
      "Step #10364\tEpoch   3 Batch  988/3125   Loss: 0.725096 mae: 0.665451 (1840.6565146794226 steps/sec)\n",
      "Step #10365\tEpoch   3 Batch  989/3125   Loss: 0.823121 mae: 0.732868 (1687.7671903167654 steps/sec)\n",
      "Step #10366\tEpoch   3 Batch  990/3125   Loss: 0.673280 mae: 0.633055 (1870.3529957369387 steps/sec)\n",
      "Step #10367\tEpoch   3 Batch  991/3125   Loss: 0.825268 mae: 0.702916 (2307.326357945231 steps/sec)\n",
      "Step #10368\tEpoch   3 Batch  992/3125   Loss: 0.843154 mae: 0.694077 (2098.033173933052 steps/sec)\n",
      "Step #10369\tEpoch   3 Batch  993/3125   Loss: 0.839070 mae: 0.717836 (2031.0415960486175 steps/sec)\n",
      "Step #10370\tEpoch   3 Batch  994/3125   Loss: 0.730039 mae: 0.666092 (2140.58445866634 steps/sec)\n",
      "Step #10371\tEpoch   3 Batch  995/3125   Loss: 0.716416 mae: 0.665822 (2183.7826580446304 steps/sec)\n",
      "Step #10372\tEpoch   3 Batch  996/3125   Loss: 0.824055 mae: 0.733145 (1919.9941406428813 steps/sec)\n",
      "Step #10373\tEpoch   3 Batch  997/3125   Loss: 0.787459 mae: 0.713110 (1862.6946272660255 steps/sec)\n",
      "Step #10374\tEpoch   3 Batch  998/3125   Loss: 0.722992 mae: 0.700001 (1951.292858804373 steps/sec)\n",
      "Step #10375\tEpoch   3 Batch  999/3125   Loss: 0.880254 mae: 0.733134 (2183.1233994711747 steps/sec)\n",
      "Step #10376\tEpoch   3 Batch 1000/3125   Loss: 0.897232 mae: 0.750620 (1940.980693408363 steps/sec)\n",
      "Step #10377\tEpoch   3 Batch 1001/3125   Loss: 0.772759 mae: 0.709212 (2123.2467019671767 steps/sec)\n",
      "Step #10378\tEpoch   3 Batch 1002/3125   Loss: 0.796547 mae: 0.698194 (2244.8880848649633 steps/sec)\n",
      "Step #10379\tEpoch   3 Batch 1003/3125   Loss: 1.011052 mae: 0.815186 (2014.6713547370646 steps/sec)\n",
      "Step #10380\tEpoch   3 Batch 1004/3125   Loss: 0.761357 mae: 0.683360 (2103.5045838432065 steps/sec)\n",
      "Step #10381\tEpoch   3 Batch 1005/3125   Loss: 0.674943 mae: 0.666684 (2042.0970631767548 steps/sec)\n",
      "Step #10382\tEpoch   3 Batch 1006/3125   Loss: 0.848380 mae: 0.699661 (1635.7546779817014 steps/sec)\n",
      "Step #10383\tEpoch   3 Batch 1007/3125   Loss: 0.846771 mae: 0.743764 (2127.123165400493 steps/sec)\n",
      "Step #10384\tEpoch   3 Batch 1008/3125   Loss: 0.757406 mae: 0.674984 (2164.8691056239163 steps/sec)\n",
      "Step #10385\tEpoch   3 Batch 1009/3125   Loss: 0.856948 mae: 0.737484 (2099.524462642786 steps/sec)\n",
      "Step #10386\tEpoch   3 Batch 1010/3125   Loss: 0.957458 mae: 0.748740 (2354.155113770304 steps/sec)\n",
      "Step #10387\tEpoch   3 Batch 1011/3125   Loss: 0.834399 mae: 0.723644 (2310.5293890816943 steps/sec)\n",
      "Step #10388\tEpoch   3 Batch 1012/3125   Loss: 0.788772 mae: 0.709033 (2125.570882701722 steps/sec)\n",
      "Step #10389\tEpoch   3 Batch 1013/3125   Loss: 0.804576 mae: 0.702669 (1710.6202486214884 steps/sec)\n",
      "Step #10390\tEpoch   3 Batch 1014/3125   Loss: 0.833612 mae: 0.722707 (2137.6606696906374 steps/sec)\n",
      "Step #10391\tEpoch   3 Batch 1015/3125   Loss: 0.883442 mae: 0.738313 (2312.4656794098514 steps/sec)\n",
      "Step #10392\tEpoch   3 Batch 1016/3125   Loss: 0.914640 mae: 0.746306 (2116.069662785301 steps/sec)\n",
      "Step #10393\tEpoch   3 Batch 1017/3125   Loss: 0.839534 mae: 0.747295 (2137.5735151719005 steps/sec)\n",
      "Step #10394\tEpoch   3 Batch 1018/3125   Loss: 0.747893 mae: 0.677922 (2190.0998370859265 steps/sec)\n",
      "Step #10395\tEpoch   3 Batch 1019/3125   Loss: 0.860099 mae: 0.740884 (2158.229906349696 steps/sec)\n",
      "Step #10396\tEpoch   3 Batch 1020/3125   Loss: 0.819920 mae: 0.726206 (2192.206054524168 steps/sec)\n",
      "Step #10397\tEpoch   3 Batch 1021/3125   Loss: 0.884553 mae: 0.716746 (2178.2708047696206 steps/sec)\n",
      "Step #10398\tEpoch   3 Batch 1022/3125   Loss: 0.837599 mae: 0.720710 (1890.1434854711947 steps/sec)\n",
      "Step #10399\tEpoch   3 Batch 1023/3125   Loss: 0.760502 mae: 0.697543 (1907.1606554991724 steps/sec)\n",
      "Step #10400\tEpoch   3 Batch 1024/3125   Loss: 0.818300 mae: 0.711356 (2122.6879358684982 steps/sec)\n",
      "Step #10401\tEpoch   3 Batch 1025/3125   Loss: 0.790434 mae: 0.705477 (2164.690338563171 steps/sec)\n",
      "Step #10402\tEpoch   3 Batch 1026/3125   Loss: 0.815260 mae: 0.732612 (2191.450097704212 steps/sec)\n",
      "Step #10403\tEpoch   3 Batch 1027/3125   Loss: 0.850136 mae: 0.737170 (2306.818756806107 steps/sec)\n",
      "Step #10404\tEpoch   3 Batch 1028/3125   Loss: 0.921124 mae: 0.747992 (2061.9544377476477 steps/sec)\n",
      "Step #10405\tEpoch   3 Batch 1029/3125   Loss: 0.773650 mae: 0.698112 (2266.8482608037702 steps/sec)\n",
      "Step #10406\tEpoch   3 Batch 1030/3125   Loss: 0.733417 mae: 0.671888 (2014.6713547370646 steps/sec)\n",
      "Step #10407\tEpoch   3 Batch 1031/3125   Loss: 0.772612 mae: 0.678153 (1766.543402265931 steps/sec)\n",
      "Step #10408\tEpoch   3 Batch 1032/3125   Loss: 0.925655 mae: 0.746412 (1888.7305804476066 steps/sec)\n",
      "Step #10409\tEpoch   3 Batch 1033/3125   Loss: 0.838475 mae: 0.732354 (2108.1141938078003 steps/sec)\n",
      "Step #10410\tEpoch   3 Batch 1034/3125   Loss: 0.808771 mae: 0.699935 (2138.3799657394566 steps/sec)\n",
      "Step #10411\tEpoch   3 Batch 1035/3125   Loss: 0.830905 mae: 0.731802 (2147.7530621441156 steps/sec)\n",
      "Step #10412\tEpoch   3 Batch 1036/3125   Loss: 0.912681 mae: 0.744645 (1990.7843900401544 steps/sec)\n",
      "Step #10413\tEpoch   3 Batch 1037/3125   Loss: 0.687849 mae: 0.681774 (2152.3158553732155 steps/sec)\n",
      "Step #10414\tEpoch   3 Batch 1038/3125   Loss: 0.896617 mae: 0.746081 (2164.8914535825998 steps/sec)\n",
      "Step #10415\tEpoch   3 Batch 1039/3125   Loss: 0.815593 mae: 0.693677 (2033.523063347846 steps/sec)\n",
      "Step #10416\tEpoch   3 Batch 1040/3125   Loss: 0.697580 mae: 0.674991 (1721.7713994844091 steps/sec)\n",
      "Step #10417\tEpoch   3 Batch 1041/3125   Loss: 0.980733 mae: 0.767993 (1593.677427199222 steps/sec)\n",
      "Step #10418\tEpoch   3 Batch 1042/3125   Loss: 0.788510 mae: 0.682246 (2151.9183212764865 steps/sec)\n",
      "Step #10419\tEpoch   3 Batch 1043/3125   Loss: 0.843698 mae: 0.725589 (2057.786543424293 steps/sec)\n",
      "Step #10420\tEpoch   3 Batch 1044/3125   Loss: 0.843647 mae: 0.737773 (2085.3281891674205 steps/sec)\n",
      "Step #10421\tEpoch   3 Batch 1045/3125   Loss: 0.992814 mae: 0.790351 (2033.5033452923494 steps/sec)\n",
      "Step #10422\tEpoch   3 Batch 1046/3125   Loss: 0.752839 mae: 0.692206 (2069.625974538636 steps/sec)\n",
      "Step #10423\tEpoch   3 Batch 1047/3125   Loss: 0.784650 mae: 0.704728 (2070.8317287278687 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10424\tEpoch   3 Batch 1048/3125   Loss: 0.788504 mae: 0.700039 (2148.6992961137694 steps/sec)\n",
      "Step #10425\tEpoch   3 Batch 1049/3125   Loss: 0.891457 mae: 0.748332 (1944.9228857336288 steps/sec)\n",
      "Step #10426\tEpoch   3 Batch 1050/3125   Loss: 0.776316 mae: 0.712288 (2118.913238964162 steps/sec)\n",
      "Step #10427\tEpoch   3 Batch 1051/3125   Loss: 0.841277 mae: 0.741362 (2037.316028250289 steps/sec)\n",
      "Step #10428\tEpoch   3 Batch 1052/3125   Loss: 0.731834 mae: 0.658063 (1877.855978796182 steps/sec)\n",
      "Step #10429\tEpoch   3 Batch 1053/3125   Loss: 0.864869 mae: 0.734903 (2039.5549677118183 steps/sec)\n",
      "Step #10430\tEpoch   3 Batch 1054/3125   Loss: 0.735733 mae: 0.701404 (2308.4946887555725 steps/sec)\n",
      "Step #10431\tEpoch   3 Batch 1055/3125   Loss: 0.776634 mae: 0.712194 (2226.1814784934822 steps/sec)\n",
      "Step #10432\tEpoch   3 Batch 1056/3125   Loss: 0.967612 mae: 0.795362 (2273.8777811510604 steps/sec)\n",
      "Step #10433\tEpoch   3 Batch 1057/3125   Loss: 0.849328 mae: 0.737660 (1668.2326924453707 steps/sec)\n",
      "Step #10434\tEpoch   3 Batch 1058/3125   Loss: 0.828571 mae: 0.726540 (1850.0092626081741 steps/sec)\n",
      "Step #10435\tEpoch   3 Batch 1059/3125   Loss: 0.725582 mae: 0.684383 (2045.5629035719162 steps/sec)\n",
      "Step #10436\tEpoch   3 Batch 1060/3125   Loss: 0.885145 mae: 0.744785 (1903.888298789843 steps/sec)\n",
      "Step #10437\tEpoch   3 Batch 1061/3125   Loss: 0.784461 mae: 0.675525 (1953.3466217097298 steps/sec)\n",
      "Step #10438\tEpoch   3 Batch 1062/3125   Loss: 0.742991 mae: 0.686425 (2106.5260408819245 steps/sec)\n",
      "Step #10439\tEpoch   3 Batch 1063/3125   Loss: 0.646938 mae: 0.637324 (2078.034086405073 steps/sec)\n",
      "Step #10440\tEpoch   3 Batch 1064/3125   Loss: 0.670206 mae: 0.658594 (1952.2010705143123 steps/sec)\n",
      "Step #10441\tEpoch   3 Batch 1065/3125   Loss: 0.828559 mae: 0.715626 (1879.3536997374292 steps/sec)\n",
      "Step #10442\tEpoch   3 Batch 1066/3125   Loss: 0.737955 mae: 0.681400 (1864.0025598179684 steps/sec)\n",
      "Step #10443\tEpoch   3 Batch 1067/3125   Loss: 0.831724 mae: 0.721000 (1857.695101426167 steps/sec)\n",
      "Step #10444\tEpoch   3 Batch 1068/3125   Loss: 0.809272 mae: 0.728522 (1853.4919484559773 steps/sec)\n",
      "Step #10445\tEpoch   3 Batch 1069/3125   Loss: 0.820462 mae: 0.708797 (2052.0279063395924 steps/sec)\n",
      "Step #10446\tEpoch   3 Batch 1070/3125   Loss: 0.815840 mae: 0.720526 (2039.9914398552557 steps/sec)\n",
      "Step #10447\tEpoch   3 Batch 1071/3125   Loss: 0.745184 mae: 0.681129 (1988.783203254654 steps/sec)\n",
      "Step #10448\tEpoch   3 Batch 1072/3125   Loss: 0.771155 mae: 0.689807 (1980.3696044269432 steps/sec)\n",
      "Step #10449\tEpoch   3 Batch 1073/3125   Loss: 0.639732 mae: 0.633537 (1902.1441787904075 steps/sec)\n",
      "Step #10450\tEpoch   3 Batch 1074/3125   Loss: 0.830503 mae: 0.718641 (1788.7225676586236 steps/sec)\n",
      "Step #10451\tEpoch   3 Batch 1075/3125   Loss: 0.783376 mae: 0.713911 (2072.3664966994743 steps/sec)\n",
      "Step #10452\tEpoch   3 Batch 1076/3125   Loss: 0.762172 mae: 0.694142 (2182.8279989591465 steps/sec)\n",
      "Step #10453\tEpoch   3 Batch 1077/3125   Loss: 0.735120 mae: 0.662863 (2040.487657744437 steps/sec)\n",
      "Step #10454\tEpoch   3 Batch 1078/3125   Loss: 0.865605 mae: 0.729967 (2291.9944480267545 steps/sec)\n",
      "Step #10455\tEpoch   3 Batch 1079/3125   Loss: 0.819052 mae: 0.709348 (2193.0771965783365 steps/sec)\n",
      "Step #10456\tEpoch   3 Batch 1080/3125   Loss: 0.854231 mae: 0.732195 (1908.6186497752053 steps/sec)\n",
      "Step #10457\tEpoch   3 Batch 1081/3125   Loss: 0.954948 mae: 0.777917 (1997.439804937519 steps/sec)\n",
      "Step #10458\tEpoch   3 Batch 1082/3125   Loss: 0.924128 mae: 0.770794 (1814.0197910179227 steps/sec)\n",
      "Step #10459\tEpoch   3 Batch 1083/3125   Loss: 0.753549 mae: 0.685450 (1935.82070264183 steps/sec)\n",
      "Step #10460\tEpoch   3 Batch 1084/3125   Loss: 0.830456 mae: 0.692512 (2032.8528639143879 steps/sec)\n",
      "Step #10461\tEpoch   3 Batch 1085/3125   Loss: 0.715855 mae: 0.667678 (2109.810865191147 steps/sec)\n",
      "Step #10462\tEpoch   3 Batch 1086/3125   Loss: 0.906538 mae: 0.738718 (1915.9421878711469 steps/sec)\n",
      "Step #10463\tEpoch   3 Batch 1087/3125   Loss: 0.791176 mae: 0.698307 (1624.804952313068 steps/sec)\n",
      "Step #10464\tEpoch   3 Batch 1088/3125   Loss: 0.811253 mae: 0.738252 (1622.3162552506788 steps/sec)\n",
      "Step #10465\tEpoch   3 Batch 1089/3125   Loss: 0.823246 mae: 0.725397 (1582.374068149579 steps/sec)\n",
      "Step #10466\tEpoch   3 Batch 1090/3125   Loss: 0.868996 mae: 0.722047 (1328.0931181011608 steps/sec)\n",
      "Step #10467\tEpoch   3 Batch 1091/3125   Loss: 0.904476 mae: 0.753953 (1733.8712877835835 steps/sec)\n",
      "Step #10468\tEpoch   3 Batch 1092/3125   Loss: 0.786342 mae: 0.704388 (1949.0260223048326 steps/sec)\n",
      "Step #10469\tEpoch   3 Batch 1093/3125   Loss: 0.821600 mae: 0.708351 (2037.0785534585086 steps/sec)\n",
      "Step #10470\tEpoch   3 Batch 1094/3125   Loss: 0.814922 mae: 0.712956 (1849.6666078673488 steps/sec)\n",
      "Step #10471\tEpoch   3 Batch 1095/3125   Loss: 0.755645 mae: 0.686407 (1933.7144543208055 steps/sec)\n",
      "Step #10472\tEpoch   3 Batch 1096/3125   Loss: 0.702575 mae: 0.661891 (1962.0088316742758 steps/sec)\n",
      "Step #10473\tEpoch   3 Batch 1097/3125   Loss: 0.953732 mae: 0.750839 (2126.756449780951 steps/sec)\n",
      "Step #10474\tEpoch   3 Batch 1098/3125   Loss: 0.941215 mae: 0.769626 (1883.1507488955137 steps/sec)\n",
      "Step #10475\tEpoch   3 Batch 1099/3125   Loss: 0.784908 mae: 0.721699 (1920.5040385355044 steps/sec)\n",
      "Step #10476\tEpoch   3 Batch 1100/3125   Loss: 0.764713 mae: 0.690180 (2144.173729896633 steps/sec)\n",
      "Step #10477\tEpoch   3 Batch 1101/3125   Loss: 0.858830 mae: 0.715888 (2100.2183187453684 steps/sec)\n",
      "Step #10478\tEpoch   3 Batch 1102/3125   Loss: 0.956992 mae: 0.768404 (1961.1648306432005 steps/sec)\n",
      "Step #10479\tEpoch   3 Batch 1103/3125   Loss: 0.919745 mae: 0.753409 (1971.8414743077428 steps/sec)\n",
      "Step #10480\tEpoch   3 Batch 1104/3125   Loss: 0.881801 mae: 0.740333 (2249.6320611014567 steps/sec)\n",
      "Step #10481\tEpoch   3 Batch 1105/3125   Loss: 0.691498 mae: 0.657281 (2119.1915925626517 steps/sec)\n",
      "Step #10482\tEpoch   3 Batch 1106/3125   Loss: 0.871462 mae: 0.721606 (2232.2476263464896 steps/sec)\n",
      "Step #10483\tEpoch   3 Batch 1107/3125   Loss: 0.776305 mae: 0.700342 (1820.04946843133 steps/sec)\n",
      "Step #10484\tEpoch   3 Batch 1108/3125   Loss: 0.937640 mae: 0.783205 (1951.52891254583 steps/sec)\n",
      "Step #10485\tEpoch   3 Batch 1109/3125   Loss: 0.749359 mae: 0.687315 (2090.1916617663182 steps/sec)\n",
      "Step #10486\tEpoch   3 Batch 1110/3125   Loss: 0.936863 mae: 0.787462 (1996.7171284394935 steps/sec)\n",
      "Step #10487\tEpoch   3 Batch 1111/3125   Loss: 0.784643 mae: 0.706731 (2073.390939829553 steps/sec)\n",
      "Step #10488\tEpoch   3 Batch 1112/3125   Loss: 0.782841 mae: 0.702513 (2000.08774188626 steps/sec)\n",
      "Step #10489\tEpoch   3 Batch 1113/3125   Loss: 0.874740 mae: 0.747529 (1969.2862441662833 steps/sec)\n",
      "Step #10490\tEpoch   3 Batch 1114/3125   Loss: 0.785676 mae: 0.701305 (2123.2037095157584 steps/sec)\n",
      "Step #10491\tEpoch   3 Batch 1115/3125   Loss: 0.853198 mae: 0.737883 (1951.6742047759972 steps/sec)\n",
      "Step #10492\tEpoch   3 Batch 1116/3125   Loss: 0.851089 mae: 0.718067 (1822.5486021187655 steps/sec)\n",
      "Step #10493\tEpoch   3 Batch 1117/3125   Loss: 0.738497 mae: 0.685219 (1990.5009586362687 steps/sec)\n",
      "Step #10494\tEpoch   3 Batch 1118/3125   Loss: 0.749580 mae: 0.681818 (2197.880880765482 steps/sec)\n",
      "Step #10495\tEpoch   3 Batch 1119/3125   Loss: 0.796685 mae: 0.682337 (1895.3023045639404 steps/sec)\n",
      "Step #10496\tEpoch   3 Batch 1120/3125   Loss: 0.795496 mae: 0.718857 (2118.014442256224 steps/sec)\n",
      "Step #10497\tEpoch   3 Batch 1121/3125   Loss: 0.834785 mae: 0.741218 (1974.4219326655118 steps/sec)\n",
      "Step #10498\tEpoch   3 Batch 1122/3125   Loss: 0.786047 mae: 0.689935 (2120.069956226812 steps/sec)\n",
      "Step #10499\tEpoch   3 Batch 1123/3125   Loss: 0.632674 mae: 0.631366 (1859.6389174617814 steps/sec)\n",
      "Step #10500\tEpoch   3 Batch 1124/3125   Loss: 0.829228 mae: 0.701392 (1773.1901581127927 steps/sec)\n",
      "Step #10501\tEpoch   3 Batch 1125/3125   Loss: 0.747079 mae: 0.672873 (2076.0592381404927 steps/sec)\n",
      "Step #10502\tEpoch   3 Batch 1126/3125   Loss: 0.888181 mae: 0.765227 (2127.74903106674 steps/sec)\n",
      "Step #10503\tEpoch   3 Batch 1127/3125   Loss: 0.785845 mae: 0.676444 (2192.206054524168 steps/sec)\n",
      "Step #10504\tEpoch   3 Batch 1128/3125   Loss: 0.859020 mae: 0.722004 (2199.7251853949674 steps/sec)\n",
      "Step #10505\tEpoch   3 Batch 1129/3125   Loss: 0.856955 mae: 0.745168 (2142.8591864469126 steps/sec)\n",
      "Step #10506\tEpoch   3 Batch 1130/3125   Loss: 0.759252 mae: 0.706637 (2199.3560768932284 steps/sec)\n",
      "Step #10507\tEpoch   3 Batch 1131/3125   Loss: 0.737336 mae: 0.669757 (1936.62514198118 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10508\tEpoch   3 Batch 1132/3125   Loss: 0.856640 mae: 0.723232 (2054.9031904052677 steps/sec)\n",
      "Step #10509\tEpoch   3 Batch 1133/3125   Loss: 0.924380 mae: 0.768901 (1804.4829158743407 steps/sec)\n",
      "Step #10510\tEpoch   3 Batch 1134/3125   Loss: 0.708818 mae: 0.676921 (1951.52891254583 steps/sec)\n",
      "Step #10511\tEpoch   3 Batch 1135/3125   Loss: 0.754918 mae: 0.713731 (2045.4032965961183 steps/sec)\n",
      "Step #10512\tEpoch   3 Batch 1136/3125   Loss: 0.814105 mae: 0.709665 (2134.9404458922936 steps/sec)\n",
      "Step #10513\tEpoch   3 Batch 1137/3125   Loss: 0.831862 mae: 0.729178 (2128.6561104344296 steps/sec)\n",
      "Step #10514\tEpoch   3 Batch 1138/3125   Loss: 0.800579 mae: 0.706748 (2019.4049109292248 steps/sec)\n",
      "Step #10515\tEpoch   3 Batch 1139/3125   Loss: 0.827757 mae: 0.743656 (1957.1017955130837 steps/sec)\n",
      "Step #10516\tEpoch   3 Batch 1140/3125   Loss: 0.799165 mae: 0.705192 (1990.0665205302664 steps/sec)\n",
      "Step #10517\tEpoch   3 Batch 1141/3125   Loss: 0.917799 mae: 0.749927 (1869.569325951878 steps/sec)\n",
      "Step #10518\tEpoch   3 Batch 1142/3125   Loss: 0.639090 mae: 0.607065 (2093.4465995188516 steps/sec)\n",
      "Step #10519\tEpoch   3 Batch 1143/3125   Loss: 0.979906 mae: 0.796767 (2257.2108191886687 steps/sec)\n",
      "Step #10520\tEpoch   3 Batch 1144/3125   Loss: 0.829646 mae: 0.716202 (2151.476788920236 steps/sec)\n",
      "Step #10521\tEpoch   3 Batch 1145/3125   Loss: 0.761735 mae: 0.681183 (2126.2173917451564 steps/sec)\n",
      "Step #10522\tEpoch   3 Batch 1146/3125   Loss: 0.906640 mae: 0.747851 (2310.9622250628113 steps/sec)\n",
      "Step #10523\tEpoch   3 Batch 1147/3125   Loss: 0.892547 mae: 0.721959 (2110.2142260593073 steps/sec)\n",
      "Step #10524\tEpoch   3 Batch 1148/3125   Loss: 0.964108 mae: 0.793376 (2234.435731333106 steps/sec)\n",
      "Step #10525\tEpoch   3 Batch 1149/3125   Loss: 0.745185 mae: 0.686869 (1771.5424902855211 steps/sec)\n",
      "Step #10526\tEpoch   3 Batch 1150/3125   Loss: 0.782785 mae: 0.731603 (1936.1781487157707 steps/sec)\n",
      "Step #10527\tEpoch   3 Batch 1151/3125   Loss: 0.753696 mae: 0.678848 (1764.8486480572924 steps/sec)\n",
      "Step #10528\tEpoch   3 Batch 1152/3125   Loss: 0.849767 mae: 0.754929 (1756.4675533518712 steps/sec)\n",
      "Step #10529\tEpoch   3 Batch 1153/3125   Loss: 0.840028 mae: 0.726710 (1987.6899163088706 steps/sec)\n",
      "Step #10530\tEpoch   3 Batch 1154/3125   Loss: 0.738551 mae: 0.691290 (1977.7735863291712 steps/sec)\n",
      "Step #10531\tEpoch   3 Batch 1155/3125   Loss: 0.853810 mae: 0.726019 (1814.2237986072062 steps/sec)\n",
      "Step #10532\tEpoch   3 Batch 1156/3125   Loss: 0.838119 mae: 0.739468 (2018.6468249766579 steps/sec)\n",
      "Step #10533\tEpoch   3 Batch 1157/3125   Loss: 0.806921 mae: 0.733582 (1856.1659718718745 steps/sec)\n",
      "Step #10534\tEpoch   3 Batch 1158/3125   Loss: 0.777726 mae: 0.702973 (1869.4360007487899 steps/sec)\n",
      "Step #10535\tEpoch   3 Batch 1159/3125   Loss: 0.850732 mae: 0.727076 (2187.495566913529 steps/sec)\n",
      "Step #10536\tEpoch   3 Batch 1160/3125   Loss: 0.724742 mae: 0.688131 (2097.1310286897133 steps/sec)\n",
      "Step #10537\tEpoch   3 Batch 1161/3125   Loss: 0.817831 mae: 0.723180 (2145.292360571218 steps/sec)\n",
      "Step #10538\tEpoch   3 Batch 1162/3125   Loss: 0.878400 mae: 0.754943 (2212.9793385813477 steps/sec)\n",
      "Step #10539\tEpoch   3 Batch 1163/3125   Loss: 0.753178 mae: 0.705205 (2121.1635716308615 steps/sec)\n",
      "Step #10540\tEpoch   3 Batch 1164/3125   Loss: 0.825415 mae: 0.699142 (2143.538165910299 steps/sec)\n",
      "Step #10541\tEpoch   3 Batch 1165/3125   Loss: 0.905699 mae: 0.741841 (2166.0094401008046 steps/sec)\n",
      "Step #10542\tEpoch   3 Batch 1166/3125   Loss: 0.813524 mae: 0.723286 (1681.9601395516702 steps/sec)\n",
      "Step #10543\tEpoch   3 Batch 1167/3125   Loss: 0.811671 mae: 0.708661 (2010.923596195152 steps/sec)\n",
      "Step #10544\tEpoch   3 Batch 1168/3125   Loss: 0.789925 mae: 0.715445 (2213.4464779515756 steps/sec)\n",
      "Step #10545\tEpoch   3 Batch 1169/3125   Loss: 0.717561 mae: 0.659618 (2197.9960591959084 steps/sec)\n",
      "Step #10546\tEpoch   3 Batch 1170/3125   Loss: 0.830284 mae: 0.693453 (2098.033173933052 steps/sec)\n",
      "Step #10547\tEpoch   3 Batch 1171/3125   Loss: 0.804319 mae: 0.708610 (2142.5089137030945 steps/sec)\n",
      "Step #10548\tEpoch   3 Batch 1172/3125   Loss: 0.787453 mae: 0.692804 (1992.6570635855726 steps/sec)\n",
      "Step #10549\tEpoch   3 Batch 1173/3125   Loss: 0.780990 mae: 0.704581 (2092.423123740347 steps/sec)\n",
      "Step #10550\tEpoch   3 Batch 1174/3125   Loss: 0.747004 mae: 0.660965 (1927.4408345204724 steps/sec)\n",
      "Step #10551\tEpoch   3 Batch 1175/3125   Loss: 0.753267 mae: 0.686486 (1919.7305065817177 steps/sec)\n",
      "Step #10552\tEpoch   3 Batch 1176/3125   Loss: 0.832366 mae: 0.690862 (2023.965410747375 steps/sec)\n",
      "Step #10553\tEpoch   3 Batch 1177/3125   Loss: 0.849774 mae: 0.723741 (2064.6543406777323 steps/sec)\n",
      "Step #10554\tEpoch   3 Batch 1178/3125   Loss: 0.785714 mae: 0.691591 (2125.4847112003 steps/sec)\n",
      "Step #10555\tEpoch   3 Batch 1179/3125   Loss: 0.746028 mae: 0.703236 (2040.805363902648 steps/sec)\n",
      "Step #10556\tEpoch   3 Batch 1180/3125   Loss: 0.797848 mae: 0.705541 (1944.4540253864056 steps/sec)\n",
      "Step #10557\tEpoch   3 Batch 1181/3125   Loss: 0.808119 mae: 0.727531 (2228.8787331278563 steps/sec)\n",
      "Step #10558\tEpoch   3 Batch 1182/3125   Loss: 0.822420 mae: 0.717159 (2254.686979239461 steps/sec)\n",
      "Step #10559\tEpoch   3 Batch 1183/3125   Loss: 0.737748 mae: 0.672534 (2260.5928640724374 steps/sec)\n",
      "Step #10560\tEpoch   3 Batch 1184/3125   Loss: 0.835178 mae: 0.738275 (1848.166948674563 steps/sec)\n",
      "Step #10561\tEpoch   3 Batch 1185/3125   Loss: 0.783334 mae: 0.700890 (2023.965410747375 steps/sec)\n",
      "Step #10562\tEpoch   3 Batch 1186/3125   Loss: 0.871916 mae: 0.756669 (1997.5159065797995 steps/sec)\n",
      "Step #10563\tEpoch   3 Batch 1187/3125   Loss: 0.676693 mae: 0.651454 (2135.0273858245273 steps/sec)\n",
      "Step #10564\tEpoch   3 Batch 1188/3125   Loss: 0.813404 mae: 0.723826 (2246.7880865652455 steps/sec)\n",
      "Step #10565\tEpoch   3 Batch 1189/3125   Loss: 0.659383 mae: 0.637105 (2309.8677181658977 steps/sec)\n",
      "Step #10566\tEpoch   3 Batch 1190/3125   Loss: 0.864140 mae: 0.721172 (2212.208989546303 steps/sec)\n",
      "Step #10567\tEpoch   3 Batch 1191/3125   Loss: 0.814765 mae: 0.697596 (2164.802064516129 steps/sec)\n",
      "Step #10568\tEpoch   3 Batch 1192/3125   Loss: 0.883520 mae: 0.712978 (2235.3407661642755 steps/sec)\n",
      "Step #10569\tEpoch   3 Batch 1193/3125   Loss: 0.725163 mae: 0.655104 (1989.0284153420084 steps/sec)\n",
      "Step #10570\tEpoch   3 Batch 1194/3125   Loss: 0.895316 mae: 0.740833 (1990.444282039844 steps/sec)\n",
      "Step #10571\tEpoch   3 Batch 1195/3125   Loss: 0.961333 mae: 0.790613 (2144.985169274829 steps/sec)\n",
      "Step #10572\tEpoch   3 Batch 1196/3125   Loss: 0.760935 mae: 0.683240 (2270.800082292941 steps/sec)\n",
      "Step #10573\tEpoch   3 Batch 1197/3125   Loss: 0.882712 mae: 0.760089 (2200.694684925757 steps/sec)\n",
      "Step #10574\tEpoch   3 Batch 1198/3125   Loss: 0.773436 mae: 0.671784 (2055.245543370672 steps/sec)\n",
      "Step #10575\tEpoch   3 Batch 1199/3125   Loss: 0.809893 mae: 0.703840 (2122.8598325724524 steps/sec)\n",
      "Step #10576\tEpoch   3 Batch 1200/3125   Loss: 0.924398 mae: 0.757317 (2026.5079334402722 steps/sec)\n",
      "Step #10577\tEpoch   3 Batch 1201/3125   Loss: 0.858789 mae: 0.714720 (2132.856009600716 steps/sec)\n",
      "Step #10578\tEpoch   3 Batch 1202/3125   Loss: 0.895159 mae: 0.780834 (1719.9921265008857 steps/sec)\n",
      "Step #10579\tEpoch   3 Batch 1203/3125   Loss: 0.767106 mae: 0.709053 (2017.5010582214184 steps/sec)\n",
      "Step #10580\tEpoch   3 Batch 1204/3125   Loss: 0.902802 mae: 0.765886 (2188.9568503016512 steps/sec)\n",
      "Step #10581\tEpoch   3 Batch 1205/3125   Loss: 0.792041 mae: 0.709942 (2174.45383379128 steps/sec)\n",
      "Step #10582\tEpoch   3 Batch 1206/3125   Loss: 0.812835 mae: 0.724823 (2316.2968444537714 steps/sec)\n",
      "Step #10583\tEpoch   3 Batch 1207/3125   Loss: 0.870056 mae: 0.715013 (2015.329617528349 steps/sec)\n",
      "Step #10584\tEpoch   3 Batch 1208/3125   Loss: 0.836247 mae: 0.706590 (2267.2648842665167 steps/sec)\n",
      "Step #10585\tEpoch   3 Batch 1209/3125   Loss: 0.842825 mae: 0.710832 (2182.7371225762136 steps/sec)\n",
      "Step #10586\tEpoch   3 Batch 1210/3125   Loss: 0.753204 mae: 0.682251 (2302.9682747109146 steps/sec)\n",
      "Step #10587\tEpoch   3 Batch 1211/3125   Loss: 0.707116 mae: 0.664420 (2024.3561527472102 steps/sec)\n",
      "Step #10588\tEpoch   3 Batch 1212/3125   Loss: 0.852341 mae: 0.718744 (1686.966174637011 steps/sec)\n",
      "Step #10589\tEpoch   3 Batch 1213/3125   Loss: 0.873494 mae: 0.728145 (2040.725928088357 steps/sec)\n",
      "Step #10590\tEpoch   3 Batch 1214/3125   Loss: 0.883811 mae: 0.766299 (2198.387756171707 steps/sec)\n",
      "Step #10591\tEpoch   3 Batch 1215/3125   Loss: 0.661017 mae: 0.645700 (2210.436890645586 steps/sec)\n",
      "Step #10592\tEpoch   3 Batch 1216/3125   Loss: 0.818686 mae: 0.687868 (2113.0846583237612 steps/sec)\n",
      "Step #10593\tEpoch   3 Batch 1217/3125   Loss: 0.772627 mae: 0.685046 (2205.068029356718 steps/sec)\n",
      "Step #10594\tEpoch   3 Batch 1218/3125   Loss: 0.803134 mae: 0.676125 (2351.3835942054984 steps/sec)\n",
      "Step #10595\tEpoch   3 Batch 1219/3125   Loss: 0.961256 mae: 0.790191 (2184.2603007957337 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10596\tEpoch   3 Batch 1220/3125   Loss: 0.867438 mae: 0.726705 (1649.3397613860686 steps/sec)\n",
      "Step #10597\tEpoch   3 Batch 1221/3125   Loss: 0.828318 mae: 0.728622 (2066.037475617205 steps/sec)\n",
      "Step #10598\tEpoch   3 Batch 1222/3125   Loss: 0.724210 mae: 0.649856 (2035.8129556463748 steps/sec)\n",
      "Step #10599\tEpoch   3 Batch 1223/3125   Loss: 0.590819 mae: 0.638554 (1964.6556246721127 steps/sec)\n",
      "Step #10600\tEpoch   3 Batch 1224/3125   Loss: 0.758559 mae: 0.696102 (2136.2235283332143 steps/sec)\n",
      "Step #10601\tEpoch   3 Batch 1225/3125   Loss: 0.826488 mae: 0.720463 (2097.4875980156826 steps/sec)\n",
      "Step #10602\tEpoch   3 Batch 1226/3125   Loss: 0.821825 mae: 0.716837 (2285.3257197654907 steps/sec)\n",
      "Step #10603\tEpoch   3 Batch 1227/3125   Loss: 0.885254 mae: 0.753743 (2285.3755285297066 steps/sec)\n",
      "Step #10604\tEpoch   3 Batch 1228/3125   Loss: 0.880933 mae: 0.752641 (2000.7937719432148 steps/sec)\n",
      "Step #10605\tEpoch   3 Batch 1229/3125   Loss: 0.731690 mae: 0.688879 (1714.4520200781544 steps/sec)\n",
      "Step #10606\tEpoch   3 Batch 1230/3125   Loss: 0.705278 mae: 0.660450 (1997.6871564789149 steps/sec)\n",
      "Step #10607\tEpoch   3 Batch 1231/3125   Loss: 0.762865 mae: 0.693775 (2204.673948466722 steps/sec)\n",
      "Step #10608\tEpoch   3 Batch 1232/3125   Loss: 0.820505 mae: 0.693102 (2117.9074934356695 steps/sec)\n",
      "Step #10609\tEpoch   3 Batch 1233/3125   Loss: 0.759177 mae: 0.682469 (2227.884247652233 steps/sec)\n",
      "Step #10610\tEpoch   3 Batch 1234/3125   Loss: 0.819335 mae: 0.701147 (2214.451495728752 steps/sec)\n",
      "Step #10611\tEpoch   3 Batch 1235/3125   Loss: 0.910919 mae: 0.753194 (2213.3763944738203 steps/sec)\n",
      "Step #10612\tEpoch   3 Batch 1236/3125   Loss: 0.758506 mae: 0.696865 (2259.3751346692525 steps/sec)\n",
      "Step #10613\tEpoch   3 Batch 1237/3125   Loss: 0.936281 mae: 0.788019 (2273.5573118244597 steps/sec)\n",
      "Step #10614\tEpoch   3 Batch 1238/3125   Loss: 0.930183 mae: 0.740671 (2091.7135447835626 steps/sec)\n",
      "Step #10615\tEpoch   3 Batch 1239/3125   Loss: 0.849410 mae: 0.734609 (1920.715109995787 steps/sec)\n",
      "Step #10616\tEpoch   3 Batch 1240/3125   Loss: 0.756315 mae: 0.680349 (1940.6394299727017 steps/sec)\n",
      "Step #10617\tEpoch   3 Batch 1241/3125   Loss: 0.833989 mae: 0.727780 (1968.7501173466515 steps/sec)\n",
      "Step #10618\tEpoch   3 Batch 1242/3125   Loss: 0.789959 mae: 0.727168 (1987.7464361540794 steps/sec)\n",
      "Step #10619\tEpoch   3 Batch 1243/3125   Loss: 0.851094 mae: 0.717949 (2043.808595653445 steps/sec)\n",
      "Step #10620\tEpoch   3 Batch 1244/3125   Loss: 0.912857 mae: 0.729644 (2057.5240861016814 steps/sec)\n",
      "Step #10621\tEpoch   3 Batch 1245/3125   Loss: 0.968891 mae: 0.787239 (1975.5192780504347 steps/sec)\n",
      "Step #10622\tEpoch   3 Batch 1246/3125   Loss: 0.801482 mae: 0.703921 (1961.5500453639875 steps/sec)\n",
      "Step #10623\tEpoch   3 Batch 1247/3125   Loss: 0.947611 mae: 0.792021 (1773.7450627151472 steps/sec)\n",
      "Step #10624\tEpoch   3 Batch 1248/3125   Loss: 0.808848 mae: 0.713008 (2166.837493800628 steps/sec)\n",
      "Step #10625\tEpoch   3 Batch 1249/3125   Loss: 0.914474 mae: 0.756576 (2316.9622043242407 steps/sec)\n",
      "Step #10626\tEpoch   3 Batch 1250/3125   Loss: 0.815753 mae: 0.705129 (2141.436915410693 steps/sec)\n",
      "Step #10627\tEpoch   3 Batch 1251/3125   Loss: 0.856907 mae: 0.743410 (1916.6776339840608 steps/sec)\n",
      "Step #10628\tEpoch   3 Batch 1252/3125   Loss: 0.762118 mae: 0.690053 (2017.1905659651418 steps/sec)\n",
      "Step #10629\tEpoch   3 Batch 1253/3125   Loss: 0.967666 mae: 0.778363 (1943.4089203139624 steps/sec)\n",
      "Step #10630\tEpoch   3 Batch 1254/3125   Loss: 0.931255 mae: 0.757930 (2093.488395308211 steps/sec)\n",
      "Step #10631\tEpoch   3 Batch 1255/3125   Loss: 0.847136 mae: 0.721692 (1741.7916645902892 steps/sec)\n",
      "Step #10632\tEpoch   3 Batch 1256/3125   Loss: 0.814417 mae: 0.720604 (1838.2202899566994 steps/sec)\n",
      "Step #10633\tEpoch   3 Batch 1257/3125   Loss: 1.067348 mae: 0.781262 (2155.7229937399134 steps/sec)\n",
      "Step #10634\tEpoch   3 Batch 1258/3125   Loss: 0.803823 mae: 0.719335 (2068.25842973658 steps/sec)\n",
      "Step #10635\tEpoch   3 Batch 1259/3125   Loss: 0.818229 mae: 0.720979 (2123.053249645677 steps/sec)\n",
      "Step #10636\tEpoch   3 Batch 1260/3125   Loss: 0.812995 mae: 0.735207 (2146.478065955661 steps/sec)\n",
      "Step #10637\tEpoch   3 Batch 1261/3125   Loss: 0.761492 mae: 0.687531 (2249.511407639418 steps/sec)\n",
      "Step #10638\tEpoch   3 Batch 1262/3125   Loss: 0.839914 mae: 0.719365 (2132.292175044737 steps/sec)\n",
      "Step #10639\tEpoch   3 Batch 1263/3125   Loss: 0.710680 mae: 0.670499 (2111.340206185567 steps/sec)\n",
      "Step #10640\tEpoch   3 Batch 1264/3125   Loss: 0.824719 mae: 0.707773 (1981.8293501167086 steps/sec)\n",
      "Step #10641\tEpoch   3 Batch 1265/3125   Loss: 0.781178 mae: 0.701252 (1923.4632669907364 steps/sec)\n",
      "Step #10642\tEpoch   3 Batch 1266/3125   Loss: 0.796668 mae: 0.708945 (1992.2784617723057 steps/sec)\n",
      "Step #10643\tEpoch   3 Batch 1267/3125   Loss: 0.789059 mae: 0.705153 (1982.485063903804 steps/sec)\n",
      "Step #10644\tEpoch   3 Batch 1268/3125   Loss: 0.948552 mae: 0.748994 (2416.1572404576195 steps/sec)\n",
      "Step #10645\tEpoch   3 Batch 1269/3125   Loss: 0.813794 mae: 0.686682 (2443.09412861137 steps/sec)\n",
      "Step #10646\tEpoch   3 Batch 1270/3125   Loss: 0.817937 mae: 0.714322 (2209.7381592118436 steps/sec)\n",
      "Step #10647\tEpoch   3 Batch 1271/3125   Loss: 0.950691 mae: 0.768208 (1979.1174361103772 steps/sec)\n",
      "Step #10648\tEpoch   3 Batch 1272/3125   Loss: 0.883838 mae: 0.739992 (2005.0404420903685 steps/sec)\n",
      "Step #10649\tEpoch   3 Batch 1273/3125   Loss: 0.763940 mae: 0.704282 (1737.9808728225019 steps/sec)\n",
      "Step #10650\tEpoch   3 Batch 1274/3125   Loss: 0.907835 mae: 0.735255 (1951.292858804373 steps/sec)\n",
      "Step #10651\tEpoch   3 Batch 1275/3125   Loss: 0.825742 mae: 0.730189 (2205.068029356718 steps/sec)\n",
      "Step #10652\tEpoch   3 Batch 1276/3125   Loss: 0.857029 mae: 0.729258 (2200.1405805768 steps/sec)\n",
      "Step #10653\tEpoch   3 Batch 1277/3125   Loss: 0.909852 mae: 0.771286 (2227.269058391215 steps/sec)\n",
      "Step #10654\tEpoch   3 Batch 1278/3125   Loss: 0.873684 mae: 0.733863 (2217.8003384094754 steps/sec)\n",
      "Step #10655\tEpoch   3 Batch 1279/3125   Loss: 0.951669 mae: 0.767802 (2183.7826580446304 steps/sec)\n",
      "Step #10656\tEpoch   3 Batch 1280/3125   Loss: 0.846155 mae: 0.749372 (2282.5647332847175 steps/sec)\n",
      "Step #10657\tEpoch   3 Batch 1281/3125   Loss: 0.898934 mae: 0.727004 (2226.299642246733 steps/sec)\n",
      "Step #10658\tEpoch   3 Batch 1282/3125   Loss: 0.745860 mae: 0.705779 (1814.6790578543862 steps/sec)\n",
      "Step #10659\tEpoch   3 Batch 1283/3125   Loss: 0.693426 mae: 0.664154 (1838.0913983206829 steps/sec)\n",
      "Step #10660\tEpoch   3 Batch 1284/3125   Loss: 0.860863 mae: 0.741187 (2009.0741876149602 steps/sec)\n",
      "Step #10661\tEpoch   3 Batch 1285/3125   Loss: 0.842906 mae: 0.738550 (2069.442169352372 steps/sec)\n",
      "Step #10662\tEpoch   3 Batch 1286/3125   Loss: 0.749462 mae: 0.696857 (2067.3409435933836 steps/sec)\n",
      "Step #10663\tEpoch   3 Batch 1287/3125   Loss: 0.758762 mae: 0.689441 (2235.4360756390306 steps/sec)\n",
      "Step #10664\tEpoch   3 Batch 1288/3125   Loss: 0.924358 mae: 0.745694 (2139.3179569311123 steps/sec)\n",
      "Step #10665\tEpoch   3 Batch 1289/3125   Loss: 0.833707 mae: 0.713652 (2177.43399125767 steps/sec)\n",
      "Step #10666\tEpoch   3 Batch 1290/3125   Loss: 0.720732 mae: 0.694322 (2083.3394593842822 steps/sec)\n",
      "Step #10667\tEpoch   3 Batch 1291/3125   Loss: 0.897385 mae: 0.752442 (1732.20999768725 steps/sec)\n",
      "Step #10668\tEpoch   3 Batch 1292/3125   Loss: 0.814028 mae: 0.687228 (1862.4134133778552 steps/sec)\n",
      "Step #10669\tEpoch   3 Batch 1293/3125   Loss: 0.840457 mae: 0.725952 (2355.7153127246584 steps/sec)\n",
      "Step #10670\tEpoch   3 Batch 1294/3125   Loss: 0.762332 mae: 0.694074 (2113.6383793590003 steps/sec)\n",
      "Step #10671\tEpoch   3 Batch 1295/3125   Loss: 0.774649 mae: 0.716087 (2117.5011864013168 steps/sec)\n",
      "Step #10672\tEpoch   3 Batch 1296/3125   Loss: 0.714413 mae: 0.677056 (2119.1915925626517 steps/sec)\n",
      "Step #10673\tEpoch   3 Batch 1297/3125   Loss: 0.871197 mae: 0.722495 (2181.420264830398 steps/sec)\n",
      "Step #10674\tEpoch   3 Batch 1298/3125   Loss: 0.732869 mae: 0.670763 (2254.4930714569828 steps/sec)\n",
      "Step #10675\tEpoch   3 Batch 1299/3125   Loss: 0.788480 mae: 0.703349 (1913.2154651778058 steps/sec)\n",
      "Step #10676\tEpoch   3 Batch 1300/3125   Loss: 0.745350 mae: 0.669677 (1927.3699785863303 steps/sec)\n",
      "Step #10677\tEpoch   3 Batch 1301/3125   Loss: 0.757224 mae: 0.687332 (2186.948088514401 steps/sec)\n",
      "Step #10678\tEpoch   3 Batch 1302/3125   Loss: 0.752757 mae: 0.686397 (2054.1584633618368 steps/sec)\n",
      "Step #10679\tEpoch   3 Batch 1303/3125   Loss: 0.709802 mae: 0.663712 (2134.831780933476 steps/sec)\n",
      "Step #10680\tEpoch   3 Batch 1304/3125   Loss: 0.719602 mae: 0.685927 (2256.409373587829 steps/sec)\n",
      "Step #10681\tEpoch   3 Batch 1305/3125   Loss: 0.720821 mae: 0.677889 (2188.637027760384 steps/sec)\n",
      "Step #10682\tEpoch   3 Batch 1306/3125   Loss: 0.706801 mae: 0.657954 (2088.3600043815536 steps/sec)\n",
      "Step #10683\tEpoch   3 Batch 1307/3125   Loss: 1.001582 mae: 0.782051 (2202.9370364923634 steps/sec)\n",
      "Step #10684\tEpoch   3 Batch 1308/3125   Loss: 0.837836 mae: 0.698814 (2133.3116321652 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10685\tEpoch   3 Batch 1309/3125   Loss: 0.834619 mae: 0.735203 (2125.1400950518328 steps/sec)\n",
      "Step #10686\tEpoch   3 Batch 1310/3125   Loss: 0.778449 mae: 0.688887 (1808.5132804415316 steps/sec)\n",
      "Step #10687\tEpoch   3 Batch 1311/3125   Loss: 0.809485 mae: 0.723928 (1993.5662952963992 steps/sec)\n",
      "Step #10688\tEpoch   3 Batch 1312/3125   Loss: 0.801415 mae: 0.713920 (2029.2335529817024 steps/sec)\n",
      "Step #10689\tEpoch   3 Batch 1313/3125   Loss: 0.781657 mae: 0.712743 (2143.735369580995 steps/sec)\n",
      "Step #10690\tEpoch   3 Batch 1314/3125   Loss: 0.862247 mae: 0.757990 (1996.1089642306447 steps/sec)\n",
      "Step #10691\tEpoch   3 Batch 1315/3125   Loss: 0.902134 mae: 0.759201 (1978.3892908691264 steps/sec)\n",
      "Step #10692\tEpoch   3 Batch 1316/3125   Loss: 0.808795 mae: 0.701419 (2033.9964114252462 steps/sec)\n",
      "Step #10693\tEpoch   3 Batch 1317/3125   Loss: 0.688440 mae: 0.661731 (2124.946297572245 steps/sec)\n",
      "Step #10694\tEpoch   3 Batch 1318/3125   Loss: 0.849319 mae: 0.714496 (1824.2290863858177 steps/sec)\n",
      "Step #10695\tEpoch   3 Batch 1319/3125   Loss: 0.832296 mae: 0.713952 (2113.787507685485 steps/sec)\n",
      "Step #10696\tEpoch   3 Batch 1320/3125   Loss: 0.755036 mae: 0.663268 (2112.2120720738867 steps/sec)\n",
      "Step #10697\tEpoch   3 Batch 1321/3125   Loss: 0.843784 mae: 0.747579 (2314.379676429691 steps/sec)\n",
      "Step #10698\tEpoch   3 Batch 1322/3125   Loss: 0.669470 mae: 0.640365 (2084.105499572675 steps/sec)\n",
      "Step #10699\tEpoch   3 Batch 1323/3125   Loss: 0.831629 mae: 0.701472 (2127.2957812198856 steps/sec)\n",
      "Step #10700\tEpoch   3 Batch 1324/3125   Loss: 0.756199 mae: 0.700899 (2104.8135212172306 steps/sec)\n",
      "Step #10701\tEpoch   3 Batch 1325/3125   Loss: 0.763799 mae: 0.673473 (2068.5236329203817 steps/sec)\n",
      "Step #10702\tEpoch   3 Batch 1326/3125   Loss: 0.765827 mae: 0.680609 (1662.0583619965446 steps/sec)\n",
      "Step #10703\tEpoch   3 Batch 1327/3125   Loss: 0.707400 mae: 0.660973 (1533.2187950080786 steps/sec)\n",
      "Step #10704\tEpoch   3 Batch 1328/3125   Loss: 0.854880 mae: 0.730682 (1381.9782537067545 steps/sec)\n",
      "Step #10705\tEpoch   3 Batch 1329/3125   Loss: 0.734912 mae: 0.697346 (2065.528754764555 steps/sec)\n",
      "Step #10706\tEpoch   3 Batch 1330/3125   Loss: 0.916225 mae: 0.756485 (1975.9846229223986 steps/sec)\n",
      "Step #10707\tEpoch   3 Batch 1331/3125   Loss: 0.634004 mae: 0.615719 (1990.6710077931448 steps/sec)\n",
      "Step #10708\tEpoch   3 Batch 1332/3125   Loss: 0.832956 mae: 0.731287 (1969.9891973134188 steps/sec)\n",
      "Step #10709\tEpoch   3 Batch 1333/3125   Loss: 0.711870 mae: 0.647603 (1669.0691455494716 steps/sec)\n",
      "Step #10710\tEpoch   3 Batch 1334/3125   Loss: 0.682314 mae: 0.649698 (1843.520455704214 steps/sec)\n",
      "Step #10711\tEpoch   3 Batch 1335/3125   Loss: 0.826468 mae: 0.735828 (1881.461278977966 steps/sec)\n",
      "Step #10712\tEpoch   3 Batch 1336/3125   Loss: 0.772664 mae: 0.710357 (2148.2370777078936 steps/sec)\n",
      "Step #10713\tEpoch   3 Batch 1337/3125   Loss: 0.690399 mae: 0.651435 (2182.055790820839 steps/sec)\n",
      "Step #10714\tEpoch   3 Batch 1338/3125   Loss: 0.763962 mae: 0.717949 (2032.5376287810504 steps/sec)\n",
      "Step #10715\tEpoch   3 Batch 1339/3125   Loss: 0.869113 mae: 0.747708 (2130.321099519519 steps/sec)\n",
      "Step #10716\tEpoch   3 Batch 1340/3125   Loss: 0.717587 mae: 0.684630 (2049.501099438065 steps/sec)\n",
      "Step #10717\tEpoch   3 Batch 1341/3125   Loss: 0.861162 mae: 0.717933 (2072.448414894458 steps/sec)\n",
      "Step #10718\tEpoch   3 Batch 1342/3125   Loss: 0.828475 mae: 0.718617 (1774.7507743344108 steps/sec)\n",
      "Step #10719\tEpoch   3 Batch 1343/3125   Loss: 0.702152 mae: 0.658974 (1983.3287623298877 steps/sec)\n",
      "Step #10720\tEpoch   3 Batch 1344/3125   Loss: 0.743677 mae: 0.675766 (2104.560051380861 steps/sec)\n",
      "Step #10721\tEpoch   3 Batch 1345/3125   Loss: 0.832472 mae: 0.730917 (2102.8718113268087 steps/sec)\n",
      "Step #10722\tEpoch   3 Batch 1346/3125   Loss: 0.755196 mae: 0.697491 (1980.7998186523603 steps/sec)\n",
      "Step #10723\tEpoch   3 Batch 1347/3125   Loss: 0.767463 mae: 0.699370 (2254.4688353292768 steps/sec)\n",
      "Step #10724\tEpoch   3 Batch 1348/3125   Loss: 0.774832 mae: 0.722410 (2090.733448313677 steps/sec)\n",
      "Step #10725\tEpoch   3 Batch 1349/3125   Loss: 0.844483 mae: 0.752350 (1908.5491709288146 steps/sec)\n",
      "Step #10726\tEpoch   3 Batch 1350/3125   Loss: 0.768745 mae: 0.666874 (1993.6989609179668 steps/sec)\n",
      "Step #10727\tEpoch   3 Batch 1351/3125   Loss: 0.840662 mae: 0.733234 (1603.2781872114003 steps/sec)\n",
      "Step #10728\tEpoch   3 Batch 1352/3125   Loss: 0.821573 mae: 0.717952 (1996.4130000190394 steps/sec)\n",
      "Step #10729\tEpoch   3 Batch 1353/3125   Loss: 0.663846 mae: 0.642454 (1944.7605623353982 steps/sec)\n",
      "Step #10730\tEpoch   3 Batch 1354/3125   Loss: 0.704226 mae: 0.661765 (2142.77160752419 steps/sec)\n",
      "Step #10731\tEpoch   3 Batch 1355/3125   Loss: 0.830061 mae: 0.713494 (2250.5494505494507 steps/sec)\n",
      "Step #10732\tEpoch   3 Batch 1356/3125   Loss: 0.883115 mae: 0.742588 (2088.6511896580914 steps/sec)\n",
      "Step #10733\tEpoch   3 Batch 1357/3125   Loss: 0.748043 mae: 0.690142 (2032.9316879768126 steps/sec)\n",
      "Step #10734\tEpoch   3 Batch 1358/3125   Loss: 1.012411 mae: 0.785075 (1986.8048581768574 steps/sec)\n",
      "Step #10735\tEpoch   3 Batch 1359/3125   Loss: 0.984318 mae: 0.790452 (2032.6164283983524 steps/sec)\n",
      "Step #10736\tEpoch   3 Batch 1360/3125   Loss: 0.713667 mae: 0.678324 (2098.2850739399278 steps/sec)\n",
      "Step #10737\tEpoch   3 Batch 1361/3125   Loss: 0.775892 mae: 0.708474 (2055.4872729767612 steps/sec)\n",
      "Step #10738\tEpoch   3 Batch 1362/3125   Loss: 0.800978 mae: 0.709595 (2240.5470085470088 steps/sec)\n",
      "Step #10739\tEpoch   3 Batch 1363/3125   Loss: 0.982294 mae: 0.777449 (2174.093156819855 steps/sec)\n",
      "Step #10740\tEpoch   3 Batch 1364/3125   Loss: 0.815352 mae: 0.716866 (2160.2084856975102 steps/sec)\n",
      "Step #10741\tEpoch   3 Batch 1365/3125   Loss: 0.730425 mae: 0.693065 (2092.924292928285 steps/sec)\n",
      "Step #10742\tEpoch   3 Batch 1366/3125   Loss: 0.897276 mae: 0.748625 (2069.973251211592 steps/sec)\n",
      "Step #10743\tEpoch   3 Batch 1367/3125   Loss: 0.801624 mae: 0.709367 (1787.5485850664847 steps/sec)\n",
      "Step #10744\tEpoch   3 Batch 1368/3125   Loss: 0.884316 mae: 0.735771 (1978.4826128794882 steps/sec)\n",
      "Step #10745\tEpoch   3 Batch 1369/3125   Loss: 0.729525 mae: 0.688442 (2077.560603508911 steps/sec)\n",
      "Step #10746\tEpoch   3 Batch 1370/3125   Loss: 0.749019 mae: 0.704742 (2032.1831060980455 steps/sec)\n",
      "Step #10747\tEpoch   3 Batch 1371/3125   Loss: 0.805033 mae: 0.726748 (2079.332123699892 steps/sec)\n",
      "Step #10748\tEpoch   3 Batch 1372/3125   Loss: 0.919769 mae: 0.748185 (2186.925282861463 steps/sec)\n",
      "Step #10749\tEpoch   3 Batch 1373/3125   Loss: 0.960623 mae: 0.766767 (2170.515421237839 steps/sec)\n",
      "Step #10750\tEpoch   3 Batch 1374/3125   Loss: 0.764239 mae: 0.666607 (2104.0321852457537 steps/sec)\n",
      "Step #10751\tEpoch   3 Batch 1375/3125   Loss: 0.861100 mae: 0.724694 (2125.570882701722 steps/sec)\n",
      "Step #10752\tEpoch   3 Batch 1376/3125   Loss: 0.805396 mae: 0.690918 (1903.2489926307765 steps/sec)\n",
      "Step #10753\tEpoch   3 Batch 1377/3125   Loss: 0.867936 mae: 0.740671 (1832.646177239083 steps/sec)\n",
      "Step #10754\tEpoch   3 Batch 1378/3125   Loss: 0.778279 mae: 0.692001 (2113.8940408031613 steps/sec)\n",
      "Step #10755\tEpoch   3 Batch 1379/3125   Loss: 0.858816 mae: 0.739748 (2088.630388016891 steps/sec)\n",
      "Step #10756\tEpoch   3 Batch 1380/3125   Loss: 0.774662 mae: 0.690373 (2050.342676691141 steps/sec)\n",
      "Step #10757\tEpoch   3 Batch 1381/3125   Loss: 0.863284 mae: 0.763291 (2156.2106085686964 steps/sec)\n",
      "Step #10758\tEpoch   3 Batch 1382/3125   Loss: 0.632857 mae: 0.640206 (1934.5349888382561 steps/sec)\n",
      "Step #10759\tEpoch   3 Batch 1383/3125   Loss: 0.852651 mae: 0.728076 (2084.62341328615 steps/sec)\n",
      "Step #10760\tEpoch   3 Batch 1384/3125   Loss: 0.886513 mae: 0.736665 (2058.533903961679 steps/sec)\n",
      "Step #10761\tEpoch   3 Batch 1385/3125   Loss: 0.708902 mae: 0.654394 (1895.936282353792 steps/sec)\n",
      "Step #10762\tEpoch   3 Batch 1386/3125   Loss: 0.862627 mae: 0.743898 (2033.1090644692197 steps/sec)\n",
      "Step #10763\tEpoch   3 Batch 1387/3125   Loss: 0.751480 mae: 0.682821 (2137.4210118635083 steps/sec)\n",
      "Step #10764\tEpoch   3 Batch 1388/3125   Loss: 0.765614 mae: 0.705322 (1980.2013105961892 steps/sec)\n",
      "Step #10765\tEpoch   3 Batch 1389/3125   Loss: 0.785992 mae: 0.705912 (2061.447725396139 steps/sec)\n",
      "Step #10766\tEpoch   3 Batch 1390/3125   Loss: 0.833163 mae: 0.715245 (1920.5040385355044 steps/sec)\n",
      "Step #10767\tEpoch   3 Batch 1391/3125   Loss: 0.796021 mae: 0.690233 (2073.390939829553 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10768\tEpoch   3 Batch 1392/3125   Loss: 0.746944 mae: 0.707988 (1960.0833699400896 steps/sec)\n",
      "Step #10769\tEpoch   3 Batch 1393/3125   Loss: 0.761033 mae: 0.681141 (1732.6250216872247 steps/sec)\n",
      "Step #10770\tEpoch   3 Batch 1394/3125   Loss: 0.846555 mae: 0.725648 (2194.821559392988 steps/sec)\n",
      "Step #10771\tEpoch   3 Batch 1395/3125   Loss: 0.804309 mae: 0.709415 (2093.3421173464294 steps/sec)\n",
      "Step #10772\tEpoch   3 Batch 1396/3125   Loss: 0.764709 mae: 0.704110 (2247.1010532857586 steps/sec)\n",
      "Step #10773\tEpoch   3 Batch 1397/3125   Loss: 0.946934 mae: 0.772496 (2156.5653761118824 steps/sec)\n",
      "Step #10774\tEpoch   3 Batch 1398/3125   Loss: 0.862788 mae: 0.741606 (2155.3463514902364 steps/sec)\n",
      "Step #10775\tEpoch   3 Batch 1399/3125   Loss: 0.826277 mae: 0.723447 (1752.1823407526235 steps/sec)\n",
      "Step #10776\tEpoch   3 Batch 1400/3125   Loss: 0.843578 mae: 0.738301 (1748.617549944969 steps/sec)\n",
      "Step #10777\tEpoch   3 Batch 1401/3125   Loss: 0.797910 mae: 0.717990 (1812.3110692465239 steps/sec)\n",
      "Step #10778\tEpoch   3 Batch 1402/3125   Loss: 0.702815 mae: 0.687702 (2014.9810718883914 steps/sec)\n",
      "Step #10779\tEpoch   3 Batch 1403/3125   Loss: 0.748926 mae: 0.703849 (2206.9244206848653 steps/sec)\n",
      "Step #10780\tEpoch   3 Batch 1404/3125   Loss: 0.853005 mae: 0.740534 (2165.4503025421805 steps/sec)\n",
      "Step #10781\tEpoch   3 Batch 1405/3125   Loss: 0.830829 mae: 0.732620 (2103.4201921726744 steps/sec)\n",
      "Step #10782\tEpoch   3 Batch 1406/3125   Loss: 0.928930 mae: 0.772415 (2159.719061203052 steps/sec)\n",
      "Step #10783\tEpoch   3 Batch 1407/3125   Loss: 0.905477 mae: 0.769062 (2231.8437716170915 steps/sec)\n",
      "Step #10784\tEpoch   3 Batch 1408/3125   Loss: 0.895959 mae: 0.749056 (2221.018184128867 steps/sec)\n",
      "Step #10785\tEpoch   3 Batch 1409/3125   Loss: 0.790132 mae: 0.679507 (1804.6692540036315 steps/sec)\n",
      "Step #10786\tEpoch   3 Batch 1410/3125   Loss: 0.850827 mae: 0.710324 (1942.1310959233947 steps/sec)\n",
      "Step #10787\tEpoch   3 Batch 1411/3125   Loss: 0.624007 mae: 0.635855 (1688.895331513292 steps/sec)\n",
      "Step #10788\tEpoch   3 Batch 1412/3125   Loss: 0.929835 mae: 0.756747 (1946.3488881464157 steps/sec)\n",
      "Step #10789\tEpoch   3 Batch 1413/3125   Loss: 0.740438 mae: 0.675234 (1959.863557777674 steps/sec)\n",
      "Step #10790\tEpoch   3 Batch 1414/3125   Loss: 0.723496 mae: 0.664584 (1928.5752384105351 steps/sec)\n",
      "Step #10791\tEpoch   3 Batch 1415/3125   Loss: 0.798808 mae: 0.713769 (1950.0771791486118 steps/sec)\n",
      "Step #10792\tEpoch   3 Batch 1416/3125   Loss: 0.803181 mae: 0.698285 (1799.2655890730637 steps/sec)\n",
      "Step #10793\tEpoch   3 Batch 1417/3125   Loss: 0.810418 mae: 0.707521 (1799.20384351407 steps/sec)\n",
      "Step #10794\tEpoch   3 Batch 1418/3125   Loss: 0.891385 mae: 0.721111 (1717.5692055692057 steps/sec)\n",
      "Step #10795\tEpoch   3 Batch 1419/3125   Loss: 0.845751 mae: 0.689759 (2078.301802649965 steps/sec)\n",
      "Step #10796\tEpoch   3 Batch 1420/3125   Loss: 0.805137 mae: 0.722290 (2036.6631057589589 steps/sec)\n",
      "Step #10797\tEpoch   3 Batch 1421/3125   Loss: 0.752428 mae: 0.708861 (2095.559374875095 steps/sec)\n",
      "Step #10798\tEpoch   3 Batch 1422/3125   Loss: 0.810734 mae: 0.697053 (2133.485253873465 steps/sec)\n",
      "Step #10799\tEpoch   3 Batch 1423/3125   Loss: 0.748615 mae: 0.698203 (2087.7778773307846 steps/sec)\n",
      "Step #10800\tEpoch   3 Batch 1424/3125   Loss: 0.797985 mae: 0.697777 (1902.5238138437812 steps/sec)\n",
      "Step #10801\tEpoch   3 Batch 1425/3125   Loss: 0.996606 mae: 0.784888 (1888.4924672892146 steps/sec)\n",
      "Step #10802\tEpoch   3 Batch 1426/3125   Loss: 0.849150 mae: 0.726811 (1646.840055283327 steps/sec)\n",
      "Step #10803\tEpoch   3 Batch 1427/3125   Loss: 0.668209 mae: 0.633062 (2082.387870001688 steps/sec)\n",
      "Step #10804\tEpoch   3 Batch 1428/3125   Loss: 0.782427 mae: 0.693396 (2111.2339303152025 steps/sec)\n",
      "Step #10805\tEpoch   3 Batch 1429/3125   Loss: 0.892480 mae: 0.745641 (2192.6873895632716 steps/sec)\n",
      "Step #10806\tEpoch   3 Batch 1430/3125   Loss: 0.766906 mae: 0.685334 (2177.8862430290883 steps/sec)\n",
      "Step #10807\tEpoch   3 Batch 1431/3125   Loss: 0.830160 mae: 0.722944 (2043.2709451757164 steps/sec)\n",
      "Step #10808\tEpoch   3 Batch 1432/3125   Loss: 0.882467 mae: 0.727076 (1967.789517143018 steps/sec)\n",
      "Step #10809\tEpoch   3 Batch 1433/3125   Loss: 0.801939 mae: 0.713609 (2078.157639177914 steps/sec)\n",
      "Step #10810\tEpoch   3 Batch 1434/3125   Loss: 0.769004 mae: 0.701545 (1888.3564295812062 steps/sec)\n",
      "Step #10811\tEpoch   3 Batch 1435/3125   Loss: 0.813872 mae: 0.724100 (1935.7134945541814 steps/sec)\n",
      "Step #10812\tEpoch   3 Batch 1436/3125   Loss: 0.826917 mae: 0.704187 (2130.2129042743377 steps/sec)\n",
      "Step #10813\tEpoch   3 Batch 1437/3125   Loss: 0.807348 mae: 0.714954 (1828.555484832896 steps/sec)\n",
      "Step #10814\tEpoch   3 Batch 1438/3125   Loss: 0.902680 mae: 0.758351 (2125.2262386120656 steps/sec)\n",
      "Step #10815\tEpoch   3 Batch 1439/3125   Loss: 0.907813 mae: 0.744907 (2153.8632185442707 steps/sec)\n",
      "Step #10816\tEpoch   3 Batch 1440/3125   Loss: 0.714703 mae: 0.669169 (1986.5790121819525 steps/sec)\n",
      "Step #10817\tEpoch   3 Batch 1441/3125   Loss: 0.742452 mae: 0.686591 (1981.3050913110433 steps/sec)\n",
      "Step #10818\tEpoch   3 Batch 1442/3125   Loss: 0.852828 mae: 0.753669 (1912.6222092514226 steps/sec)\n",
      "Step #10819\tEpoch   3 Batch 1443/3125   Loss: 0.768658 mae: 0.704499 (1974.8867606482659 steps/sec)\n",
      "Step #10820\tEpoch   3 Batch 1444/3125   Loss: 0.775012 mae: 0.701200 (2313.843437965466 steps/sec)\n",
      "Step #10821\tEpoch   3 Batch 1445/3125   Loss: 0.857661 mae: 0.739065 (2221.7240685220304 steps/sec)\n",
      "Step #10822\tEpoch   3 Batch 1446/3125   Loss: 0.690543 mae: 0.651091 (2345.886327281676 steps/sec)\n",
      "Step #10823\tEpoch   3 Batch 1447/3125   Loss: 0.817173 mae: 0.730246 (2226.677850567512 steps/sec)\n",
      "Step #10824\tEpoch   3 Batch 1448/3125   Loss: 0.848756 mae: 0.741252 (2234.435731333106 steps/sec)\n",
      "Step #10825\tEpoch   3 Batch 1449/3125   Loss: 0.803601 mae: 0.705132 (2472.2985876971684 steps/sec)\n",
      "Step #10826\tEpoch   3 Batch 1450/3125   Loss: 0.830493 mae: 0.731188 (1840.866557820263 steps/sec)\n",
      "Step #10827\tEpoch   3 Batch 1451/3125   Loss: 0.813007 mae: 0.708347 (2154.3942553650495 steps/sec)\n",
      "Step #10828\tEpoch   3 Batch 1452/3125   Loss: 0.794224 mae: 0.735049 (2261.324131981885 steps/sec)\n",
      "Step #10829\tEpoch   3 Batch 1453/3125   Loss: 0.836004 mae: 0.714720 (2338.9790432852633 steps/sec)\n",
      "Step #10830\tEpoch   3 Batch 1454/3125   Loss: 0.840596 mae: 0.718902 (2197.5123908920395 steps/sec)\n",
      "Step #10831\tEpoch   3 Batch 1455/3125   Loss: 0.771766 mae: 0.706090 (2267.7797482590076 steps/sec)\n",
      "Step #10832\tEpoch   3 Batch 1456/3125   Loss: 0.831983 mae: 0.709767 (2151.653380118399 steps/sec)\n",
      "Step #10833\tEpoch   3 Batch 1457/3125   Loss: 0.801549 mae: 0.731277 (2076.6554110925167 steps/sec)\n",
      "Step #10834\tEpoch   3 Batch 1458/3125   Loss: 0.825007 mae: 0.696050 (1871.471278522921 steps/sec)\n",
      "Step #10835\tEpoch   3 Batch 1459/3125   Loss: 0.816140 mae: 0.684275 (1829.0977279665083 steps/sec)\n",
      "Step #10836\tEpoch   3 Batch 1460/3125   Loss: 0.819930 mae: 0.727660 (2000.7174203396298 steps/sec)\n",
      "Step #10837\tEpoch   3 Batch 1461/3125   Loss: 0.774411 mae: 0.698155 (2120.6059012680244 steps/sec)\n",
      "Step #10838\tEpoch   3 Batch 1462/3125   Loss: 0.902121 mae: 0.723086 (2156.5875529595655 steps/sec)\n",
      "Step #10839\tEpoch   3 Batch 1463/3125   Loss: 0.805575 mae: 0.731880 (2308.291415803551 steps/sec)\n",
      "Step #10840\tEpoch   3 Batch 1464/3125   Loss: 0.827578 mae: 0.727469 (2102.2604929979852 steps/sec)\n",
      "Step #10841\tEpoch   3 Batch 1465/3125   Loss: 0.810109 mae: 0.705844 (2281.223961449348 steps/sec)\n",
      "Step #10842\tEpoch   3 Batch 1466/3125   Loss: 0.703841 mae: 0.649056 (2162.7258477023347 steps/sec)\n",
      "Step #10843\tEpoch   3 Batch 1467/3125   Loss: 0.749189 mae: 0.669507 (1764.9674720798512 steps/sec)\n",
      "Step #10844\tEpoch   3 Batch 1468/3125   Loss: 0.942362 mae: 0.765932 (1879.8590880162067 steps/sec)\n",
      "Step #10845\tEpoch   3 Batch 1469/3125   Loss: 0.856337 mae: 0.729052 (1990.0476362187092 steps/sec)\n",
      "Step #10846\tEpoch   3 Batch 1470/3125   Loss: 0.885856 mae: 0.733001 (2092.151757300053 steps/sec)\n",
      "Step #10847\tEpoch   3 Batch 1471/3125   Loss: 0.752774 mae: 0.684085 (2136.7459015558297 steps/sec)\n",
      "Step #10848\tEpoch   3 Batch 1472/3125   Loss: 0.772328 mae: 0.702402 (2077.9723155276797 steps/sec)\n",
      "Step #10849\tEpoch   3 Batch 1473/3125   Loss: 0.870638 mae: 0.727735 (2111.7654166834495 steps/sec)\n",
      "Step #10850\tEpoch   3 Batch 1474/3125   Loss: 0.706032 mae: 0.653833 (2191.9998327636845 steps/sec)\n",
      "Step #10851\tEpoch   3 Batch 1475/3125   Loss: 0.881432 mae: 0.738580 (2072.0388886693277 steps/sec)\n",
      "Step #10852\tEpoch   3 Batch 1476/3125   Loss: 0.707750 mae: 0.671435 (2294.6276560824563 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10853\tEpoch   3 Batch 1477/3125   Loss: 0.837585 mae: 0.747057 (1980.9869266228368 steps/sec)\n",
      "Step #10854\tEpoch   3 Batch 1478/3125   Loss: 0.771227 mae: 0.686735 (2021.8190231957271 steps/sec)\n",
      "Step #10855\tEpoch   3 Batch 1479/3125   Loss: 0.675430 mae: 0.667120 (2211.182691395253 steps/sec)\n",
      "Step #10856\tEpoch   3 Batch 1480/3125   Loss: 0.745992 mae: 0.662000 (2032.7149365125522 steps/sec)\n",
      "Step #10857\tEpoch   3 Batch 1481/3125   Loss: 0.762450 mae: 0.708440 (2213.960559098011 steps/sec)\n",
      "Step #10858\tEpoch   3 Batch 1482/3125   Loss: 0.848142 mae: 0.714500 (2087.757093081135 steps/sec)\n",
      "Step #10859\tEpoch   3 Batch 1483/3125   Loss: 0.888869 mae: 0.730782 (2004.3505686705535 steps/sec)\n",
      "Step #10860\tEpoch   3 Batch 1484/3125   Loss: 0.833588 mae: 0.739941 (2218.762365238735 steps/sec)\n",
      "Step #10861\tEpoch   3 Batch 1485/3125   Loss: 0.866398 mae: 0.715636 (2236.0319440446106 steps/sec)\n",
      "Step #10862\tEpoch   3 Batch 1486/3125   Loss: 0.843256 mae: 0.720000 (2051.626409962923 steps/sec)\n",
      "Step #10863\tEpoch   3 Batch 1487/3125   Loss: 0.802158 mae: 0.703127 (1845.8407780662765 steps/sec)\n",
      "Step #10864\tEpoch   3 Batch 1488/3125   Loss: 0.885822 mae: 0.725607 (1993.2820712663124 steps/sec)\n",
      "Step #10865\tEpoch   3 Batch 1489/3125   Loss: 0.740222 mae: 0.667680 (2001.118331281787 steps/sec)\n",
      "Step #10866\tEpoch   3 Batch 1490/3125   Loss: 0.886689 mae: 0.745388 (1370.2039802945367 steps/sec)\n",
      "Step #10867\tEpoch   3 Batch 1491/3125   Loss: 0.824970 mae: 0.705437 (1404.9763509439524 steps/sec)\n",
      "Step #10868\tEpoch   3 Batch 1492/3125   Loss: 0.767639 mae: 0.703018 (1300.4948591697776 steps/sec)\n",
      "Step #10869\tEpoch   3 Batch 1493/3125   Loss: 0.854782 mae: 0.753155 (1519.1579679384558 steps/sec)\n",
      "Step #10870\tEpoch   3 Batch 1494/3125   Loss: 0.737007 mae: 0.695608 (1638.2848080994306 steps/sec)\n",
      "Step #10871\tEpoch   3 Batch 1495/3125   Loss: 0.839183 mae: 0.722558 (1324.3943718897617 steps/sec)\n",
      "Step #10872\tEpoch   3 Batch 1496/3125   Loss: 0.746278 mae: 0.702421 (1472.8845937745814 steps/sec)\n",
      "Step #10873\tEpoch   3 Batch 1497/3125   Loss: 0.898201 mae: 0.741938 (1543.2944778052513 steps/sec)\n",
      "Step #10874\tEpoch   3 Batch 1498/3125   Loss: 0.826742 mae: 0.722501 (1390.3247833783041 steps/sec)\n",
      "Step #10875\tEpoch   3 Batch 1499/3125   Loss: 0.852828 mae: 0.728972 (1501.7737835225034 steps/sec)\n",
      "Step #10876\tEpoch   3 Batch 1500/3125   Loss: 0.848299 mae: 0.725534 (1495.210255386502 steps/sec)\n",
      "Step #10877\tEpoch   3 Batch 1501/3125   Loss: 0.695266 mae: 0.667991 (1677.735021880175 steps/sec)\n",
      "Step #10878\tEpoch   3 Batch 1502/3125   Loss: 0.741821 mae: 0.672782 (1720.0485548374397 steps/sec)\n",
      "Step #10879\tEpoch   3 Batch 1503/3125   Loss: 0.908182 mae: 0.757004 (1631.4789603478991 steps/sec)\n",
      "Step #10880\tEpoch   3 Batch 1504/3125   Loss: 0.891422 mae: 0.744340 (1596.3705564436325 steps/sec)\n",
      "Step #10881\tEpoch   3 Batch 1505/3125   Loss: 0.623992 mae: 0.612052 (1818.2347841165251 steps/sec)\n",
      "Step #10882\tEpoch   3 Batch 1506/3125   Loss: 0.750225 mae: 0.673929 (1399.8184439579218 steps/sec)\n",
      "Step #10883\tEpoch   3 Batch 1507/3125   Loss: 0.860049 mae: 0.739278 (1554.2979114477566 steps/sec)\n",
      "Step #10884\tEpoch   3 Batch 1508/3125   Loss: 0.811148 mae: 0.705064 (2090.400007974243 steps/sec)\n",
      "Step #10885\tEpoch   3 Batch 1509/3125   Loss: 0.736972 mae: 0.669116 (1905.5317293014466 steps/sec)\n",
      "Step #10886\tEpoch   3 Batch 1510/3125   Loss: 0.876094 mae: 0.730588 (1676.5011071939628 steps/sec)\n",
      "Step #10887\tEpoch   3 Batch 1511/3125   Loss: 0.845869 mae: 0.743912 (1415.6363489084797 steps/sec)\n",
      "Step #10888\tEpoch   3 Batch 1512/3125   Loss: 0.833645 mae: 0.707795 (1123.8515787442927 steps/sec)\n",
      "Step #10889\tEpoch   3 Batch 1513/3125   Loss: 0.784046 mae: 0.688960 (1143.9842897665285 steps/sec)\n",
      "Step #10890\tEpoch   3 Batch 1514/3125   Loss: 0.647710 mae: 0.665132 (1357.6171887643795 steps/sec)\n",
      "Step #10891\tEpoch   3 Batch 1515/3125   Loss: 0.724874 mae: 0.689922 (1190.1368246023233 steps/sec)\n",
      "Step #10892\tEpoch   3 Batch 1516/3125   Loss: 0.783387 mae: 0.727045 (1358.2502703998032 steps/sec)\n",
      "Step #10893\tEpoch   3 Batch 1517/3125   Loss: 0.804555 mae: 0.698668 (1413.2609120499221 steps/sec)\n",
      "Step #10894\tEpoch   3 Batch 1518/3125   Loss: 0.889953 mae: 0.762968 (1549.428888067972 steps/sec)\n",
      "Step #10895\tEpoch   3 Batch 1519/3125   Loss: 0.740576 mae: 0.700821 (1227.9198309024587 steps/sec)\n",
      "Step #10896\tEpoch   3 Batch 1520/3125   Loss: 0.749546 mae: 0.684951 (1398.0174522861962 steps/sec)\n",
      "Step #10897\tEpoch   3 Batch 1521/3125   Loss: 0.828924 mae: 0.715448 (1471.2416604112443 steps/sec)\n",
      "Step #10898\tEpoch   3 Batch 1522/3125   Loss: 0.876112 mae: 0.738892 (1467.4326338401686 steps/sec)\n",
      "Step #10899\tEpoch   3 Batch 1523/3125   Loss: 0.744857 mae: 0.697601 (1313.4783044393225 steps/sec)\n",
      "Step #10900\tEpoch   3 Batch 1524/3125   Loss: 0.747562 mae: 0.685971 (1537.321135350692 steps/sec)\n",
      "Step #10901\tEpoch   3 Batch 1525/3125   Loss: 0.753426 mae: 0.671758 (1416.7935630755096 steps/sec)\n",
      "Step #10902\tEpoch   3 Batch 1526/3125   Loss: 0.888907 mae: 0.752097 (1439.3531959286483 steps/sec)\n",
      "Step #10903\tEpoch   3 Batch 1527/3125   Loss: 0.668745 mae: 0.631857 (1609.3685010244878 steps/sec)\n",
      "Step #10904\tEpoch   3 Batch 1528/3125   Loss: 0.848239 mae: 0.725042 (1517.113858485311 steps/sec)\n",
      "Step #10905\tEpoch   3 Batch 1529/3125   Loss: 0.891585 mae: 0.718929 (1506.7154260096129 steps/sec)\n",
      "Step #10906\tEpoch   3 Batch 1530/3125   Loss: 0.869414 mae: 0.736297 (1687.7536074426391 steps/sec)\n",
      "Step #10907\tEpoch   3 Batch 1531/3125   Loss: 0.714269 mae: 0.679328 (1988.0856227366664 steps/sec)\n",
      "Step #10908\tEpoch   3 Batch 1532/3125   Loss: 0.771507 mae: 0.677329 (1799.4045320771877 steps/sec)\n",
      "Step #10909\tEpoch   3 Batch 1533/3125   Loss: 0.930959 mae: 0.749812 (1669.4943319322379 steps/sec)\n",
      "Step #10910\tEpoch   3 Batch 1534/3125   Loss: 0.893456 mae: 0.741893 (1757.1445328864684 steps/sec)\n",
      "Step #10911\tEpoch   3 Batch 1535/3125   Loss: 0.655949 mae: 0.647768 (1144.8148613165781 steps/sec)\n",
      "Step #10912\tEpoch   3 Batch 1536/3125   Loss: 0.814478 mae: 0.722380 (1237.498746068556 steps/sec)\n",
      "Step #10913\tEpoch   3 Batch 1537/3125   Loss: 0.979351 mae: 0.775424 (1277.9174562937596 steps/sec)\n",
      "Step #10914\tEpoch   3 Batch 1538/3125   Loss: 0.793769 mae: 0.697516 (956.6426420946993 steps/sec)\n",
      "Step #10915\tEpoch   3 Batch 1539/3125   Loss: 0.786798 mae: 0.683663 (1407.6640645451434 steps/sec)\n",
      "Step #10916\tEpoch   3 Batch 1540/3125   Loss: 0.884269 mae: 0.745592 (1358.9984188288965 steps/sec)\n",
      "Step #10917\tEpoch   3 Batch 1541/3125   Loss: 0.783090 mae: 0.711166 (1331.7702942129026 steps/sec)\n",
      "Step #10918\tEpoch   3 Batch 1542/3125   Loss: 0.796622 mae: 0.705577 (1320.9990236527983 steps/sec)\n",
      "Step #10919\tEpoch   3 Batch 1543/3125   Loss: 0.855765 mae: 0.740952 (1406.1916224679321 steps/sec)\n",
      "Step #10920\tEpoch   3 Batch 1544/3125   Loss: 0.930561 mae: 0.787622 (1117.8431508419196 steps/sec)\n",
      "Step #10921\tEpoch   3 Batch 1545/3125   Loss: 0.796195 mae: 0.720123 (1363.3539847747086 steps/sec)\n",
      "Step #10922\tEpoch   3 Batch 1546/3125   Loss: 0.858511 mae: 0.745544 (1315.3401322144032 steps/sec)\n",
      "Step #10923\tEpoch   3 Batch 1547/3125   Loss: 0.729228 mae: 0.671772 (1528.4918807031866 steps/sec)\n",
      "Step #10924\tEpoch   3 Batch 1548/3125   Loss: 0.905455 mae: 0.779789 (1571.4418451301572 steps/sec)\n",
      "Step #10925\tEpoch   3 Batch 1549/3125   Loss: 0.903525 mae: 0.728887 (1579.4304822298707 steps/sec)\n",
      "Step #10926\tEpoch   3 Batch 1550/3125   Loss: 0.808509 mae: 0.721747 (1466.7449993006014 steps/sec)\n",
      "Step #10927\tEpoch   3 Batch 1551/3125   Loss: 0.919598 mae: 0.761970 (1463.6126348701198 steps/sec)\n",
      "Step #10928\tEpoch   3 Batch 1552/3125   Loss: 0.695691 mae: 0.657536 (1598.1588593463036 steps/sec)\n",
      "Step #10929\tEpoch   3 Batch 1553/3125   Loss: 0.929569 mae: 0.758459 (1555.6814978561786 steps/sec)\n",
      "Step #10930\tEpoch   3 Batch 1554/3125   Loss: 0.729268 mae: 0.683606 (1583.8798845973747 steps/sec)\n",
      "Step #10931\tEpoch   3 Batch 1555/3125   Loss: 0.790716 mae: 0.720028 (1894.9426679075818 steps/sec)\n",
      "Step #10932\tEpoch   3 Batch 1556/3125   Loss: 0.886740 mae: 0.750433 (1938.7377393201505 steps/sec)\n",
      "Step #10933\tEpoch   3 Batch 1557/3125   Loss: 1.039842 mae: 0.794112 (1366.3474193086015 steps/sec)\n",
      "Step #10934\tEpoch   3 Batch 1558/3125   Loss: 0.744415 mae: 0.683483 (1221.831740852948 steps/sec)\n",
      "Step #10935\tEpoch   3 Batch 1559/3125   Loss: 0.762914 mae: 0.707529 (1459.548317500087 steps/sec)\n",
      "Step #10936\tEpoch   3 Batch 1560/3125   Loss: 0.736136 mae: 0.693670 (1301.1645726694587 steps/sec)\n",
      "Step #10937\tEpoch   3 Batch 1561/3125   Loss: 0.966336 mae: 0.742794 (1242.9114141611697 steps/sec)\n",
      "Step #10938\tEpoch   3 Batch 1562/3125   Loss: 0.715553 mae: 0.666188 (1307.0846707594503 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #10939\tEpoch   3 Batch 1563/3125   Loss: 0.862614 mae: 0.737220 (1185.9774132071097 steps/sec)\n",
      "Step #10940\tEpoch   3 Batch 1564/3125   Loss: 0.754483 mae: 0.692205 (1292.1931802777674 steps/sec)\n",
      "Step #10941\tEpoch   3 Batch 1565/3125   Loss: 0.857424 mae: 0.730669 (1519.6643502583315 steps/sec)\n",
      "Step #10942\tEpoch   3 Batch 1566/3125   Loss: 0.863156 mae: 0.740996 (1472.9259727489816 steps/sec)\n",
      "Step #10943\tEpoch   3 Batch 1567/3125   Loss: 0.752924 mae: 0.692529 (1666.1783166224398 steps/sec)\n",
      "Step #10944\tEpoch   3 Batch 1568/3125   Loss: 0.764373 mae: 0.675792 (1518.3770399223852 steps/sec)\n",
      "Step #10945\tEpoch   3 Batch 1569/3125   Loss: 0.777507 mae: 0.702048 (1710.9831116912785 steps/sec)\n",
      "Step #10946\tEpoch   3 Batch 1570/3125   Loss: 0.792827 mae: 0.676703 (1264.0908488143602 steps/sec)\n",
      "Step #10947\tEpoch   3 Batch 1571/3125   Loss: 0.962757 mae: 0.768734 (1635.4740347347324 steps/sec)\n",
      "Step #10948\tEpoch   3 Batch 1572/3125   Loss: 0.895838 mae: 0.741512 (1816.2664010739186 steps/sec)\n",
      "Step #10949\tEpoch   3 Batch 1573/3125   Loss: 0.689107 mae: 0.655283 (1766.0376087377579 steps/sec)\n",
      "Step #10950\tEpoch   3 Batch 1574/3125   Loss: 0.786889 mae: 0.718138 (1136.267439655406 steps/sec)\n",
      "Step #10951\tEpoch   3 Batch 1575/3125   Loss: 0.742556 mae: 0.674961 (1244.7113986408285 steps/sec)\n",
      "Step #10952\tEpoch   3 Batch 1576/3125   Loss: 0.843800 mae: 0.708852 (1092.1415254502087 steps/sec)\n",
      "Step #10953\tEpoch   3 Batch 1577/3125   Loss: 0.660800 mae: 0.636210 (1185.3071005883717 steps/sec)\n",
      "Step #10954\tEpoch   3 Batch 1578/3125   Loss: 0.714866 mae: 0.667603 (1090.4832721486734 steps/sec)\n",
      "Step #10955\tEpoch   3 Batch 1579/3125   Loss: 0.754211 mae: 0.698231 (1251.7545378035898 steps/sec)\n",
      "Step #10956\tEpoch   3 Batch 1580/3125   Loss: 0.896008 mae: 0.757619 (1085.3307250021994 steps/sec)\n",
      "Step #10957\tEpoch   3 Batch 1581/3125   Loss: 0.938105 mae: 0.760647 (1292.1931802777674 steps/sec)\n",
      "Step #10958\tEpoch   3 Batch 1582/3125   Loss: 0.786654 mae: 0.697650 (1380.3772889433014 steps/sec)\n",
      "Step #10959\tEpoch   3 Batch 1583/3125   Loss: 0.706766 mae: 0.671496 (1561.8800783490105 steps/sec)\n",
      "Step #10960\tEpoch   3 Batch 1584/3125   Loss: 0.700364 mae: 0.689710 (1682.918452180333 steps/sec)\n",
      "Step #10961\tEpoch   3 Batch 1585/3125   Loss: 0.725445 mae: 0.707072 (1864.3671212417546 steps/sec)\n",
      "Step #10962\tEpoch   3 Batch 1586/3125   Loss: 0.812661 mae: 0.697555 (1172.7135979063798 steps/sec)\n",
      "Step #10963\tEpoch   3 Batch 1587/3125   Loss: 0.728178 mae: 0.705366 (1328.0931181011608 steps/sec)\n",
      "Step #10964\tEpoch   3 Batch 1588/3125   Loss: 0.728608 mae: 0.694430 (1205.0243055954584 steps/sec)\n",
      "Step #10965\tEpoch   3 Batch 1589/3125   Loss: 0.737567 mae: 0.686133 (1247.599274219935 steps/sec)\n",
      "Step #10966\tEpoch   3 Batch 1590/3125   Loss: 0.891423 mae: 0.758655 (1084.9489122843322 steps/sec)\n",
      "Step #10967\tEpoch   3 Batch 1591/3125   Loss: 0.767318 mae: 0.707443 (1035.390304472542 steps/sec)\n",
      "Step #10968\tEpoch   3 Batch 1592/3125   Loss: 0.745547 mae: 0.673005 (1043.1256683827005 steps/sec)\n",
      "Step #10969\tEpoch   3 Batch 1593/3125   Loss: 0.897181 mae: 0.747854 (1233.0748199323828 steps/sec)\n",
      "Step #10970\tEpoch   3 Batch 1594/3125   Loss: 0.665812 mae: 0.658282 (1336.3529194359305 steps/sec)\n",
      "Step #10971\tEpoch   3 Batch 1595/3125   Loss: 0.940942 mae: 0.769960 (1324.4110997435994 steps/sec)\n",
      "Step #10972\tEpoch   3 Batch 1596/3125   Loss: 0.899993 mae: 0.737061 (1520.6781283310008 steps/sec)\n",
      "Step #10973\tEpoch   3 Batch 1597/3125   Loss: 0.702582 mae: 0.683050 (1286.7384128308647 steps/sec)\n",
      "Step #10974\tEpoch   3 Batch 1598/3125   Loss: 0.828937 mae: 0.735145 (1328.5558624534374 steps/sec)\n",
      "Step #10975\tEpoch   3 Batch 1599/3125   Loss: 0.808568 mae: 0.703641 (1493.2938378501544 steps/sec)\n",
      "Step #10976\tEpoch   3 Batch 1600/3125   Loss: 0.739022 mae: 0.678147 (1590.1850911048598 steps/sec)\n",
      "Step #10977\tEpoch   3 Batch 1601/3125   Loss: 0.721510 mae: 0.660241 (1436.6219567332064 steps/sec)\n",
      "Step #10978\tEpoch   3 Batch 1602/3125   Loss: 0.745349 mae: 0.689958 (1304.4585987261146 steps/sec)\n",
      "Step #10979\tEpoch   3 Batch 1603/3125   Loss: 0.705500 mae: 0.693265 (1469.7671810829374 steps/sec)\n",
      "Step #10980\tEpoch   3 Batch 1604/3125   Loss: 0.806238 mae: 0.704148 (1652.0682836908486 steps/sec)\n",
      "Step #10981\tEpoch   3 Batch 1605/3125   Loss: 0.783125 mae: 0.689242 (1523.9268975039058 steps/sec)\n",
      "Step #10982\tEpoch   3 Batch 1606/3125   Loss: 0.744221 mae: 0.671194 (1665.9003709676138 steps/sec)\n",
      "Step #10983\tEpoch   3 Batch 1607/3125   Loss: 0.951263 mae: 0.761597 (1751.4214130616335 steps/sec)\n",
      "Step #10984\tEpoch   3 Batch 1608/3125   Loss: 0.789985 mae: 0.714651 (1609.331450672233 steps/sec)\n",
      "Step #10985\tEpoch   3 Batch 1609/3125   Loss: 0.854773 mae: 0.753478 (1242.3297474053363 steps/sec)\n",
      "Step #10986\tEpoch   3 Batch 1610/3125   Loss: 0.747941 mae: 0.665580 (1470.632950449503 steps/sec)\n",
      "Step #10987\tEpoch   3 Batch 1611/3125   Loss: 0.725452 mae: 0.680153 (1558.5603127299212 steps/sec)\n",
      "Step #10988\tEpoch   3 Batch 1612/3125   Loss: 0.844149 mae: 0.736269 (1699.8743626946366 steps/sec)\n",
      "Step #10989\tEpoch   3 Batch 1613/3125   Loss: 0.700689 mae: 0.670766 (1847.3366630550638 steps/sec)\n",
      "Step #10990\tEpoch   3 Batch 1614/3125   Loss: 0.810222 mae: 0.701213 (2116.4754205899867 steps/sec)\n",
      "Step #10991\tEpoch   3 Batch 1615/3125   Loss: 0.819483 mae: 0.721446 (2076.408677313637 steps/sec)\n",
      "Step #10992\tEpoch   3 Batch 1616/3125   Loss: 0.889712 mae: 0.740985 (2141.1526877329115 steps/sec)\n",
      "Step #10993\tEpoch   3 Batch 1617/3125   Loss: 0.861073 mae: 0.729133 (1675.5367001430136 steps/sec)\n",
      "Step #10994\tEpoch   3 Batch 1618/3125   Loss: 0.696965 mae: 0.642239 (1714.6763036972839 steps/sec)\n",
      "Step #10995\tEpoch   3 Batch 1619/3125   Loss: 0.905106 mae: 0.742395 (1883.895077254761 steps/sec)\n",
      "Step #10996\tEpoch   3 Batch 1620/3125   Loss: 0.764861 mae: 0.690418 (1510.8402314004336 steps/sec)\n",
      "Step #10997\tEpoch   3 Batch 1621/3125   Loss: 0.828228 mae: 0.717941 (1286.1702258147607 steps/sec)\n",
      "Step #10998\tEpoch   3 Batch 1622/3125   Loss: 0.772589 mae: 0.682683 (1336.7106680519348 steps/sec)\n",
      "Step #10999\tEpoch   3 Batch 1623/3125   Loss: 0.856915 mae: 0.743880 (1119.6514756759066 steps/sec)\n",
      "Step #11000\tEpoch   3 Batch 1624/3125   Loss: 0.875437 mae: 0.745774 (1279.6173019543717 steps/sec)\n",
      "Step #11001\tEpoch   3 Batch 1625/3125   Loss: 0.842698 mae: 0.747058 (1417.9526707234618 steps/sec)\n",
      "Step #11002\tEpoch   3 Batch 1626/3125   Loss: 0.748171 mae: 0.692145 (1257.134636134756 steps/sec)\n",
      "Step #11003\tEpoch   3 Batch 1627/3125   Loss: 0.856990 mae: 0.749925 (1422.1064908997207 steps/sec)\n",
      "Step #11004\tEpoch   3 Batch 1628/3125   Loss: 0.827536 mae: 0.745192 (1405.993644323469 steps/sec)\n",
      "Step #11005\tEpoch   3 Batch 1629/3125   Loss: 0.752033 mae: 0.682893 (1387.6201756069158 steps/sec)\n",
      "Step #11006\tEpoch   3 Batch 1630/3125   Loss: 0.941857 mae: 0.769886 (1241.3370189945722 steps/sec)\n",
      "Step #11007\tEpoch   3 Batch 1631/3125   Loss: 0.907010 mae: 0.747707 (1650.7158880711559 steps/sec)\n",
      "Step #11008\tEpoch   3 Batch 1632/3125   Loss: 0.799520 mae: 0.685207 (1937.0902339672834 steps/sec)\n",
      "Step #11009\tEpoch   3 Batch 1633/3125   Loss: 0.952000 mae: 0.741888 (1705.736618217606 steps/sec)\n",
      "Step #11010\tEpoch   3 Batch 1634/3125   Loss: 0.963560 mae: 0.787951 (1705.2650409412836 steps/sec)\n",
      "Step #11011\tEpoch   3 Batch 1635/3125   Loss: 0.852101 mae: 0.741703 (1176.4700602497503 steps/sec)\n",
      "Step #11012\tEpoch   3 Batch 1636/3125   Loss: 0.834681 mae: 0.727302 (1376.4271931321457 steps/sec)\n",
      "Step #11013\tEpoch   3 Batch 1637/3125   Loss: 0.811320 mae: 0.708107 (1314.3344196540486 steps/sec)\n",
      "Step #11014\tEpoch   3 Batch 1638/3125   Loss: 0.817123 mae: 0.694805 (1288.7943855164021 steps/sec)\n",
      "Step #11015\tEpoch   3 Batch 1639/3125   Loss: 0.756381 mae: 0.669846 (1499.829788451361 steps/sec)\n",
      "Step #11016\tEpoch   3 Batch 1640/3125   Loss: 0.894374 mae: 0.772824 (1446.4713347679744 steps/sec)\n",
      "Step #11017\tEpoch   3 Batch 1641/3125   Loss: 0.636920 mae: 0.620372 (1313.2397788256217 steps/sec)\n",
      "Step #11018\tEpoch   3 Batch 1642/3125   Loss: 0.814425 mae: 0.728228 (1296.402851014113 steps/sec)\n",
      "Step #11019\tEpoch   3 Batch 1643/3125   Loss: 0.840639 mae: 0.701855 (1862.4630331879823 steps/sec)\n",
      "Step #11020\tEpoch   3 Batch 1644/3125   Loss: 0.741354 mae: 0.676357 (1872.7246749535648 steps/sec)\n",
      "Step #11021\tEpoch   3 Batch 1645/3125   Loss: 0.863100 mae: 0.728224 (1703.7549760337963 steps/sec)\n",
      "Step #11022\tEpoch   3 Batch 1646/3125   Loss: 0.731213 mae: 0.673728 (1657.5525011658144 steps/sec)\n",
      "Step #11023\tEpoch   3 Batch 1647/3125   Loss: 0.836603 mae: 0.725232 (1290.2295420847663 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11024\tEpoch   3 Batch 1648/3125   Loss: 0.859768 mae: 0.704718 (1100.4281756359667 steps/sec)\n",
      "Step #11025\tEpoch   3 Batch 1649/3125   Loss: 0.857959 mae: 0.745582 (1311.5807248506833 steps/sec)\n",
      "Step #11026\tEpoch   3 Batch 1650/3125   Loss: 0.721274 mae: 0.669453 (1334.3632488149397 steps/sec)\n",
      "Step #11027\tEpoch   3 Batch 1651/3125   Loss: 0.818896 mae: 0.692188 (1360.9827958803564 steps/sec)\n",
      "Step #11028\tEpoch   3 Batch 1652/3125   Loss: 0.700328 mae: 0.662350 (1339.939045817866 steps/sec)\n",
      "Step #11029\tEpoch   3 Batch 1653/3125   Loss: 0.798553 mae: 0.719165 (1272.2271764912855 steps/sec)\n",
      "Step #11030\tEpoch   3 Batch 1654/3125   Loss: 0.715236 mae: 0.671469 (1200.2243461340354 steps/sec)\n",
      "Step #11031\tEpoch   3 Batch 1655/3125   Loss: 0.873812 mae: 0.717610 (1335.4934026185745 steps/sec)\n",
      "Step #11032\tEpoch   3 Batch 1656/3125   Loss: 0.835410 mae: 0.707085 (1534.0599534768041 steps/sec)\n",
      "Step #11033\tEpoch   3 Batch 1657/3125   Loss: 0.776457 mae: 0.697789 (1454.8905615872907 steps/sec)\n",
      "Step #11034\tEpoch   3 Batch 1658/3125   Loss: 0.655993 mae: 0.645373 (1330.789976330535 steps/sec)\n",
      "Step #11035\tEpoch   3 Batch 1659/3125   Loss: 0.712221 mae: 0.679830 (1485.5718009747252 steps/sec)\n",
      "Step #11036\tEpoch   3 Batch 1660/3125   Loss: 0.890686 mae: 0.740634 (1278.11216343048 steps/sec)\n",
      "Step #11037\tEpoch   3 Batch 1661/3125   Loss: 0.956879 mae: 0.773060 (1270.9318885636542 steps/sec)\n",
      "Step #11038\tEpoch   3 Batch 1662/3125   Loss: 0.845315 mae: 0.725199 (1427.148563767889 steps/sec)\n",
      "Step #11039\tEpoch   3 Batch 1663/3125   Loss: 0.836426 mae: 0.745738 (1297.1164908027067 steps/sec)\n",
      "Step #11040\tEpoch   3 Batch 1664/3125   Loss: 0.838737 mae: 0.727981 (1337.8618727432793 steps/sec)\n",
      "Step #11041\tEpoch   3 Batch 1665/3125   Loss: 0.822629 mae: 0.715989 (1209.5976375046143 steps/sec)\n",
      "Step #11042\tEpoch   3 Batch 1666/3125   Loss: 0.776718 mae: 0.705395 (1283.8553272767344 steps/sec)\n",
      "Step #11043\tEpoch   3 Batch 1667/3125   Loss: 0.751546 mae: 0.698016 (1816.7226861637616 steps/sec)\n",
      "Step #11044\tEpoch   3 Batch 1668/3125   Loss: 0.759701 mae: 0.668261 (1545.6375937854689 steps/sec)\n",
      "Step #11045\tEpoch   3 Batch 1669/3125   Loss: 0.832544 mae: 0.722741 (1288.4934873433276 steps/sec)\n",
      "Step #11046\tEpoch   3 Batch 1670/3125   Loss: 0.726295 mae: 0.641304 (1282.4421505797172 steps/sec)\n",
      "Step #11047\tEpoch   3 Batch 1671/3125   Loss: 0.807769 mae: 0.728874 (1366.2762062360744 steps/sec)\n",
      "Step #11048\tEpoch   3 Batch 1672/3125   Loss: 0.824506 mae: 0.715625 (1154.0123371870993 steps/sec)\n",
      "Step #11049\tEpoch   3 Batch 1673/3125   Loss: 0.719005 mae: 0.696208 (1210.3306381333286 steps/sec)\n",
      "Step #11050\tEpoch   3 Batch 1674/3125   Loss: 0.763023 mae: 0.685545 (1414.0232349589714 steps/sec)\n",
      "Step #11051\tEpoch   3 Batch 1675/3125   Loss: 0.821109 mae: 0.723818 (1328.5811123288713 steps/sec)\n",
      "Step #11052\tEpoch   3 Batch 1676/3125   Loss: 0.729135 mae: 0.664729 (1252.8238766025067 steps/sec)\n",
      "Step #11053\tEpoch   3 Batch 1677/3125   Loss: 0.716229 mae: 0.669763 (1527.96846653212 steps/sec)\n",
      "Step #11054\tEpoch   3 Batch 1678/3125   Loss: 0.871093 mae: 0.724792 (1604.3084455324358 steps/sec)\n",
      "Step #11055\tEpoch   3 Batch 1679/3125   Loss: 0.804914 mae: 0.679044 (1253.093368706605 steps/sec)\n",
      "Step #11056\tEpoch   3 Batch 1680/3125   Loss: 0.795558 mae: 0.708521 (1145.002375011602 steps/sec)\n",
      "Step #11057\tEpoch   3 Batch 1681/3125   Loss: 0.853068 mae: 0.717852 (1348.147957674951 steps/sec)\n",
      "Step #11058\tEpoch   3 Batch 1682/3125   Loss: 0.854377 mae: 0.748333 (1552.5029241497757 steps/sec)\n",
      "Step #11059\tEpoch   3 Batch 1683/3125   Loss: 0.733036 mae: 0.683617 (1375.5153709424583 steps/sec)\n",
      "Step #11060\tEpoch   3 Batch 1684/3125   Loss: 0.661415 mae: 0.644631 (1055.7445051902419 steps/sec)\n",
      "Step #11061\tEpoch   3 Batch 1685/3125   Loss: 0.839582 mae: 0.721399 (1267.6208897485494 steps/sec)\n",
      "Step #11062\tEpoch   3 Batch 1686/3125   Loss: 0.656162 mae: 0.642273 (1362.8401167135644 steps/sec)\n",
      "Step #11063\tEpoch   3 Batch 1687/3125   Loss: 0.930147 mae: 0.766851 (1709.0589040649347 steps/sec)\n",
      "Step #11064\tEpoch   3 Batch 1688/3125   Loss: 0.840634 mae: 0.731668 (1446.7507381550263 steps/sec)\n",
      "Step #11065\tEpoch   3 Batch 1689/3125   Loss: 0.833990 mae: 0.743069 (1280.9381871487906 steps/sec)\n",
      "Step #11066\tEpoch   3 Batch 1690/3125   Loss: 0.780605 mae: 0.694482 (956.1715786402101 steps/sec)\n",
      "Step #11067\tEpoch   3 Batch 1691/3125   Loss: 0.909683 mae: 0.757757 (1100.5667744237794 steps/sec)\n",
      "Step #11068\tEpoch   3 Batch 1692/3125   Loss: 0.811993 mae: 0.725379 (1092.7561303500527 steps/sec)\n",
      "Step #11069\tEpoch   3 Batch 1693/3125   Loss: 1.010934 mae: 0.781277 (919.8761305717118 steps/sec)\n",
      "Step #11070\tEpoch   3 Batch 1694/3125   Loss: 0.851150 mae: 0.710486 (1190.616555013058 steps/sec)\n",
      "Step #11071\tEpoch   3 Batch 1695/3125   Loss: 0.845025 mae: 0.687447 (1072.712020460358 steps/sec)\n",
      "Step #11072\tEpoch   3 Batch 1696/3125   Loss: 0.821311 mae: 0.711890 (1648.3026935259488 steps/sec)\n",
      "Step #11073\tEpoch   3 Batch 1697/3125   Loss: 0.761590 mae: 0.707213 (1393.262069744421 steps/sec)\n",
      "Step #11074\tEpoch   3 Batch 1698/3125   Loss: 0.750507 mae: 0.684363 (1205.2528433744633 steps/sec)\n",
      "Step #11075\tEpoch   3 Batch 1699/3125   Loss: 0.878273 mae: 0.720681 (1213.5102449411806 steps/sec)\n",
      "Step #11076\tEpoch   3 Batch 1700/3125   Loss: 0.766370 mae: 0.697601 (1041.4938418752483 steps/sec)\n",
      "Step #11077\tEpoch   3 Batch 1701/3125   Loss: 0.819107 mae: 0.723638 (810.6345064842195 steps/sec)\n",
      "Step #11078\tEpoch   3 Batch 1702/3125   Loss: 0.773201 mae: 0.689527 (1005.4907225391955 steps/sec)\n",
      "Step #11079\tEpoch   3 Batch 1703/3125   Loss: 0.900685 mae: 0.747106 (1173.9346070095105 steps/sec)\n",
      "Step #11080\tEpoch   3 Batch 1704/3125   Loss: 0.819683 mae: 0.727153 (1243.6558578646489 steps/sec)\n",
      "Step #11081\tEpoch   3 Batch 1705/3125   Loss: 0.811728 mae: 0.721872 (1347.4982812128533 steps/sec)\n",
      "Step #11082\tEpoch   3 Batch 1706/3125   Loss: 0.768698 mae: 0.700553 (1698.0850357486984 steps/sec)\n",
      "Step #11083\tEpoch   3 Batch 1707/3125   Loss: 0.859618 mae: 0.741226 (1214.0300909443508 steps/sec)\n",
      "Step #11084\tEpoch   3 Batch 1708/3125   Loss: 0.840119 mae: 0.706643 (885.113279535402 steps/sec)\n",
      "Step #11085\tEpoch   3 Batch 1709/3125   Loss: 0.676852 mae: 0.657051 (779.5089858196889 steps/sec)\n",
      "Step #11086\tEpoch   3 Batch 1710/3125   Loss: 0.768672 mae: 0.669276 (708.7966201943389 steps/sec)\n",
      "Step #11087\tEpoch   3 Batch 1711/3125   Loss: 0.740245 mae: 0.673840 (668.2275568924094 steps/sec)\n",
      "Step #11088\tEpoch   3 Batch 1712/3125   Loss: 0.668212 mae: 0.650824 (1285.6419467757064 steps/sec)\n",
      "Step #11089\tEpoch   3 Batch 1713/3125   Loss: 0.758373 mae: 0.707952 (1503.21623384536 steps/sec)\n",
      "Step #11090\tEpoch   3 Batch 1714/3125   Loss: 0.846147 mae: 0.747759 (2039.1385094073605 steps/sec)\n",
      "Step #11091\tEpoch   3 Batch 1715/3125   Loss: 0.690950 mae: 0.637741 (1882.6773915541512 steps/sec)\n",
      "Step #11092\tEpoch   3 Batch 1716/3125   Loss: 0.982718 mae: 0.758562 (1431.9722502936115 steps/sec)\n",
      "Step #11093\tEpoch   3 Batch 1717/3125   Loss: 0.788136 mae: 0.686543 (1698.2500465628518 steps/sec)\n",
      "Step #11094\tEpoch   3 Batch 1718/3125   Loss: 0.796931 mae: 0.689265 (1408.212297630319 steps/sec)\n",
      "Step #11095\tEpoch   3 Batch 1719/3125   Loss: 0.807020 mae: 0.718821 (1795.0304285677603 steps/sec)\n",
      "Step #11096\tEpoch   3 Batch 1720/3125   Loss: 0.824633 mae: 0.714944 (1609.2079618176517 steps/sec)\n",
      "Step #11097\tEpoch   3 Batch 1721/3125   Loss: 0.848260 mae: 0.717820 (1854.1310440556288 steps/sec)\n",
      "Step #11098\tEpoch   3 Batch 1722/3125   Loss: 0.711861 mae: 0.678643 (1659.585648038238 steps/sec)\n",
      "Step #11099\tEpoch   3 Batch 1723/3125   Loss: 0.878521 mae: 0.721923 (1320.6579510818913 steps/sec)\n",
      "Step #11100\tEpoch   3 Batch 1724/3125   Loss: 0.910211 mae: 0.772362 (1420.8154361052018 steps/sec)\n",
      "Step #11101\tEpoch   3 Batch 1725/3125   Loss: 0.768189 mae: 0.685647 (1449.1500594267393 steps/sec)\n",
      "Step #11102\tEpoch   3 Batch 1726/3125   Loss: 0.850773 mae: 0.750150 (1412.3091635183278 steps/sec)\n",
      "Step #11103\tEpoch   3 Batch 1727/3125   Loss: 0.735500 mae: 0.679943 (1590.2936180538704 steps/sec)\n",
      "Step #11104\tEpoch   3 Batch 1728/3125   Loss: 0.927313 mae: 0.751456 (1434.754529035083 steps/sec)\n",
      "Step #11105\tEpoch   3 Batch 1729/3125   Loss: 0.730048 mae: 0.684418 (1330.7730869540387 steps/sec)\n",
      "Step #11106\tEpoch   3 Batch 1730/3125   Loss: 0.823075 mae: 0.727928 (1475.2780454862016 steps/sec)\n",
      "Step #11107\tEpoch   3 Batch 1731/3125   Loss: 0.798294 mae: 0.705018 (1437.488518747001 steps/sec)\n",
      "Step #11108\tEpoch   3 Batch 1732/3125   Loss: 0.887151 mae: 0.715114 (1491.8703582505761 steps/sec)\n",
      "Step #11109\tEpoch   3 Batch 1733/3125   Loss: 0.792341 mae: 0.711646 (1527.8460171059726 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11110\tEpoch   3 Batch 1734/3125   Loss: 0.852050 mae: 0.741893 (1566.1725278746555 steps/sec)\n",
      "Step #11111\tEpoch   3 Batch 1735/3125   Loss: 0.853145 mae: 0.736515 (1190.2178786485736 steps/sec)\n",
      "Step #11112\tEpoch   3 Batch 1736/3125   Loss: 0.684933 mae: 0.650105 (1354.6443428158025 steps/sec)\n",
      "Step #11113\tEpoch   3 Batch 1737/3125   Loss: 0.750885 mae: 0.694166 (1730.8803988081975 steps/sec)\n",
      "Step #11114\tEpoch   3 Batch 1738/3125   Loss: 0.824574 mae: 0.706138 (2079.888921947833 steps/sec)\n",
      "Step #11115\tEpoch   3 Batch 1739/3125   Loss: 0.811393 mae: 0.709514 (1750.719604634855 steps/sec)\n",
      "Step #11116\tEpoch   3 Batch 1740/3125   Loss: 0.922758 mae: 0.758547 (1832.646177239083 steps/sec)\n",
      "Step #11117\tEpoch   3 Batch 1741/3125   Loss: 0.820405 mae: 0.700052 (1441.5793670433611 steps/sec)\n",
      "Step #11118\tEpoch   3 Batch 1742/3125   Loss: 0.865740 mae: 0.735287 (1827.6790071812034 steps/sec)\n",
      "Step #11119\tEpoch   3 Batch 1743/3125   Loss: 0.809731 mae: 0.727960 (1863.4228694809983 steps/sec)\n",
      "Step #11120\tEpoch   3 Batch 1744/3125   Loss: 0.819755 mae: 0.723497 (1913.1456512616542 steps/sec)\n",
      "Step #11121\tEpoch   3 Batch 1745/3125   Loss: 0.663013 mae: 0.651669 (1813.8315170385747 steps/sec)\n",
      "Step #11122\tEpoch   3 Batch 1746/3125   Loss: 0.774460 mae: 0.701704 (1998.6581274778896 steps/sec)\n",
      "Step #11123\tEpoch   3 Batch 1747/3125   Loss: 0.796379 mae: 0.728961 (1745.0817557728312 steps/sec)\n",
      "Step #11124\tEpoch   3 Batch 1748/3125   Loss: 0.705263 mae: 0.652779 (1923.022328183027 steps/sec)\n",
      "Step #11125\tEpoch   3 Batch 1749/3125   Loss: 0.827047 mae: 0.731115 (1575.562150182187 steps/sec)\n",
      "Step #11126\tEpoch   3 Batch 1750/3125   Loss: 0.716446 mae: 0.677181 (1592.6728688057717 steps/sec)\n",
      "Step #11127\tEpoch   3 Batch 1751/3125   Loss: 0.840371 mae: 0.724338 (1689.1946097896916 steps/sec)\n",
      "Step #11128\tEpoch   3 Batch 1752/3125   Loss: 0.786150 mae: 0.696948 (1690.2020519516107 steps/sec)\n",
      "Step #11129\tEpoch   3 Batch 1753/3125   Loss: 0.631648 mae: 0.631428 (1671.2904742550666 steps/sec)\n",
      "Step #11130\tEpoch   3 Batch 1754/3125   Loss: 0.854915 mae: 0.727615 (1776.7656228819303 steps/sec)\n",
      "Step #11131\tEpoch   3 Batch 1755/3125   Loss: 0.754012 mae: 0.695281 (1891.4220262092228 steps/sec)\n",
      "Step #11132\tEpoch   3 Batch 1756/3125   Loss: 0.874856 mae: 0.756271 (1671.3304324264013 steps/sec)\n",
      "Step #11133\tEpoch   3 Batch 1757/3125   Loss: 0.744563 mae: 0.705179 (1389.6706646345503 steps/sec)\n",
      "Step #11134\tEpoch   3 Batch 1758/3125   Loss: 0.889573 mae: 0.769196 (1784.8102127659574 steps/sec)\n",
      "Step #11135\tEpoch   3 Batch 1759/3125   Loss: 0.979701 mae: 0.775337 (2068.686868686869 steps/sec)\n",
      "Step #11136\tEpoch   3 Batch 1760/3125   Loss: 0.820404 mae: 0.736780 (2012.4673729464148 steps/sec)\n",
      "Step #11137\tEpoch   3 Batch 1761/3125   Loss: 0.746794 mae: 0.705909 (2009.632504431987 steps/sec)\n",
      "Step #11138\tEpoch   3 Batch 1762/3125   Loss: 0.688152 mae: 0.666884 (2046.0817983140805 steps/sec)\n",
      "Step #11139\tEpoch   3 Batch 1763/3125   Loss: 0.729122 mae: 0.701268 (2072.18220443654 steps/sec)\n",
      "Step #11140\tEpoch   3 Batch 1764/3125   Loss: 0.948964 mae: 0.778958 (1888.3564295812062 steps/sec)\n",
      "Step #11141\tEpoch   3 Batch 1765/3125   Loss: 0.811385 mae: 0.737127 (1533.8019001089747 steps/sec)\n",
      "Step #11142\tEpoch   3 Batch 1766/3125   Loss: 0.737450 mae: 0.701742 (1926.325458353235 steps/sec)\n",
      "Step #11143\tEpoch   3 Batch 1767/3125   Loss: 0.686083 mae: 0.644522 (1887.5236260868 steps/sec)\n",
      "Step #11144\tEpoch   3 Batch 1768/3125   Loss: 0.809857 mae: 0.678354 (1949.7145832171213 steps/sec)\n",
      "Step #11145\tEpoch   3 Batch 1769/3125   Loss: 0.865237 mae: 0.719792 (1984.811660041643 steps/sec)\n",
      "Step #11146\tEpoch   3 Batch 1770/3125   Loss: 0.927528 mae: 0.734363 (1958.1799676928392 steps/sec)\n",
      "Step #11147\tEpoch   3 Batch 1771/3125   Loss: 0.831101 mae: 0.727373 (2071.7727834033094 steps/sec)\n",
      "Step #11148\tEpoch   3 Batch 1772/3125   Loss: 0.902954 mae: 0.740356 (1716.9223715880994 steps/sec)\n",
      "Step #11149\tEpoch   3 Batch 1773/3125   Loss: 0.759211 mae: 0.680715 (1822.136880609594 steps/sec)\n",
      "Step #11150\tEpoch   3 Batch 1774/3125   Loss: 0.765892 mae: 0.683055 (1900.3515894015732 steps/sec)\n",
      "Step #11151\tEpoch   3 Batch 1775/3125   Loss: 0.859672 mae: 0.733735 (1950.0953124854707 steps/sec)\n",
      "Step #11152\tEpoch   3 Batch 1776/3125   Loss: 0.853823 mae: 0.723332 (2034.016139044072 steps/sec)\n",
      "Step #11153\tEpoch   3 Batch 1777/3125   Loss: 0.806715 mae: 0.731547 (1989.4246549352558 steps/sec)\n",
      "Step #11154\tEpoch   3 Batch 1778/3125   Loss: 0.848992 mae: 0.721723 (2007.0552881164524 steps/sec)\n",
      "Step #11155\tEpoch   3 Batch 1779/3125   Loss: 0.747176 mae: 0.670484 (1917.0981424601434 steps/sec)\n",
      "Step #11156\tEpoch   3 Batch 1780/3125   Loss: 0.856783 mae: 0.743922 (1638.7456729153805 steps/sec)\n",
      "Step #11157\tEpoch   3 Batch 1781/3125   Loss: 0.737738 mae: 0.674740 (1764.9229111963914 steps/sec)\n",
      "Step #11158\tEpoch   3 Batch 1782/3125   Loss: 0.804628 mae: 0.737745 (1976.0590984471582 steps/sec)\n",
      "Step #11159\tEpoch   3 Batch 1783/3125   Loss: 0.907787 mae: 0.738477 (1775.5020488333503 steps/sec)\n",
      "Step #11160\tEpoch   3 Batch 1784/3125   Loss: 0.935225 mae: 0.752613 (1937.5197479651513 steps/sec)\n",
      "Step #11161\tEpoch   3 Batch 1785/3125   Loss: 0.904051 mae: 0.737720 (1951.8921836898048 steps/sec)\n",
      "Step #11162\tEpoch   3 Batch 1786/3125   Loss: 0.741179 mae: 0.672564 (1998.0678169570974 steps/sec)\n",
      "Step #11163\tEpoch   3 Batch 1787/3125   Loss: 0.735613 mae: 0.670162 (1927.812913663774 steps/sec)\n",
      "Step #11164\tEpoch   3 Batch 1788/3125   Loss: 0.814294 mae: 0.711383 (1516.433710546296 steps/sec)\n",
      "Step #11165\tEpoch   3 Batch 1789/3125   Loss: 0.720020 mae: 0.683169 (1784.384997617589 steps/sec)\n",
      "Step #11166\tEpoch   3 Batch 1790/3125   Loss: 0.847908 mae: 0.728303 (1997.3637090936797 steps/sec)\n",
      "Step #11167\tEpoch   3 Batch 1791/3125   Loss: 0.923846 mae: 0.749134 (2118.6563620750617 steps/sec)\n",
      "Step #11168\tEpoch   3 Batch 1792/3125   Loss: 0.800317 mae: 0.717223 (2011.4250637816272 steps/sec)\n",
      "Step #11169\tEpoch   3 Batch 1793/3125   Loss: 0.728845 mae: 0.651300 (2196.637722449749 steps/sec)\n",
      "Step #11170\tEpoch   3 Batch 1794/3125   Loss: 0.764801 mae: 0.670218 (1925.971640584821 steps/sec)\n",
      "Step #11171\tEpoch   3 Batch 1795/3125   Loss: 0.848448 mae: 0.716637 (2174.5665698880134 steps/sec)\n",
      "Step #11172\tEpoch   3 Batch 1796/3125   Loss: 0.808316 mae: 0.712545 (2021.5851471977483 steps/sec)\n",
      "Step #11173\tEpoch   3 Batch 1797/3125   Loss: 0.778391 mae: 0.691007 (1724.305435649507 steps/sec)\n",
      "Step #11174\tEpoch   3 Batch 1798/3125   Loss: 0.997724 mae: 0.778766 (1851.1687027752278 steps/sec)\n",
      "Step #11175\tEpoch   3 Batch 1799/3125   Loss: 0.922413 mae: 0.753578 (2118.1000090898992 steps/sec)\n",
      "Step #11176\tEpoch   3 Batch 1800/3125   Loss: 0.765974 mae: 0.695664 (2098.894082088133 steps/sec)\n",
      "Step #11177\tEpoch   3 Batch 1801/3125   Loss: 0.773957 mae: 0.700251 (2144.634201214898 steps/sec)\n",
      "Step #11178\tEpoch   3 Batch 1802/3125   Loss: 0.818786 mae: 0.726003 (2169.931502597107 steps/sec)\n",
      "Step #11179\tEpoch   3 Batch 1803/3125   Loss: 0.747422 mae: 0.688918 (1991.9567633286158 steps/sec)\n",
      "Step #11180\tEpoch   3 Batch 1804/3125   Loss: 0.847465 mae: 0.731696 (1888.237412662969 steps/sec)\n",
      "Step #11181\tEpoch   3 Batch 1805/3125   Loss: 0.866843 mae: 0.730474 (1686.2201495537508 steps/sec)\n",
      "Step #11182\tEpoch   3 Batch 1806/3125   Loss: 0.785014 mae: 0.702325 (1850.3520443275866 steps/sec)\n",
      "Step #11183\tEpoch   3 Batch 1807/3125   Loss: 0.867537 mae: 0.743680 (2047.5001220405175 steps/sec)\n",
      "Step #11184\tEpoch   3 Batch 1808/3125   Loss: 0.774898 mae: 0.673570 (1974.5706538113889 steps/sec)\n",
      "Step #11185\tEpoch   3 Batch 1809/3125   Loss: 0.779615 mae: 0.693860 (1841.8851386363835 steps/sec)\n",
      "Step #11186\tEpoch   3 Batch 1810/3125   Loss: 0.890485 mae: 0.749918 (1999.5347152037527 steps/sec)\n",
      "Step #11187\tEpoch   3 Batch 1811/3125   Loss: 0.838472 mae: 0.733023 (1631.745537728949 steps/sec)\n",
      "Step #11188\tEpoch   3 Batch 1812/3125   Loss: 0.767072 mae: 0.704537 (1522.3449817796425 steps/sec)\n",
      "Step #11189\tEpoch   3 Batch 1813/3125   Loss: 0.775695 mae: 0.702256 (1819.1022249208484 steps/sec)\n",
      "Step #11190\tEpoch   3 Batch 1814/3125   Loss: 0.788703 mae: 0.698088 (2064.0650374496818 steps/sec)\n",
      "Step #11191\tEpoch   3 Batch 1815/3125   Loss: 0.774201 mae: 0.687895 (1991.2380482154217 steps/sec)\n",
      "Step #11192\tEpoch   3 Batch 1816/3125   Loss: 0.835492 mae: 0.714367 (2114.5334650829823 steps/sec)\n",
      "Step #11193\tEpoch   3 Batch 1817/3125   Loss: 0.776144 mae: 0.701621 (2128.6561104344296 steps/sec)\n",
      "Step #11194\tEpoch   3 Batch 1818/3125   Loss: 0.849478 mae: 0.759977 (2168.4282361213072 steps/sec)\n",
      "Step #11195\tEpoch   3 Batch 1819/3125   Loss: 0.756441 mae: 0.698465 (1909.8701346010237 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11196\tEpoch   3 Batch 1820/3125   Loss: 0.774251 mae: 0.685043 (2083.235983629356 steps/sec)\n",
      "Step #11197\tEpoch   3 Batch 1821/3125   Loss: 0.817632 mae: 0.713050 (1623.4465354275849 steps/sec)\n",
      "Step #11198\tEpoch   3 Batch 1822/3125   Loss: 0.798847 mae: 0.703310 (1857.0864363703986 steps/sec)\n",
      "Step #11199\tEpoch   3 Batch 1823/3125   Loss: 0.839229 mae: 0.745082 (2005.1171240080314 steps/sec)\n",
      "Step #11200\tEpoch   3 Batch 1824/3125   Loss: 0.709735 mae: 0.682066 (2080.65242625976 steps/sec)\n",
      "Step #11201\tEpoch   3 Batch 1825/3125   Loss: 0.917626 mae: 0.773626 (2164.802064516129 steps/sec)\n",
      "Step #11202\tEpoch   3 Batch 1826/3125   Loss: 0.859490 mae: 0.739942 (2060.2326312480354 steps/sec)\n",
      "Step #11203\tEpoch   3 Batch 1827/3125   Loss: 0.828661 mae: 0.725513 (1996.5840608547464 steps/sec)\n",
      "Step #11204\tEpoch   3 Batch 1828/3125   Loss: 0.738363 mae: 0.669589 (2003.6994573110144 steps/sec)\n",
      "Step #11205\tEpoch   3 Batch 1829/3125   Loss: 0.801992 mae: 0.728317 (1784.506466984343 steps/sec)\n",
      "Step #11206\tEpoch   3 Batch 1830/3125   Loss: 0.841108 mae: 0.741237 (1897.9094644246954 steps/sec)\n",
      "Step #11207\tEpoch   3 Batch 1831/3125   Loss: 0.712157 mae: 0.669890 (1924.4870241897002 steps/sec)\n",
      "Step #11208\tEpoch   3 Batch 1832/3125   Loss: 0.712201 mae: 0.675071 (2087.6739602205985 steps/sec)\n",
      "Step #11209\tEpoch   3 Batch 1833/3125   Loss: 0.743021 mae: 0.682984 (2168.181630205533 steps/sec)\n",
      "Step #11210\tEpoch   3 Batch 1834/3125   Loss: 0.828744 mae: 0.728601 (2064.8372963127063 steps/sec)\n",
      "Step #11211\tEpoch   3 Batch 1835/3125   Loss: 0.909466 mae: 0.752744 (2051.305325964689 steps/sec)\n",
      "Step #11212\tEpoch   3 Batch 1836/3125   Loss: 0.832622 mae: 0.739764 (2016.1239773502919 steps/sec)\n",
      "Step #11213\tEpoch   3 Batch 1837/3125   Loss: 0.778882 mae: 0.729751 (2001.8441977453442 steps/sec)\n",
      "Step #11214\tEpoch   3 Batch 1838/3125   Loss: 0.805613 mae: 0.717591 (1963.404860877055 steps/sec)\n",
      "Step #11215\tEpoch   3 Batch 1839/3125   Loss: 0.731489 mae: 0.667897 (1902.0234175894939 steps/sec)\n",
      "Step #11216\tEpoch   3 Batch 1840/3125   Loss: 0.832694 mae: 0.688080 (1821.7728204593627 steps/sec)\n",
      "Step #11217\tEpoch   3 Batch 1841/3125   Loss: 0.824495 mae: 0.693369 (1949.1709420774778 steps/sec)\n",
      "Step #11218\tEpoch   3 Batch 1842/3125   Loss: 0.764373 mae: 0.676668 (2021.5851471977483 steps/sec)\n",
      "Step #11219\tEpoch   3 Batch 1843/3125   Loss: 0.583737 mae: 0.613798 (1786.4217932773395 steps/sec)\n",
      "Step #11220\tEpoch   3 Batch 1844/3125   Loss: 0.825117 mae: 0.722822 (1971.3780785862004 steps/sec)\n",
      "Step #11221\tEpoch   3 Batch 1845/3125   Loss: 0.815942 mae: 0.733851 (1793.2192665178839 steps/sec)\n",
      "Step #11222\tEpoch   3 Batch 1846/3125   Loss: 1.091574 mae: 0.812786 (1766.0524808838886 steps/sec)\n",
      "Step #11223\tEpoch   3 Batch 1847/3125   Loss: 0.815212 mae: 0.718723 (2208.7839403450384 steps/sec)\n",
      "Step #11224\tEpoch   3 Batch 1848/3125   Loss: 0.951419 mae: 0.776433 (2182.101199704496 steps/sec)\n",
      "Step #11225\tEpoch   3 Batch 1849/3125   Loss: 0.756374 mae: 0.680233 (2008.0161625446433 steps/sec)\n",
      "Step #11226\tEpoch   3 Batch 1850/3125   Loss: 0.773631 mae: 0.701766 (2124.515763027798 steps/sec)\n",
      "Step #11227\tEpoch   3 Batch 1851/3125   Loss: 0.839832 mae: 0.722459 (2201.1104463826528 steps/sec)\n",
      "Step #11228\tEpoch   3 Batch 1852/3125   Loss: 0.890656 mae: 0.762152 (1949.460846285417 steps/sec)\n",
      "Step #11229\tEpoch   3 Batch 1853/3125   Loss: 0.794420 mae: 0.716214 (1909.6962191301814 steps/sec)\n",
      "Step #11230\tEpoch   3 Batch 1854/3125   Loss: 0.921780 mae: 0.767571 (2090.837670235888 steps/sec)\n",
      "Step #11231\tEpoch   3 Batch 1855/3125   Loss: 0.762650 mae: 0.681353 (2221.700531813463 steps/sec)\n",
      "Step #11232\tEpoch   3 Batch 1856/3125   Loss: 0.903473 mae: 0.738699 (2153.7968573482594 steps/sec)\n",
      "Step #11233\tEpoch   3 Batch 1857/3125   Loss: 0.856210 mae: 0.709447 (2073.6779654313173 steps/sec)\n",
      "Step #11234\tEpoch   3 Batch 1858/3125   Loss: 0.716730 mae: 0.645398 (2040.0509732584947 steps/sec)\n",
      "Step #11235\tEpoch   3 Batch 1859/3125   Loss: 0.885792 mae: 0.726934 (2077.0050510052492 steps/sec)\n",
      "Step #11236\tEpoch   3 Batch 1860/3125   Loss: 0.870584 mae: 0.742926 (2166.4793388429753 steps/sec)\n",
      "Step #11237\tEpoch   3 Batch 1861/3125   Loss: 0.840843 mae: 0.707955 (1703.0631801201885 steps/sec)\n",
      "Step #11238\tEpoch   3 Batch 1862/3125   Loss: 0.721761 mae: 0.656191 (1728.6546815368004 steps/sec)\n",
      "Step #11239\tEpoch   3 Batch 1863/3125   Loss: 0.765289 mae: 0.677400 (2165.8081173190126 steps/sec)\n",
      "Step #11240\tEpoch   3 Batch 1864/3125   Loss: 0.708799 mae: 0.661891 (2225.0949602122014 steps/sec)\n",
      "Step #11241\tEpoch   3 Batch 1865/3125   Loss: 0.664481 mae: 0.667986 (2042.4749457035168 steps/sec)\n",
      "Step #11242\tEpoch   3 Batch 1866/3125   Loss: 0.819991 mae: 0.731421 (2205.346288935159 steps/sec)\n",
      "Step #11243\tEpoch   3 Batch 1867/3125   Loss: 0.679982 mae: 0.663823 (2079.4145935172974 steps/sec)\n",
      "Step #11244\tEpoch   3 Batch 1868/3125   Loss: 0.895829 mae: 0.761707 (2107.1398428551333 steps/sec)\n",
      "Step #11245\tEpoch   3 Batch 1869/3125   Loss: 0.864044 mae: 0.734396 (2161.700373142021 steps/sec)\n",
      "Step #11246\tEpoch   3 Batch 1870/3125   Loss: 0.866964 mae: 0.734715 (1769.464811549203 steps/sec)\n",
      "Step #11247\tEpoch   3 Batch 1871/3125   Loss: 0.703010 mae: 0.671004 (2015.7364065398554 steps/sec)\n",
      "Step #11248\tEpoch   3 Batch 1872/3125   Loss: 0.828032 mae: 0.728023 (2093.174967561633 steps/sec)\n",
      "Step #11249\tEpoch   3 Batch 1873/3125   Loss: 0.694048 mae: 0.669650 (1820.0968565030985 steps/sec)\n",
      "Step #11250\tEpoch   3 Batch 1874/3125   Loss: 0.883757 mae: 0.744614 (2113.191120605395 steps/sec)\n",
      "Step #11251\tEpoch   3 Batch 1875/3125   Loss: 0.799674 mae: 0.698008 (2242.9433155080214 steps/sec)\n",
      "Step #11252\tEpoch   3 Batch 1876/3125   Loss: 0.855189 mae: 0.734384 (2239.3268625001333 steps/sec)\n",
      "Step #11253\tEpoch   3 Batch 1877/3125   Loss: 0.843596 mae: 0.715545 (2098.36905405135 steps/sec)\n",
      "Step #11254\tEpoch   3 Batch 1878/3125   Loss: 0.790729 mae: 0.718659 (2193.8572265461544 steps/sec)\n",
      "Step #11255\tEpoch   3 Batch 1879/3125   Loss: 0.742448 mae: 0.674260 (1928.3447045625908 steps/sec)\n",
      "Step #11256\tEpoch   3 Batch 1880/3125   Loss: 0.828845 mae: 0.729334 (2089.2546175456773 steps/sec)\n",
      "Step #11257\tEpoch   3 Batch 1881/3125   Loss: 0.745374 mae: 0.681429 (2036.7422255890294 steps/sec)\n",
      "Step #11258\tEpoch   3 Batch 1882/3125   Loss: 0.749993 mae: 0.695386 (2115.066614222464 steps/sec)\n",
      "Step #11259\tEpoch   3 Batch 1883/3125   Loss: 0.828136 mae: 0.720311 (2218.997132548223 steps/sec)\n",
      "Step #11260\tEpoch   3 Batch 1884/3125   Loss: 0.894860 mae: 0.757675 (2372.827046231133 steps/sec)\n",
      "Step #11261\tEpoch   3 Batch 1885/3125   Loss: 0.818579 mae: 0.702439 (2166.1884251081983 steps/sec)\n",
      "Step #11262\tEpoch   3 Batch 1886/3125   Loss: 0.792050 mae: 0.715136 (2166.3003057598544 steps/sec)\n",
      "Step #11263\tEpoch   3 Batch 1887/3125   Loss: 0.729483 mae: 0.656425 (2308.088178646504 steps/sec)\n",
      "Step #11264\tEpoch   3 Batch 1888/3125   Loss: 0.778096 mae: 0.684231 (1850.890958033626 steps/sec)\n",
      "Step #11265\tEpoch   3 Batch 1889/3125   Loss: 0.882775 mae: 0.732374 (1862.26456980988 steps/sec)\n",
      "Step #11266\tEpoch   3 Batch 1890/3125   Loss: 0.709276 mae: 0.645667 (2191.9998327636845 steps/sec)\n",
      "Step #11267\tEpoch   3 Batch 1891/3125   Loss: 0.802019 mae: 0.718745 (2059.888614955456 steps/sec)\n",
      "Step #11268\tEpoch   3 Batch 1892/3125   Loss: 0.781135 mae: 0.679332 (2046.9205692310695 steps/sec)\n",
      "Step #11269\tEpoch   3 Batch 1893/3125   Loss: 0.797137 mae: 0.727321 (1988.6134764550816 steps/sec)\n",
      "Step #11270\tEpoch   3 Batch 1894/3125   Loss: 0.675421 mae: 0.648950 (2134.3103430728993 steps/sec)\n",
      "Step #11271\tEpoch   3 Batch 1895/3125   Loss: 0.743279 mae: 0.694486 (2188.7283960925106 steps/sec)\n",
      "Step #11272\tEpoch   3 Batch 1896/3125   Loss: 0.904965 mae: 0.763659 (2149.2718421726877 steps/sec)\n",
      "Step #11273\tEpoch   3 Batch 1897/3125   Loss: 0.802380 mae: 0.725252 (1986.014621765976 steps/sec)\n",
      "Step #11274\tEpoch   3 Batch 1898/3125   Loss: 0.768135 mae: 0.720273 (1961.2565347099478 steps/sec)\n",
      "Step #11275\tEpoch   3 Batch 1899/3125   Loss: 0.868998 mae: 0.726175 (2162.056949627827 steps/sec)\n",
      "Step #11276\tEpoch   3 Batch 1900/3125   Loss: 0.693818 mae: 0.652792 (2156.099767647482 steps/sec)\n",
      "Step #11277\tEpoch   3 Batch 1901/3125   Loss: 0.862593 mae: 0.738772 (2173.2595494207135 steps/sec)\n",
      "Step #11278\tEpoch   3 Batch 1902/3125   Loss: 0.813607 mae: 0.717104 (1961.2381932105116 steps/sec)\n",
      "Step #11279\tEpoch   3 Batch 1903/3125   Loss: 0.832445 mae: 0.736427 (1930.0661715305962 steps/sec)\n",
      "Step #11280\tEpoch   3 Batch 1904/3125   Loss: 0.844572 mae: 0.728076 (2089.8583942042274 steps/sec)\n",
      "Step #11281\tEpoch   3 Batch 1905/3125   Loss: 0.742290 mae: 0.691097 (2225.9924425763174 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11282\tEpoch   3 Batch 1906/3125   Loss: 0.897892 mae: 0.782322 (1634.0849943118951 steps/sec)\n",
      "Step #11283\tEpoch   3 Batch 1907/3125   Loss: 0.890870 mae: 0.743248 (2135.7449105333376 steps/sec)\n",
      "Step #11284\tEpoch   3 Batch 1908/3125   Loss: 0.815019 mae: 0.727045 (2140.125724548943 steps/sec)\n",
      "Step #11285\tEpoch   3 Batch 1909/3125   Loss: 0.711600 mae: 0.644760 (2069.013417521705 steps/sec)\n",
      "Step #11286\tEpoch   3 Batch 1910/3125   Loss: 0.722649 mae: 0.698637 (2074.867918554722 steps/sec)\n",
      "Step #11287\tEpoch   3 Batch 1911/3125   Loss: 0.798355 mae: 0.716494 (2208.1560022322133 steps/sec)\n",
      "Step #11288\tEpoch   3 Batch 1912/3125   Loss: 0.870629 mae: 0.751678 (2171.9316051658607 steps/sec)\n",
      "Step #11289\tEpoch   3 Batch 1913/3125   Loss: 0.799152 mae: 0.694910 (2204.3726875210227 steps/sec)\n",
      "Step #11290\tEpoch   3 Batch 1914/3125   Loss: 0.849697 mae: 0.712365 (2233.5314290582996 steps/sec)\n",
      "Step #11291\tEpoch   3 Batch 1915/3125   Loss: 0.721268 mae: 0.675951 (1982.9349470499244 steps/sec)\n",
      "Step #11292\tEpoch   3 Batch 1916/3125   Loss: 0.749505 mae: 0.675937 (1850.433677746111 steps/sec)\n",
      "Step #11293\tEpoch   3 Batch 1917/3125   Loss: 0.779733 mae: 0.703794 (2092.026534989276 steps/sec)\n",
      "Step #11294\tEpoch   3 Batch 1918/3125   Loss: 0.706908 mae: 0.664889 (1933.8749389079978 steps/sec)\n",
      "Step #11295\tEpoch   3 Batch 1919/3125   Loss: 0.737312 mae: 0.675473 (2151.3002267061947 steps/sec)\n",
      "Step #11296\tEpoch   3 Batch 1920/3125   Loss: 0.838469 mae: 0.679673 (2099.524462642786 steps/sec)\n",
      "Step #11297\tEpoch   3 Batch 1921/3125   Loss: 0.680302 mae: 0.650855 (2040.3089914968964 steps/sec)\n",
      "Step #11298\tEpoch   3 Batch 1922/3125   Loss: 0.930122 mae: 0.760610 (2031.0415960486175 steps/sec)\n",
      "Step #11299\tEpoch   3 Batch 1923/3125   Loss: 0.822977 mae: 0.714772 (1936.6966800572563 steps/sec)\n",
      "Step #11300\tEpoch   3 Batch 1924/3125   Loss: 0.979757 mae: 0.785641 (1865.1962005051762 steps/sec)\n",
      "Step #11301\tEpoch   3 Batch 1925/3125   Loss: 0.765067 mae: 0.686717 (1905.0470549762 steps/sec)\n",
      "Step #11302\tEpoch   3 Batch 1926/3125   Loss: 0.791668 mae: 0.701587 (1751.6262131867766 steps/sec)\n",
      "Step #11303\tEpoch   3 Batch 1927/3125   Loss: 0.863042 mae: 0.739883 (1897.875113122172 steps/sec)\n",
      "Step #11304\tEpoch   3 Batch 1928/3125   Loss: 0.830318 mae: 0.728516 (1801.0271207983374 steps/sec)\n",
      "Step #11305\tEpoch   3 Batch 1929/3125   Loss: 0.777237 mae: 0.706036 (1792.4376068376068 steps/sec)\n",
      "Step #11306\tEpoch   3 Batch 1930/3125   Loss: 0.913472 mae: 0.751895 (1600.91605151263 steps/sec)\n",
      "Step #11307\tEpoch   3 Batch 1931/3125   Loss: 0.657853 mae: 0.656267 (1557.4722801910123 steps/sec)\n",
      "Step #11308\tEpoch   3 Batch 1932/3125   Loss: 0.865199 mae: 0.741298 (2092.0474043334266 steps/sec)\n",
      "Step #11309\tEpoch   3 Batch 1933/3125   Loss: 0.633147 mae: 0.619093 (2070.9339758655424 steps/sec)\n",
      "Step #11310\tEpoch   3 Batch 1934/3125   Loss: 0.715939 mae: 0.677660 (2350.6456241032997 steps/sec)\n",
      "Step #11311\tEpoch   3 Batch 1935/3125   Loss: 0.731522 mae: 0.669665 (2161.2102724761944 steps/sec)\n",
      "Step #11312\tEpoch   3 Batch 1936/3125   Loss: 0.768150 mae: 0.689192 (2350.408517792099 steps/sec)\n",
      "Step #11313\tEpoch   3 Batch 1937/3125   Loss: 0.973621 mae: 0.754125 (2071.343065405053 steps/sec)\n",
      "Step #11314\tEpoch   3 Batch 1938/3125   Loss: 0.861533 mae: 0.731054 (2220.453799485426 steps/sec)\n",
      "Step #11315\tEpoch   3 Batch 1939/3125   Loss: 0.793249 mae: 0.688823 (2306.7933826117564 steps/sec)\n",
      "Step #11316\tEpoch   3 Batch 1940/3125   Loss: 0.768310 mae: 0.694605 (2138.488992219605 steps/sec)\n",
      "Step #11317\tEpoch   3 Batch 1941/3125   Loss: 0.696276 mae: 0.677004 (2071.588596716518 steps/sec)\n",
      "Step #11318\tEpoch   3 Batch 1942/3125   Loss: 0.681139 mae: 0.662209 (2107.83874242409 steps/sec)\n",
      "Step #11319\tEpoch   3 Batch 1943/3125   Loss: 0.761206 mae: 0.696611 (2102.5344882900226 steps/sec)\n",
      "Step #11320\tEpoch   3 Batch 1944/3125   Loss: 0.838288 mae: 0.734990 (2255.317409960532 steps/sec)\n",
      "Step #11321\tEpoch   3 Batch 1945/3125   Loss: 0.849569 mae: 0.726686 (1942.4548923715313 steps/sec)\n",
      "Step #11322\tEpoch   3 Batch 1946/3125   Loss: 0.842835 mae: 0.751134 (2164.489260906811 steps/sec)\n",
      "Step #11323\tEpoch   3 Batch 1947/3125   Loss: 0.757518 mae: 0.708393 (2164.779718402907 steps/sec)\n",
      "Step #11324\tEpoch   3 Batch 1948/3125   Loss: 0.756238 mae: 0.704088 (2000.7937719432148 steps/sec)\n",
      "Step #11325\tEpoch   3 Batch 1949/3125   Loss: 0.785179 mae: 0.690233 (2042.5744117188717 steps/sec)\n",
      "Step #11326\tEpoch   3 Batch 1950/3125   Loss: 0.776289 mae: 0.698561 (2056.7572868855673 steps/sec)\n",
      "Step #11327\tEpoch   3 Batch 1951/3125   Loss: 0.781885 mae: 0.694465 (2063.0497870205504 steps/sec)\n",
      "Step #11328\tEpoch   3 Batch 1952/3125   Loss: 0.799122 mae: 0.694225 (2061.5085177284745 steps/sec)\n",
      "Step #11329\tEpoch   3 Batch 1953/3125   Loss: 1.080035 mae: 0.816154 (2256.725026633236 steps/sec)\n",
      "Step #11330\tEpoch   3 Batch 1954/3125   Loss: 0.682281 mae: 0.660483 (2326.446574369897 steps/sec)\n",
      "Step #11331\tEpoch   3 Batch 1955/3125   Loss: 0.723057 mae: 0.674511 (2222.1478145695364 steps/sec)\n",
      "Step #11332\tEpoch   3 Batch 1956/3125   Loss: 0.794159 mae: 0.731025 (2234.435731333106 steps/sec)\n",
      "Step #11333\tEpoch   3 Batch 1957/3125   Loss: 0.856111 mae: 0.723017 (2393.2714802515206 steps/sec)\n",
      "Step #11334\tEpoch   3 Batch 1958/3125   Loss: 0.860132 mae: 0.721020 (1896.313443227749 steps/sec)\n",
      "Step #11335\tEpoch   3 Batch 1959/3125   Loss: 0.897267 mae: 0.732324 (2043.2311304669765 steps/sec)\n",
      "Step #11336\tEpoch   3 Batch 1960/3125   Loss: 0.689544 mae: 0.661739 (1932.0042746066256 steps/sec)\n",
      "Step #11337\tEpoch   3 Batch 1961/3125   Loss: 0.956034 mae: 0.765908 (2200.7177786639245 steps/sec)\n",
      "Step #11338\tEpoch   3 Batch 1962/3125   Loss: 0.784463 mae: 0.719387 (2176.1009421823765 steps/sec)\n",
      "Step #11339\tEpoch   3 Batch 1963/3125   Loss: 0.748300 mae: 0.700011 (2106.779984529299 steps/sec)\n",
      "Step #11340\tEpoch   3 Batch 1964/3125   Loss: 0.941601 mae: 0.744121 (2218.7858397342306 steps/sec)\n",
      "Step #11341\tEpoch   3 Batch 1965/3125   Loss: 0.733801 mae: 0.676542 (2108.813739982101 steps/sec)\n",
      "Step #11342\tEpoch   3 Batch 1966/3125   Loss: 0.813910 mae: 0.730359 (2118.9988784366824 steps/sec)\n",
      "Step #11343\tEpoch   3 Batch 1967/3125   Loss: 0.825715 mae: 0.728563 (1837.2862349313148 steps/sec)\n",
      "Step #11344\tEpoch   3 Batch 1968/3125   Loss: 0.807998 mae: 0.703862 (1764.5516579861842 steps/sec)\n",
      "Step #11345\tEpoch   3 Batch 1969/3125   Loss: 0.661818 mae: 0.653695 (2312.6696882478136 steps/sec)\n",
      "Step #11346\tEpoch   3 Batch 1970/3125   Loss: 0.856747 mae: 0.726122 (1906.8658562089126 steps/sec)\n",
      "Step #11347\tEpoch   3 Batch 1971/3125   Loss: 0.743004 mae: 0.676154 (2180.4676696576175 steps/sec)\n",
      "Step #11348\tEpoch   3 Batch 1972/3125   Loss: 0.747896 mae: 0.682094 (2111.403976843695 steps/sec)\n",
      "Step #11349\tEpoch   3 Batch 1973/3125   Loss: 0.787894 mae: 0.727423 (2164.3999050499006 steps/sec)\n",
      "Step #11350\tEpoch   3 Batch 1974/3125   Loss: 0.801802 mae: 0.702168 (2155.2577489106307 steps/sec)\n",
      "Step #11351\tEpoch   3 Batch 1975/3125   Loss: 0.870111 mae: 0.719265 (2314.813956312019 steps/sec)\n",
      "Step #11352\tEpoch   3 Batch 1976/3125   Loss: 0.802291 mae: 0.708402 (1925.2290461764435 steps/sec)\n",
      "Step #11353\tEpoch   3 Batch 1977/3125   Loss: 0.883260 mae: 0.754178 (1968.5837925111 steps/sec)\n",
      "Step #11354\tEpoch   3 Batch 1978/3125   Loss: 0.914656 mae: 0.749125 (2213.3763944738203 steps/sec)\n",
      "Step #11355\tEpoch   3 Batch 1979/3125   Loss: 0.750237 mae: 0.696455 (2281.0254625349417 steps/sec)\n",
      "Step #11356\tEpoch   3 Batch 1980/3125   Loss: 0.776380 mae: 0.706921 (2105.172708017547 steps/sec)\n",
      "Step #11357\tEpoch   3 Batch 1981/3125   Loss: 0.877979 mae: 0.736627 (2172.4040772354356 steps/sec)\n",
      "Step #11358\tEpoch   3 Batch 1982/3125   Loss: 0.914034 mae: 0.771230 (2185.034070307779 steps/sec)\n",
      "Step #11359\tEpoch   3 Batch 1983/3125   Loss: 0.775714 mae: 0.685731 (2134.375508874776 steps/sec)\n",
      "Step #11360\tEpoch   3 Batch 1984/3125   Loss: 0.706996 mae: 0.690110 (2100.134190550582 steps/sec)\n",
      "Step #11361\tEpoch   3 Batch 1985/3125   Loss: 0.814464 mae: 0.725914 (1901.350885782154 steps/sec)\n",
      "Step #11362\tEpoch   3 Batch 1986/3125   Loss: 0.833461 mae: 0.716395 (2147.203309135959 steps/sec)\n",
      "Step #11363\tEpoch   3 Batch 1987/3125   Loss: 0.779547 mae: 0.739573 (2060.0100193511 steps/sec)\n",
      "Step #11364\tEpoch   3 Batch 1988/3125   Loss: 0.706252 mae: 0.677728 (2087.6116149198165 steps/sec)\n",
      "Step #11365\tEpoch   3 Batch 1989/3125   Loss: 0.786549 mae: 0.684195 (2117.009549575014 steps/sec)\n",
      "Step #11366\tEpoch   3 Batch 1990/3125   Loss: 0.741190 mae: 0.669322 (2209.435512758381 steps/sec)\n",
      "Step #11367\tEpoch   3 Batch 1991/3125   Loss: 0.716149 mae: 0.673340 (2313.435042084478 steps/sec)\n",
      "Step #11368\tEpoch   3 Batch 1992/3125   Loss: 0.896686 mae: 0.763893 (2149.051596044474 steps/sec)\n",
      "Step #11369\tEpoch   3 Batch 1993/3125   Loss: 0.797914 mae: 0.701278 (2379.287967143927 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11370\tEpoch   3 Batch 1994/3125   Loss: 0.828918 mae: 0.721576 (1989.1604776674349 steps/sec)\n",
      "Step #11371\tEpoch   3 Batch 1995/3125   Loss: 0.886313 mae: 0.750943 (1954.2931693225235 steps/sec)\n",
      "Step #11372\tEpoch   3 Batch 1996/3125   Loss: 0.813601 mae: 0.700086 (2204.3726875210227 steps/sec)\n",
      "Step #11373\tEpoch   3 Batch 1997/3125   Loss: 0.902327 mae: 0.723148 (2221.62992467981 steps/sec)\n",
      "Step #11374\tEpoch   3 Batch 1998/3125   Loss: 0.897671 mae: 0.727643 (2285.425339465138 steps/sec)\n",
      "Step #11375\tEpoch   3 Batch 1999/3125   Loss: 0.876213 mae: 0.746498 (2162.257575601357 steps/sec)\n",
      "Step #11376\tEpoch   3 Batch 2000/3125   Loss: 0.933803 mae: 0.776159 (2127.123165400493 steps/sec)\n",
      "Step #11377\tEpoch   3 Batch 2001/3125   Loss: 0.727813 mae: 0.668175 (2176.6655941544627 steps/sec)\n",
      "Step #11378\tEpoch   3 Batch 2002/3125   Loss: 0.760033 mae: 0.707185 (2062.441116017427 steps/sec)\n",
      "Step #11379\tEpoch   3 Batch 2003/3125   Loss: 0.698658 mae: 0.663844 (2068.6664628071458 steps/sec)\n",
      "Step #11380\tEpoch   3 Batch 2004/3125   Loss: 0.881707 mae: 0.752534 (1850.0092626081741 steps/sec)\n",
      "Step #11381\tEpoch   3 Batch 2005/3125   Loss: 0.799431 mae: 0.714280 (2098.36905405135 steps/sec)\n",
      "Step #11382\tEpoch   3 Batch 2006/3125   Loss: 0.883922 mae: 0.755354 (2353.7059483726152 steps/sec)\n",
      "Step #11383\tEpoch   3 Batch 2007/3125   Loss: 0.739520 mae: 0.676775 (2177.1175267578146 steps/sec)\n",
      "Step #11384\tEpoch   3 Batch 2008/3125   Loss: 0.830156 mae: 0.738160 (2013.9554983626394 steps/sec)\n",
      "Step #11385\tEpoch   3 Batch 2009/3125   Loss: 0.784818 mae: 0.710184 (2012.3901278163744 steps/sec)\n",
      "Step #11386\tEpoch   3 Batch 2010/3125   Loss: 0.722706 mae: 0.640420 (2059.3020287122686 steps/sec)\n",
      "Step #11387\tEpoch   3 Batch 2011/3125   Loss: 0.729652 mae: 0.665862 (2225.6144670373988 steps/sec)\n",
      "Step #11388\tEpoch   3 Batch 2012/3125   Loss: 0.810767 mae: 0.731030 (2037.1577055709358 steps/sec)\n",
      "Step #11389\tEpoch   3 Batch 2013/3125   Loss: 0.721098 mae: 0.685877 (1995.9379847912364 steps/sec)\n",
      "Step #11390\tEpoch   3 Batch 2014/3125   Loss: 0.850364 mae: 0.702306 (1804.2034808193605 steps/sec)\n",
      "Step #11391\tEpoch   3 Batch 2015/3125   Loss: 0.921800 mae: 0.759695 (2059.6458490881055 steps/sec)\n",
      "Step #11392\tEpoch   3 Batch 2016/3125   Loss: 0.742764 mae: 0.695871 (2069.3809082117978 steps/sec)\n",
      "Step #11393\tEpoch   3 Batch 2017/3125   Loss: 0.773243 mae: 0.712589 (1908.3233996087174 steps/sec)\n",
      "Step #11394\tEpoch   3 Batch 2018/3125   Loss: 0.724842 mae: 0.679557 (1971.1371988758658 steps/sec)\n",
      "Step #11395\tEpoch   3 Batch 2019/3125   Loss: 0.880457 mae: 0.712663 (1992.013526092821 steps/sec)\n",
      "Step #11396\tEpoch   3 Batch 2020/3125   Loss: 0.813558 mae: 0.715525 (1864.549455434541 steps/sec)\n",
      "Step #11397\tEpoch   3 Batch 2021/3125   Loss: 0.724634 mae: 0.678460 (1836.8034753972008 steps/sec)\n",
      "Step #11398\tEpoch   3 Batch 2022/3125   Loss: 0.875177 mae: 0.766293 (2057.2011535971433 steps/sec)\n",
      "Step #11399\tEpoch   3 Batch 2023/3125   Loss: 1.054857 mae: 0.809207 (2147.7750581198857 steps/sec)\n",
      "Step #11400\tEpoch   3 Batch 2024/3125   Loss: 0.856547 mae: 0.711065 (2099.587521524969 steps/sec)\n",
      "Step #11401\tEpoch   3 Batch 2025/3125   Loss: 0.816404 mae: 0.725765 (2045.8223180403672 steps/sec)\n",
      "Step #11402\tEpoch   3 Batch 2026/3125   Loss: 0.862918 mae: 0.733056 (1946.6740926390048 steps/sec)\n",
      "Step #11403\tEpoch   3 Batch 2027/3125   Loss: 0.799776 mae: 0.695809 (2215.317009274713 steps/sec)\n",
      "Step #11404\tEpoch   3 Batch 2028/3125   Loss: 0.773269 mae: 0.714156 (2088.734400366523 steps/sec)\n",
      "Step #11405\tEpoch   3 Batch 2029/3125   Loss: 0.972229 mae: 0.789627 (1850.5479766337821 steps/sec)\n",
      "Step #11406\tEpoch   3 Batch 2030/3125   Loss: 0.770709 mae: 0.691495 (1904.1821780739826 steps/sec)\n",
      "Step #11407\tEpoch   3 Batch 2031/3125   Loss: 0.887716 mae: 0.737044 (2309.994933139471 steps/sec)\n",
      "Step #11408\tEpoch   3 Batch 2032/3125   Loss: 0.763104 mae: 0.712134 (2064.2275702544416 steps/sec)\n",
      "Step #11409\tEpoch   3 Batch 2033/3125   Loss: 0.882616 mae: 0.748902 (2253.475602548811 steps/sec)\n",
      "Step #11410\tEpoch   3 Batch 2034/3125   Loss: 0.823096 mae: 0.701599 (2190.9921957437027 steps/sec)\n",
      "Step #11411\tEpoch   3 Batch 2035/3125   Loss: 0.972854 mae: 0.778705 (2155.545733932224 steps/sec)\n",
      "Step #11412\tEpoch   3 Batch 2036/3125   Loss: 0.676310 mae: 0.655130 (1972.2123477688438 steps/sec)\n",
      "Step #11413\tEpoch   3 Batch 2037/3125   Loss: 0.829258 mae: 0.708322 (1853.7049313639698 steps/sec)\n",
      "Step #11414\tEpoch   3 Batch 2038/3125   Loss: 0.890581 mae: 0.750145 (1581.6222331158792 steps/sec)\n",
      "Step #11415\tEpoch   3 Batch 2039/3125   Loss: 0.920299 mae: 0.758819 (1242.977714556662 steps/sec)\n",
      "Step #11416\tEpoch   3 Batch 2040/3125   Loss: 0.780813 mae: 0.724599 (1567.7414049592956 steps/sec)\n",
      "Step #11417\tEpoch   3 Batch 2041/3125   Loss: 0.785265 mae: 0.700722 (1610.5055407512075 steps/sec)\n",
      "Step #11418\tEpoch   3 Batch 2042/3125   Loss: 0.746996 mae: 0.686154 (1703.3398310591292 steps/sec)\n",
      "Step #11419\tEpoch   3 Batch 2043/3125   Loss: 0.782390 mae: 0.688407 (1369.7922926192032 steps/sec)\n",
      "Step #11420\tEpoch   3 Batch 2044/3125   Loss: 0.955141 mae: 0.773019 (1463.6126348701198 steps/sec)\n",
      "Step #11421\tEpoch   3 Batch 2045/3125   Loss: 0.645481 mae: 0.652239 (1503.9924267959466 steps/sec)\n",
      "Step #11422\tEpoch   3 Batch 2046/3125   Loss: 0.738407 mae: 0.681563 (1447.3897799740496 steps/sec)\n",
      "Step #11423\tEpoch   3 Batch 2047/3125   Loss: 0.904378 mae: 0.746807 (1560.450615354852 steps/sec)\n",
      "Step #11424\tEpoch   3 Batch 2048/3125   Loss: 0.870800 mae: 0.747047 (1592.3463576862919 steps/sec)\n",
      "Step #11425\tEpoch   3 Batch 2049/3125   Loss: 0.771849 mae: 0.675392 (1453.065975638485 steps/sec)\n",
      "Step #11426\tEpoch   3 Batch 2050/3125   Loss: 0.855326 mae: 0.733996 (1619.9602956966405 steps/sec)\n",
      "Step #11427\tEpoch   3 Batch 2051/3125   Loss: 0.790509 mae: 0.723389 (1588.1018371272357 steps/sec)\n",
      "Step #11428\tEpoch   3 Batch 2052/3125   Loss: 0.827343 mae: 0.718106 (1695.4355102106813 steps/sec)\n",
      "Step #11429\tEpoch   3 Batch 2053/3125   Loss: 0.900913 mae: 0.734056 (1646.840055283327 steps/sec)\n",
      "Step #11430\tEpoch   3 Batch 2054/3125   Loss: 0.662062 mae: 0.662142 (1605.598131914405 steps/sec)\n",
      "Step #11431\tEpoch   3 Batch 2055/3125   Loss: 0.750145 mae: 0.676796 (1252.1955122463846 steps/sec)\n",
      "Step #11432\tEpoch   3 Batch 2056/3125   Loss: 0.813087 mae: 0.695557 (1502.8499767100218 steps/sec)\n",
      "Step #11433\tEpoch   3 Batch 2057/3125   Loss: 0.738986 mae: 0.692369 (1355.9669212018543 steps/sec)\n",
      "Step #11434\tEpoch   3 Batch 2058/3125   Loss: 0.747470 mae: 0.674834 (1627.6044051564234 steps/sec)\n",
      "Step #11435\tEpoch   3 Batch 2059/3125   Loss: 0.890794 mae: 0.735010 (1628.6661748145848 steps/sec)\n",
      "Step #11436\tEpoch   3 Batch 2060/3125   Loss: 0.741442 mae: 0.676174 (1519.7083994579593 steps/sec)\n",
      "Step #11437\tEpoch   3 Batch 2061/3125   Loss: 0.705449 mae: 0.673278 (1654.322857503471 steps/sec)\n",
      "Step #11438\tEpoch   3 Batch 2062/3125   Loss: 0.774522 mae: 0.706828 (1534.4079019571977 steps/sec)\n",
      "Step #11439\tEpoch   3 Batch 2063/3125   Loss: 0.766237 mae: 0.695431 (1419.2865505782988 steps/sec)\n",
      "Step #11440\tEpoch   3 Batch 2064/3125   Loss: 0.680987 mae: 0.644599 (1633.3468332346802 steps/sec)\n",
      "Step #11441\tEpoch   3 Batch 2065/3125   Loss: 0.623876 mae: 0.632450 (1632.9780027253262 steps/sec)\n",
      "Step #11442\tEpoch   3 Batch 2066/3125   Loss: 0.721709 mae: 0.665081 (1689.1946097896916 steps/sec)\n",
      "Step #11443\tEpoch   3 Batch 2067/3125   Loss: 0.832415 mae: 0.705823 (1799.9914169720794 steps/sec)\n",
      "Step #11444\tEpoch   3 Batch 2068/3125   Loss: 0.730268 mae: 0.669095 (1966.6269681255098 steps/sec)\n",
      "Step #11445\tEpoch   3 Batch 2069/3125   Loss: 0.863166 mae: 0.719129 (1769.9128189114601 steps/sec)\n",
      "Step #11446\tEpoch   3 Batch 2070/3125   Loss: 0.794938 mae: 0.706420 (1576.6400529267596 steps/sec)\n",
      "Step #11447\tEpoch   3 Batch 2071/3125   Loss: 0.916180 mae: 0.747266 (2059.888614955456 steps/sec)\n",
      "Step #11448\tEpoch   3 Batch 2072/3125   Loss: 0.742948 mae: 0.679648 (1858.5347264686854 steps/sec)\n",
      "Step #11449\tEpoch   3 Batch 2073/3125   Loss: 0.697977 mae: 0.654989 (1779.947547550946 steps/sec)\n",
      "Step #11450\tEpoch   3 Batch 2074/3125   Loss: 0.869733 mae: 0.710607 (1711.0389504430266 steps/sec)\n",
      "Step #11451\tEpoch   3 Batch 2075/3125   Loss: 0.889834 mae: 0.761025 (1949.8414764539073 steps/sec)\n",
      "Step #11452\tEpoch   3 Batch 2076/3125   Loss: 0.916189 mae: 0.749676 (1766.3499764166836 steps/sec)\n",
      "Step #11453\tEpoch   3 Batch 2077/3125   Loss: 0.775835 mae: 0.703655 (1540.879200005878 steps/sec)\n",
      "Step #11454\tEpoch   3 Batch 2078/3125   Loss: 0.834574 mae: 0.713527 (1664.8423793533227 steps/sec)\n",
      "Step #11455\tEpoch   3 Batch 2079/3125   Loss: 0.823144 mae: 0.701384 (1852.755077700524 steps/sec)\n",
      "Step #11456\tEpoch   3 Batch 2080/3125   Loss: 0.935899 mae: 0.764798 (1928.5752384105351 steps/sec)\n",
      "Step #11457\tEpoch   3 Batch 2081/3125   Loss: 0.802524 mae: 0.726684 (1831.9737934046734 steps/sec)\n",
      "Step #11458\tEpoch   3 Batch 2082/3125   Loss: 0.839440 mae: 0.720461 (2055.084421884034 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11459\tEpoch   3 Batch 2083/3125   Loss: 0.744165 mae: 0.697787 (1932.680858907013 steps/sec)\n",
      "Step #11460\tEpoch   3 Batch 2084/3125   Loss: 0.877180 mae: 0.755792 (1479.336639320556 steps/sec)\n",
      "Step #11461\tEpoch   3 Batch 2085/3125   Loss: 0.848896 mae: 0.730666 (1839.6554295288472 steps/sec)\n",
      "Step #11462\tEpoch   3 Batch 2086/3125   Loss: 0.906510 mae: 0.739318 (1775.3818021739867 steps/sec)\n",
      "Step #11463\tEpoch   3 Batch 2087/3125   Loss: 0.822883 mae: 0.729768 (1993.2820712663124 steps/sec)\n",
      "Step #11464\tEpoch   3 Batch 2088/3125   Loss: 0.799612 mae: 0.712995 (1910.9491179472227 steps/sec)\n",
      "Step #11465\tEpoch   3 Batch 2089/3125   Loss: 0.804453 mae: 0.695751 (2009.1704270016 steps/sec)\n",
      "Step #11466\tEpoch   3 Batch 2090/3125   Loss: 0.708804 mae: 0.651691 (1772.9203300419315 steps/sec)\n",
      "Step #11467\tEpoch   3 Batch 2091/3125   Loss: 0.681658 mae: 0.660110 (1403.8195583342817 steps/sec)\n",
      "Step #11468\tEpoch   3 Batch 2092/3125   Loss: 0.961469 mae: 0.777455 (1769.9128189114601 steps/sec)\n",
      "Step #11469\tEpoch   3 Batch 2093/3125   Loss: 0.816471 mae: 0.712371 (1822.4852482380443 steps/sec)\n",
      "Step #11470\tEpoch   3 Batch 2094/3125   Loss: 0.797875 mae: 0.725754 (1997.7823080000762 steps/sec)\n",
      "Step #11471\tEpoch   3 Batch 2095/3125   Loss: 0.819656 mae: 0.698717 (1905.1855081943384 steps/sec)\n",
      "Step #11472\tEpoch   3 Batch 2096/3125   Loss: 0.607984 mae: 0.595927 (1915.1373465809468 steps/sec)\n",
      "Step #11473\tEpoch   3 Batch 2097/3125   Loss: 0.708197 mae: 0.656631 (1809.8867716100524 steps/sec)\n",
      "Step #11474\tEpoch   3 Batch 2098/3125   Loss: 0.762047 mae: 0.704609 (1810.7306290904696 steps/sec)\n",
      "Step #11475\tEpoch   3 Batch 2099/3125   Loss: 0.856285 mae: 0.712533 (1545.8426701261942 steps/sec)\n",
      "Step #11476\tEpoch   3 Batch 2100/3125   Loss: 0.785723 mae: 0.703289 (1590.0765789673212 steps/sec)\n",
      "Step #11477\tEpoch   3 Batch 2101/3125   Loss: 0.747106 mae: 0.670293 (1724.7734188666832 steps/sec)\n",
      "Step #11478\tEpoch   3 Batch 2102/3125   Loss: 0.868331 mae: 0.725910 (1837.0770079802378 steps/sec)\n",
      "Step #11479\tEpoch   3 Batch 2103/3125   Loss: 0.942278 mae: 0.779493 (1687.3054952128089 steps/sec)\n",
      "Step #11480\tEpoch   3 Batch 2104/3125   Loss: 0.764060 mae: 0.659290 (1737.1458864848746 steps/sec)\n",
      "Step #11481\tEpoch   3 Batch 2105/3125   Loss: 0.773735 mae: 0.706664 (1707.361393796304 steps/sec)\n",
      "Step #11482\tEpoch   3 Batch 2106/3125   Loss: 0.749382 mae: 0.684855 (1391.7087511364466 steps/sec)\n",
      "Step #11483\tEpoch   3 Batch 2107/3125   Loss: 0.880641 mae: 0.743917 (1695.6548456475687 steps/sec)\n",
      "Step #11484\tEpoch   3 Batch 2108/3125   Loss: 0.853670 mae: 0.718867 (1999.7635167350052 steps/sec)\n",
      "Step #11485\tEpoch   3 Batch 2109/3125   Loss: 0.778067 mae: 0.719592 (1934.017614238945 steps/sec)\n",
      "Step #11486\tEpoch   3 Batch 2110/3125   Loss: 0.822457 mae: 0.713813 (1784.7038899810225 steps/sec)\n",
      "Step #11487\tEpoch   3 Batch 2111/3125   Loss: 0.802694 mae: 0.696132 (1606.840645447998 steps/sec)\n",
      "Step #11488\tEpoch   3 Batch 2112/3125   Loss: 0.854305 mae: 0.713323 (1968.4174957762343 steps/sec)\n",
      "Step #11489\tEpoch   3 Batch 2113/3125   Loss: 0.870337 mae: 0.726141 (1854.590153786291 steps/sec)\n",
      "Step #11490\tEpoch   3 Batch 2114/3125   Loss: 0.871454 mae: 0.728492 (1543.2604072381541 steps/sec)\n",
      "Step #11491\tEpoch   3 Batch 2115/3125   Loss: 0.973773 mae: 0.778342 (1851.1033435723616 steps/sec)\n",
      "Step #11492\tEpoch   3 Batch 2116/3125   Loss: 0.804448 mae: 0.698606 (1847.6784550051982 steps/sec)\n",
      "Step #11493\tEpoch   3 Batch 2117/3125   Loss: 0.718300 mae: 0.666571 (1866.8565731377296 steps/sec)\n",
      "Step #11494\tEpoch   3 Batch 2118/3125   Loss: 0.794354 mae: 0.702328 (1685.1903636918823 steps/sec)\n",
      "Step #11495\tEpoch   3 Batch 2119/3125   Loss: 0.836669 mae: 0.706304 (1915.5571793934964 steps/sec)\n",
      "Step #11496\tEpoch   3 Batch 2120/3125   Loss: 0.753077 mae: 0.687236 (1744.2834567079765 steps/sec)\n",
      "Step #11497\tEpoch   3 Batch 2121/3125   Loss: 0.797802 mae: 0.698323 (1831.9097825802112 steps/sec)\n",
      "Step #11498\tEpoch   3 Batch 2122/3125   Loss: 0.843531 mae: 0.719800 (1950.839069767442 steps/sec)\n",
      "Step #11499\tEpoch   3 Batch 2123/3125   Loss: 0.707533 mae: 0.682768 (1684.2429888528382 steps/sec)\n",
      "Step #11500\tEpoch   3 Batch 2124/3125   Loss: 0.662001 mae: 0.618154 (1609.7885242755708 steps/sec)\n",
      "Step #11501\tEpoch   3 Batch 2125/3125   Loss: 0.904294 mae: 0.750912 (1538.844006134384 steps/sec)\n",
      "Step #11502\tEpoch   3 Batch 2126/3125   Loss: 0.869579 mae: 0.734644 (1698.9517004488082 steps/sec)\n",
      "Step #11503\tEpoch   3 Batch 2127/3125   Loss: 0.815951 mae: 0.705040 (1479.8585874267005 steps/sec)\n",
      "Step #11504\tEpoch   3 Batch 2128/3125   Loss: 0.728842 mae: 0.661785 (1637.4661325962536 steps/sec)\n",
      "Step #11505\tEpoch   3 Batch 2129/3125   Loss: 0.709451 mae: 0.690172 (1939.7957673523754 steps/sec)\n",
      "Step #11506\tEpoch   3 Batch 2130/3125   Loss: 0.805395 mae: 0.708815 (1824.0069580343552 steps/sec)\n",
      "Step #11507\tEpoch   3 Batch 2131/3125   Loss: 0.848106 mae: 0.720148 (2070.034547428684 steps/sec)\n",
      "Step #11508\tEpoch   3 Batch 2132/3125   Loss: 0.750977 mae: 0.691280 (2023.535768733476 steps/sec)\n",
      "Step #11509\tEpoch   3 Batch 2133/3125   Loss: 0.744644 mae: 0.682104 (1924.275122953828 steps/sec)\n",
      "Step #11510\tEpoch   3 Batch 2134/3125   Loss: 0.715238 mae: 0.660918 (1887.1499532071125 steps/sec)\n",
      "Step #11511\tEpoch   3 Batch 2135/3125   Loss: 0.782642 mae: 0.686820 (1919.6075021281658 steps/sec)\n",
      "Step #11512\tEpoch   3 Batch 2136/3125   Loss: 0.930277 mae: 0.755023 (1800.8879271109738 steps/sec)\n",
      "Step #11513\tEpoch   3 Batch 2137/3125   Loss: 0.809384 mae: 0.709114 (1895.5250051971764 steps/sec)\n",
      "Step #11514\tEpoch   3 Batch 2138/3125   Loss: 0.706821 mae: 0.676755 (1978.4639477730923 steps/sec)\n",
      "Step #11515\tEpoch   3 Batch 2139/3125   Loss: 0.863711 mae: 0.731747 (2063.313656040929 steps/sec)\n",
      "Step #11516\tEpoch   3 Batch 2140/3125   Loss: 0.795719 mae: 0.696140 (1987.0307555286047 steps/sec)\n",
      "Step #11517\tEpoch   3 Batch 2141/3125   Loss: 0.841958 mae: 0.736806 (2116.026960487549 steps/sec)\n",
      "Step #11518\tEpoch   3 Batch 2142/3125   Loss: 0.654893 mae: 0.650071 (2124.8601767042232 steps/sec)\n",
      "Step #11519\tEpoch   3 Batch 2143/3125   Loss: 0.733757 mae: 0.675720 (2001.194713488239 steps/sec)\n",
      "Step #11520\tEpoch   3 Batch 2144/3125   Loss: 0.668479 mae: 0.642955 (1870.1361702886595 steps/sec)\n",
      "Step #11521\tEpoch   3 Batch 2145/3125   Loss: 0.912264 mae: 0.754973 (1933.6431363871063 steps/sec)\n",
      "Step #11522\tEpoch   3 Batch 2146/3125   Loss: 0.632924 mae: 0.653869 (1982.335148215366 steps/sec)\n",
      "Step #11523\tEpoch   3 Batch 2147/3125   Loss: 0.776222 mae: 0.704688 (2031.6121907271424 steps/sec)\n",
      "Step #11524\tEpoch   3 Batch 2148/3125   Loss: 0.750714 mae: 0.679779 (2125.592426668829 steps/sec)\n",
      "Step #11525\tEpoch   3 Batch 2149/3125   Loss: 0.735214 mae: 0.678568 (2095.8944633220067 steps/sec)\n",
      "Step #11526\tEpoch   3 Batch 2150/3125   Loss: 0.821910 mae: 0.707937 (2199.079326797043 steps/sec)\n",
      "Step #11527\tEpoch   3 Batch 2151/3125   Loss: 0.838350 mae: 0.725497 (2106.5895210543235 steps/sec)\n",
      "Step #11528\tEpoch   3 Batch 2152/3125   Loss: 0.931307 mae: 0.751478 (1796.7528851343825 steps/sec)\n",
      "Step #11529\tEpoch   3 Batch 2153/3125   Loss: 0.915408 mae: 0.786232 (2041.8187128809268 steps/sec)\n",
      "Step #11530\tEpoch   3 Batch 2154/3125   Loss: 0.761737 mae: 0.692631 (2182.5099647202073 steps/sec)\n",
      "Step #11531\tEpoch   3 Batch 2155/3125   Loss: 0.812756 mae: 0.714302 (2110.8726723704076 steps/sec)\n",
      "Step #11532\tEpoch   3 Batch 2156/3125   Loss: 0.846862 mae: 0.710296 (2188.2716307024502 steps/sec)\n",
      "Step #11533\tEpoch   3 Batch 2157/3125   Loss: 0.765802 mae: 0.699127 (1999.4393966840505 steps/sec)\n",
      "Step #11534\tEpoch   3 Batch 2158/3125   Loss: 0.712590 mae: 0.656283 (2146.2144626153877 steps/sec)\n",
      "Step #11535\tEpoch   3 Batch 2159/3125   Loss: 0.755066 mae: 0.695184 (2106.779984529299 steps/sec)\n",
      "Step #11536\tEpoch   3 Batch 2160/3125   Loss: 0.760612 mae: 0.693335 (1601.4784155905645 steps/sec)\n",
      "Step #11537\tEpoch   3 Batch 2161/3125   Loss: 0.870567 mae: 0.752618 (2005.1938117912532 steps/sec)\n",
      "Step #11538\tEpoch   3 Batch 2162/3125   Loss: 0.879115 mae: 0.725488 (2028.2326544033733 steps/sec)\n",
      "Step #11539\tEpoch   3 Batch 2163/3125   Loss: 0.938822 mae: 0.760108 (2046.8406566593142 steps/sec)\n",
      "Step #11540\tEpoch   3 Batch 2164/3125   Loss: 0.720227 mae: 0.676110 (2162.547434416763 steps/sec)\n",
      "Step #11541\tEpoch   3 Batch 2165/3125   Loss: 0.754452 mae: 0.695754 (2001.6149198743951 steps/sec)\n",
      "Step #11542\tEpoch   3 Batch 2166/3125   Loss: 0.726488 mae: 0.687219 (2082.1191000972976 steps/sec)\n",
      "Step #11543\tEpoch   3 Batch 2167/3125   Loss: 0.806962 mae: 0.744113 (1916.5900512698659 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11544\tEpoch   3 Batch 2168/3125   Loss: 0.862060 mae: 0.742383 (2038.405163196672 steps/sec)\n",
      "Step #11545\tEpoch   3 Batch 2169/3125   Loss: 0.671225 mae: 0.636329 (1869.3693452778891 steps/sec)\n",
      "Step #11546\tEpoch   3 Batch 2170/3125   Loss: 0.787436 mae: 0.698216 (2145.2045826513913 steps/sec)\n",
      "Step #11547\tEpoch   3 Batch 2171/3125   Loss: 0.808118 mae: 0.709131 (2214.825688848522 steps/sec)\n",
      "Step #11548\tEpoch   3 Batch 2172/3125   Loss: 0.789877 mae: 0.688320 (1977.3819738442535 steps/sec)\n",
      "Step #11549\tEpoch   3 Batch 2173/3125   Loss: 0.772526 mae: 0.675534 (2261.616771633164 steps/sec)\n",
      "Step #11550\tEpoch   3 Batch 2174/3125   Loss: 0.710062 mae: 0.647078 (2068.686868686869 steps/sec)\n",
      "Step #11551\tEpoch   3 Batch 2175/3125   Loss: 0.789192 mae: 0.699344 (2179.7877537444524 steps/sec)\n",
      "Step #11552\tEpoch   3 Batch 2176/3125   Loss: 0.746293 mae: 0.700091 (2122.430143003168 steps/sec)\n",
      "Step #11553\tEpoch   3 Batch 2177/3125   Loss: 0.841578 mae: 0.740100 (1703.630411294974 steps/sec)\n",
      "Step #11554\tEpoch   3 Batch 2178/3125   Loss: 0.741717 mae: 0.691798 (1804.2655700188416 steps/sec)\n",
      "Step #11555\tEpoch   3 Batch 2179/3125   Loss: 0.840680 mae: 0.719236 (2012.815049428928 steps/sec)\n",
      "Step #11556\tEpoch   3 Batch 2180/3125   Loss: 0.843165 mae: 0.711735 (2100.5969790456347 steps/sec)\n",
      "Step #11557\tEpoch   3 Batch 2181/3125   Loss: 0.925488 mae: 0.752866 (2203.700940471812 steps/sec)\n",
      "Step #11558\tEpoch   3 Batch 2182/3125   Loss: 0.682964 mae: 0.655292 (2051.2651975312265 steps/sec)\n",
      "Step #11559\tEpoch   3 Batch 2183/3125   Loss: 0.701391 mae: 0.640894 (1933.6253077256424 steps/sec)\n",
      "Step #11560\tEpoch   3 Batch 2184/3125   Loss: 0.859805 mae: 0.728043 (2107.41511159346 steps/sec)\n",
      "Step #11561\tEpoch   3 Batch 2185/3125   Loss: 0.858247 mae: 0.732001 (1906.0859448847525 steps/sec)\n",
      "Step #11562\tEpoch   3 Batch 2186/3125   Loss: 0.826590 mae: 0.712897 (1793.9095326079519 steps/sec)\n",
      "Step #11563\tEpoch   3 Batch 2187/3125   Loss: 0.838256 mae: 0.711304 (1779.9324404610345 steps/sec)\n",
      "Step #11564\tEpoch   3 Batch 2188/3125   Loss: 0.766589 mae: 0.689547 (2015.0778780279227 steps/sec)\n",
      "Step #11565\tEpoch   3 Batch 2189/3125   Loss: 0.890970 mae: 0.710002 (1979.1734694840554 steps/sec)\n",
      "Step #11566\tEpoch   3 Batch 2190/3125   Loss: 0.831858 mae: 0.696228 (1950.458050055338 steps/sec)\n",
      "Step #11567\tEpoch   3 Batch 2191/3125   Loss: 0.857144 mae: 0.721492 (1983.1787190180337 steps/sec)\n",
      "Step #11568\tEpoch   3 Batch 2192/3125   Loss: 0.813082 mae: 0.706343 (1988.9340958450698 steps/sec)\n",
      "Step #11569\tEpoch   3 Batch 2193/3125   Loss: 0.864917 mae: 0.748540 (1863.0421263969583 steps/sec)\n",
      "Step #11570\tEpoch   3 Batch 2194/3125   Loss: 0.782771 mae: 0.694996 (1872.5741787433144 steps/sec)\n",
      "Step #11571\tEpoch   3 Batch 2195/3125   Loss: 0.858877 mae: 0.715263 (2091.254661853573 steps/sec)\n",
      "Step #11572\tEpoch   3 Batch 2196/3125   Loss: 0.710142 mae: 0.666490 (2229.2341217114003 steps/sec)\n",
      "Step #11573\tEpoch   3 Batch 2197/3125   Loss: 0.772318 mae: 0.672639 (1976.2825587093369 steps/sec)\n",
      "Step #11574\tEpoch   3 Batch 2198/3125   Loss: 0.806151 mae: 0.705502 (2128.3104652106845 steps/sec)\n",
      "Step #11575\tEpoch   3 Batch 2199/3125   Loss: 0.739097 mae: 0.670781 (1827.1694430891475 steps/sec)\n",
      "Step #11576\tEpoch   3 Batch 2200/3125   Loss: 0.760067 mae: 0.679603 (2270.4804798302407 steps/sec)\n",
      "Step #11577\tEpoch   3 Batch 2201/3125   Loss: 0.910000 mae: 0.764612 (2256.2151694459385 steps/sec)\n",
      "Step #11578\tEpoch   3 Batch 2202/3125   Loss: 0.862206 mae: 0.731896 (1874.6330562259766 steps/sec)\n",
      "Step #11579\tEpoch   3 Batch 2203/3125   Loss: 0.812631 mae: 0.729649 (2049.1005911378184 steps/sec)\n",
      "Step #11580\tEpoch   3 Batch 2204/3125   Loss: 0.795281 mae: 0.712148 (2105.891449515489 steps/sec)\n",
      "Step #11581\tEpoch   3 Batch 2205/3125   Loss: 0.882879 mae: 0.720708 (1908.3233996087174 steps/sec)\n",
      "Step #11582\tEpoch   3 Batch 2206/3125   Loss: 0.663818 mae: 0.641874 (1960.99978493216 steps/sec)\n",
      "Step #11583\tEpoch   3 Batch 2207/3125   Loss: 0.803514 mae: 0.686767 (2301.7044768584065 steps/sec)\n",
      "Step #11584\tEpoch   3 Batch 2208/3125   Loss: 0.827073 mae: 0.741649 (2210.134052778012 steps/sec)\n",
      "Step #11585\tEpoch   3 Batch 2209/3125   Loss: 0.731000 mae: 0.662758 (2335.358574610245 steps/sec)\n",
      "Step #11586\tEpoch   3 Batch 2210/3125   Loss: 0.857137 mae: 0.725943 (2084.105499572675 steps/sec)\n",
      "Step #11587\tEpoch   3 Batch 2211/3125   Loss: 0.730813 mae: 0.679714 (1948.1026650936824 steps/sec)\n",
      "Step #11588\tEpoch   3 Batch 2212/3125   Loss: 0.777491 mae: 0.698895 (2055.910436640982 steps/sec)\n",
      "Step #11589\tEpoch   3 Batch 2213/3125   Loss: 0.743164 mae: 0.681692 (2417.3269552187194 steps/sec)\n",
      "Step #11590\tEpoch   3 Batch 2214/3125   Loss: 0.822893 mae: 0.711067 (2181.102640638163 steps/sec)\n",
      "Step #11591\tEpoch   3 Batch 2215/3125   Loss: 0.833146 mae: 0.721816 (2103.842218254048 steps/sec)\n",
      "Step #11592\tEpoch   3 Batch 2216/3125   Loss: 0.826879 mae: 0.716648 (2186.2641257662317 steps/sec)\n",
      "Step #11593\tEpoch   3 Batch 2217/3125   Loss: 0.797317 mae: 0.731006 (2099.7767209011263 steps/sec)\n",
      "Step #11594\tEpoch   3 Batch 2218/3125   Loss: 0.871961 mae: 0.759704 (2131.6636342383185 steps/sec)\n",
      "Step #11595\tEpoch   3 Batch 2219/3125   Loss: 0.927705 mae: 0.757027 (2098.7260445334 steps/sec)\n",
      "Step #11596\tEpoch   3 Batch 2220/3125   Loss: 0.801437 mae: 0.685815 (1900.1966203053505 steps/sec)\n",
      "Step #11597\tEpoch   3 Batch 2221/3125   Loss: 0.812055 mae: 0.712751 (2138.3145551873567 steps/sec)\n",
      "Step #11598\tEpoch   3 Batch 2222/3125   Loss: 0.934867 mae: 0.774283 (2118.185582837577 steps/sec)\n",
      "Step #11599\tEpoch   3 Batch 2223/3125   Loss: 0.786222 mae: 0.693863 (2152.382125336125 steps/sec)\n",
      "Step #11600\tEpoch   3 Batch 2224/3125   Loss: 0.702140 mae: 0.646233 (2130.2778201025953 steps/sec)\n",
      "Step #11601\tEpoch   3 Batch 2225/3125   Loss: 0.709576 mae: 0.674951 (2200.8101584636374 steps/sec)\n",
      "Step #11602\tEpoch   3 Batch 2226/3125   Loss: 0.767348 mae: 0.713782 (2196.844817833274 steps/sec)\n",
      "Step #11603\tEpoch   3 Batch 2227/3125   Loss: 0.894877 mae: 0.708048 (2147.7530621441156 steps/sec)\n",
      "Step #11604\tEpoch   3 Batch 2228/3125   Loss: 0.779251 mae: 0.694779 (2110.3203992915796 steps/sec)\n",
      "Step #11605\tEpoch   3 Batch 2229/3125   Loss: 0.812814 mae: 0.717694 (2119.27725453737 steps/sec)\n",
      "Step #11606\tEpoch   3 Batch 2230/3125   Loss: 0.718812 mae: 0.667729 (2067.3817034700314 steps/sec)\n",
      "Step #11607\tEpoch   3 Batch 2231/3125   Loss: 0.873558 mae: 0.731112 (2263.9606183608257 steps/sec)\n",
      "Step #11608\tEpoch   3 Batch 2232/3125   Loss: 0.703439 mae: 0.661014 (2182.555392508872 steps/sec)\n",
      "Step #11609\tEpoch   3 Batch 2233/3125   Loss: 0.750830 mae: 0.688560 (2114.704043561561 steps/sec)\n",
      "Step #11610\tEpoch   3 Batch 2234/3125   Loss: 0.793657 mae: 0.713563 (2072.5098577908666 steps/sec)\n",
      "Step #11611\tEpoch   3 Batch 2235/3125   Loss: 0.926515 mae: 0.777120 (2174.0030062717046 steps/sec)\n",
      "Step #11612\tEpoch   3 Batch 2236/3125   Loss: 0.771644 mae: 0.699585 (2249.583798163563 steps/sec)\n",
      "Step #11613\tEpoch   3 Batch 2237/3125   Loss: 0.726563 mae: 0.695215 (2323.122085230357 steps/sec)\n",
      "Step #11614\tEpoch   3 Batch 2238/3125   Loss: 0.841131 mae: 0.726855 (1816.9115608538953 steps/sec)\n",
      "Step #11615\tEpoch   3 Batch 2239/3125   Loss: 0.589634 mae: 0.607155 (1924.999311566599 steps/sec)\n",
      "Step #11616\tEpoch   3 Batch 2240/3125   Loss: 0.824987 mae: 0.720106 (1843.1964000070313 steps/sec)\n",
      "Step #11617\tEpoch   3 Batch 2241/3125   Loss: 0.884356 mae: 0.747377 (1899.9211820874968 steps/sec)\n",
      "Step #11618\tEpoch   3 Batch 2242/3125   Loss: 0.718279 mae: 0.701875 (1960.7064389158463 steps/sec)\n",
      "Step #11619\tEpoch   3 Batch 2243/3125   Loss: 0.669715 mae: 0.642398 (2038.0683971661533 steps/sec)\n",
      "Step #11620\tEpoch   3 Batch 2244/3125   Loss: 0.841870 mae: 0.730890 (2126.2389488198555 steps/sec)\n",
      "Step #11621\tEpoch   3 Batch 2245/3125   Loss: 0.714815 mae: 0.659723 (2222.807296469416 steps/sec)\n",
      "Step #11622\tEpoch   3 Batch 2246/3125   Loss: 0.732334 mae: 0.674479 (1909.922315419433 steps/sec)\n",
      "Step #11623\tEpoch   3 Batch 2247/3125   Loss: 0.841802 mae: 0.722129 (1907.8893740902474 steps/sec)\n",
      "Step #11624\tEpoch   3 Batch 2248/3125   Loss: 0.849090 mae: 0.717444 (2111.6590980032825 steps/sec)\n",
      "Step #11625\tEpoch   3 Batch 2249/3125   Loss: 0.753184 mae: 0.691069 (2106.5895210543235 steps/sec)\n",
      "Step #11626\tEpoch   3 Batch 2250/3125   Loss: 0.898413 mae: 0.749905 (2207.2263795480617 steps/sec)\n",
      "Step #11627\tEpoch   3 Batch 2251/3125   Loss: 0.840324 mae: 0.709422 (2046.4212180056402 steps/sec)\n",
      "Step #11628\tEpoch   3 Batch 2252/3125   Loss: 0.880106 mae: 0.741110 (2091.0878452487786 steps/sec)\n",
      "Step #11629\tEpoch   3 Batch 2253/3125   Loss: 0.712258 mae: 0.662143 (2107.6692696555815 steps/sec)\n",
      "Step #11630\tEpoch   3 Batch 2254/3125   Loss: 0.887452 mae: 0.745548 (2049.260775672533 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11631\tEpoch   3 Batch 2255/3125   Loss: 0.913199 mae: 0.751084 (1599.4386735612197 steps/sec)\n",
      "Step #11632\tEpoch   3 Batch 2256/3125   Loss: 0.698917 mae: 0.675503 (2020.7864789600978 steps/sec)\n",
      "Step #11633\tEpoch   3 Batch 2257/3125   Loss: 0.801827 mae: 0.717957 (2229.660737637815 steps/sec)\n",
      "Step #11634\tEpoch   3 Batch 2258/3125   Loss: 0.850710 mae: 0.750628 (2060.6577512257913 steps/sec)\n",
      "Step #11635\tEpoch   3 Batch 2259/3125   Loss: 0.794053 mae: 0.697604 (2122.7738807405385 steps/sec)\n",
      "Step #11636\tEpoch   3 Batch 2260/3125   Loss: 0.826272 mae: 0.692542 (2073.821508034611 steps/sec)\n",
      "Step #11637\tEpoch   3 Batch 2261/3125   Loss: 0.829981 mae: 0.685631 (2031.6909186026235 steps/sec)\n",
      "Step #11638\tEpoch   3 Batch 2262/3125   Loss: 0.897606 mae: 0.747041 (2023.8091561800354 steps/sec)\n",
      "Step #11639\tEpoch   3 Batch 2263/3125   Loss: 0.776812 mae: 0.709528 (1960.7981001178077 steps/sec)\n",
      "Step #11640\tEpoch   3 Batch 2264/3125   Loss: 0.735081 mae: 0.676186 (1758.3378748878586 steps/sec)\n",
      "Step #11641\tEpoch   3 Batch 2265/3125   Loss: 0.833415 mae: 0.719110 (1960.0833699400896 steps/sec)\n",
      "Step #11642\tEpoch   3 Batch 2266/3125   Loss: 0.838013 mae: 0.730340 (1972.2494427882218 steps/sec)\n",
      "Step #11643\tEpoch   3 Batch 2267/3125   Loss: 0.730975 mae: 0.680680 (1869.9193951066409 steps/sec)\n",
      "Step #11644\tEpoch   3 Batch 2268/3125   Loss: 0.868405 mae: 0.726672 (2193.7195338814618 steps/sec)\n",
      "Step #11645\tEpoch   3 Batch 2269/3125   Loss: 0.814904 mae: 0.705981 (1869.0194820241343 steps/sec)\n",
      "Step #11646\tEpoch   3 Batch 2270/3125   Loss: 0.857358 mae: 0.697155 (1659.4280650113153 steps/sec)\n",
      "Step #11647\tEpoch   3 Batch 2271/3125   Loss: 0.903269 mae: 0.739069 (1334.7199327915073 steps/sec)\n",
      "Step #11648\tEpoch   3 Batch 2272/3125   Loss: 0.832260 mae: 0.731617 (1071.073907425472 steps/sec)\n",
      "Step #11649\tEpoch   3 Batch 2273/3125   Loss: 0.800117 mae: 0.707977 (1509.5243579408038 steps/sec)\n",
      "Step #11650\tEpoch   3 Batch 2274/3125   Loss: 0.845041 mae: 0.722198 (1186.1249837393316 steps/sec)\n",
      "Step #11651\tEpoch   3 Batch 2275/3125   Loss: 0.695946 mae: 0.656542 (1107.2491314769643 steps/sec)\n",
      "Step #11652\tEpoch   3 Batch 2276/3125   Loss: 0.796104 mae: 0.696144 (1224.2282725447158 steps/sec)\n",
      "Step #11653\tEpoch   3 Batch 2277/3125   Loss: 0.789315 mae: 0.714185 (1323.241169567028 steps/sec)\n",
      "Step #11654\tEpoch   3 Batch 2278/3125   Loss: 0.863249 mae: 0.741100 (1465.586716331337 steps/sec)\n",
      "Step #11655\tEpoch   3 Batch 2279/3125   Loss: 0.762053 mae: 0.714899 (1267.7894787749822 steps/sec)\n",
      "Step #11656\tEpoch   3 Batch 2280/3125   Loss: 0.810258 mae: 0.716884 (1294.0589904973467 steps/sec)\n",
      "Step #11657\tEpoch   3 Batch 2281/3125   Loss: 0.885972 mae: 0.747257 (1314.3344196540486 steps/sec)\n",
      "Step #11658\tEpoch   3 Batch 2282/3125   Loss: 0.806643 mae: 0.731418 (1093.234634832925 steps/sec)\n",
      "Step #11659\tEpoch   3 Batch 2283/3125   Loss: 0.728593 mae: 0.703975 (1477.2733356344347 steps/sec)\n",
      "Step #11660\tEpoch   3 Batch 2284/3125   Loss: 0.818602 mae: 0.731170 (1497.7303566582393 steps/sec)\n",
      "Step #11661\tEpoch   3 Batch 2285/3125   Loss: 0.956215 mae: 0.766100 (1825.562993462573 steps/sec)\n",
      "Step #11662\tEpoch   3 Batch 2286/3125   Loss: 0.805153 mae: 0.735944 (1709.7973992091638 steps/sec)\n",
      "Step #11663\tEpoch   3 Batch 2287/3125   Loss: 0.749615 mae: 0.694550 (1694.3533727064869 steps/sec)\n",
      "Step #11664\tEpoch   3 Batch 2288/3125   Loss: 0.799332 mae: 0.686111 (1599.9389671719675 steps/sec)\n",
      "Step #11665\tEpoch   3 Batch 2289/3125   Loss: 0.986259 mae: 0.762773 (1200.6022613424932 steps/sec)\n",
      "Step #11666\tEpoch   3 Batch 2290/3125   Loss: 0.733017 mae: 0.680924 (1958.5457194355463 steps/sec)\n",
      "Step #11667\tEpoch   3 Batch 2291/3125   Loss: 0.632020 mae: 0.633997 (2067.076043566113 steps/sec)\n",
      "Step #11668\tEpoch   3 Batch 2292/3125   Loss: 0.786630 mae: 0.731240 (2046.740774718679 steps/sec)\n",
      "Step #11669\tEpoch   3 Batch 2293/3125   Loss: 0.796285 mae: 0.716378 (1888.9857683300306 steps/sec)\n",
      "Step #11670\tEpoch   3 Batch 2294/3125   Loss: 0.728164 mae: 0.656681 (1857.2180058271858 steps/sec)\n",
      "Step #11671\tEpoch   3 Batch 2295/3125   Loss: 1.024445 mae: 0.788545 (2172.8542417837457 steps/sec)\n",
      "Step #11672\tEpoch   3 Batch 2296/3125   Loss: 0.843486 mae: 0.742446 (2090.295829678654 steps/sec)\n",
      "Step #11673\tEpoch   3 Batch 2297/3125   Loss: 0.906151 mae: 0.786043 (2012.1584280012282 steps/sec)\n",
      "Step #11674\tEpoch   3 Batch 2298/3125   Loss: 0.846592 mae: 0.726999 (1703.2844936811669 steps/sec)\n",
      "Step #11675\tEpoch   3 Batch 2299/3125   Loss: 0.854729 mae: 0.707471 (1913.8789515952399 steps/sec)\n",
      "Step #11676\tEpoch   3 Batch 2300/3125   Loss: 0.810303 mae: 0.710749 (2184.8519575771465 steps/sec)\n",
      "Step #11677\tEpoch   3 Batch 2301/3125   Loss: 0.860523 mae: 0.723488 (2166.747943959995 steps/sec)\n",
      "Step #11678\tEpoch   3 Batch 2302/3125   Loss: 0.819067 mae: 0.714461 (2169.3927795593254 steps/sec)\n",
      "Step #11679\tEpoch   3 Batch 2303/3125   Loss: 0.835661 mae: 0.718627 (2228.168295792605 steps/sec)\n",
      "Step #11680\tEpoch   3 Batch 2304/3125   Loss: 0.743034 mae: 0.673409 (2129.7369757286483 steps/sec)\n",
      "Step #11681\tEpoch   3 Batch 2305/3125   Loss: 0.779714 mae: 0.696893 (2092.924292928285 steps/sec)\n",
      "Step #11682\tEpoch   3 Batch 2306/3125   Loss: 0.750579 mae: 0.686525 (1729.923779984822 steps/sec)\n",
      "Step #11683\tEpoch   3 Batch 2307/3125   Loss: 0.956832 mae: 0.779985 (2046.0019512195122 steps/sec)\n",
      "Step #11684\tEpoch   3 Batch 2308/3125   Loss: 0.900255 mae: 0.756444 (2120.4129298403486 steps/sec)\n",
      "Step #11685\tEpoch   3 Batch 2309/3125   Loss: 0.879182 mae: 0.733757 (2184.169305115814 steps/sec)\n",
      "Step #11686\tEpoch   3 Batch 2310/3125   Loss: 0.806341 mae: 0.726222 (2100.134190550582 steps/sec)\n",
      "Step #11687\tEpoch   3 Batch 2311/3125   Loss: 0.838608 mae: 0.719005 (1888.5775008104895 steps/sec)\n",
      "Step #11688\tEpoch   3 Batch 2312/3125   Loss: 0.893777 mae: 0.711262 (1893.0781729554071 steps/sec)\n",
      "Step #11689\tEpoch   3 Batch 2313/3125   Loss: 0.726296 mae: 0.672449 (2134.744856930547 steps/sec)\n",
      "Step #11690\tEpoch   3 Batch 2314/3125   Loss: 0.799524 mae: 0.699260 (1520.9648760180733 steps/sec)\n",
      "Step #11691\tEpoch   3 Batch 2315/3125   Loss: 0.891326 mae: 0.744657 (1423.3226099822182 steps/sec)\n",
      "Step #11692\tEpoch   3 Batch 2316/3125   Loss: 0.959556 mae: 0.789983 (1590.1850911048598 steps/sec)\n",
      "Step #11693\tEpoch   3 Batch 2317/3125   Loss: 0.838981 mae: 0.733001 (1564.5484251204846 steps/sec)\n",
      "Step #11694\tEpoch   3 Batch 2318/3125   Loss: 0.837625 mae: 0.711278 (1726.5768176317067 steps/sec)\n",
      "Step #11695\tEpoch   3 Batch 2319/3125   Loss: 0.842087 mae: 0.726379 (1630.8319206183803 steps/sec)\n",
      "Step #11696\tEpoch   3 Batch 2320/3125   Loss: 0.828689 mae: 0.733303 (1380.5772066568359 steps/sec)\n",
      "Step #11697\tEpoch   3 Batch 2321/3125   Loss: 0.755610 mae: 0.678716 (1450.6235776687947 steps/sec)\n",
      "Step #11698\tEpoch   3 Batch 2322/3125   Loss: 0.782214 mae: 0.687890 (1459.1319594227905 steps/sec)\n",
      "Step #11699\tEpoch   3 Batch 2323/3125   Loss: 0.759955 mae: 0.689535 (1804.2034808193605 steps/sec)\n",
      "Step #11700\tEpoch   3 Batch 2324/3125   Loss: 0.669021 mae: 0.632041 (1827.2331230613738 steps/sec)\n",
      "Step #11701\tEpoch   3 Batch 2325/3125   Loss: 0.755775 mae: 0.690943 (1452.220760335157 steps/sec)\n",
      "Step #11702\tEpoch   3 Batch 2326/3125   Loss: 0.731292 mae: 0.674994 (1582.7203912364248 steps/sec)\n",
      "Step #11703\tEpoch   3 Batch 2327/3125   Loss: 0.803804 mae: 0.689265 (1575.1597954017982 steps/sec)\n",
      "Step #11704\tEpoch   3 Batch 2328/3125   Loss: 0.851878 mae: 0.730001 (1661.1368100880807 steps/sec)\n",
      "Step #11705\tEpoch   3 Batch 2329/3125   Loss: 0.794593 mae: 0.704789 (1522.5218161490322 steps/sec)\n",
      "Step #11706\tEpoch   3 Batch 2330/3125   Loss: 0.890422 mae: 0.761553 (1674.1456249451173 steps/sec)\n",
      "Step #11707\tEpoch   3 Batch 2331/3125   Loss: 0.724038 mae: 0.688771 (1994.4194539281605 steps/sec)\n",
      "Step #11708\tEpoch   3 Batch 2332/3125   Loss: 0.821849 mae: 0.711231 (1907.0912827601258 steps/sec)\n",
      "Step #11709\tEpoch   3 Batch 2333/3125   Loss: 0.740024 mae: 0.684743 (2252.459051608399 steps/sec)\n",
      "Step #11710\tEpoch   3 Batch 2334/3125   Loss: 0.733927 mae: 0.670811 (1980.2200084981823 steps/sec)\n",
      "Step #11711\tEpoch   3 Batch 2335/3125   Loss: 0.841384 mae: 0.739624 (2034.3318329970512 steps/sec)\n",
      "Step #11712\tEpoch   3 Batch 2336/3125   Loss: 0.712748 mae: 0.658717 (1738.0961063501798 steps/sec)\n",
      "Step #11713\tEpoch   3 Batch 2337/3125   Loss: 0.876252 mae: 0.739874 (1732.0383217707301 steps/sec)\n",
      "Step #11714\tEpoch   3 Batch 2338/3125   Loss: 0.798425 mae: 0.702430 (1486.098158987514 steps/sec)\n",
      "Step #11715\tEpoch   3 Batch 2339/3125   Loss: 0.706783 mae: 0.663599 (1490.3966285507174 steps/sec)\n",
      "Step #11716\tEpoch   3 Batch 2340/3125   Loss: 0.798639 mae: 0.696602 (1444.9166322171695 steps/sec)\n",
      "Step #11717\tEpoch   3 Batch 2341/3125   Loss: 0.768601 mae: 0.714596 (1561.1824523006603 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11718\tEpoch   3 Batch 2342/3125   Loss: 0.779486 mae: 0.704744 (1219.5154856192503 steps/sec)\n",
      "Step #11719\tEpoch   3 Batch 2343/3125   Loss: 0.715431 mae: 0.663050 (1366.4542528376141 steps/sec)\n",
      "Step #11720\tEpoch   3 Batch 2344/3125   Loss: 0.876554 mae: 0.748072 (1549.0855370069435 steps/sec)\n",
      "Step #11721\tEpoch   3 Batch 2345/3125   Loss: 0.836188 mae: 0.689844 (1470.375174405968 steps/sec)\n",
      "Step #11722\tEpoch   3 Batch 2346/3125   Loss: 0.701474 mae: 0.653163 (1495.6687943515317 steps/sec)\n",
      "Step #11723\tEpoch   3 Batch 2347/3125   Loss: 0.884587 mae: 0.739357 (1424.1441832985868 steps/sec)\n",
      "Step #11724\tEpoch   3 Batch 2348/3125   Loss: 0.995609 mae: 0.776107 (1569.372146972985 steps/sec)\n",
      "Step #11725\tEpoch   3 Batch 2349/3125   Loss: 0.788047 mae: 0.685971 (1482.4529035450464 steps/sec)\n",
      "Step #11726\tEpoch   3 Batch 2350/3125   Loss: 0.902446 mae: 0.754468 (1588.8116974127809 steps/sec)\n",
      "Step #11727\tEpoch   3 Batch 2351/3125   Loss: 0.783464 mae: 0.701710 (1515.8966055629446 steps/sec)\n",
      "Step #11728\tEpoch   3 Batch 2352/3125   Loss: 0.776545 mae: 0.703503 (1474.3549725116352 steps/sec)\n",
      "Step #11729\tEpoch   3 Batch 2353/3125   Loss: 0.816738 mae: 0.699839 (1742.3560396467353 steps/sec)\n",
      "Step #11730\tEpoch   3 Batch 2354/3125   Loss: 0.791829 mae: 0.697583 (1860.810463083735 steps/sec)\n",
      "Step #11731\tEpoch   3 Batch 2355/3125   Loss: 0.835704 mae: 0.739112 (1404.2613598317955 steps/sec)\n",
      "Step #11732\tEpoch   3 Batch 2356/3125   Loss: 0.874673 mae: 0.763681 (1813.502131596925 steps/sec)\n",
      "Step #11733\tEpoch   3 Batch 2357/3125   Loss: 0.767124 mae: 0.694489 (1553.9523993005128 steps/sec)\n",
      "Step #11734\tEpoch   3 Batch 2358/3125   Loss: 0.753907 mae: 0.696294 (1172.6873675440215 steps/sec)\n",
      "Step #11735\tEpoch   3 Batch 2359/3125   Loss: 0.835707 mae: 0.709868 (1295.2978598560885 steps/sec)\n",
      "Step #11736\tEpoch   3 Batch 2360/3125   Loss: 0.788971 mae: 0.699469 (1390.9610665251707 steps/sec)\n",
      "Step #11737\tEpoch   3 Batch 2361/3125   Loss: 0.694101 mae: 0.657133 (1217.3422416745514 steps/sec)\n",
      "Step #11738\tEpoch   3 Batch 2362/3125   Loss: 0.869917 mae: 0.738683 (1450.964818210122 steps/sec)\n",
      "Step #11739\tEpoch   3 Batch 2363/3125   Loss: 0.736641 mae: 0.689380 (1508.2975525204795 steps/sec)\n",
      "Step #11740\tEpoch   3 Batch 2364/3125   Loss: 0.839077 mae: 0.718701 (1431.6789776218238 steps/sec)\n",
      "Step #11741\tEpoch   3 Batch 2365/3125   Loss: 0.657242 mae: 0.620278 (1558.259215502701 steps/sec)\n",
      "Step #11742\tEpoch   3 Batch 2366/3125   Loss: 0.776459 mae: 0.709561 (1343.4325834059346 steps/sec)\n",
      "Step #11743\tEpoch   3 Batch 2367/3125   Loss: 0.880117 mae: 0.725440 (1632.0249027237355 steps/sec)\n",
      "Step #11744\tEpoch   3 Batch 2368/3125   Loss: 0.897946 mae: 0.749490 (1331.1025071405902 steps/sec)\n",
      "Step #11745\tEpoch   3 Batch 2369/3125   Loss: 0.786669 mae: 0.710430 (1530.4662584746072 steps/sec)\n",
      "Step #11746\tEpoch   3 Batch 2370/3125   Loss: 0.820283 mae: 0.713130 (1609.6279012648902 steps/sec)\n",
      "Step #11747\tEpoch   3 Batch 2371/3125   Loss: 0.819399 mae: 0.710817 (1513.0857641719756 steps/sec)\n",
      "Step #11748\tEpoch   3 Batch 2372/3125   Loss: 0.788415 mae: 0.718244 (1457.9755283648499 steps/sec)\n",
      "Step #11749\tEpoch   3 Batch 2373/3125   Loss: 0.722368 mae: 0.673706 (1481.7092471173412 steps/sec)\n",
      "Step #11750\tEpoch   3 Batch 2374/3125   Loss: 0.773351 mae: 0.699572 (1469.6435829514078 steps/sec)\n",
      "Step #11751\tEpoch   3 Batch 2375/3125   Loss: 0.855784 mae: 0.741507 (1720.231972504532 steps/sec)\n",
      "Step #11752\tEpoch   3 Batch 2376/3125   Loss: 0.786330 mae: 0.692888 (1855.968848179123 steps/sec)\n",
      "Step #11753\tEpoch   3 Batch 2377/3125   Loss: 0.752338 mae: 0.681011 (1669.0558619646795 steps/sec)\n",
      "Step #11754\tEpoch   3 Batch 2378/3125   Loss: 0.915255 mae: 0.762592 (1541.445487354007 steps/sec)\n",
      "Step #11755\tEpoch   3 Batch 2379/3125   Loss: 0.872905 mae: 0.750907 (1387.1153795274756 steps/sec)\n",
      "Step #11756\tEpoch   3 Batch 2380/3125   Loss: 0.794893 mae: 0.708033 (1067.828956078088 steps/sec)\n",
      "Step #11757\tEpoch   3 Batch 2381/3125   Loss: 0.767102 mae: 0.698361 (1300.2529636426764 steps/sec)\n",
      "Step #11758\tEpoch   3 Batch 2382/3125   Loss: 0.793409 mae: 0.705562 (1299.5680814014736 steps/sec)\n",
      "Step #11759\tEpoch   3 Batch 2383/3125   Loss: 0.772483 mae: 0.698299 (1494.2301389383683 steps/sec)\n",
      "Step #11760\tEpoch   3 Batch 2384/3125   Loss: 0.902121 mae: 0.767862 (1395.2086008342703 steps/sec)\n",
      "Step #11761\tEpoch   3 Batch 2385/3125   Loss: 0.860940 mae: 0.733555 (1377.0327325256903 steps/sec)\n",
      "Step #11762\tEpoch   3 Batch 2386/3125   Loss: 0.774099 mae: 0.678487 (1178.1355684639843 steps/sec)\n",
      "Step #11763\tEpoch   3 Batch 2387/3125   Loss: 0.803892 mae: 0.728472 (1481.0081707308461 steps/sec)\n",
      "Step #11764\tEpoch   3 Batch 2388/3125   Loss: 0.772418 mae: 0.676919 (1633.5631216943582 steps/sec)\n",
      "Step #11765\tEpoch   3 Batch 2389/3125   Loss: 0.816166 mae: 0.744621 (1531.9084281728 steps/sec)\n",
      "Step #11766\tEpoch   3 Batch 2390/3125   Loss: 0.846311 mae: 0.735243 (1440.2131663164257 steps/sec)\n",
      "Step #11767\tEpoch   3 Batch 2391/3125   Loss: 0.827362 mae: 0.713869 (1517.223616908908 steps/sec)\n",
      "Step #11768\tEpoch   3 Batch 2392/3125   Loss: 0.794750 mae: 0.705616 (1477.710525017792 steps/sec)\n",
      "Step #11769\tEpoch   3 Batch 2393/3125   Loss: 0.802871 mae: 0.707037 (1287.0621881539944 steps/sec)\n",
      "Step #11770\tEpoch   3 Batch 2394/3125   Loss: 0.756555 mae: 0.672277 (1952.8191375441145 steps/sec)\n",
      "Step #11771\tEpoch   3 Batch 2395/3125   Loss: 0.760352 mae: 0.711571 (1957.8691861007899 steps/sec)\n",
      "Step #11772\tEpoch   3 Batch 2396/3125   Loss: 0.900819 mae: 0.765325 (1438.346261736727 steps/sec)\n",
      "Step #11773\tEpoch   3 Batch 2397/3125   Loss: 0.719014 mae: 0.664963 (1829.8957288076435 steps/sec)\n",
      "Step #11774\tEpoch   3 Batch 2398/3125   Loss: 0.727812 mae: 0.679454 (1843.520455704214 steps/sec)\n",
      "Step #11775\tEpoch   3 Batch 2399/3125   Loss: 0.724849 mae: 0.672082 (1781.2628467562472 steps/sec)\n",
      "Step #11776\tEpoch   3 Batch 2400/3125   Loss: 0.848813 mae: 0.713787 (1794.4467737381171 steps/sec)\n",
      "Step #11777\tEpoch   3 Batch 2401/3125   Loss: 0.821146 mae: 0.692265 (1939.562543352601 steps/sec)\n",
      "Step #11778\tEpoch   3 Batch 2402/3125   Loss: 0.747700 mae: 0.683320 (1883.759701063524 steps/sec)\n",
      "Step #11779\tEpoch   3 Batch 2403/3125   Loss: 0.780008 mae: 0.707649 (1937.3049671596568 steps/sec)\n",
      "Step #11780\tEpoch   3 Batch 2404/3125   Loss: 0.829190 mae: 0.720904 (1785.2204336315579 steps/sec)\n",
      "Step #11781\tEpoch   3 Batch 2405/3125   Loss: 0.813772 mae: 0.712839 (2030.1175195059147 steps/sec)\n",
      "Step #11782\tEpoch   3 Batch 2406/3125   Loss: 0.823242 mae: 0.711319 (1874.7503642848842 steps/sec)\n",
      "Step #11783\tEpoch   3 Batch 2407/3125   Loss: 0.711770 mae: 0.693459 (1598.8777408435242 steps/sec)\n",
      "Step #11784\tEpoch   3 Batch 2408/3125   Loss: 0.904794 mae: 0.753477 (1490.8946141159074 steps/sec)\n",
      "Step #11785\tEpoch   3 Batch 2409/3125   Loss: 0.678702 mae: 0.663244 (1434.0187222636298 steps/sec)\n",
      "Step #11786\tEpoch   3 Batch 2410/3125   Loss: 0.858384 mae: 0.717705 (1396.7697461753128 steps/sec)\n",
      "Step #11787\tEpoch   3 Batch 2411/3125   Loss: 0.880330 mae: 0.757869 (1543.3512654268745 steps/sec)\n",
      "Step #11788\tEpoch   3 Batch 2412/3125   Loss: 0.863774 mae: 0.763460 (1385.658123385333 steps/sec)\n",
      "Step #11789\tEpoch   3 Batch 2413/3125   Loss: 0.823976 mae: 0.717488 (1193.5981787137166 steps/sec)\n",
      "Step #11790\tEpoch   3 Batch 2414/3125   Loss: 0.821371 mae: 0.720366 (1372.9399210469462 steps/sec)\n",
      "Step #11791\tEpoch   3 Batch 2415/3125   Loss: 0.834270 mae: 0.732803 (1381.7506176906604 steps/sec)\n",
      "Step #11792\tEpoch   3 Batch 2416/3125   Loss: 0.782547 mae: 0.703927 (1306.441404400588 steps/sec)\n",
      "Step #11793\tEpoch   3 Batch 2417/3125   Loss: 0.759979 mae: 0.687124 (1487.6900267438477 steps/sec)\n",
      "Step #11794\tEpoch   3 Batch 2418/3125   Loss: 0.704863 mae: 0.651089 (1541.2189224742965 steps/sec)\n",
      "Step #11795\tEpoch   3 Batch 2419/3125   Loss: 0.757187 mae: 0.696684 (1501.7845378247544 steps/sec)\n",
      "Step #11796\tEpoch   3 Batch 2420/3125   Loss: 0.737867 mae: 0.666810 (1578.039971105225 steps/sec)\n",
      "Step #11797\tEpoch   3 Batch 2421/3125   Loss: 0.711311 mae: 0.668415 (1951.6742047759972 steps/sec)\n",
      "Step #11798\tEpoch   3 Batch 2422/3125   Loss: 0.784283 mae: 0.691851 (1936.0351544469268 steps/sec)\n",
      "Step #11799\tEpoch   3 Batch 2423/3125   Loss: 0.669061 mae: 0.625631 (1870.7534209915968 steps/sec)\n",
      "Step #11800\tEpoch   3 Batch 2424/3125   Loss: 0.765895 mae: 0.696890 (1439.6002086822812 steps/sec)\n",
      "Step #11801\tEpoch   3 Batch 2425/3125   Loss: 0.728394 mae: 0.680339 (1102.7079324015942 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11802\tEpoch   3 Batch 2426/3125   Loss: 0.949700 mae: 0.775109 (1164.3664184863778 steps/sec)\n",
      "Step #11803\tEpoch   3 Batch 2427/3125   Loss: 0.857067 mae: 0.718872 (1318.839103229255 steps/sec)\n",
      "Step #11804\tEpoch   3 Batch 2428/3125   Loss: 0.757094 mae: 0.676300 (1165.745033296646 steps/sec)\n",
      "Step #11805\tEpoch   3 Batch 2429/3125   Loss: 0.867422 mae: 0.706857 (1263.0706593750754 steps/sec)\n",
      "Step #11806\tEpoch   3 Batch 2430/3125   Loss: 0.783272 mae: 0.712056 (1327.1769947347104 steps/sec)\n",
      "Step #11807\tEpoch   3 Batch 2431/3125   Loss: 0.758952 mae: 0.711840 (1213.4751362673733 steps/sec)\n",
      "Step #11808\tEpoch   3 Batch 2432/3125   Loss: 0.796325 mae: 0.702708 (1343.6477681174276 steps/sec)\n",
      "Step #11809\tEpoch   3 Batch 2433/3125   Loss: 0.683702 mae: 0.657734 (1609.010419064279 steps/sec)\n",
      "Step #11810\tEpoch   3 Batch 2434/3125   Loss: 0.932082 mae: 0.746988 (1880.1118840995482 steps/sec)\n",
      "Step #11811\tEpoch   3 Batch 2435/3125   Loss: 0.747846 mae: 0.655912 (1877.7214691187794 steps/sec)\n",
      "Step #11812\tEpoch   3 Batch 2436/3125   Loss: 0.765769 mae: 0.694744 (1419.5651585303117 steps/sec)\n",
      "Step #11813\tEpoch   3 Batch 2437/3125   Loss: 0.751819 mae: 0.682104 (1005.1244691966297 steps/sec)\n",
      "Step #11814\tEpoch   3 Batch 2438/3125   Loss: 0.792219 mae: 0.723277 (1108.8943057619197 steps/sec)\n",
      "Step #11815\tEpoch   3 Batch 2439/3125   Loss: 0.771675 mae: 0.687126 (1150.0381124729238 steps/sec)\n",
      "Step #11816\tEpoch   3 Batch 2440/3125   Loss: 0.758173 mae: 0.712333 (1424.2602465278958 steps/sec)\n",
      "Step #11817\tEpoch   3 Batch 2441/3125   Loss: 0.734557 mae: 0.672899 (866.3733521715601 steps/sec)\n",
      "Step #11818\tEpoch   3 Batch 2442/3125   Loss: 0.733649 mae: 0.657407 (815.0673538076471 steps/sec)\n",
      "Step #11819\tEpoch   3 Batch 2443/3125   Loss: 0.770537 mae: 0.700117 (1576.3319302465425 steps/sec)\n",
      "Step #11820\tEpoch   3 Batch 2444/3125   Loss: 0.763918 mae: 0.700958 (1811.7161245734526 steps/sec)\n",
      "Step #11821\tEpoch   3 Batch 2445/3125   Loss: 0.887334 mae: 0.750951 (1702.8142710989137 steps/sec)\n",
      "Step #11822\tEpoch   3 Batch 2446/3125   Loss: 0.733629 mae: 0.674787 (1655.7988235758557 steps/sec)\n",
      "Step #11823\tEpoch   3 Batch 2447/3125   Loss: 0.746571 mae: 0.686085 (1378.5443836769036 steps/sec)\n",
      "Step #11824\tEpoch   3 Batch 2448/3125   Loss: 0.746924 mae: 0.691640 (1577.0431643856218 steps/sec)\n",
      "Step #11825\tEpoch   3 Batch 2449/3125   Loss: 0.795931 mae: 0.712996 (1692.9718907922565 steps/sec)\n",
      "Step #11826\tEpoch   3 Batch 2450/3125   Loss: 0.870196 mae: 0.746252 (1475.0601375778974 steps/sec)\n",
      "Step #11827\tEpoch   3 Batch 2451/3125   Loss: 0.808141 mae: 0.717245 (1696.807288379695 steps/sec)\n",
      "Step #11828\tEpoch   3 Batch 2452/3125   Loss: 0.843851 mae: 0.735879 (1806.8149118196934 steps/sec)\n",
      "Step #11829\tEpoch   3 Batch 2453/3125   Loss: 0.860076 mae: 0.724759 (1681.6903893187923 steps/sec)\n",
      "Step #11830\tEpoch   3 Batch 2454/3125   Loss: 0.725322 mae: 0.655407 (1858.7323956145249 steps/sec)\n",
      "Step #11831\tEpoch   3 Batch 2455/3125   Loss: 0.872652 mae: 0.731084 (2049.1005911378184 steps/sec)\n",
      "Step #11832\tEpoch   3 Batch 2456/3125   Loss: 0.731459 mae: 0.708345 (1901.523284491513 steps/sec)\n",
      "Step #11833\tEpoch   3 Batch 2457/3125   Loss: 0.727650 mae: 0.657941 (1431.503071672355 steps/sec)\n",
      "Step #11834\tEpoch   3 Batch 2458/3125   Loss: 0.797750 mae: 0.717895 (1701.6122357904985 steps/sec)\n",
      "Step #11835\tEpoch   3 Batch 2459/3125   Loss: 0.760289 mae: 0.700953 (1690.9516053603393 steps/sec)\n",
      "Step #11836\tEpoch   3 Batch 2460/3125   Loss: 0.815886 mae: 0.716354 (2037.9099575344728 steps/sec)\n",
      "Step #11837\tEpoch   3 Batch 2461/3125   Loss: 0.705302 mae: 0.658730 (1840.882716970532 steps/sec)\n",
      "Step #11838\tEpoch   3 Batch 2462/3125   Loss: 0.853267 mae: 0.730131 (1581.0379662856972 steps/sec)\n",
      "Step #11839\tEpoch   3 Batch 2463/3125   Loss: 0.797742 mae: 0.696395 (1089.780604662281 steps/sec)\n",
      "Step #11840\tEpoch   3 Batch 2464/3125   Loss: 0.842548 mae: 0.713269 (1240.5806703500823 steps/sec)\n",
      "Step #11841\tEpoch   3 Batch 2465/3125   Loss: 0.696606 mae: 0.677079 (1796.891440322166 steps/sec)\n",
      "Step #11842\tEpoch   3 Batch 2466/3125   Loss: 0.738007 mae: 0.655802 (1881.3262523324242 steps/sec)\n",
      "Step #11843\tEpoch   3 Batch 2467/3125   Loss: 0.731773 mae: 0.678891 (1888.3394262457455 steps/sec)\n",
      "Step #11844\tEpoch   3 Batch 2468/3125   Loss: 1.007312 mae: 0.804859 (2017.015956065517 steps/sec)\n",
      "Step #11845\tEpoch   3 Batch 2469/3125   Loss: 0.814810 mae: 0.690403 (2040.725928088357 steps/sec)\n",
      "Step #11846\tEpoch   3 Batch 2470/3125   Loss: 0.865125 mae: 0.752159 (1902.299465725715 steps/sec)\n",
      "Step #11847\tEpoch   3 Batch 2471/3125   Loss: 0.817712 mae: 0.713963 (1311.4740976061235 steps/sec)\n",
      "Step #11848\tEpoch   3 Batch 2472/3125   Loss: 0.748985 mae: 0.662074 (1807.7182336157778 steps/sec)\n",
      "Step #11849\tEpoch   3 Batch 2473/3125   Loss: 0.818298 mae: 0.714368 (1618.122897441437 steps/sec)\n",
      "Step #11850\tEpoch   3 Batch 2474/3125   Loss: 0.888316 mae: 0.730417 (1904.3896769037976 steps/sec)\n",
      "Step #11851\tEpoch   3 Batch 2475/3125   Loss: 0.850496 mae: 0.739473 (1652.2374889701248 steps/sec)\n",
      "Step #11852\tEpoch   3 Batch 2476/3125   Loss: 0.797241 mae: 0.706438 (1918.0625040013902 steps/sec)\n",
      "Step #11853\tEpoch   3 Batch 2477/3125   Loss: 0.837444 mae: 0.718810 (1463.7454370327976 steps/sec)\n",
      "Step #11854\tEpoch   3 Batch 2478/3125   Loss: 0.770575 mae: 0.710057 (1690.6108168678002 steps/sec)\n",
      "Step #11855\tEpoch   3 Batch 2479/3125   Loss: 0.807787 mae: 0.718084 (2094.78489307083 steps/sec)\n",
      "Step #11856\tEpoch   3 Batch 2480/3125   Loss: 0.882096 mae: 0.724521 (1916.2923299036897 steps/sec)\n",
      "Step #11857\tEpoch   3 Batch 2481/3125   Loss: 0.780935 mae: 0.698681 (1952.3646384151336 steps/sec)\n",
      "Step #11858\tEpoch   3 Batch 2482/3125   Loss: 0.861732 mae: 0.729252 (2061.346412809499 steps/sec)\n",
      "Step #11859\tEpoch   3 Batch 2483/3125   Loss: 0.872592 mae: 0.756385 (1901.299172264984 steps/sec)\n",
      "Step #11860\tEpoch   3 Batch 2484/3125   Loss: 0.731571 mae: 0.690909 (1975.8915364104882 steps/sec)\n",
      "Step #11861\tEpoch   3 Batch 2485/3125   Loss: 0.752075 mae: 0.680862 (1881.8158161571387 steps/sec)\n",
      "Step #11862\tEpoch   3 Batch 2486/3125   Loss: 0.827130 mae: 0.684979 (1475.7971330654525 steps/sec)\n",
      "Step #11863\tEpoch   3 Batch 2487/3125   Loss: 0.730621 mae: 0.656894 (2016.1239773502919 steps/sec)\n",
      "Step #11864\tEpoch   3 Batch 2488/3125   Loss: 0.799216 mae: 0.710214 (1888.8496595453398 steps/sec)\n",
      "Step #11865\tEpoch   3 Batch 2489/3125   Loss: 0.881499 mae: 0.753134 (1823.2140838948055 steps/sec)\n",
      "Step #11866\tEpoch   3 Batch 2490/3125   Loss: 0.890815 mae: 0.739290 (2050.1823229805163 steps/sec)\n",
      "Step #11867\tEpoch   3 Batch 2491/3125   Loss: 0.839763 mae: 0.717437 (1861.9338914882849 steps/sec)\n",
      "Step #11868\tEpoch   3 Batch 2492/3125   Loss: 0.924696 mae: 0.748210 (1756.3351618441438 steps/sec)\n",
      "Step #11869\tEpoch   3 Batch 2493/3125   Loss: 0.852105 mae: 0.722341 (1203.572002479282 steps/sec)\n",
      "Step #11870\tEpoch   3 Batch 2494/3125   Loss: 0.782237 mae: 0.691486 (1434.4404924760602 steps/sec)\n",
      "Step #11871\tEpoch   3 Batch 2495/3125   Loss: 0.834079 mae: 0.712410 (1666.3239442215247 steps/sec)\n",
      "Step #11872\tEpoch   3 Batch 2496/3125   Loss: 0.818829 mae: 0.731399 (1692.5619834710744 steps/sec)\n",
      "Step #11873\tEpoch   3 Batch 2497/3125   Loss: 0.702558 mae: 0.679006 (1807.7182336157778 steps/sec)\n",
      "Step #11874\tEpoch   3 Batch 2498/3125   Loss: 0.851766 mae: 0.717183 (1946.3669522121265 steps/sec)\n",
      "Step #11875\tEpoch   3 Batch 2499/3125   Loss: 0.893557 mae: 0.760736 (2022.4234533969816 steps/sec)\n",
      "Step #11876\tEpoch   3 Batch 2500/3125   Loss: 0.740603 mae: 0.698780 (2042.5545177407887 steps/sec)\n",
      "Step #11877\tEpoch   3 Batch 2501/3125   Loss: 0.854279 mae: 0.714305 (1507.711995398828 steps/sec)\n",
      "Step #11878\tEpoch   3 Batch 2502/3125   Loss: 0.725540 mae: 0.681619 (1729.8096275033818 steps/sec)\n",
      "Step #11879\tEpoch   3 Batch 2503/3125   Loss: 0.817954 mae: 0.722347 (1989.802172778595 steps/sec)\n",
      "Step #11880\tEpoch   3 Batch 2504/3125   Loss: 0.709156 mae: 0.666848 (2041.3218474716505 steps/sec)\n",
      "Step #11881\tEpoch   3 Batch 2505/3125   Loss: 0.766741 mae: 0.678932 (2130.2129042743377 steps/sec)\n",
      "Step #11882\tEpoch   3 Batch 2506/3125   Loss: 0.924814 mae: 0.752889 (2074.2522551036554 steps/sec)\n",
      "Step #11883\tEpoch   3 Batch 2507/3125   Loss: 0.644276 mae: 0.628803 (2118.720575458164 steps/sec)\n",
      "Step #11884\tEpoch   3 Batch 2508/3125   Loss: 0.836208 mae: 0.718039 (1971.3780785862004 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11885\tEpoch   3 Batch 2509/3125   Loss: 0.768241 mae: 0.707903 (1617.3367163580556 steps/sec)\n",
      "Step #11886\tEpoch   3 Batch 2510/3125   Loss: 0.875090 mae: 0.745399 (1100.1106850407332 steps/sec)\n",
      "Step #11887\tEpoch   3 Batch 2511/3125   Loss: 0.747348 mae: 0.679492 (1320.757758968158 steps/sec)\n",
      "Step #11888\tEpoch   3 Batch 2512/3125   Loss: 0.854123 mae: 0.732993 (1579.1688315600034 steps/sec)\n",
      "Step #11889\tEpoch   3 Batch 2513/3125   Loss: 0.878884 mae: 0.736626 (1501.7200143215182 steps/sec)\n",
      "Step #11890\tEpoch   3 Batch 2514/3125   Loss: 0.804395 mae: 0.711379 (1432.4710896783492 steps/sec)\n",
      "Step #11891\tEpoch   3 Batch 2515/3125   Loss: 0.857477 mae: 0.731757 (1157.3814280510821 steps/sec)\n",
      "Step #11892\tEpoch   3 Batch 2516/3125   Loss: 0.699764 mae: 0.672252 (1353.57765241974 steps/sec)\n",
      "Step #11893\tEpoch   3 Batch 2517/3125   Loss: 0.718130 mae: 0.684023 (1598.768039154399 steps/sec)\n",
      "Step #11894\tEpoch   3 Batch 2518/3125   Loss: 0.859956 mae: 0.733902 (1515.0861882125157 steps/sec)\n",
      "Step #11895\tEpoch   3 Batch 2519/3125   Loss: 0.782859 mae: 0.699660 (1501.4189779349647 steps/sec)\n",
      "Step #11896\tEpoch   3 Batch 2520/3125   Loss: 0.867920 mae: 0.747756 (1509.2527689218657 steps/sec)\n",
      "Step #11897\tEpoch   3 Batch 2521/3125   Loss: 0.933294 mae: 0.765899 (1351.928469666007 steps/sec)\n",
      "Step #11898\tEpoch   3 Batch 2522/3125   Loss: 0.716614 mae: 0.667392 (1098.1060744898655 steps/sec)\n",
      "Step #11899\tEpoch   3 Batch 2523/3125   Loss: 0.842304 mae: 0.732967 (1027.6630567942373 steps/sec)\n",
      "Step #11900\tEpoch   3 Batch 2524/3125   Loss: 0.763934 mae: 0.706337 (1320.0594203993253 steps/sec)\n",
      "Step #11901\tEpoch   3 Batch 2525/3125   Loss: 0.781224 mae: 0.699090 (1224.6786692439312 steps/sec)\n",
      "Step #11902\tEpoch   3 Batch 2526/3125   Loss: 0.930998 mae: 0.732801 (1255.1182610361006 steps/sec)\n",
      "Step #11903\tEpoch   3 Batch 2527/3125   Loss: 0.749373 mae: 0.683525 (1552.8018007345102 steps/sec)\n",
      "Step #11904\tEpoch   3 Batch 2528/3125   Loss: 0.781865 mae: 0.697563 (1557.969808629502 steps/sec)\n",
      "Step #11905\tEpoch   3 Batch 2529/3125   Loss: 0.852389 mae: 0.736116 (1789.6995195384839 steps/sec)\n",
      "Step #11906\tEpoch   3 Batch 2530/3125   Loss: 0.694181 mae: 0.658613 (1899.9383946367095 steps/sec)\n",
      "Step #11907\tEpoch   3 Batch 2531/3125   Loss: 0.891037 mae: 0.749838 (1890.7399226448606 steps/sec)\n",
      "Step #11908\tEpoch   3 Batch 2532/3125   Loss: 0.804850 mae: 0.713399 (2027.3503282000715 steps/sec)\n",
      "Step #11909\tEpoch   3 Batch 2533/3125   Loss: 0.850950 mae: 0.724154 (1816.3450545643514 steps/sec)\n",
      "Step #11910\tEpoch   3 Batch 2534/3125   Loss: 0.925575 mae: 0.755349 (1415.4643628509718 steps/sec)\n",
      "Step #11911\tEpoch   3 Batch 2535/3125   Loss: 0.961890 mae: 0.764081 (1827.2331230613738 steps/sec)\n",
      "Step #11912\tEpoch   3 Batch 2536/3125   Loss: 0.751245 mae: 0.699987 (1776.3291857599038 steps/sec)\n",
      "Step #11913\tEpoch   3 Batch 2537/3125   Loss: 0.819508 mae: 0.718462 (1898.8889995563243 steps/sec)\n",
      "Step #11914\tEpoch   3 Batch 2538/3125   Loss: 0.791042 mae: 0.725259 (1741.7916645902892 steps/sec)\n",
      "Step #11915\tEpoch   3 Batch 2539/3125   Loss: 0.864675 mae: 0.744912 (1921.5950740360652 steps/sec)\n",
      "Step #11916\tEpoch   3 Batch 2540/3125   Loss: 0.745398 mae: 0.704350 (1940.6753467884475 steps/sec)\n",
      "Step #11917\tEpoch   3 Batch 2541/3125   Loss: 0.770257 mae: 0.697644 (1923.6749894512832 steps/sec)\n",
      "Step #11918\tEpoch   3 Batch 2542/3125   Loss: 0.961615 mae: 0.769363 (1929.0187276941756 steps/sec)\n",
      "Step #11919\tEpoch   3 Batch 2543/3125   Loss: 0.765793 mae: 0.695858 (1623.0570389288755 steps/sec)\n",
      "Step #11920\tEpoch   3 Batch 2544/3125   Loss: 0.902719 mae: 0.751771 (1951.6015559567459 steps/sec)\n",
      "Step #11921\tEpoch   3 Batch 2545/3125   Loss: 0.873278 mae: 0.749173 (1988.4626325071588 steps/sec)\n",
      "Step #11922\tEpoch   3 Batch 2546/3125   Loss: 0.751701 mae: 0.706114 (2149.249815528409 steps/sec)\n",
      "Step #11923\tEpoch   3 Batch 2547/3125   Loss: 1.006529 mae: 0.788051 (2053.313751407451 steps/sec)\n",
      "Step #11924\tEpoch   3 Batch 2548/3125   Loss: 0.859538 mae: 0.723971 (1928.3978997894271 steps/sec)\n",
      "Step #11925\tEpoch   3 Batch 2549/3125   Loss: 0.616139 mae: 0.638158 (2084.2919188606297 steps/sec)\n",
      "Step #11926\tEpoch   3 Batch 2550/3125   Loss: 0.883993 mae: 0.737669 (2012.641196172708 steps/sec)\n",
      "Step #11927\tEpoch   3 Batch 2551/3125   Loss: 0.762555 mae: 0.689168 (1820.9818870152649 steps/sec)\n",
      "Step #11928\tEpoch   3 Batch 2552/3125   Loss: 0.788469 mae: 0.689735 (1984.5299266619352 steps/sec)\n",
      "Step #11929\tEpoch   3 Batch 2553/3125   Loss: 0.854247 mae: 0.739437 (2022.501470715877 steps/sec)\n",
      "Step #11930\tEpoch   3 Batch 2554/3125   Loss: 0.831974 mae: 0.706887 (1984.6801745104906 steps/sec)\n",
      "Step #11931\tEpoch   3 Batch 2555/3125   Loss: 0.702406 mae: 0.650189 (2281.0254625349417 steps/sec)\n",
      "Step #11932\tEpoch   3 Batch 2556/3125   Loss: 0.983545 mae: 0.785605 (2058.796618988249 steps/sec)\n",
      "Step #11933\tEpoch   3 Batch 2557/3125   Loss: 0.911463 mae: 0.753235 (2077.4371217149255 steps/sec)\n",
      "Step #11934\tEpoch   3 Batch 2558/3125   Loss: 0.837033 mae: 0.740465 (1952.5827715913747 steps/sec)\n",
      "Step #11935\tEpoch   3 Batch 2559/3125   Loss: 0.905217 mae: 0.757828 (1814.098250045414 steps/sec)\n",
      "Step #11936\tEpoch   3 Batch 2560/3125   Loss: 0.615742 mae: 0.621702 (1940.3162384463792 steps/sec)\n",
      "Step #11937\tEpoch   3 Batch 2561/3125   Loss: 0.639592 mae: 0.628097 (2085.2245157697967 steps/sec)\n",
      "Step #11938\tEpoch   3 Batch 2562/3125   Loss: 0.861156 mae: 0.730595 (1917.3961142857142 steps/sec)\n",
      "Step #11939\tEpoch   3 Batch 2563/3125   Loss: 0.768846 mae: 0.681572 (1967.346479296047 steps/sec)\n",
      "Step #11940\tEpoch   3 Batch 2564/3125   Loss: 0.814095 mae: 0.720553 (1848.7208871806627 steps/sec)\n",
      "Step #11941\tEpoch   3 Batch 2565/3125   Loss: 0.787961 mae: 0.705127 (1703.8795589896085 steps/sec)\n",
      "Step #11942\tEpoch   3 Batch 2566/3125   Loss: 0.752909 mae: 0.672533 (1401.8114610001137 steps/sec)\n",
      "Step #11943\tEpoch   3 Batch 2567/3125   Loss: 0.806559 mae: 0.703496 (1644.5800233690666 steps/sec)\n",
      "Step #11944\tEpoch   3 Batch 2568/3125   Loss: 0.873985 mae: 0.723798 (1636.0737076968685 steps/sec)\n",
      "Step #11945\tEpoch   3 Batch 2569/3125   Loss: 0.787095 mae: 0.694193 (1657.2643291213265 steps/sec)\n",
      "Step #11946\tEpoch   3 Batch 2570/3125   Loss: 0.891711 mae: 0.769472 (1734.5022661858604 steps/sec)\n",
      "Step #11947\tEpoch   3 Batch 2571/3125   Loss: 0.725435 mae: 0.676230 (2017.7534035695387 steps/sec)\n",
      "Step #11948\tEpoch   3 Batch 2572/3125   Loss: 0.880695 mae: 0.742142 (1939.7957673523754 steps/sec)\n",
      "Step #11949\tEpoch   3 Batch 2573/3125   Loss: 0.812299 mae: 0.731014 (1800.3931904226367 steps/sec)\n",
      "Step #11950\tEpoch   3 Batch 2574/3125   Loss: 0.883507 mae: 0.716865 (1632.7618691705206 steps/sec)\n",
      "Step #11951\tEpoch   3 Batch 2575/3125   Loss: 0.747779 mae: 0.667846 (1888.8496595453398 steps/sec)\n",
      "Step #11952\tEpoch   3 Batch 2576/3125   Loss: 0.873365 mae: 0.736597 (2070.484163968091 steps/sec)\n",
      "Step #11953\tEpoch   3 Batch 2577/3125   Loss: 0.759945 mae: 0.684330 (1999.5347152037527 steps/sec)\n",
      "Step #11954\tEpoch   3 Batch 2578/3125   Loss: 0.816358 mae: 0.727850 (2041.4808180906675 steps/sec)\n",
      "Step #11955\tEpoch   3 Batch 2579/3125   Loss: 0.698082 mae: 0.656030 (2056.5152584921943 steps/sec)\n",
      "Step #11956\tEpoch   3 Batch 2580/3125   Loss: 0.810424 mae: 0.701568 (2247.39002304024 steps/sec)\n",
      "Step #11957\tEpoch   3 Batch 2581/3125   Loss: 0.764149 mae: 0.703665 (1992.7706721906536 steps/sec)\n",
      "Step #11958\tEpoch   3 Batch 2582/3125   Loss: 0.901351 mae: 0.756996 (1826.2624855224544 steps/sec)\n",
      "Step #11959\tEpoch   3 Batch 2583/3125   Loss: 0.789685 mae: 0.711586 (2044.326600639476 steps/sec)\n",
      "Step #11960\tEpoch   3 Batch 2584/3125   Loss: 0.958164 mae: 0.775913 (1884.4367766515707 steps/sec)\n",
      "Step #11961\tEpoch   3 Batch 2585/3125   Loss: 0.719947 mae: 0.689490 (2048.0 steps/sec)\n",
      "Step #11962\tEpoch   3 Batch 2586/3125   Loss: 0.880618 mae: 0.721546 (2016.3759783089438 steps/sec)\n",
      "Step #11963\tEpoch   3 Batch 2587/3125   Loss: 0.788498 mae: 0.729205 (2162.458238812126 steps/sec)\n",
      "Step #11964\tEpoch   3 Batch 2588/3125   Loss: 0.804802 mae: 0.710764 (2057.927894333994 steps/sec)\n",
      "Step #11965\tEpoch   3 Batch 2589/3125   Loss: 0.849201 mae: 0.727853 (2005.4622652335233 steps/sec)\n",
      "Step #11966\tEpoch   3 Batch 2590/3125   Loss: 0.858120 mae: 0.732698 (1904.1130218453213 steps/sec)\n",
      "Step #11967\tEpoch   3 Batch 2591/3125   Loss: 0.975610 mae: 0.766826 (1921.3662058287296 steps/sec)\n",
      "Step #11968\tEpoch   3 Batch 2592/3125   Loss: 0.760228 mae: 0.694074 (2123.956328870344 steps/sec)\n",
      "Step #11969\tEpoch   3 Batch 2593/3125   Loss: 0.797763 mae: 0.699219 (2026.0184907884186 steps/sec)\n",
      "Step #11970\tEpoch   3 Batch 2594/3125   Loss: 0.729448 mae: 0.696815 (2106.5895210543235 steps/sec)\n",
      "Step #11971\tEpoch   3 Batch 2595/3125   Loss: 0.771031 mae: 0.691209 (2013.066223830596 steps/sec)\n",
      "Step #11972\tEpoch   3 Batch 2596/3125   Loss: 0.828455 mae: 0.717199 (1994.2487637885126 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #11973\tEpoch   3 Batch 2597/3125   Loss: 0.811965 mae: 0.693918 (1980.519222959892 steps/sec)\n",
      "Step #11974\tEpoch   3 Batch 2598/3125   Loss: 0.819181 mae: 0.727983 (1851.0216510587218 steps/sec)\n",
      "Step #11975\tEpoch   3 Batch 2599/3125   Loss: 0.951157 mae: 0.732485 (2032.8528639143879 steps/sec)\n",
      "Step #11976\tEpoch   3 Batch 2600/3125   Loss: 0.780160 mae: 0.679147 (1951.5833945970091 steps/sec)\n",
      "Step #11977\tEpoch   3 Batch 2601/3125   Loss: 0.841880 mae: 0.721519 (1332.1255931245196 steps/sec)\n",
      "Step #11978\tEpoch   3 Batch 2602/3125   Loss: 0.787432 mae: 0.711288 (1666.0459499825224 steps/sec)\n",
      "Step #11979\tEpoch   3 Batch 2603/3125   Loss: 0.948747 mae: 0.763786 (1724.021933033549 steps/sec)\n",
      "Step #11980\tEpoch   3 Batch 2604/3125   Loss: 0.903476 mae: 0.727972 (1382.5886882511554 steps/sec)\n",
      "Step #11981\tEpoch   3 Batch 2605/3125   Loss: 0.737542 mae: 0.685352 (1431.3565163976384 steps/sec)\n",
      "Step #11982\tEpoch   3 Batch 2606/3125   Loss: 0.798259 mae: 0.710785 (1636.993208961049 steps/sec)\n",
      "Step #11983\tEpoch   3 Batch 2607/3125   Loss: 0.755575 mae: 0.712485 (1491.1596356629384 steps/sec)\n",
      "Step #11984\tEpoch   3 Batch 2608/3125   Loss: 0.751293 mae: 0.670859 (1561.6125813513634 steps/sec)\n",
      "Step #11985\tEpoch   3 Batch 2609/3125   Loss: 0.870776 mae: 0.722885 (1597.9518439500152 steps/sec)\n",
      "Step #11986\tEpoch   3 Batch 2610/3125   Loss: 0.862698 mae: 0.739074 (1401.0248050932946 steps/sec)\n",
      "Step #11987\tEpoch   3 Batch 2611/3125   Loss: 0.770920 mae: 0.692465 (1349.5968234968564 steps/sec)\n",
      "Step #11988\tEpoch   3 Batch 2612/3125   Loss: 0.666013 mae: 0.650262 (1494.1875543269161 steps/sec)\n",
      "Step #11989\tEpoch   3 Batch 2613/3125   Loss: 0.962079 mae: 0.770677 (1472.1918414051147 steps/sec)\n",
      "Step #11990\tEpoch   3 Batch 2614/3125   Loss: 0.898522 mae: 0.760523 (1387.5375474719137 steps/sec)\n",
      "Step #11991\tEpoch   3 Batch 2615/3125   Loss: 0.959303 mae: 0.777018 (1489.518019233774 steps/sec)\n",
      "Step #11992\tEpoch   3 Batch 2616/3125   Loss: 0.779501 mae: 0.693608 (1246.5684701071127 steps/sec)\n",
      "Step #11993\tEpoch   3 Batch 2617/3125   Loss: 0.743515 mae: 0.687737 (1283.0384455375279 steps/sec)\n",
      "Step #11994\tEpoch   3 Batch 2618/3125   Loss: 0.760596 mae: 0.709860 (1482.232871096787 steps/sec)\n",
      "Step #11995\tEpoch   3 Batch 2619/3125   Loss: 0.917147 mae: 0.759796 (1652.732287808338 steps/sec)\n",
      "Step #11996\tEpoch   3 Batch 2620/3125   Loss: 0.724801 mae: 0.678894 (1528.4361813000605 steps/sec)\n",
      "Step #11997\tEpoch   3 Batch 2621/3125   Loss: 0.759652 mae: 0.685433 (1730.280603615422 steps/sec)\n",
      "Step #11998\tEpoch   3 Batch 2622/3125   Loss: 0.737783 mae: 0.666981 (1619.747594111559 steps/sec)\n",
      "Step #11999\tEpoch   3 Batch 2623/3125   Loss: 0.853646 mae: 0.733986 (1397.6075120124224 steps/sec)\n",
      "Step #12000\tEpoch   3 Batch 2624/3125   Loss: 0.933373 mae: 0.754283 (1751.3044059190968 steps/sec)\n",
      "Step #12001\tEpoch   3 Batch 2625/3125   Loss: 0.887808 mae: 0.755652 (1947.198261854579 steps/sec)\n",
      "Step #12002\tEpoch   3 Batch 2626/3125   Loss: 0.763872 mae: 0.688686 (1984.2294992004995 steps/sec)\n",
      "Step #12003\tEpoch   3 Batch 2627/3125   Loss: 0.735751 mae: 0.694367 (1961.6417854603958 steps/sec)\n",
      "Step #12004\tEpoch   3 Batch 2628/3125   Loss: 0.949249 mae: 0.784218 (2120.0913888271093 steps/sec)\n",
      "Step #12005\tEpoch   3 Batch 2629/3125   Loss: 0.807226 mae: 0.708089 (1715.8547560995567 steps/sec)\n",
      "Step #12006\tEpoch   3 Batch 2630/3125   Loss: 0.585666 mae: 0.605649 (1303.7935965184954 steps/sec)\n",
      "Step #12007\tEpoch   3 Batch 2631/3125   Loss: 0.614832 mae: 0.622461 (1309.9995002748487 steps/sec)\n",
      "Step #12008\tEpoch   3 Batch 2632/3125   Loss: 0.889155 mae: 0.735758 (1705.3343741868332 steps/sec)\n",
      "Step #12009\tEpoch   3 Batch 2633/3125   Loss: 0.666303 mae: 0.637067 (1660.676417253312 steps/sec)\n",
      "Step #12010\tEpoch   3 Batch 2634/3125   Loss: 0.850542 mae: 0.725434 (1686.6812508042724 steps/sec)\n",
      "Step #12011\tEpoch   3 Batch 2635/3125   Loss: 0.775988 mae: 0.698488 (1677.0105475278483 steps/sec)\n",
      "Step #12012\tEpoch   3 Batch 2636/3125   Loss: 0.956054 mae: 0.759087 (1445.7932327716956 steps/sec)\n",
      "Step #12013\tEpoch   3 Batch 2637/3125   Loss: 0.713991 mae: 0.664031 (1193.0006598857715 steps/sec)\n",
      "Step #12014\tEpoch   3 Batch 2638/3125   Loss: 0.858203 mae: 0.706428 (1522.8645496728657 steps/sec)\n",
      "Step #12015\tEpoch   3 Batch 2639/3125   Loss: 0.821515 mae: 0.734165 (1628.1351168802937 steps/sec)\n",
      "Step #12016\tEpoch   3 Batch 2640/3125   Loss: 0.741622 mae: 0.693506 (1399.3794332156704 steps/sec)\n",
      "Step #12017\tEpoch   3 Batch 2641/3125   Loss: 0.705578 mae: 0.675740 (1615.45548383121 steps/sec)\n",
      "Step #12018\tEpoch   3 Batch 2642/3125   Loss: 0.798374 mae: 0.728942 (1630.046014177341 steps/sec)\n",
      "Step #12019\tEpoch   3 Batch 2643/3125   Loss: 0.738991 mae: 0.675339 (1444.3394537114837 steps/sec)\n",
      "Step #12020\tEpoch   3 Batch 2644/3125   Loss: 0.930685 mae: 0.754418 (1395.1714732395303 steps/sec)\n",
      "Step #12021\tEpoch   3 Batch 2645/3125   Loss: 0.801030 mae: 0.724260 (2009.9599378941516 steps/sec)\n",
      "Step #12022\tEpoch   3 Batch 2646/3125   Loss: 0.716022 mae: 0.655000 (2012.9502893946228 steps/sec)\n",
      "Step #12023\tEpoch   3 Batch 2647/3125   Loss: 0.734605 mae: 0.687538 (1914.1759234750225 steps/sec)\n",
      "Step #12024\tEpoch   3 Batch 2648/3125   Loss: 0.797005 mae: 0.705594 (1616.1900137948041 steps/sec)\n",
      "Step #12025\tEpoch   3 Batch 2649/3125   Loss: 0.858771 mae: 0.740453 (1255.6818930262914 steps/sec)\n",
      "Step #12026\tEpoch   3 Batch 2650/3125   Loss: 0.727841 mae: 0.694541 (1422.878389014031 steps/sec)\n",
      "Step #12027\tEpoch   3 Batch 2651/3125   Loss: 0.904361 mae: 0.740119 (1279.8125274617976 steps/sec)\n",
      "Step #12028\tEpoch   3 Batch 2652/3125   Loss: 0.817920 mae: 0.724685 (1483.7219832465473 steps/sec)\n",
      "Step #12029\tEpoch   3 Batch 2653/3125   Loss: 0.822161 mae: 0.729910 (1522.4776037053707 steps/sec)\n",
      "Step #12030\tEpoch   3 Batch 2654/3125   Loss: 0.855365 mae: 0.745411 (1263.397854126379 steps/sec)\n",
      "Step #12031\tEpoch   3 Batch 2655/3125   Loss: 0.888092 mae: 0.727541 (1580.978371491681 steps/sec)\n",
      "Step #12032\tEpoch   3 Batch 2656/3125   Loss: 0.934620 mae: 0.773308 (1445.0361060581004 steps/sec)\n",
      "Step #12033\tEpoch   3 Batch 2657/3125   Loss: 0.846611 mae: 0.738990 (1500.774306201606 steps/sec)\n",
      "Step #12034\tEpoch   3 Batch 2658/3125   Loss: 0.861049 mae: 0.718853 (1620.9995826054694 steps/sec)\n",
      "Step #12035\tEpoch   3 Batch 2659/3125   Loss: 0.713176 mae: 0.669965 (1394.9394705334576 steps/sec)\n",
      "Step #12036\tEpoch   3 Batch 2660/3125   Loss: 0.881247 mae: 0.732989 (1722.5209241965026 steps/sec)\n",
      "Step #12037\tEpoch   3 Batch 2661/3125   Loss: 0.792111 mae: 0.697829 (1925.7417287260907 steps/sec)\n",
      "Step #12038\tEpoch   3 Batch 2662/3125   Loss: 0.676430 mae: 0.670962 (1760.0053711101414 steps/sec)\n",
      "Step #12039\tEpoch   3 Batch 2663/3125   Loss: 0.741102 mae: 0.685315 (1185.333898544581 steps/sec)\n",
      "Step #12040\tEpoch   3 Batch 2664/3125   Loss: 0.754133 mae: 0.707915 (1695.6548456475687 steps/sec)\n",
      "Step #12041\tEpoch   3 Batch 2665/3125   Loss: 0.820771 mae: 0.727651 (1422.5598795286967 steps/sec)\n",
      "Step #12042\tEpoch   3 Batch 2666/3125   Loss: 0.876464 mae: 0.738541 (1571.3358758607255 steps/sec)\n",
      "Step #12043\tEpoch   3 Batch 2667/3125   Loss: 0.678624 mae: 0.665711 (1530.1200951422027 steps/sec)\n",
      "Step #12044\tEpoch   3 Batch 2668/3125   Loss: 0.838148 mae: 0.742869 (1324.8880212775366 steps/sec)\n",
      "Step #12045\tEpoch   3 Batch 2669/3125   Loss: 0.771699 mae: 0.692115 (1516.3021394434122 steps/sec)\n",
      "Step #12046\tEpoch   3 Batch 2670/3125   Loss: 0.798310 mae: 0.704830 (1391.7549308486634 steps/sec)\n",
      "Step #12047\tEpoch   3 Batch 2671/3125   Loss: 0.733959 mae: 0.645024 (1352.4515841948112 steps/sec)\n",
      "Step #12048\tEpoch   3 Batch 2672/3125   Loss: 0.845296 mae: 0.730537 (1783.944809751865 steps/sec)\n",
      "Step #12049\tEpoch   3 Batch 2673/3125   Loss: 0.795361 mae: 0.707730 (1817.7934955966994 steps/sec)\n",
      "Step #12050\tEpoch   3 Batch 2674/3125   Loss: 0.805102 mae: 0.710878 (2109.5349702755175 steps/sec)\n",
      "Step #12051\tEpoch   3 Batch 2675/3125   Loss: 0.788295 mae: 0.702483 (2031.7106015248835 steps/sec)\n",
      "Step #12052\tEpoch   3 Batch 2676/3125   Loss: 0.762622 mae: 0.684930 (1745.8662515297076 steps/sec)\n",
      "Step #12053\tEpoch   3 Batch 2677/3125   Loss: 0.780969 mae: 0.722105 (1609.6279012648902 steps/sec)\n",
      "Step #12054\tEpoch   3 Batch 2678/3125   Loss: 0.813786 mae: 0.700993 (1215.0077634353781 steps/sec)\n",
      "Step #12055\tEpoch   3 Batch 2679/3125   Loss: 0.703702 mae: 0.636262 (1476.3269788528144 steps/sec)\n",
      "Step #12056\tEpoch   3 Batch 2680/3125   Loss: 0.724911 mae: 0.677854 (1736.3979598595747 steps/sec)\n",
      "Step #12057\tEpoch   3 Batch 2681/3125   Loss: 0.732951 mae: 0.666567 (1416.037812288994 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12058\tEpoch   3 Batch 2682/3125   Loss: 0.907737 mae: 0.759405 (1487.7427959308181 steps/sec)\n",
      "Step #12059\tEpoch   3 Batch 2683/3125   Loss: 0.840970 mae: 0.692549 (1121.2737859094384 steps/sec)\n",
      "Step #12060\tEpoch   3 Batch 2684/3125   Loss: 0.854189 mae: 0.720771 (1555.8892483010357 steps/sec)\n",
      "Step #12061\tEpoch   3 Batch 2685/3125   Loss: 0.795476 mae: 0.695849 (1382.8621919777386 steps/sec)\n",
      "Step #12062\tEpoch   3 Batch 2686/3125   Loss: 0.735132 mae: 0.694325 (1387.1520795851413 steps/sec)\n",
      "Step #12063\tEpoch   3 Batch 2687/3125   Loss: 0.781668 mae: 0.700611 (1529.0379497648646 steps/sec)\n",
      "Step #12064\tEpoch   3 Batch 2688/3125   Loss: 0.666845 mae: 0.646052 (1336.6851082273156 steps/sec)\n",
      "Step #12065\tEpoch   3 Batch 2689/3125   Loss: 0.800585 mae: 0.716357 (1543.0673691026282 steps/sec)\n",
      "Step #12066\tEpoch   3 Batch 2690/3125   Loss: 0.880175 mae: 0.742819 (1895.7134850758412 steps/sec)\n",
      "Step #12067\tEpoch   3 Batch 2691/3125   Loss: 0.778912 mae: 0.704410 (1812.7810384917925 steps/sec)\n",
      "Step #12068\tEpoch   3 Batch 2692/3125   Loss: 0.925297 mae: 0.767281 (1245.0808917386528 steps/sec)\n",
      "Step #12069\tEpoch   3 Batch 2693/3125   Loss: 0.725328 mae: 0.675413 (1421.6630285938961 steps/sec)\n",
      "Step #12070\tEpoch   3 Batch 2694/3125   Loss: 0.813949 mae: 0.724424 (1337.043034746573 steps/sec)\n",
      "Step #12071\tEpoch   3 Batch 2695/3125   Loss: 0.815515 mae: 0.720036 (1304.758882853961 steps/sec)\n",
      "Step #12072\tEpoch   3 Batch 2696/3125   Loss: 0.670276 mae: 0.663420 (1264.3194752580303 steps/sec)\n",
      "Step #12073\tEpoch   3 Batch 2697/3125   Loss: 0.665710 mae: 0.646816 (1211.2743739025968 steps/sec)\n",
      "Step #12074\tEpoch   3 Batch 2698/3125   Loss: 0.809341 mae: 0.705771 (1309.1653661277232 steps/sec)\n",
      "Step #12075\tEpoch   3 Batch 2699/3125   Loss: 0.769901 mae: 0.695750 (1169.2939024934208 steps/sec)\n",
      "Step #12076\tEpoch   3 Batch 2700/3125   Loss: 0.792422 mae: 0.694308 (1160.7638235456911 steps/sec)\n",
      "Step #12077\tEpoch   3 Batch 2701/3125   Loss: 0.839403 mae: 0.718543 (1516.3459939408401 steps/sec)\n",
      "Step #12078\tEpoch   3 Batch 2702/3125   Loss: 0.831789 mae: 0.741572 (1503.2593346570422 steps/sec)\n",
      "Step #12079\tEpoch   3 Batch 2703/3125   Loss: 0.794092 mae: 0.721487 (1452.9351041645016 steps/sec)\n",
      "Step #12080\tEpoch   3 Batch 2704/3125   Loss: 0.762038 mae: 0.678747 (1810.167970031246 steps/sec)\n",
      "Step #12081\tEpoch   3 Batch 2705/3125   Loss: 0.757415 mae: 0.675391 (1761.3061443880808 steps/sec)\n",
      "Step #12082\tEpoch   3 Batch 2706/3125   Loss: 0.698997 mae: 0.682408 (1593.616875764645 steps/sec)\n",
      "Step #12083\tEpoch   3 Batch 2707/3125   Loss: 0.820857 mae: 0.711077 (1744.5301256935606 steps/sec)\n",
      "Step #12084\tEpoch   3 Batch 2708/3125   Loss: 0.709343 mae: 0.683897 (1534.4079019571977 steps/sec)\n",
      "Step #12085\tEpoch   3 Batch 2709/3125   Loss: 0.708441 mae: 0.672484 (1354.8631345009594 steps/sec)\n",
      "Step #12086\tEpoch   3 Batch 2710/3125   Loss: 0.729849 mae: 0.687578 (1399.4728166936932 steps/sec)\n",
      "Step #12087\tEpoch   3 Batch 2711/3125   Loss: 0.803577 mae: 0.721911 (1215.923559493025 steps/sec)\n",
      "Step #12088\tEpoch   3 Batch 2712/3125   Loss: 0.852843 mae: 0.717532 (1150.5365489696944 steps/sec)\n",
      "Step #12089\tEpoch   3 Batch 2713/3125   Loss: 0.835172 mae: 0.745647 (1403.5470960660696 steps/sec)\n",
      "Step #12090\tEpoch   3 Batch 2714/3125   Loss: 0.838798 mae: 0.737691 (1594.2347163729798 steps/sec)\n",
      "Step #12091\tEpoch   3 Batch 2715/3125   Loss: 0.777793 mae: 0.685390 (1285.8784359651972 steps/sec)\n",
      "Step #12092\tEpoch   3 Batch 2716/3125   Loss: 0.750710 mae: 0.690538 (1501.3222418693222 steps/sec)\n",
      "Step #12093\tEpoch   3 Batch 2717/3125   Loss: 0.787099 mae: 0.713789 (1064.9225613162037 steps/sec)\n",
      "Step #12094\tEpoch   3 Batch 2718/3125   Loss: 0.738138 mae: 0.688275 (1574.5566483970267 steps/sec)\n",
      "Step #12095\tEpoch   3 Batch 2719/3125   Loss: 0.762700 mae: 0.692453 (1596.9905344999581 steps/sec)\n",
      "Step #12096\tEpoch   3 Batch 2720/3125   Loss: 0.700770 mae: 0.657418 (1575.1006429129363 steps/sec)\n",
      "Step #12097\tEpoch   3 Batch 2721/3125   Loss: 0.806159 mae: 0.720306 (1573.2691167975754 steps/sec)\n",
      "Step #12098\tEpoch   3 Batch 2722/3125   Loss: 0.784267 mae: 0.707561 (1324.5784015259653 steps/sec)\n",
      "Step #12099\tEpoch   3 Batch 2723/3125   Loss: 0.729918 mae: 0.680994 (1323.8676607053803 steps/sec)\n",
      "Step #12100\tEpoch   3 Batch 2724/3125   Loss: 0.761949 mae: 0.697463 (1327.3197931632478 steps/sec)\n",
      "Step #12101\tEpoch   3 Batch 2725/3125   Loss: 0.849793 mae: 0.709693 (1515.009571970381 steps/sec)\n",
      "Step #12102\tEpoch   3 Batch 2726/3125   Loss: 0.756101 mae: 0.686846 (1233.5897978871085 steps/sec)\n",
      "Step #12103\tEpoch   3 Batch 2727/3125   Loss: 0.791638 mae: 0.695697 (1406.2387683394577 steps/sec)\n",
      "Step #12104\tEpoch   3 Batch 2728/3125   Loss: 0.884491 mae: 0.743004 (1361.8666025936582 steps/sec)\n",
      "Step #12105\tEpoch   3 Batch 2729/3125   Loss: 0.905703 mae: 0.751848 (1042.1200661899531 steps/sec)\n",
      "Step #12106\tEpoch   3 Batch 2730/3125   Loss: 0.927451 mae: 0.742074 (1468.4293076406004 steps/sec)\n",
      "Step #12107\tEpoch   3 Batch 2731/3125   Loss: 0.700639 mae: 0.656074 (1489.3804995490282 steps/sec)\n",
      "Step #12108\tEpoch   3 Batch 2732/3125   Loss: 0.774716 mae: 0.710984 (1512.7692418668398 steps/sec)\n",
      "Step #12109\tEpoch   3 Batch 2733/3125   Loss: 0.804477 mae: 0.718011 (1447.8394443792415 steps/sec)\n",
      "Step #12110\tEpoch   3 Batch 2734/3125   Loss: 0.717768 mae: 0.688329 (1879.4884433729758 steps/sec)\n",
      "Step #12111\tEpoch   3 Batch 2735/3125   Loss: 0.700323 mae: 0.650421 (1981.3612486300594 steps/sec)\n",
      "Step #12112\tEpoch   3 Batch 2736/3125   Loss: 0.818209 mae: 0.721549 (1436.6613232493457 steps/sec)\n",
      "Step #12113\tEpoch   3 Batch 2737/3125   Loss: 0.780911 mae: 0.713344 (1491.1596356629384 steps/sec)\n",
      "Step #12114\tEpoch   3 Batch 2738/3125   Loss: 0.796724 mae: 0.738218 (1638.656040006251 steps/sec)\n",
      "Step #12115\tEpoch   3 Batch 2739/3125   Loss: 0.850796 mae: 0.733383 (1540.019239666023 steps/sec)\n",
      "Step #12116\tEpoch   3 Batch 2740/3125   Loss: 0.806399 mae: 0.700100 (1447.010280825226 steps/sec)\n",
      "Step #12117\tEpoch   3 Batch 2741/3125   Loss: 0.766010 mae: 0.696482 (1818.1874929557928 steps/sec)\n",
      "Step #12118\tEpoch   3 Batch 2742/3125   Loss: 0.792685 mae: 0.690170 (1910.3746686464378 steps/sec)\n",
      "Step #12119\tEpoch   3 Batch 2743/3125   Loss: 0.711573 mae: 0.655528 (1715.7986025886473 steps/sec)\n",
      "Step #12120\tEpoch   3 Batch 2744/3125   Loss: 0.829360 mae: 0.718189 (1322.99908526007 steps/sec)\n",
      "Step #12121\tEpoch   3 Batch 2745/3125   Loss: 0.835165 mae: 0.711611 (1470.117488713793 steps/sec)\n",
      "Step #12122\tEpoch   3 Batch 2746/3125   Loss: 0.654932 mae: 0.650058 (1637.7858303136325 steps/sec)\n",
      "Step #12123\tEpoch   3 Batch 2747/3125   Loss: 0.881488 mae: 0.734414 (1833.9282747282537 steps/sec)\n",
      "Step #12124\tEpoch   3 Batch 2748/3125   Loss: 0.792191 mae: 0.703347 (1985.563340276463 steps/sec)\n",
      "Step #12125\tEpoch   3 Batch 2749/3125   Loss: 0.947986 mae: 0.759828 (1905.479788113648 steps/sec)\n",
      "Step #12126\tEpoch   3 Batch 2750/3125   Loss: 0.914026 mae: 0.756203 (1480.255514381507 steps/sec)\n",
      "Step #12127\tEpoch   3 Batch 2751/3125   Loss: 0.950416 mae: 0.762951 (1575.3609471011553 steps/sec)\n",
      "Step #12128\tEpoch   3 Batch 2752/3125   Loss: 0.796072 mae: 0.718842 (1422.6756846596884 steps/sec)\n",
      "Step #12129\tEpoch   3 Batch 2753/3125   Loss: 0.789918 mae: 0.706537 (1444.747411423493 steps/sec)\n",
      "Step #12130\tEpoch   3 Batch 2754/3125   Loss: 0.741303 mae: 0.695009 (1808.7160512993005 steps/sec)\n",
      "Step #12131\tEpoch   3 Batch 2755/3125   Loss: 0.864104 mae: 0.718733 (1881.7482749647816 steps/sec)\n",
      "Step #12132\tEpoch   3 Batch 2756/3125   Loss: 0.930345 mae: 0.747281 (2030.1175195059147 steps/sec)\n",
      "Step #12133\tEpoch   3 Batch 2757/3125   Loss: 0.772364 mae: 0.724570 (1822.6911644561874 steps/sec)\n",
      "Step #12134\tEpoch   3 Batch 2758/3125   Loss: 0.801833 mae: 0.721413 (1446.1322042780896 steps/sec)\n",
      "Step #12135\tEpoch   3 Batch 2759/3125   Loss: 0.790949 mae: 0.694846 (1854.721369758824 steps/sec)\n",
      "Step #12136\tEpoch   3 Batch 2760/3125   Loss: 0.727102 mae: 0.669183 (1937.9136364896458 steps/sec)\n",
      "Step #12137\tEpoch   3 Batch 2761/3125   Loss: 0.852694 mae: 0.733400 (1778.091297564946 steps/sec)\n",
      "Step #12138\tEpoch   3 Batch 2762/3125   Loss: 0.680908 mae: 0.637288 (1537.0957811721246 steps/sec)\n",
      "Step #12139\tEpoch   3 Batch 2763/3125   Loss: 0.695119 mae: 0.654055 (1509.6656228629017 steps/sec)\n",
      "Step #12140\tEpoch   3 Batch 2764/3125   Loss: 0.751373 mae: 0.693162 (1479.8481448551308 steps/sec)\n",
      "Step #12141\tEpoch   3 Batch 2765/3125   Loss: 0.765799 mae: 0.685507 (1272.39699306512 steps/sec)\n",
      "Step #12142\tEpoch   3 Batch 2766/3125   Loss: 0.696737 mae: 0.657139 (1431.9233631714428 steps/sec)\n",
      "Step #12143\tEpoch   3 Batch 2767/3125   Loss: 0.724262 mae: 0.698906 (1586.3839572758836 steps/sec)\n",
      "Step #12144\tEpoch   3 Batch 2768/3125   Loss: 0.834427 mae: 0.718994 (2037.8505490234186 steps/sec)\n",
      "Step #12145\tEpoch   3 Batch 2769/3125   Loss: 0.770972 mae: 0.694446 (1840.1396895592584 steps/sec)\n",
      "Step #12146\tEpoch   3 Batch 2770/3125   Loss: 0.782897 mae: 0.697540 (1668.4583194106322 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12147\tEpoch   3 Batch 2771/3125   Loss: 0.857691 mae: 0.723775 (1326.5809332835718 steps/sec)\n",
      "Step #12148\tEpoch   3 Batch 2772/3125   Loss: 0.907579 mae: 0.761765 (1515.0642970668978 steps/sec)\n",
      "Step #12149\tEpoch   3 Batch 2773/3125   Loss: 0.948763 mae: 0.757564 (1598.5730510942228 steps/sec)\n",
      "Step #12150\tEpoch   3 Batch 2774/3125   Loss: 0.741257 mae: 0.678271 (1565.3892662536389 steps/sec)\n",
      "Step #12151\tEpoch   3 Batch 2775/3125   Loss: 0.838047 mae: 0.719871 (1475.5790717964594 steps/sec)\n",
      "Step #12152\tEpoch   3 Batch 2776/3125   Loss: 0.752968 mae: 0.693086 (1850.0092626081741 steps/sec)\n",
      "Step #12153\tEpoch   3 Batch 2777/3125   Loss: 0.907605 mae: 0.738466 (1857.4976528316593 steps/sec)\n",
      "Step #12154\tEpoch   3 Batch 2778/3125   Loss: 0.731226 mae: 0.662430 (1590.7278740253041 steps/sec)\n",
      "Step #12155\tEpoch   3 Batch 2779/3125   Loss: 0.778930 mae: 0.674335 (1537.5691012801149 steps/sec)\n",
      "Step #12156\tEpoch   3 Batch 2780/3125   Loss: 0.822364 mae: 0.703248 (1583.9277352305855 steps/sec)\n",
      "Step #12157\tEpoch   3 Batch 2781/3125   Loss: 0.911795 mae: 0.747308 (1545.159293861071 steps/sec)\n",
      "Step #12158\tEpoch   3 Batch 2782/3125   Loss: 0.922030 mae: 0.756850 (1534.744776610926 steps/sec)\n",
      "Step #12159\tEpoch   3 Batch 2783/3125   Loss: 0.794696 mae: 0.703619 (1500.0121594461016 steps/sec)\n",
      "Step #12160\tEpoch   3 Batch 2784/3125   Loss: 0.766666 mae: 0.701038 (1433.9402807502172 steps/sec)\n",
      "Step #12161\tEpoch   3 Batch 2785/3125   Loss: 0.842962 mae: 0.746234 (1129.0185733512785 steps/sec)\n",
      "Step #12162\tEpoch   3 Batch 2786/3125   Loss: 0.905970 mae: 0.744759 (1255.3962562331265 steps/sec)\n",
      "Step #12163\tEpoch   3 Batch 2787/3125   Loss: 0.731695 mae: 0.675936 (1274.1750663774615 steps/sec)\n",
      "Step #12164\tEpoch   3 Batch 2788/3125   Loss: 0.914978 mae: 0.743469 (1310.2041071328165 steps/sec)\n",
      "Step #12165\tEpoch   3 Batch 2789/3125   Loss: 0.789471 mae: 0.697416 (1321.3902260755601 steps/sec)\n",
      "Step #12166\tEpoch   3 Batch 2790/3125   Loss: 0.788293 mae: 0.720911 (1386.4642765058609 steps/sec)\n",
      "Step #12167\tEpoch   3 Batch 2791/3125   Loss: 0.777900 mae: 0.698210 (1635.1042430101827 steps/sec)\n",
      "Step #12168\tEpoch   3 Batch 2792/3125   Loss: 0.677544 mae: 0.662475 (1782.9134962805526 steps/sec)\n",
      "Step #12169\tEpoch   3 Batch 2793/3125   Loss: 0.930869 mae: 0.779385 (2132.2704950535317 steps/sec)\n",
      "Step #12170\tEpoch   3 Batch 2794/3125   Loss: 0.763779 mae: 0.704005 (1837.4311122793183 steps/sec)\n",
      "Step #12171\tEpoch   3 Batch 2795/3125   Loss: 0.838590 mae: 0.725388 (1917.6941787523547 steps/sec)\n",
      "Step #12172\tEpoch   3 Batch 2796/3125   Loss: 0.814265 mae: 0.709956 (2004.503832845864 steps/sec)\n",
      "Step #12173\tEpoch   3 Batch 2797/3125   Loss: 0.948716 mae: 0.763937 (1525.0905388699005 steps/sec)\n",
      "Step #12174\tEpoch   3 Batch 2798/3125   Loss: 0.745560 mae: 0.686186 (1977.7549345983007 steps/sec)\n",
      "Step #12175\tEpoch   3 Batch 2799/3125   Loss: 0.819235 mae: 0.715021 (1971.2298379516487 steps/sec)\n",
      "Step #12176\tEpoch   3 Batch 2800/3125   Loss: 0.963981 mae: 0.754376 (1994.32462246567 steps/sec)\n",
      "Step #12177\tEpoch   3 Batch 2801/3125   Loss: 0.733486 mae: 0.694625 (2027.997292331496 steps/sec)\n",
      "Step #12178\tEpoch   3 Batch 2802/3125   Loss: 0.809954 mae: 0.722187 (1914.7002163810498 steps/sec)\n",
      "Step #12179\tEpoch   3 Batch 2803/3125   Loss: 0.850336 mae: 0.735243 (2096.0829976711875 steps/sec)\n",
      "Step #12180\tEpoch   3 Batch 2804/3125   Loss: 0.695113 mae: 0.647009 (2020.6891235643259 steps/sec)\n",
      "Step #12181\tEpoch   3 Batch 2805/3125   Loss: 0.787976 mae: 0.692567 (1861.6363813903117 steps/sec)\n",
      "Step #12182\tEpoch   3 Batch 2806/3125   Loss: 0.778417 mae: 0.691151 (1603.7440925011088 steps/sec)\n",
      "Step #12183\tEpoch   3 Batch 2807/3125   Loss: 0.892502 mae: 0.756079 (2021.994465709575 steps/sec)\n",
      "Step #12184\tEpoch   3 Batch 2808/3125   Loss: 0.895994 mae: 0.743557 (2017.7534035695387 steps/sec)\n",
      "Step #12185\tEpoch   3 Batch 2809/3125   Loss: 0.839713 mae: 0.741510 (1876.814032575622 steps/sec)\n",
      "Step #12186\tEpoch   3 Batch 2810/3125   Loss: 0.752146 mae: 0.687520 (1997.1164376386787 steps/sec)\n",
      "Step #12187\tEpoch   3 Batch 2811/3125   Loss: 0.736386 mae: 0.673045 (2031.8483926599106 steps/sec)\n",
      "Step #12188\tEpoch   3 Batch 2812/3125   Loss: 0.847892 mae: 0.751708 (1869.719339538533 steps/sec)\n",
      "Step #12189\tEpoch   3 Batch 2813/3125   Loss: 0.890600 mae: 0.758975 (1488.3551921875887 steps/sec)\n",
      "Step #12190\tEpoch   3 Batch 2814/3125   Loss: 0.781756 mae: 0.696310 (1667.6224782715872 steps/sec)\n",
      "Step #12191\tEpoch   3 Batch 2815/3125   Loss: 0.750143 mae: 0.675877 (1970.1187434240192 steps/sec)\n",
      "Step #12192\tEpoch   3 Batch 2816/3125   Loss: 0.845840 mae: 0.704770 (1952.6736748014414 steps/sec)\n",
      "Step #12193\tEpoch   3 Batch 2817/3125   Loss: 0.738908 mae: 0.666885 (1822.56444127718 steps/sec)\n",
      "Step #12194\tEpoch   3 Batch 2818/3125   Loss: 0.897564 mae: 0.721278 (1877.2340330304794 steps/sec)\n",
      "Step #12195\tEpoch   3 Batch 2819/3125   Loss: 0.870872 mae: 0.747216 (1964.15880716674 steps/sec)\n",
      "Step #12196\tEpoch   3 Batch 2820/3125   Loss: 0.994921 mae: 0.780370 (1916.0822293284605 steps/sec)\n",
      "Step #12197\tEpoch   3 Batch 2821/3125   Loss: 0.783294 mae: 0.707955 (1775.637345796608 steps/sec)\n",
      "Step #12198\tEpoch   3 Batch 2822/3125   Loss: 0.785145 mae: 0.709364 (1506.845338602479 steps/sec)\n",
      "Step #12199\tEpoch   3 Batch 2823/3125   Loss: 0.732318 mae: 0.673736 (1827.6630790012637 steps/sec)\n",
      "Step #12200\tEpoch   3 Batch 2824/3125   Loss: 0.817801 mae: 0.737457 (1955.0948110305224 steps/sec)\n",
      "Step #12201\tEpoch   3 Batch 2825/3125   Loss: 0.785853 mae: 0.670922 (1888.4414508518532 steps/sec)\n",
      "Step #12202\tEpoch   3 Batch 2826/3125   Loss: 0.829016 mae: 0.717723 (1950.3310765568037 steps/sec)\n",
      "Step #12203\tEpoch   3 Batch 2827/3125   Loss: 0.770883 mae: 0.720024 (2148.787360267221 steps/sec)\n",
      "Step #12204\tEpoch   3 Batch 2828/3125   Loss: 0.951778 mae: 0.754594 (1889.564449570216 steps/sec)\n",
      "Step #12205\tEpoch   3 Batch 2829/3125   Loss: 0.690153 mae: 0.654656 (1453.6199236159728 steps/sec)\n",
      "Step #12206\tEpoch   3 Batch 2830/3125   Loss: 0.711402 mae: 0.647297 (1798.1856532848592 steps/sec)\n",
      "Step #12207\tEpoch   3 Batch 2831/3125   Loss: 0.863317 mae: 0.736826 (2062.0963618485744 steps/sec)\n",
      "Step #12208\tEpoch   3 Batch 2832/3125   Loss: 0.731439 mae: 0.679260 (1985.5445413317427 steps/sec)\n",
      "Step #12209\tEpoch   3 Batch 2833/3125   Loss: 0.673475 mae: 0.646213 (2030.628606839924 steps/sec)\n",
      "Step #12210\tEpoch   3 Batch 2834/3125   Loss: 0.727829 mae: 0.666643 (1895.8163080817212 steps/sec)\n",
      "Step #12211\tEpoch   3 Batch 2835/3125   Loss: 0.879158 mae: 0.720880 (2262.055873152842 steps/sec)\n",
      "Step #12212\tEpoch   3 Batch 2836/3125   Loss: 0.785623 mae: 0.686984 (2022.579494054221 steps/sec)\n",
      "Step #12213\tEpoch   3 Batch 2837/3125   Loss: 0.728783 mae: 0.681558 (1913.564611201343 steps/sec)\n",
      "Step #12214\tEpoch   3 Batch 2838/3125   Loss: 0.593173 mae: 0.590138 (1518.7839022023304 steps/sec)\n",
      "Step #12215\tEpoch   3 Batch 2839/3125   Loss: 0.799682 mae: 0.717465 (2051.706696668786 steps/sec)\n",
      "Step #12216\tEpoch   3 Batch 2840/3125   Loss: 0.798251 mae: 0.707220 (2274.099697459309 steps/sec)\n",
      "Step #12217\tEpoch   3 Batch 2841/3125   Loss: 0.853045 mae: 0.713833 (2127.4900075070504 steps/sec)\n",
      "Step #12218\tEpoch   3 Batch 2842/3125   Loss: 0.792223 mae: 0.710614 (1810.0273598991912 steps/sec)\n",
      "Step #12219\tEpoch   3 Batch 2843/3125   Loss: 0.842075 mae: 0.726528 (1945.319790362228 steps/sec)\n",
      "Step #12220\tEpoch   3 Batch 2844/3125   Loss: 0.833814 mae: 0.719177 (1971.8414743077428 steps/sec)\n",
      "Step #12221\tEpoch   3 Batch 2845/3125   Loss: 0.804557 mae: 0.708318 (1778.664359744203 steps/sec)\n",
      "Step #12222\tEpoch   3 Batch 2846/3125   Loss: 0.733936 mae: 0.675769 (1526.6114884293129 steps/sec)\n",
      "Step #12223\tEpoch   3 Batch 2847/3125   Loss: 0.773517 mae: 0.694067 (1865.9430025535853 steps/sec)\n",
      "Step #12224\tEpoch   3 Batch 2848/3125   Loss: 0.730337 mae: 0.711941 (1930.1372258474225 steps/sec)\n",
      "Step #12225\tEpoch   3 Batch 2849/3125   Loss: 0.790542 mae: 0.721317 (2088.734400366523 steps/sec)\n",
      "Step #12226\tEpoch   3 Batch 2850/3125   Loss: 0.825771 mae: 0.721537 (2179.969023191027 steps/sec)\n",
      "Step #12227\tEpoch   3 Batch 2851/3125   Loss: 0.796946 mae: 0.688417 (2141.0652482414316 steps/sec)\n",
      "Step #12228\tEpoch   3 Batch 2852/3125   Loss: 0.843415 mae: 0.722556 (2143.2533802082803 steps/sec)\n",
      "Step #12229\tEpoch   3 Batch 2853/3125   Loss: 0.816912 mae: 0.734638 (1814.286579404971 steps/sec)\n",
      "Step #12230\tEpoch   3 Batch 2854/3125   Loss: 0.873546 mae: 0.730502 (1670.797813859367 steps/sec)\n",
      "Step #12231\tEpoch   3 Batch 2855/3125   Loss: 0.887766 mae: 0.752677 (2116.2832001291677 steps/sec)\n",
      "Step #12232\tEpoch   3 Batch 2856/3125   Loss: 0.796660 mae: 0.703725 (2188.02049099085 steps/sec)\n",
      "Step #12233\tEpoch   3 Batch 2857/3125   Loss: 0.675102 mae: 0.645241 (2038.325914118539 steps/sec)\n",
      "Step #12234\tEpoch   3 Batch 2858/3125   Loss: 1.018460 mae: 0.787887 (1826.1988731854715 steps/sec)\n",
      "Step #12235\tEpoch   3 Batch 2859/3125   Loss: 0.765368 mae: 0.688386 (2175.2206698405785 steps/sec)\n",
      "Step #12236\tEpoch   3 Batch 2860/3125   Loss: 0.761588 mae: 0.696543 (2131.8369877913656 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12237\tEpoch   3 Batch 2861/3125   Loss: 0.712770 mae: 0.677987 (2024.3756938076162 steps/sec)\n",
      "Step #12238\tEpoch   3 Batch 2862/3125   Loss: 0.765918 mae: 0.694974 (1540.777312467857 steps/sec)\n",
      "Step #12239\tEpoch   3 Batch 2863/3125   Loss: 0.725627 mae: 0.669431 (1863.5222193589664 steps/sec)\n",
      "Step #12240\tEpoch   3 Batch 2864/3125   Loss: 0.846120 mae: 0.725191 (1372.9489093146883 steps/sec)\n",
      "Step #12241\tEpoch   3 Batch 2865/3125   Loss: 0.737408 mae: 0.698040 (1691.5517269192922 steps/sec)\n",
      "Step #12242\tEpoch   3 Batch 2866/3125   Loss: 0.869111 mae: 0.710182 (1534.4977207372665 steps/sec)\n",
      "Step #12243\tEpoch   3 Batch 2867/3125   Loss: 0.914289 mae: 0.786275 (1414.5478091949062 steps/sec)\n",
      "Step #12244\tEpoch   3 Batch 2868/3125   Loss: 0.757626 mae: 0.670765 (1221.447458851212 steps/sec)\n",
      "Step #12245\tEpoch   3 Batch 2869/3125   Loss: 0.691665 mae: 0.679046 (1448.4794485540428 steps/sec)\n",
      "Step #12246\tEpoch   3 Batch 2870/3125   Loss: 0.660530 mae: 0.643682 (1334.7963898825058 steps/sec)\n",
      "Step #12247\tEpoch   3 Batch 2871/3125   Loss: 0.762939 mae: 0.694149 (1358.1095345719707 steps/sec)\n",
      "Step #12248\tEpoch   3 Batch 2872/3125   Loss: 0.833265 mae: 0.720119 (1366.4631563858138 steps/sec)\n",
      "Step #12249\tEpoch   3 Batch 2873/3125   Loss: 0.759520 mae: 0.703041 (1307.0520850862267 steps/sec)\n",
      "Step #12250\tEpoch   3 Batch 2874/3125   Loss: 0.838255 mae: 0.700351 (1497.6234003656307 steps/sec)\n",
      "Step #12251\tEpoch   3 Batch 2875/3125   Loss: 0.856247 mae: 0.733755 (1387.7395447326628 steps/sec)\n",
      "Step #12252\tEpoch   3 Batch 2876/3125   Loss: 0.725517 mae: 0.687717 (1228.2938086062188 steps/sec)\n",
      "Step #12253\tEpoch   3 Batch 2877/3125   Loss: 0.769912 mae: 0.693382 (1411.3871914286483 steps/sec)\n",
      "Step #12254\tEpoch   3 Batch 2878/3125   Loss: 0.806865 mae: 0.701039 (1458.0262105885215 steps/sec)\n",
      "Step #12255\tEpoch   3 Batch 2879/3125   Loss: 0.689529 mae: 0.673236 (1406.1916224679321 steps/sec)\n",
      "Step #12256\tEpoch   3 Batch 2880/3125   Loss: 0.788599 mae: 0.726062 (1599.170352295257 steps/sec)\n",
      "Step #12257\tEpoch   3 Batch 2881/3125   Loss: 0.738949 mae: 0.685431 (1619.8476820169 steps/sec)\n",
      "Step #12258\tEpoch   3 Batch 2882/3125   Loss: 0.829479 mae: 0.732062 (1327.8072191514552 steps/sec)\n",
      "Step #12259\tEpoch   3 Batch 2883/3125   Loss: 0.944156 mae: 0.783571 (1308.5527095747668 steps/sec)\n",
      "Step #12260\tEpoch   3 Batch 2884/3125   Loss: 0.729198 mae: 0.666875 (1525.589786491107 steps/sec)\n",
      "Step #12261\tEpoch   3 Batch 2885/3125   Loss: 0.821688 mae: 0.733332 (1415.301969941354 steps/sec)\n",
      "Step #12262\tEpoch   3 Batch 2886/3125   Loss: 0.721606 mae: 0.667899 (1295.962230104683 steps/sec)\n",
      "Step #12263\tEpoch   3 Batch 2887/3125   Loss: 0.878286 mae: 0.741796 (1570.7474178544412 steps/sec)\n",
      "Step #12264\tEpoch   3 Batch 2888/3125   Loss: 0.843373 mae: 0.737084 (1499.6903581976417 steps/sec)\n",
      "Step #12265\tEpoch   3 Batch 2889/3125   Loss: 0.753840 mae: 0.699518 (1548.7652132813423 steps/sec)\n",
      "Step #12266\tEpoch   3 Batch 2890/3125   Loss: 0.746993 mae: 0.675990 (1549.8525640551907 steps/sec)\n",
      "Step #12267\tEpoch   3 Batch 2891/3125   Loss: 0.880945 mae: 0.746821 (1436.700691922998 steps/sec)\n",
      "Step #12268\tEpoch   3 Batch 2892/3125   Loss: 0.766804 mae: 0.696100 (1320.6579510818913 steps/sec)\n",
      "Step #12269\tEpoch   3 Batch 2893/3125   Loss: 0.721101 mae: 0.680959 (1643.9225523242142 steps/sec)\n",
      "Step #12270\tEpoch   3 Batch 2894/3125   Loss: 0.823543 mae: 0.720050 (1635.6908869684585 steps/sec)\n",
      "Step #12271\tEpoch   3 Batch 2895/3125   Loss: 0.776912 mae: 0.691014 (1664.5120325100006 steps/sec)\n",
      "Step #12272\tEpoch   3 Batch 2896/3125   Loss: 0.717650 mae: 0.677283 (1498.4794786784041 steps/sec)\n",
      "Step #12273\tEpoch   3 Batch 2897/3125   Loss: 0.743890 mae: 0.684318 (1777.413148683352 steps/sec)\n",
      "Step #12274\tEpoch   3 Batch 2898/3125   Loss: 0.813517 mae: 0.704707 (1411.7102198527134 steps/sec)\n",
      "Step #12275\tEpoch   3 Batch 2899/3125   Loss: 0.838357 mae: 0.734973 (1672.6367841761046 steps/sec)\n",
      "Step #12276\tEpoch   3 Batch 2900/3125   Loss: 0.754018 mae: 0.692897 (1727.8286302780639 steps/sec)\n",
      "Step #12277\tEpoch   3 Batch 2901/3125   Loss: 0.697927 mae: 0.671172 (1375.1726218188733 steps/sec)\n",
      "Step #12278\tEpoch   3 Batch 2902/3125   Loss: 0.762785 mae: 0.671208 (1503.89535884344 steps/sec)\n",
      "Step #12279\tEpoch   3 Batch 2903/3125   Loss: 0.711221 mae: 0.665096 (1540.6414833752076 steps/sec)\n",
      "Step #12280\tEpoch   3 Batch 2904/3125   Loss: 0.763513 mae: 0.694106 (1560.5318966864354 steps/sec)\n",
      "Step #12281\tEpoch   3 Batch 2905/3125   Loss: 0.893826 mae: 0.741125 (1455.1833245441173 steps/sec)\n",
      "Step #12282\tEpoch   3 Batch 2906/3125   Loss: 0.953513 mae: 0.764058 (1739.2492826220373 steps/sec)\n",
      "Step #12283\tEpoch   3 Batch 2907/3125   Loss: 0.658706 mae: 0.650607 (1557.2872344375385 steps/sec)\n",
      "Step #12284\tEpoch   3 Batch 2908/3125   Loss: 0.857618 mae: 0.728939 (1713.807531380753 steps/sec)\n",
      "Step #12285\tEpoch   3 Batch 2909/3125   Loss: 0.809214 mae: 0.722316 (1934.0711229157444 steps/sec)\n",
      "Step #12286\tEpoch   3 Batch 2910/3125   Loss: 0.861155 mae: 0.735836 (2003.6037413178688 steps/sec)\n",
      "Step #12287\tEpoch   3 Batch 2911/3125   Loss: 0.846634 mae: 0.745951 (1965.9264119990626 steps/sec)\n",
      "Step #12288\tEpoch   3 Batch 2912/3125   Loss: 0.721241 mae: 0.680821 (1871.6048941999625 steps/sec)\n",
      "Step #12289\tEpoch   3 Batch 2913/3125   Loss: 0.698113 mae: 0.654464 (1195.3398235334353 steps/sec)\n",
      "Step #12290\tEpoch   3 Batch 2914/3125   Loss: 0.759738 mae: 0.695631 (1799.0649314997984 steps/sec)\n",
      "Step #12291\tEpoch   3 Batch 2915/3125   Loss: 0.834953 mae: 0.723498 (1627.6044051564234 steps/sec)\n",
      "Step #12292\tEpoch   3 Batch 2916/3125   Loss: 1.065079 mae: 0.808097 (1606.8898934947513 steps/sec)\n",
      "Step #12293\tEpoch   3 Batch 2917/3125   Loss: 0.878288 mae: 0.740687 (1470.1587123548875 steps/sec)\n",
      "Step #12294\tEpoch   3 Batch 2918/3125   Loss: 0.736806 mae: 0.687192 (1914.018691588785 steps/sec)\n",
      "Step #12295\tEpoch   3 Batch 2919/3125   Loss: 0.746844 mae: 0.677884 (1174.118635053047 steps/sec)\n",
      "Step #12296\tEpoch   3 Batch 2920/3125   Loss: 0.780418 mae: 0.711899 (1375.208692630019 steps/sec)\n",
      "Step #12297\tEpoch   3 Batch 2921/3125   Loss: 0.761435 mae: 0.698734 (1575.9410248510217 steps/sec)\n",
      "Step #12298\tEpoch   3 Batch 2922/3125   Loss: 0.995270 mae: 0.795614 (1629.944662065535 steps/sec)\n",
      "Step #12299\tEpoch   3 Batch 2923/3125   Loss: 0.735111 mae: 0.690502 (1907.2300332854363 steps/sec)\n",
      "Step #12300\tEpoch   3 Batch 2924/3125   Loss: 0.845825 mae: 0.742372 (1977.3819738442535 steps/sec)\n",
      "Step #12301\tEpoch   3 Batch 2925/3125   Loss: 0.839541 mae: 0.731107 (2075.5455706099506 steps/sec)\n",
      "Step #12302\tEpoch   3 Batch 2926/3125   Loss: 0.693432 mae: 0.657195 (1972.6761358291787 steps/sec)\n",
      "Step #12303\tEpoch   3 Batch 2927/3125   Loss: 0.731544 mae: 0.677346 (1302.0451243589584 steps/sec)\n",
      "Step #12304\tEpoch   3 Batch 2928/3125   Loss: 0.851459 mae: 0.735748 (1678.1243498439626 steps/sec)\n",
      "Step #12305\tEpoch   3 Batch 2929/3125   Loss: 0.817973 mae: 0.721895 (1475.6206023079087 steps/sec)\n",
      "Step #12306\tEpoch   3 Batch 2930/3125   Loss: 0.640233 mae: 0.623512 (1740.837400803533 steps/sec)\n",
      "Step #12307\tEpoch   3 Batch 2931/3125   Loss: 0.737548 mae: 0.677099 (1666.3504247018348 steps/sec)\n",
      "Step #12308\tEpoch   3 Batch 2932/3125   Loss: 0.651232 mae: 0.657902 (1934.8205554017898 steps/sec)\n",
      "Step #12309\tEpoch   3 Batch 2933/3125   Loss: 0.793576 mae: 0.670202 (1275.9969334485318 steps/sec)\n",
      "Step #12310\tEpoch   3 Batch 2934/3125   Loss: 0.668200 mae: 0.660341 (1525.3678583118158 steps/sec)\n",
      "Step #12311\tEpoch   3 Batch 2935/3125   Loss: 0.848624 mae: 0.750793 (1474.5415682303972 steps/sec)\n",
      "Step #12312\tEpoch   3 Batch 2936/3125   Loss: 0.729577 mae: 0.676099 (1511.5807379323767 steps/sec)\n",
      "Step #12313\tEpoch   3 Batch 2937/3125   Loss: 0.829517 mae: 0.722708 (1932.9658782974175 steps/sec)\n",
      "Step #12314\tEpoch   3 Batch 2938/3125   Loss: 0.796786 mae: 0.722487 (1960.1016898459698 steps/sec)\n",
      "Step #12315\tEpoch   3 Batch 2939/3125   Loss: 0.777669 mae: 0.688880 (1845.6295983384377 steps/sec)\n",
      "Step #12316\tEpoch   3 Batch 2940/3125   Loss: 0.830616 mae: 0.728429 (1105.8187052787548 steps/sec)\n",
      "Step #12317\tEpoch   3 Batch 2941/3125   Loss: 0.784954 mae: 0.717410 (1241.64569777562 steps/sec)\n",
      "Step #12318\tEpoch   3 Batch 2942/3125   Loss: 0.911659 mae: 0.752057 (1493.687366899096 steps/sec)\n",
      "Step #12319\tEpoch   3 Batch 2943/3125   Loss: 1.012476 mae: 0.809972 (1603.5846733802828 steps/sec)\n",
      "Step #12320\tEpoch   3 Batch 2944/3125   Loss: 0.851113 mae: 0.726004 (1762.6681011296396 steps/sec)\n",
      "Step #12321\tEpoch   3 Batch 2945/3125   Loss: 0.793994 mae: 0.721313 (2004.4271978284557 steps/sec)\n",
      "Step #12322\tEpoch   3 Batch 2946/3125   Loss: 0.725090 mae: 0.666713 (2020.66965361083 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12323\tEpoch   3 Batch 2947/3125   Loss: 0.892728 mae: 0.744296 (1663.337061095645 steps/sec)\n",
      "Step #12324\tEpoch   3 Batch 2948/3125   Loss: 0.761742 mae: 0.694243 (1474.1269752010346 steps/sec)\n",
      "Step #12325\tEpoch   3 Batch 2949/3125   Loss: 0.853407 mae: 0.733050 (1529.6067219045397 steps/sec)\n",
      "Step #12326\tEpoch   3 Batch 2950/3125   Loss: 0.905582 mae: 0.751130 (1527.0450143445905 steps/sec)\n",
      "Step #12327\tEpoch   3 Batch 2951/3125   Loss: 0.819979 mae: 0.725058 (1512.0384723534034 steps/sec)\n",
      "Step #12328\tEpoch   3 Batch 2952/3125   Loss: 0.842811 mae: 0.732826 (1528.5921498596888 steps/sec)\n",
      "Step #12329\tEpoch   3 Batch 2953/3125   Loss: 0.849186 mae: 0.699596 (1504.2513359394613 steps/sec)\n",
      "Step #12330\tEpoch   3 Batch 2954/3125   Loss: 0.764299 mae: 0.669385 (1207.7864037411596 steps/sec)\n",
      "Step #12331\tEpoch   3 Batch 2955/3125   Loss: 0.785996 mae: 0.686949 (1672.9703641657693 steps/sec)\n",
      "Step #12332\tEpoch   3 Batch 2956/3125   Loss: 0.860003 mae: 0.728204 (1601.4417276295494 steps/sec)\n",
      "Step #12333\tEpoch   3 Batch 2957/3125   Loss: 0.863568 mae: 0.720878 (1547.6908089917492 steps/sec)\n",
      "Step #12334\tEpoch   3 Batch 2958/3125   Loss: 0.973594 mae: 0.796732 (1529.4282380396733 steps/sec)\n",
      "Step #12335\tEpoch   3 Batch 2959/3125   Loss: 0.732999 mae: 0.687646 (1448.6895732305438 steps/sec)\n",
      "Step #12336\tEpoch   3 Batch 2960/3125   Loss: 0.781899 mae: 0.706586 (1237.2651166083576 steps/sec)\n",
      "Step #12337\tEpoch   3 Batch 2961/3125   Loss: 0.770304 mae: 0.704437 (1511.5807379323767 steps/sec)\n",
      "Step #12338\tEpoch   3 Batch 2962/3125   Loss: 0.817120 mae: 0.705406 (1506.022936998657 steps/sec)\n",
      "Step #12339\tEpoch   3 Batch 2963/3125   Loss: 0.780235 mae: 0.707079 (1376.5356087955365 steps/sec)\n",
      "Step #12340\tEpoch   3 Batch 2964/3125   Loss: 0.975978 mae: 0.770796 (1427.4302672238937 steps/sec)\n",
      "Step #12341\tEpoch   3 Batch 2965/3125   Loss: 0.851624 mae: 0.753283 (1523.461938005332 steps/sec)\n",
      "Step #12342\tEpoch   3 Batch 2966/3125   Loss: 0.672031 mae: 0.660016 (1365.8223973427985 steps/sec)\n",
      "Step #12343\tEpoch   3 Batch 2967/3125   Loss: 0.933380 mae: 0.781612 (1964.5451990632318 steps/sec)\n",
      "Step #12344\tEpoch   3 Batch 2968/3125   Loss: 0.706824 mae: 0.673968 (2063.212159968518 steps/sec)\n",
      "Step #12345\tEpoch   3 Batch 2969/3125   Loss: 0.679384 mae: 0.642922 (1973.23296951449 steps/sec)\n",
      "Step #12346\tEpoch   3 Batch 2970/3125   Loss: 0.858705 mae: 0.731513 (2071.0771388222283 steps/sec)\n",
      "Step #12347\tEpoch   3 Batch 2971/3125   Loss: 0.704704 mae: 0.676190 (1879.926493657837 steps/sec)\n",
      "Step #12348\tEpoch   3 Batch 2972/3125   Loss: 0.704312 mae: 0.672047 (1898.3217770697177 steps/sec)\n",
      "Step #12349\tEpoch   3 Batch 2973/3125   Loss: 0.855962 mae: 0.718437 (1554.4361593311294 steps/sec)\n",
      "Step #12350\tEpoch   3 Batch 2974/3125   Loss: 0.716216 mae: 0.673382 (1869.7860199714694 steps/sec)\n",
      "Step #12351\tEpoch   3 Batch 2975/3125   Loss: 0.701453 mae: 0.667176 (1821.2981779653658 steps/sec)\n",
      "Step #12352\tEpoch   3 Batch 2976/3125   Loss: 0.919589 mae: 0.762571 (1511.5698428715584 steps/sec)\n",
      "Step #12353\tEpoch   3 Batch 2977/3125   Loss: 0.880000 mae: 0.745124 (1466.662936749937 steps/sec)\n",
      "Step #12354\tEpoch   3 Batch 2978/3125   Loss: 0.721085 mae: 0.654899 (1551.1594020665834 steps/sec)\n",
      "Step #12355\tEpoch   3 Batch 2979/3125   Loss: 0.879823 mae: 0.734788 (1535.621343369921 steps/sec)\n",
      "Step #12356\tEpoch   3 Batch 2980/3125   Loss: 0.769588 mae: 0.693037 (1082.602186728889 steps/sec)\n",
      "Step #12357\tEpoch   3 Batch 2981/3125   Loss: 0.664820 mae: 0.674750 (1451.4669342838356 steps/sec)\n",
      "Step #12358\tEpoch   3 Batch 2982/3125   Loss: 0.806878 mae: 0.714637 (1564.793577125973 steps/sec)\n",
      "Step #12359\tEpoch   3 Batch 2983/3125   Loss: 0.749301 mae: 0.681069 (1460.5851667676536 steps/sec)\n",
      "Step #12360\tEpoch   3 Batch 2984/3125   Loss: 0.740390 mae: 0.686234 (1412.0714266476339 steps/sec)\n",
      "Step #12361\tEpoch   3 Batch 2985/3125   Loss: 0.783231 mae: 0.702226 (1648.5877571555473 steps/sec)\n",
      "Step #12362\tEpoch   3 Batch 2986/3125   Loss: 0.807393 mae: 0.711992 (2034.5094539139884 steps/sec)\n",
      "Step #12363\tEpoch   3 Batch 2987/3125   Loss: 0.759673 mae: 0.686952 (1395.793621212928 steps/sec)\n",
      "Step #12364\tEpoch   3 Batch 2988/3125   Loss: 0.758299 mae: 0.692022 (1766.4094875509586 steps/sec)\n",
      "Step #12365\tEpoch   3 Batch 2989/3125   Loss: 0.826640 mae: 0.703430 (1415.55035065575 steps/sec)\n",
      "Step #12366\tEpoch   3 Batch 2990/3125   Loss: 0.741433 mae: 0.681204 (1959.6254835635127 steps/sec)\n",
      "Step #12367\tEpoch   3 Batch 2991/3125   Loss: 0.855453 mae: 0.738920 (1929.1429412468149 steps/sec)\n",
      "Step #12368\tEpoch   3 Batch 2992/3125   Loss: 0.812382 mae: 0.716709 (2078.0546775136495 steps/sec)\n",
      "Step #12369\tEpoch   3 Batch 2993/3125   Loss: 0.679345 mae: 0.642914 (2011.5794118211293 steps/sec)\n",
      "Step #12370\tEpoch   3 Batch 2994/3125   Loss: 0.777553 mae: 0.672168 (2045.084157353773 steps/sec)\n",
      "Step #12371\tEpoch   3 Batch 2995/3125   Loss: 0.740719 mae: 0.697758 (1161.4130886254009 steps/sec)\n",
      "Step #12372\tEpoch   3 Batch 2996/3125   Loss: 0.773705 mae: 0.691717 (1240.8816307209845 steps/sec)\n",
      "Step #12373\tEpoch   3 Batch 2997/3125   Loss: 0.948936 mae: 0.758793 (1386.1526970844652 steps/sec)\n",
      "Step #12374\tEpoch   3 Batch 2998/3125   Loss: 0.809130 mae: 0.721064 (1770.8394200646812 steps/sec)\n",
      "Step #12375\tEpoch   3 Batch 2999/3125   Loss: 0.754419 mae: 0.703181 (1854.4753550395274 steps/sec)\n",
      "Step #12376\tEpoch   3 Batch 3000/3125   Loss: 0.887321 mae: 0.722271 (2070.7294916860856 steps/sec)\n",
      "Step #12377\tEpoch   3 Batch 3001/3125   Loss: 0.776160 mae: 0.710535 (2031.5334689528238 steps/sec)\n",
      "Step #12378\tEpoch   3 Batch 3002/3125   Loss: 0.705200 mae: 0.660574 (1401.633449626392 steps/sec)\n",
      "Step #12379\tEpoch   3 Batch 3003/3125   Loss: 0.762338 mae: 0.677699 (1529.3836235815759 steps/sec)\n",
      "Step #12380\tEpoch   3 Batch 3004/3125   Loss: 0.798566 mae: 0.705063 (1645.3929199096158 steps/sec)\n",
      "Step #12381\tEpoch   3 Batch 3005/3125   Loss: 0.711094 mae: 0.666294 (1632.228137355624 steps/sec)\n",
      "Step #12382\tEpoch   3 Batch 3006/3125   Loss: 0.680158 mae: 0.659510 (1569.6070653394206 steps/sec)\n",
      "Step #12383\tEpoch   3 Batch 3007/3125   Loss: 0.794088 mae: 0.687034 (1878.2259797950849 steps/sec)\n",
      "Step #12384\tEpoch   3 Batch 3008/3125   Loss: 0.810952 mae: 0.687941 (1770.525462650277 steps/sec)\n",
      "Step #12385\tEpoch   3 Batch 3009/3125   Loss: 0.815079 mae: 0.700001 (1737.5056959875392 steps/sec)\n",
      "Step #12386\tEpoch   3 Batch 3010/3125   Loss: 0.807059 mae: 0.693885 (1098.7561954460198 steps/sec)\n",
      "Step #12387\tEpoch   3 Batch 3011/3125   Loss: 0.801919 mae: 0.684758 (1596.6865635278352 steps/sec)\n",
      "Step #12388\tEpoch   3 Batch 3012/3125   Loss: 0.799062 mae: 0.717485 (1803.7534618891164 steps/sec)\n",
      "Step #12389\tEpoch   3 Batch 3013/3125   Loss: 0.896478 mae: 0.742417 (1663.6669416767152 steps/sec)\n",
      "Step #12390\tEpoch   3 Batch 3014/3125   Loss: 0.856690 mae: 0.715729 (1524.2148718284166 steps/sec)\n",
      "Step #12391\tEpoch   3 Batch 3015/3125   Loss: 0.860257 mae: 0.724146 (1281.8229048879327 steps/sec)\n",
      "Step #12392\tEpoch   3 Batch 3016/3125   Loss: 0.782379 mae: 0.712724 (1260.8153523353753 steps/sec)\n",
      "Step #12393\tEpoch   3 Batch 3017/3125   Loss: 0.831458 mae: 0.711128 (1965.8895544494128 steps/sec)\n",
      "Step #12394\tEpoch   3 Batch 3018/3125   Loss: 0.818831 mae: 0.714592 (2044.4063170208617 steps/sec)\n",
      "Step #12395\tEpoch   3 Batch 3019/3125   Loss: 0.872004 mae: 0.717047 (1839.8652442448063 steps/sec)\n",
      "Step #12396\tEpoch   3 Batch 3020/3125   Loss: 0.909977 mae: 0.736066 (1590.0765789673212 steps/sec)\n",
      "Step #12397\tEpoch   3 Batch 3021/3125   Loss: 0.740933 mae: 0.686169 (1511.6679040733507 steps/sec)\n",
      "Step #12398\tEpoch   3 Batch 3022/3125   Loss: 0.613433 mae: 0.623468 (1184.8651091838753 steps/sec)\n",
      "Step #12399\tEpoch   3 Batch 3023/3125   Loss: 0.858589 mae: 0.713150 (1760.4486006413376 steps/sec)\n",
      "Step #12400\tEpoch   3 Batch 3024/3125   Loss: 0.681327 mae: 0.672943 (1607.1977621948884 steps/sec)\n",
      "Step #12401\tEpoch   3 Batch 3025/3125   Loss: 0.785991 mae: 0.713645 (1671.397033624764 steps/sec)\n",
      "Step #12402\tEpoch   3 Batch 3026/3125   Loss: 0.695385 mae: 0.649717 (1869.719339538533 steps/sec)\n",
      "Step #12403\tEpoch   3 Batch 3027/3125   Loss: 0.818883 mae: 0.731487 (2051.3454559682295 steps/sec)\n",
      "Step #12404\tEpoch   3 Batch 3028/3125   Loss: 0.737983 mae: 0.670813 (1866.3581510421302 steps/sec)\n",
      "Step #12405\tEpoch   3 Batch 3029/3125   Loss: 0.758625 mae: 0.696056 (2010.345290362162 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12406\tEpoch   3 Batch 3030/3125   Loss: 0.815526 mae: 0.691793 (1701.83317238637 steps/sec)\n",
      "Step #12407\tEpoch   3 Batch 3031/3125   Loss: 0.778805 mae: 0.698721 (1236.4116592774267 steps/sec)\n",
      "Step #12408\tEpoch   3 Batch 3032/3125   Loss: 0.770888 mae: 0.680707 (1684.9737269206666 steps/sec)\n",
      "Step #12409\tEpoch   3 Batch 3033/3125   Loss: 0.771705 mae: 0.710493 (1595.7753445087849 steps/sec)\n",
      "Step #12410\tEpoch   3 Batch 3034/3125   Loss: 0.737205 mae: 0.689758 (1371.700667813483 steps/sec)\n",
      "Step #12411\tEpoch   3 Batch 3035/3125   Loss: 0.862489 mae: 0.748380 (1502.5377219252869 steps/sec)\n",
      "Step #12412\tEpoch   3 Batch 3036/3125   Loss: 0.562535 mae: 0.592840 (1457.4688998540553 steps/sec)\n",
      "Step #12413\tEpoch   3 Batch 3037/3125   Loss: 0.834590 mae: 0.730982 (1122.7144486142415 steps/sec)\n",
      "Step #12414\tEpoch   3 Batch 3038/3125   Loss: 0.789783 mae: 0.734034 (1449.110005527916 steps/sec)\n",
      "Step #12415\tEpoch   3 Batch 3039/3125   Loss: 0.726041 mae: 0.682463 (1495.3595162716408 steps/sec)\n",
      "Step #12416\tEpoch   3 Batch 3040/3125   Loss: 0.876257 mae: 0.729116 (1486.8989379050213 steps/sec)\n",
      "Step #12417\tEpoch   3 Batch 3041/3125   Loss: 0.801692 mae: 0.715417 (1526.5337021400494 steps/sec)\n",
      "Step #12418\tEpoch   3 Batch 3042/3125   Loss: 0.801921 mae: 0.703037 (1416.5447459252805 steps/sec)\n",
      "Step #12419\tEpoch   3 Batch 3043/3125   Loss: 0.893876 mae: 0.750733 (1186.661913572912 steps/sec)\n",
      "Step #12420\tEpoch   3 Batch 3044/3125   Loss: 0.829811 mae: 0.709739 (1401.0716051362222 steps/sec)\n",
      "Step #12421\tEpoch   3 Batch 3045/3125   Loss: 0.854841 mae: 0.715223 (1372.4277842493095 steps/sec)\n",
      "Step #12422\tEpoch   3 Batch 3046/3125   Loss: 0.618223 mae: 0.624501 (1505.6769311401965 steps/sec)\n",
      "Step #12423\tEpoch   3 Batch 3047/3125   Loss: 0.657598 mae: 0.644966 (1471.1487737807959 steps/sec)\n",
      "Step #12424\tEpoch   3 Batch 3048/3125   Loss: 0.721303 mae: 0.675495 (1247.1392805532926 steps/sec)\n",
      "Step #12425\tEpoch   3 Batch 3049/3125   Loss: 0.763567 mae: 0.711719 (1826.882938132655 steps/sec)\n",
      "Step #12426\tEpoch   3 Batch 3050/3125   Loss: 0.796105 mae: 0.713677 (1862.9759260904327 steps/sec)\n",
      "Step #12427\tEpoch   3 Batch 3051/3125   Loss: 0.788481 mae: 0.685708 (1917.9747946809096 steps/sec)\n",
      "Step #12428\tEpoch   3 Batch 3052/3125   Loss: 0.714092 mae: 0.668899 (1937.842008482642 steps/sec)\n",
      "Step #12429\tEpoch   3 Batch 3053/3125   Loss: 0.817629 mae: 0.697136 (1926.1839156471583 steps/sec)\n",
      "Step #12430\tEpoch   3 Batch 3054/3125   Loss: 0.750693 mae: 0.675032 (1979.491051876463 steps/sec)\n",
      "Step #12431\tEpoch   3 Batch 3055/3125   Loss: 0.860578 mae: 0.735566 (1785.311619433543 steps/sec)\n",
      "Step #12432\tEpoch   3 Batch 3056/3125   Loss: 0.827078 mae: 0.721706 (1077.313333162099 steps/sec)\n",
      "Step #12433\tEpoch   3 Batch 3057/3125   Loss: 0.945153 mae: 0.753863 (1374.037359051806 steps/sec)\n",
      "Step #12434\tEpoch   3 Batch 3058/3125   Loss: 0.884745 mae: 0.748337 (1552.8018007345102 steps/sec)\n",
      "Step #12435\tEpoch   3 Batch 3059/3125   Loss: 0.683396 mae: 0.643033 (1395.552124785392 steps/sec)\n",
      "Step #12436\tEpoch   3 Batch 3060/3125   Loss: 0.685511 mae: 0.674954 (1589.6306290600105 steps/sec)\n",
      "Step #12437\tEpoch   3 Batch 3061/3125   Loss: 0.874817 mae: 0.730779 (1542.3977876984857 steps/sec)\n",
      "Step #12438\tEpoch   3 Batch 3062/3125   Loss: 0.722128 mae: 0.658862 (1262.2420175148213 steps/sec)\n",
      "Step #12439\tEpoch   3 Batch 3063/3125   Loss: 0.866803 mae: 0.768250 (1594.3316760175767 steps/sec)\n",
      "Step #12440\tEpoch   3 Batch 3064/3125   Loss: 0.831583 mae: 0.710394 (1493.0280572107956 steps/sec)\n",
      "Step #12441\tEpoch   3 Batch 3065/3125   Loss: 0.748003 mae: 0.682423 (1554.9202206536568 steps/sec)\n",
      "Step #12442\tEpoch   3 Batch 3066/3125   Loss: 0.809995 mae: 0.696672 (1486.2350731724603 steps/sec)\n",
      "Step #12443\tEpoch   3 Batch 3067/3125   Loss: 0.705694 mae: 0.657166 (1967.1250351749368 steps/sec)\n",
      "Step #12444\tEpoch   3 Batch 3068/3125   Loss: 0.914913 mae: 0.749681 (2018.6468249766579 steps/sec)\n",
      "Step #12445\tEpoch   3 Batch 3069/3125   Loss: 0.668009 mae: 0.654932 (1520.3033136875376 steps/sec)\n",
      "Step #12446\tEpoch   3 Batch 3070/3125   Loss: 0.766434 mae: 0.683700 (1635.9077967159405 steps/sec)\n",
      "Step #12447\tEpoch   3 Batch 3071/3125   Loss: 0.847364 mae: 0.743574 (1532.8937943132812 steps/sec)\n",
      "Step #12448\tEpoch   3 Batch 3072/3125   Loss: 0.832505 mae: 0.714981 (1420.4978494259492 steps/sec)\n",
      "Step #12449\tEpoch   3 Batch 3073/3125   Loss: 1.003829 mae: 0.788693 (1103.1893907911142 steps/sec)\n",
      "Step #12450\tEpoch   3 Batch 3074/3125   Loss: 0.783052 mae: 0.713370 (1131.8825561312608 steps/sec)\n",
      "Step #12451\tEpoch   3 Batch 3075/3125   Loss: 0.755053 mae: 0.696042 (832.9171035490738 steps/sec)\n",
      "Step #12452\tEpoch   3 Batch 3076/3125   Loss: 0.886920 mae: 0.750304 (1163.4943355192347 steps/sec)\n",
      "Step #12453\tEpoch   3 Batch 3077/3125   Loss: 0.689176 mae: 0.656730 (1768.0923354494946 steps/sec)\n",
      "Step #12454\tEpoch   3 Batch 3078/3125   Loss: 0.644919 mae: 0.630474 (1976.357056694813 steps/sec)\n",
      "Step #12455\tEpoch   3 Batch 3079/3125   Loss: 0.776240 mae: 0.689867 (1803.0866055077422 steps/sec)\n",
      "Step #12456\tEpoch   3 Batch 3080/3125   Loss: 0.925740 mae: 0.764311 (2031.5137894624677 steps/sec)\n",
      "Step #12457\tEpoch   3 Batch 3081/3125   Loss: 0.713695 mae: 0.639978 (1957.9423023060406 steps/sec)\n",
      "Step #12458\tEpoch   3 Batch 3082/3125   Loss: 0.794880 mae: 0.694299 (1582.5770667471606 steps/sec)\n",
      "Step #12459\tEpoch   3 Batch 3083/3125   Loss: 0.753513 mae: 0.706229 (1114.4808234975262 steps/sec)\n",
      "Step #12460\tEpoch   3 Batch 3084/3125   Loss: 0.800004 mae: 0.709952 (1403.951129707113 steps/sec)\n",
      "Step #12461\tEpoch   3 Batch 3085/3125   Loss: 0.785938 mae: 0.704018 (1474.406799918446 steps/sec)\n",
      "Step #12462\tEpoch   3 Batch 3086/3125   Loss: 0.794356 mae: 0.704060 (1511.297517385508 steps/sec)\n",
      "Step #12463\tEpoch   3 Batch 3087/3125   Loss: 0.892162 mae: 0.722795 (1820.3654355279718 steps/sec)\n",
      "Step #12464\tEpoch   3 Batch 3088/3125   Loss: 0.652482 mae: 0.650241 (1799.20384351407 steps/sec)\n",
      "Step #12465\tEpoch   3 Batch 3089/3125   Loss: 0.794785 mae: 0.711990 (1243.8771515676342 steps/sec)\n",
      "Step #12466\tEpoch   3 Batch 3090/3125   Loss: 0.644072 mae: 0.650761 (1388.3826547500828 steps/sec)\n",
      "Step #12467\tEpoch   3 Batch 3091/3125   Loss: 0.870680 mae: 0.750088 (1310.0322330776341 steps/sec)\n",
      "Step #12468\tEpoch   3 Batch 3092/3125   Loss: 0.641704 mae: 0.642263 (1616.5637598378157 steps/sec)\n",
      "Step #12469\tEpoch   3 Batch 3093/3125   Loss: 0.795687 mae: 0.715368 (1762.6681011296396 steps/sec)\n",
      "Step #12470\tEpoch   3 Batch 3094/3125   Loss: 0.846374 mae: 0.746661 (1582.529297685615 steps/sec)\n",
      "Step #12471\tEpoch   3 Batch 3095/3125   Loss: 0.733384 mae: 0.668208 (1255.9676598293158 steps/sec)\n",
      "Step #12472\tEpoch   3 Batch 3096/3125   Loss: 0.747524 mae: 0.689474 (1693.0675644037556 steps/sec)\n",
      "Step #12473\tEpoch   3 Batch 3097/3125   Loss: 0.695536 mae: 0.640814 (1637.1976829515825 steps/sec)\n",
      "Step #12474\tEpoch   3 Batch 3098/3125   Loss: 0.922897 mae: 0.741126 (1589.7270294650505 steps/sec)\n",
      "Step #12475\tEpoch   3 Batch 3099/3125   Loss: 0.861228 mae: 0.748664 (1264.0679907175793 steps/sec)\n",
      "Step #12476\tEpoch   3 Batch 3100/3125   Loss: 0.972859 mae: 0.762425 (616.6371160999132 steps/sec)\n",
      "Step #12477\tEpoch   3 Batch 3101/3125   Loss: 0.837672 mae: 0.740915 (867.2116843859453 steps/sec)\n",
      "Step #12478\tEpoch   3 Batch 3102/3125   Loss: 0.806779 mae: 0.691447 (1652.9016291368805 steps/sec)\n",
      "Step #12479\tEpoch   3 Batch 3103/3125   Loss: 0.861221 mae: 0.725587 (1533.757030124403 steps/sec)\n",
      "Step #12480\tEpoch   3 Batch 3104/3125   Loss: 0.754922 mae: 0.658989 (2019.0743932143992 steps/sec)\n",
      "Step #12481\tEpoch   3 Batch 3105/3125   Loss: 0.799538 mae: 0.735886 (1979.3229073268335 steps/sec)\n",
      "Step #12482\tEpoch   3 Batch 3106/3125   Loss: 0.796146 mae: 0.712463 (1971.1371988758658 steps/sec)\n",
      "Step #12483\tEpoch   3 Batch 3107/3125   Loss: 0.667408 mae: 0.652013 (1286.4463651476208 steps/sec)\n",
      "Step #12484\tEpoch   3 Batch 3108/3125   Loss: 0.774522 mae: 0.666334 (1229.294598998816 steps/sec)\n",
      "Step #12485\tEpoch   3 Batch 3109/3125   Loss: 0.680802 mae: 0.648654 (1447.010280825226 steps/sec)\n",
      "Step #12486\tEpoch   3 Batch 3110/3125   Loss: 0.693988 mae: 0.660927 (1653.6706145815263 steps/sec)\n",
      "Step #12487\tEpoch   3 Batch 3111/3125   Loss: 0.889485 mae: 0.749731 (1547.1085110620938 steps/sec)\n",
      "Step #12488\tEpoch   3 Batch 3112/3125   Loss: 0.825740 mae: 0.716051 (1318.2505060156143 steps/sec)\n",
      "Step #12489\tEpoch   3 Batch 3113/3125   Loss: 0.825392 mae: 0.709626 (1297.6542438324125 steps/sec)\n",
      "Step #12490\tEpoch   3 Batch 3114/3125   Loss: 0.844161 mae: 0.717656 (1597.1486451494982 steps/sec)\n",
      "Step #12491\tEpoch   3 Batch 3115/3125   Loss: 0.722923 mae: 0.655537 (1541.3435249154784 steps/sec)\n",
      "Step #12492\tEpoch   3 Batch 3116/3125   Loss: 0.726891 mae: 0.688085 (1510.8511159459965 steps/sec)\n",
      "Step #12493\tEpoch   3 Batch 3117/3125   Loss: 0.723797 mae: 0.681469 (1437.0649544654054 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12494\tEpoch   3 Batch 3118/3125   Loss: 0.816320 mae: 0.708660 (1914.7526614685098 steps/sec)\n",
      "Step #12495\tEpoch   3 Batch 3119/3125   Loss: 0.859446 mae: 0.752518 (1396.695326704451 steps/sec)\n",
      "Step #12496\tEpoch   3 Batch 3120/3125   Loss: 0.735432 mae: 0.674449 (1634.8365671699967 steps/sec)\n",
      "Step #12497\tEpoch   3 Batch 3121/3125   Loss: 0.814741 mae: 0.715473 (1446.8505512397721 steps/sec)\n",
      "Step #12498\tEpoch   3 Batch 3122/3125   Loss: 0.938253 mae: 0.772131 (1591.5970978415955 steps/sec)\n",
      "Step #12499\tEpoch   3 Batch 3123/3125   Loss: 0.787039 mae: 0.714343 (1574.1548069416922 steps/sec)\n",
      "Step #12500\tEpoch   3 Batch 3124/3125   Loss: 0.833875 mae: 0.718748 (1506.434025557958 steps/sec)\n",
      "\n",
      "Train time for epoch #4 (12500 total steps): 91.3540952205658\n",
      "Model test set loss: 0.815985 mae: 0.713444\n",
      "best loss = 0.8159849643707275\n",
      "Step #12501\tEpoch   4 Batch    0/3125   Loss: 0.880601 mae: 0.713475 (689.6189777805546 steps/sec)\n",
      "Step #12502\tEpoch   4 Batch    1/3125   Loss: 0.906051 mae: 0.743577 (2171.5492782736555 steps/sec)\n",
      "Step #12503\tEpoch   4 Batch    2/3125   Loss: 0.748701 mae: 0.684436 (1939.1141932501155 steps/sec)\n",
      "Step #12504\tEpoch   4 Batch    3/3125   Loss: 0.846603 mae: 0.727785 (2191.7478366289033 steps/sec)\n",
      "Step #12505\tEpoch   4 Batch    4/3125   Loss: 0.807865 mae: 0.724030 (2066.8112114164073 steps/sec)\n",
      "Step #12506\tEpoch   4 Batch    5/3125   Loss: 0.769804 mae: 0.707946 (2256.6278932133905 steps/sec)\n",
      "Step #12507\tEpoch   4 Batch    6/3125   Loss: 0.713645 mae: 0.672629 (2287.3946096877285 steps/sec)\n",
      "Step #12508\tEpoch   4 Batch    7/3125   Loss: 0.810776 mae: 0.715705 (2224.6936892019476 steps/sec)\n",
      "Step #12509\tEpoch   4 Batch    8/3125   Loss: 0.787491 mae: 0.697525 (2337.883905777956 steps/sec)\n",
      "Step #12510\tEpoch   4 Batch    9/3125   Loss: 0.690786 mae: 0.640009 (2393.599269531473 steps/sec)\n",
      "Step #12511\tEpoch   4 Batch   10/3125   Loss: 0.792110 mae: 0.704094 (1723.6535189736087 steps/sec)\n",
      "Step #12512\tEpoch   4 Batch   11/3125   Loss: 0.836519 mae: 0.713812 (2505.168850716137 steps/sec)\n",
      "Step #12513\tEpoch   4 Batch   12/3125   Loss: 0.813258 mae: 0.722291 (2369.797163681564 steps/sec)\n",
      "Step #12514\tEpoch   4 Batch   13/3125   Loss: 0.671387 mae: 0.658015 (2208.853733292607 steps/sec)\n",
      "Step #12515\tEpoch   4 Batch   14/3125   Loss: 0.749282 mae: 0.692805 (2327.5310203991035 steps/sec)\n",
      "Step #12516\tEpoch   4 Batch   15/3125   Loss: 0.855907 mae: 0.714032 (2127.5547574844527 steps/sec)\n",
      "Step #12517\tEpoch   4 Batch   16/3125   Loss: 0.890930 mae: 0.750538 (2382.4504402158477 steps/sec)\n",
      "Step #12518\tEpoch   4 Batch   17/3125   Loss: 0.777514 mae: 0.695506 (2163.2612642350223 steps/sec)\n",
      "Step #12519\tEpoch   4 Batch   18/3125   Loss: 0.748316 mae: 0.676003 (2377.345999501213 steps/sec)\n",
      "Step #12520\tEpoch   4 Batch   19/3125   Loss: 0.865589 mae: 0.757602 (1755.3208229405561 steps/sec)\n",
      "Step #12521\tEpoch   4 Batch   20/3125   Loss: 0.767202 mae: 0.697973 (2210.553388847897 steps/sec)\n",
      "Step #12522\tEpoch   4 Batch   21/3125   Loss: 0.811772 mae: 0.706786 (1888.8496595453398 steps/sec)\n",
      "Step #12523\tEpoch   4 Batch   22/3125   Loss: 0.807467 mae: 0.713507 (2182.3509823509826 steps/sec)\n",
      "Step #12524\tEpoch   4 Batch   23/3125   Loss: 0.822722 mae: 0.711010 (2358.390967466235 steps/sec)\n",
      "Step #12525\tEpoch   4 Batch   24/3125   Loss: 0.706158 mae: 0.659665 (2229.4711104023813 steps/sec)\n",
      "Step #12526\tEpoch   4 Batch   25/3125   Loss: 0.832611 mae: 0.716181 (2198.779592778209 steps/sec)\n",
      "Step #12527\tEpoch   4 Batch   26/3125   Loss: 0.738002 mae: 0.676903 (2029.881719805641 steps/sec)\n",
      "Step #12528\tEpoch   4 Batch   27/3125   Loss: 0.818998 mae: 0.712472 (1421.0176106680399 steps/sec)\n",
      "Step #12529\tEpoch   4 Batch   28/3125   Loss: 0.895423 mae: 0.725402 (2009.2474251497006 steps/sec)\n",
      "Step #12530\tEpoch   4 Batch   29/3125   Loss: 0.724287 mae: 0.676922 (2221.347541018335 steps/sec)\n",
      "Step #12531\tEpoch   4 Batch   30/3125   Loss: 0.833967 mae: 0.725139 (1921.3133978305482 steps/sec)\n",
      "Step #12532\tEpoch   4 Batch   31/3125   Loss: 0.746651 mae: 0.694325 (2133.0078621629596 steps/sec)\n",
      "Step #12533\tEpoch   4 Batch   32/3125   Loss: 0.724908 mae: 0.684563 (2133.203132946801 steps/sec)\n",
      "Step #12534\tEpoch   4 Batch   33/3125   Loss: 0.729777 mae: 0.678187 (2237.5110693823553 steps/sec)\n",
      "Step #12535\tEpoch   4 Batch   34/3125   Loss: 0.805921 mae: 0.724382 (2366.4278218480945 steps/sec)\n",
      "Step #12536\tEpoch   4 Batch   35/3125   Loss: 0.826399 mae: 0.713183 (2346.437522377372 steps/sec)\n",
      "Step #12537\tEpoch   4 Batch   36/3125   Loss: 0.969074 mae: 0.784684 (1545.7401251538627 steps/sec)\n",
      "Step #12538\tEpoch   4 Batch   37/3125   Loss: 0.773385 mae: 0.677695 (2061.184333382476 steps/sec)\n",
      "Step #12539\tEpoch   4 Batch   38/3125   Loss: 0.854367 mae: 0.734127 (2274.8890841441853 steps/sec)\n",
      "Step #12540\tEpoch   4 Batch   39/3125   Loss: 0.848485 mae: 0.732446 (2327.0661340434976 steps/sec)\n",
      "Step #12541\tEpoch   4 Batch   40/3125   Loss: 0.819572 mae: 0.725740 (2229.874106839061 steps/sec)\n",
      "Step #12542\tEpoch   4 Batch   41/3125   Loss: 0.744573 mae: 0.694098 (2156.5653761118824 steps/sec)\n",
      "Step #12543\tEpoch   4 Batch   42/3125   Loss: 0.712812 mae: 0.647168 (2185.603368315738 steps/sec)\n",
      "Step #12544\tEpoch   4 Batch   43/3125   Loss: 0.803380 mae: 0.697984 (2050.4429104988367 steps/sec)\n",
      "Step #12545\tEpoch   4 Batch   44/3125   Loss: 0.662944 mae: 0.653314 (2195.8787066510304 steps/sec)\n",
      "Step #12546\tEpoch   4 Batch   45/3125   Loss: 0.731556 mae: 0.653114 (2221.62992467981 steps/sec)\n",
      "Step #12547\tEpoch   4 Batch   46/3125   Loss: 0.838929 mae: 0.715921 (1664.6177292354585 steps/sec)\n",
      "Step #12548\tEpoch   4 Batch   47/3125   Loss: 0.895203 mae: 0.742320 (2137.5735151719005 steps/sec)\n",
      "Step #12549\tEpoch   4 Batch   48/3125   Loss: 0.920749 mae: 0.778385 (2211.9056659494577 steps/sec)\n",
      "Step #12550\tEpoch   4 Batch   49/3125   Loss: 0.906557 mae: 0.740847 (2479.6944651366275 steps/sec)\n",
      "Step #12551\tEpoch   4 Batch   50/3125   Loss: 0.864808 mae: 0.719396 (2327.866887189335 steps/sec)\n",
      "Step #12552\tEpoch   4 Batch   51/3125   Loss: 0.818166 mae: 0.710091 (2318.5760088446655 steps/sec)\n",
      "Step #12553\tEpoch   4 Batch   52/3125   Loss: 0.779413 mae: 0.709299 (2288.5428375001366 steps/sec)\n",
      "Step #12554\tEpoch   4 Batch   53/3125   Loss: 0.782523 mae: 0.697680 (2165.8081173190126 steps/sec)\n",
      "Step #12555\tEpoch   4 Batch   54/3125   Loss: 0.759834 mae: 0.683580 (2081.3338626439063 steps/sec)\n",
      "Step #12556\tEpoch   4 Batch   55/3125   Loss: 0.852631 mae: 0.721290 (1681.474651421974 steps/sec)\n",
      "Step #12557\tEpoch   4 Batch   56/3125   Loss: 0.806720 mae: 0.711331 (2224.8825045884214 steps/sec)\n",
      "Step #12558\tEpoch   4 Batch   57/3125   Loss: 0.640049 mae: 0.638186 (2356.032894441199 steps/sec)\n",
      "Step #12559\tEpoch   4 Batch   58/3125   Loss: 0.701816 mae: 0.678521 (2324.512574956495 steps/sec)\n",
      "Step #12560\tEpoch   4 Batch   59/3125   Loss: 0.766603 mae: 0.705923 (2162.1461121306475 steps/sec)\n",
      "Step #12561\tEpoch   4 Batch   60/3125   Loss: 0.684269 mae: 0.648010 (1977.0650677828685 steps/sec)\n",
      "Step #12562\tEpoch   4 Batch   61/3125   Loss: 0.739814 mae: 0.693669 (2150.925128205128 steps/sec)\n",
      "Step #12563\tEpoch   4 Batch   62/3125   Loss: 0.757618 mae: 0.695021 (2338.613883468079 steps/sec)\n",
      "Step #12564\tEpoch   4 Batch   63/3125   Loss: 0.636362 mae: 0.653325 (2102.2604929979852 steps/sec)\n",
      "Step #12565\tEpoch   4 Batch   64/3125   Loss: 0.718540 mae: 0.671371 (1701.2119343900579 steps/sec)\n",
      "Step #12566\tEpoch   4 Batch   65/3125   Loss: 0.852969 mae: 0.724744 (2097.466619992999 steps/sec)\n",
      "Step #12567\tEpoch   4 Batch   66/3125   Loss: 0.818799 mae: 0.728469 (2318.550375341345 steps/sec)\n",
      "Step #12568\tEpoch   4 Batch   67/3125   Loss: 0.809366 mae: 0.681279 (2214.919256888776 steps/sec)\n",
      "Step #12569\tEpoch   4 Batch   68/3125   Loss: 0.755351 mae: 0.671974 (2358.6031603216557 steps/sec)\n",
      "Step #12570\tEpoch   4 Batch   69/3125   Loss: 0.732539 mae: 0.673090 (2313.9710912501378 steps/sec)\n",
      "Step #12571\tEpoch   4 Batch   70/3125   Loss: 0.714796 mae: 0.673990 (2392.2340728911195 steps/sec)\n",
      "Step #12572\tEpoch   4 Batch   71/3125   Loss: 0.821865 mae: 0.727034 (2151.7416865887567 steps/sec)\n",
      "Step #12573\tEpoch   4 Batch   72/3125   Loss: 0.825087 mae: 0.699725 (2194.683745656997 steps/sec)\n",
      "Step #12574\tEpoch   4 Batch   73/3125   Loss: 0.777431 mae: 0.684851 (1590.5830956859413 steps/sec)\n",
      "Step #12575\tEpoch   4 Batch   74/3125   Loss: 0.858361 mae: 0.733453 (2161.9900825764685 steps/sec)\n",
      "Step #12576\tEpoch   4 Batch   75/3125   Loss: 0.758092 mae: 0.698299 (2229.7318561677334 steps/sec)\n",
      "Step #12577\tEpoch   4 Batch   76/3125   Loss: 0.723662 mae: 0.668549 (2433.2014526215644 steps/sec)\n",
      "Step #12578\tEpoch   4 Batch   77/3125   Loss: 0.743109 mae: 0.660741 (2238.5380641304814 steps/sec)\n",
      "Step #12579\tEpoch   4 Batch   78/3125   Loss: 0.623905 mae: 0.614664 (2281.8693215820686 steps/sec)\n",
      "Step #12580\tEpoch   4 Batch   79/3125   Loss: 0.750294 mae: 0.687770 (2224.8116950626977 steps/sec)\n",
      "Step #12581\tEpoch   4 Batch   80/3125   Loss: 0.752381 mae: 0.701762 (2198.5951816828465 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12582\tEpoch   4 Batch   81/3125   Loss: 0.855386 mae: 0.717991 (2167.4197482378718 steps/sec)\n",
      "Step #12583\tEpoch   4 Batch   82/3125   Loss: 0.835953 mae: 0.737008 (1585.1848492407235 steps/sec)\n",
      "Step #12584\tEpoch   4 Batch   83/3125   Loss: 0.717641 mae: 0.662890 (2206.715420634503 steps/sec)\n",
      "Step #12585\tEpoch   4 Batch   84/3125   Loss: 0.806078 mae: 0.717499 (2241.4570017742244 steps/sec)\n",
      "Step #12586\tEpoch   4 Batch   85/3125   Loss: 0.755418 mae: 0.674927 (2111.9142808229526 steps/sec)\n",
      "Step #12587\tEpoch   4 Batch   86/3125   Loss: 0.960383 mae: 0.776780 (2082.718759000129 steps/sec)\n",
      "Step #12588\tEpoch   4 Batch   87/3125   Loss: 0.710032 mae: 0.650850 (2279.5625991869388 steps/sec)\n",
      "Step #12589\tEpoch   4 Batch   88/3125   Loss: 1.058921 mae: 0.781096 (2154.6155979986233 steps/sec)\n",
      "Step #12590\tEpoch   4 Batch   89/3125   Loss: 0.814613 mae: 0.714883 (2366.85514361492 steps/sec)\n",
      "Step #12591\tEpoch   4 Batch   90/3125   Loss: 0.872401 mae: 0.740087 (2056.434595018631 steps/sec)\n",
      "Step #12592\tEpoch   4 Batch   91/3125   Loss: 0.850269 mae: 0.729691 (2087.2583952067203 steps/sec)\n",
      "Step #12593\tEpoch   4 Batch   92/3125   Loss: 0.911261 mae: 0.755805 (2104.8135212172306 steps/sec)\n",
      "Step #12594\tEpoch   4 Batch   93/3125   Loss: 0.811213 mae: 0.716856 (2351.1726983272792 steps/sec)\n",
      "Step #12595\tEpoch   4 Batch   94/3125   Loss: 0.820321 mae: 0.722025 (2325.388922769862 steps/sec)\n",
      "Step #12596\tEpoch   4 Batch   95/3125   Loss: 0.675128 mae: 0.637579 (2093.8019169329073 steps/sec)\n",
      "Step #12597\tEpoch   4 Batch   96/3125   Loss: 0.582636 mae: 0.611148 (2311.089560627266 steps/sec)\n",
      "Step #12598\tEpoch   4 Batch   97/3125   Loss: 0.824555 mae: 0.733312 (2388.1206158331056 steps/sec)\n",
      "Step #12599\tEpoch   4 Batch   98/3125   Loss: 0.755128 mae: 0.709508 (2280.405376016702 steps/sec)\n",
      "Step #12600\tEpoch   4 Batch   99/3125   Loss: 1.011608 mae: 0.806820 (2188.1803005008346 steps/sec)\n",
      "Step #12601\tEpoch   4 Batch  100/3125   Loss: 0.891493 mae: 0.758697 (1879.9096417942558 steps/sec)\n",
      "Step #12602\tEpoch   4 Batch  101/3125   Loss: 0.774531 mae: 0.687130 (2070.300206323978 steps/sec)\n",
      "Step #12603\tEpoch   4 Batch  102/3125   Loss: 0.847510 mae: 0.725185 (2103.7577994904 steps/sec)\n",
      "Step #12604\tEpoch   4 Batch  103/3125   Loss: 0.787372 mae: 0.692447 (2029.8620723031506 steps/sec)\n",
      "Step #12605\tEpoch   4 Batch  104/3125   Loss: 0.816693 mae: 0.719623 (2029.6459748756363 steps/sec)\n",
      "Step #12606\tEpoch   4 Batch  105/3125   Loss: 0.670558 mae: 0.640251 (2226.772422726935 steps/sec)\n",
      "Step #12607\tEpoch   4 Batch  106/3125   Loss: 0.733625 mae: 0.687600 (2081.499126568207 steps/sec)\n",
      "Step #12608\tEpoch   4 Batch  107/3125   Loss: 0.899795 mae: 0.749891 (2013.704102012598 steps/sec)\n",
      "Step #12609\tEpoch   4 Batch  108/3125   Loss: 0.756692 mae: 0.678227 (1727.6435891521403 steps/sec)\n",
      "Step #12610\tEpoch   4 Batch  109/3125   Loss: 0.863358 mae: 0.705657 (2131.1870573051633 steps/sec)\n",
      "Step #12611\tEpoch   4 Batch  110/3125   Loss: 0.696141 mae: 0.672426 (2414.627181872611 steps/sec)\n",
      "Step #12612\tEpoch   4 Batch  111/3125   Loss: 0.755959 mae: 0.695109 (2170.627749314289 steps/sec)\n",
      "Step #12613\tEpoch   4 Batch  112/3125   Loss: 0.761678 mae: 0.689744 (2077.6429328604404 steps/sec)\n",
      "Step #12614\tEpoch   4 Batch  113/3125   Loss: 0.843632 mae: 0.718214 (1959.4973137117495 steps/sec)\n",
      "Step #12615\tEpoch   4 Batch  114/3125   Loss: 0.868446 mae: 0.731035 (2194.522984837228 steps/sec)\n",
      "Step #12616\tEpoch   4 Batch  115/3125   Loss: 0.783469 mae: 0.704104 (2231.8437716170915 steps/sec)\n",
      "Step #12617\tEpoch   4 Batch  116/3125   Loss: 0.913477 mae: 0.753708 (2343.9984799204194 steps/sec)\n",
      "Step #12618\tEpoch   4 Batch  117/3125   Loss: 0.833879 mae: 0.712085 (1791.9168788557172 steps/sec)\n",
      "Step #12619\tEpoch   4 Batch  118/3125   Loss: 0.733700 mae: 0.688217 (2071.424902708362 steps/sec)\n",
      "Step #12620\tEpoch   4 Batch  119/3125   Loss: 0.800523 mae: 0.718327 (2177.049724903976 steps/sec)\n",
      "Step #12621\tEpoch   4 Batch  120/3125   Loss: 0.878730 mae: 0.749726 (1952.0011914069771 steps/sec)\n",
      "Step #12622\tEpoch   4 Batch  121/3125   Loss: 0.960531 mae: 0.798750 (1609.5290722662246 steps/sec)\n",
      "Step #12623\tEpoch   4 Batch  122/3125   Loss: 0.986310 mae: 0.807144 (1407.4656716017234 steps/sec)\n",
      "Step #12624\tEpoch   4 Batch  123/3125   Loss: 0.785647 mae: 0.702108 (1607.357900545711 steps/sec)\n",
      "Step #12625\tEpoch   4 Batch  124/3125   Loss: 0.684430 mae: 0.671637 (1367.8356889883185 steps/sec)\n",
      "Step #12626\tEpoch   4 Batch  125/3125   Loss: 0.801577 mae: 0.734687 (1419.5171148730515 steps/sec)\n",
      "Step #12627\tEpoch   4 Batch  126/3125   Loss: 0.875553 mae: 0.737310 (2111.2126764249906 steps/sec)\n",
      "Step #12628\tEpoch   4 Batch  127/3125   Loss: 0.916033 mae: 0.758391 (2271.4887625236934 steps/sec)\n",
      "Step #12629\tEpoch   4 Batch  128/3125   Loss: 0.847321 mae: 0.739440 (2299.2566604538974 steps/sec)\n",
      "Step #12630\tEpoch   4 Batch  129/3125   Loss: 0.916391 mae: 0.769305 (2022.4234533969816 steps/sec)\n",
      "Step #12631\tEpoch   4 Batch  130/3125   Loss: 0.751379 mae: 0.689608 (2195.212125652916 steps/sec)\n",
      "Step #12632\tEpoch   4 Batch  131/3125   Loss: 0.779736 mae: 0.696956 (2083.3394593842822 steps/sec)\n",
      "Step #12633\tEpoch   4 Batch  132/3125   Loss: 0.820564 mae: 0.720804 (1913.1456512616542 steps/sec)\n",
      "Step #12634\tEpoch   4 Batch  133/3125   Loss: 0.802664 mae: 0.712480 (1842.8400702987697 steps/sec)\n",
      "Step #12635\tEpoch   4 Batch  134/3125   Loss: 0.790025 mae: 0.688472 (2309.0284506297894 steps/sec)\n",
      "Step #12636\tEpoch   4 Batch  135/3125   Loss: 0.748019 mae: 0.718334 (2140.890390681626 steps/sec)\n",
      "Step #12637\tEpoch   4 Batch  136/3125   Loss: 0.689645 mae: 0.673557 (1960.0833699400896 steps/sec)\n",
      "Step #12638\tEpoch   4 Batch  137/3125   Loss: 0.678084 mae: 0.650354 (2111.4890103804837 steps/sec)\n",
      "Step #12639\tEpoch   4 Batch  138/3125   Loss: 0.749414 mae: 0.697236 (2131.2087152700146 steps/sec)\n",
      "Step #12640\tEpoch   4 Batch  139/3125   Loss: 0.881918 mae: 0.753633 (2062.7048293498574 steps/sec)\n",
      "Step #12641\tEpoch   4 Batch  140/3125   Loss: 0.837501 mae: 0.721988 (2130.5591677503253 steps/sec)\n",
      "Step #12642\tEpoch   4 Batch  141/3125   Loss: 0.804908 mae: 0.726084 (1828.4917126590115 steps/sec)\n",
      "Step #12643\tEpoch   4 Batch  142/3125   Loss: 0.760134 mae: 0.694849 (2005.366380752938 steps/sec)\n",
      "Step #12644\tEpoch   4 Batch  143/3125   Loss: 0.765143 mae: 0.680333 (2212.2556620990117 steps/sec)\n",
      "Step #12645\tEpoch   4 Batch  144/3125   Loss: 0.654403 mae: 0.633533 (2099.2512512512512 steps/sec)\n",
      "Step #12646\tEpoch   4 Batch  145/3125   Loss: 0.803499 mae: 0.700355 (2049.5211289629024 steps/sec)\n",
      "Step #12647\tEpoch   4 Batch  146/3125   Loss: 0.678628 mae: 0.662358 (2059.3222502626745 steps/sec)\n",
      "Step #12648\tEpoch   4 Batch  147/3125   Loss: 0.864907 mae: 0.731443 (2170.133592723284 steps/sec)\n",
      "Step #12649\tEpoch   4 Batch  148/3125   Loss: 0.668635 mae: 0.660010 (1996.3369823893383 steps/sec)\n",
      "Step #12650\tEpoch   4 Batch  149/3125   Loss: 0.765030 mae: 0.700486 (2007.8623608145758 steps/sec)\n",
      "Step #12651\tEpoch   4 Batch  150/3125   Loss: 0.813798 mae: 0.706064 (1625.548011037733 steps/sec)\n",
      "Step #12652\tEpoch   4 Batch  151/3125   Loss: 0.801455 mae: 0.695531 (2213.8670720378345 steps/sec)\n",
      "Step #12653\tEpoch   4 Batch  152/3125   Loss: 0.698134 mae: 0.665024 (2024.7079495645794 steps/sec)\n",
      "Step #12654\tEpoch   4 Batch  153/3125   Loss: 0.945515 mae: 0.769179 (2214.147556906964 steps/sec)\n",
      "Step #12655\tEpoch   4 Batch  154/3125   Loss: 0.841379 mae: 0.734850 (2169.011345889313 steps/sec)\n",
      "Step #12656\tEpoch   4 Batch  155/3125   Loss: 0.764797 mae: 0.695039 (2107.0551592484676 steps/sec)\n",
      "Step #12657\tEpoch   4 Batch  156/3125   Loss: 0.899014 mae: 0.748005 (2244.4556225050032 steps/sec)\n",
      "Step #12658\tEpoch   4 Batch  157/3125   Loss: 0.822303 mae: 0.723570 (2086.365490414557 steps/sec)\n",
      "Step #12659\tEpoch   4 Batch  158/3125   Loss: 0.762205 mae: 0.697735 (2091.504936670988 steps/sec)\n",
      "Step #12660\tEpoch   4 Batch  159/3125   Loss: 0.725953 mae: 0.664568 (1733.8856231035709 steps/sec)\n",
      "Step #12661\tEpoch   4 Batch  160/3125   Loss: 0.700062 mae: 0.625711 (2106.3144679355196 steps/sec)\n",
      "Step #12662\tEpoch   4 Batch  161/3125   Loss: 0.729501 mae: 0.652449 (2116.2191344009525 steps/sec)\n",
      "Step #12663\tEpoch   4 Batch  162/3125   Loss: 0.836380 mae: 0.747713 (2158.7186560711493 steps/sec)\n",
      "Step #12664\tEpoch   4 Batch  163/3125   Loss: 0.757266 mae: 0.688445 (2248.4984292744643 steps/sec)\n",
      "Step #12665\tEpoch   4 Batch  164/3125   Loss: 0.804165 mae: 0.700801 (2019.8133469454583 steps/sec)\n",
      "Step #12666\tEpoch   4 Batch  165/3125   Loss: 0.773474 mae: 0.703694 (1904.251339326251 steps/sec)\n",
      "Step #12667\tEpoch   4 Batch  166/3125   Loss: 0.941723 mae: 0.767389 (2054.641468026531 steps/sec)\n",
      "Step #12668\tEpoch   4 Batch  167/3125   Loss: 0.785279 mae: 0.699722 (2088.547185595347 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12669\tEpoch   4 Batch  168/3125   Loss: 0.963169 mae: 0.787240 (1882.119811532421 steps/sec)\n",
      "Step #12670\tEpoch   4 Batch  169/3125   Loss: 0.913874 mae: 0.741301 (2042.5744117188717 steps/sec)\n",
      "Step #12671\tEpoch   4 Batch  170/3125   Loss: 0.823112 mae: 0.722908 (2168.54034826488 steps/sec)\n",
      "Step #12672\tEpoch   4 Batch  171/3125   Loss: 0.846425 mae: 0.735607 (2162.815065385091 steps/sec)\n",
      "Step #12673\tEpoch   4 Batch  172/3125   Loss: 0.662896 mae: 0.654433 (1992.9032319373568 steps/sec)\n",
      "Step #12674\tEpoch   4 Batch  173/3125   Loss: 0.782219 mae: 0.692629 (1915.9947010186834 steps/sec)\n",
      "Step #12675\tEpoch   4 Batch  174/3125   Loss: 0.737969 mae: 0.660718 (2045.0642144577607 steps/sec)\n",
      "Step #12676\tEpoch   4 Batch  175/3125   Loss: 0.792413 mae: 0.709785 (2049.5211289629024 steps/sec)\n",
      "Step #12677\tEpoch   4 Batch  176/3125   Loss: 0.843074 mae: 0.720910 (1994.1539485570295 steps/sec)\n",
      "Step #12678\tEpoch   4 Batch  177/3125   Loss: 0.652491 mae: 0.639703 (1856.2481191028342 steps/sec)\n",
      "Step #12679\tEpoch   4 Batch  178/3125   Loss: 0.675864 mae: 0.664836 (2169.6621076372367 steps/sec)\n",
      "Step #12680\tEpoch   4 Batch  179/3125   Loss: 0.819823 mae: 0.714497 (2103.0405134376256 steps/sec)\n",
      "Step #12681\tEpoch   4 Batch  180/3125   Loss: 0.797285 mae: 0.702528 (2270.849259888902 steps/sec)\n",
      "Step #12682\tEpoch   4 Batch  181/3125   Loss: 0.727848 mae: 0.679173 (2236.651984258183 steps/sec)\n",
      "Step #12683\tEpoch   4 Batch  182/3125   Loss: 0.803673 mae: 0.717981 (2146.104646998025 steps/sec)\n",
      "Step #12684\tEpoch   4 Batch  183/3125   Loss: 0.770835 mae: 0.690789 (2070.1980217567275 steps/sec)\n",
      "Step #12685\tEpoch   4 Batch  184/3125   Loss: 0.946765 mae: 0.779745 (2102.2604929979852 steps/sec)\n",
      "Step #12686\tEpoch   4 Batch  185/3125   Loss: 0.674495 mae: 0.642697 (1568.8673105264338 steps/sec)\n",
      "Step #12687\tEpoch   4 Batch  186/3125   Loss: 0.769351 mae: 0.670733 (2190.076965652642 steps/sec)\n",
      "Step #12688\tEpoch   4 Batch  187/3125   Loss: 0.803147 mae: 0.714332 (2063.0497870205504 steps/sec)\n",
      "Step #12689\tEpoch   4 Batch  188/3125   Loss: 0.778289 mae: 0.693773 (1896.1762764581958 steps/sec)\n",
      "Step #12690\tEpoch   4 Batch  189/3125   Loss: 0.872328 mae: 0.734302 (2110.0656014810643 steps/sec)\n",
      "Step #12691\tEpoch   4 Batch  190/3125   Loss: 0.723698 mae: 0.686867 (2126.2173917451564 steps/sec)\n",
      "Step #12692\tEpoch   4 Batch  191/3125   Loss: 0.816659 mae: 0.721498 (1868.6697497037255 steps/sec)\n",
      "Step #12693\tEpoch   4 Batch  192/3125   Loss: 0.912662 mae: 0.735529 (1829.7520372729334 steps/sec)\n",
      "Step #12694\tEpoch   4 Batch  193/3125   Loss: 0.848385 mae: 0.715831 (2074.0060920131336 steps/sec)\n",
      "Step #12695\tEpoch   4 Batch  194/3125   Loss: 0.787871 mae: 0.681621 (1817.320924106137 steps/sec)\n",
      "Step #12696\tEpoch   4 Batch  195/3125   Loss: 0.824280 mae: 0.713856 (2132.292175044737 steps/sec)\n",
      "Step #12697\tEpoch   4 Batch  196/3125   Loss: 0.870986 mae: 0.757844 (2165.0702538637042 steps/sec)\n",
      "Step #12698\tEpoch   4 Batch  197/3125   Loss: 0.784580 mae: 0.708987 (2173.8452608010616 steps/sec)\n",
      "Step #12699\tEpoch   4 Batch  198/3125   Loss: 0.890412 mae: 0.770937 (2167.3301502656 steps/sec)\n",
      "Step #12700\tEpoch   4 Batch  199/3125   Loss: 0.853641 mae: 0.721810 (2320.0893894303636 steps/sec)\n",
      "Step #12701\tEpoch   4 Batch  200/3125   Loss: 0.958340 mae: 0.739802 (2193.3524379275 steps/sec)\n",
      "Step #12702\tEpoch   4 Batch  201/3125   Loss: 0.822296 mae: 0.706723 (2097.7603505016455 steps/sec)\n",
      "Step #12703\tEpoch   4 Batch  202/3125   Loss: 0.892914 mae: 0.750885 (1747.3354440926512 steps/sec)\n",
      "Step #12704\tEpoch   4 Batch  203/3125   Loss: 0.870825 mae: 0.719021 (2094.324661706696 steps/sec)\n",
      "Step #12705\tEpoch   4 Batch  204/3125   Loss: 0.753697 mae: 0.680243 (2385.1600796133066 steps/sec)\n",
      "Step #12706\tEpoch   4 Batch  205/3125   Loss: 0.690035 mae: 0.647581 (2147.203309135959 steps/sec)\n",
      "Step #12707\tEpoch   4 Batch  206/3125   Loss: 0.826485 mae: 0.716094 (2090.837670235888 steps/sec)\n",
      "Step #12708\tEpoch   4 Batch  207/3125   Loss: 0.828342 mae: 0.696913 (2152.006649495644 steps/sec)\n",
      "Step #12709\tEpoch   4 Batch  208/3125   Loss: 0.818251 mae: 0.704130 (2002.169097991293 steps/sec)\n",
      "Step #12710\tEpoch   4 Batch  209/3125   Loss: 0.689291 mae: 0.669161 (2020.8643700313178 steps/sec)\n",
      "Step #12711\tEpoch   4 Batch  210/3125   Loss: 0.751800 mae: 0.673602 (1947.813164665125 steps/sec)\n",
      "Step #12712\tEpoch   4 Batch  211/3125   Loss: 0.786577 mae: 0.689083 (1918.8698062969504 steps/sec)\n",
      "Step #12713\tEpoch   4 Batch  212/3125   Loss: 0.836958 mae: 0.729496 (1818.0613952198073 steps/sec)\n",
      "Step #12714\tEpoch   4 Batch  213/3125   Loss: 0.703442 mae: 0.665929 (2036.4851086143776 steps/sec)\n",
      "Step #12715\tEpoch   4 Batch  214/3125   Loss: 0.860710 mae: 0.719486 (2029.1353819955104 steps/sec)\n",
      "Step #12716\tEpoch   4 Batch  215/3125   Loss: 0.837032 mae: 0.719311 (2341.9305847143432 steps/sec)\n",
      "Step #12717\tEpoch   4 Batch  216/3125   Loss: 0.848555 mae: 0.741227 (2371.9145855953675 steps/sec)\n",
      "Step #12718\tEpoch   4 Batch  217/3125   Loss: 0.749298 mae: 0.679895 (2231.4638065140825 steps/sec)\n",
      "Step #12719\tEpoch   4 Batch  218/3125   Loss: 0.798626 mae: 0.720072 (2061.589579749324 steps/sec)\n",
      "Step #12720\tEpoch   4 Batch  219/3125   Loss: 0.958344 mae: 0.769517 (2204.41902999979 steps/sec)\n",
      "Step #12721\tEpoch   4 Batch  220/3125   Loss: 0.778089 mae: 0.687877 (1956.1707724309047 steps/sec)\n",
      "Step #12722\tEpoch   4 Batch  221/3125   Loss: 0.726928 mae: 0.659074 (1843.180200212693 steps/sec)\n",
      "Step #12723\tEpoch   4 Batch  222/3125   Loss: 0.842035 mae: 0.744666 (2148.4791673069635 steps/sec)\n",
      "Step #12724\tEpoch   4 Batch  223/3125   Loss: 0.874560 mae: 0.720375 (2175.5365830886853 steps/sec)\n",
      "Step #12725\tEpoch   4 Batch  224/3125   Loss: 0.606908 mae: 0.625567 (2098.978110956532 steps/sec)\n",
      "Step #12726\tEpoch   4 Batch  225/3125   Loss: 0.710255 mae: 0.667103 (2064.5120642639863 steps/sec)\n",
      "Step #12727\tEpoch   4 Batch  226/3125   Loss: 0.754024 mae: 0.679974 (2037.0785534585086 steps/sec)\n",
      "Step #12728\tEpoch   4 Batch  227/3125   Loss: 0.831099 mae: 0.741121 (2190.0083542188804 steps/sec)\n",
      "Step #12729\tEpoch   4 Batch  228/3125   Loss: 0.891665 mae: 0.745099 (2124.3005611717754 steps/sec)\n",
      "Step #12730\tEpoch   4 Batch  229/3125   Loss: 0.782829 mae: 0.676093 (1732.1384619196683 steps/sec)\n",
      "Step #12731\tEpoch   4 Batch  230/3125   Loss: 0.840289 mae: 0.735523 (1979.9582699987727 steps/sec)\n",
      "Step #12732\tEpoch   4 Batch  231/3125   Loss: 0.717441 mae: 0.655467 (2098.7260445334 steps/sec)\n",
      "Step #12733\tEpoch   4 Batch  232/3125   Loss: 0.746405 mae: 0.701952 (1995.3113107017812 steps/sec)\n",
      "Step #12734\tEpoch   4 Batch  233/3125   Loss: 0.707651 mae: 0.679092 (2043.7090094040832 steps/sec)\n",
      "Step #12735\tEpoch   4 Batch  234/3125   Loss: 0.739940 mae: 0.670472 (2097.844288615243 steps/sec)\n",
      "Step #12736\tEpoch   4 Batch  235/3125   Loss: 0.796489 mae: 0.707870 (2255.8996590041197 steps/sec)\n",
      "Step #12737\tEpoch   4 Batch  236/3125   Loss: 0.747155 mae: 0.693135 (2018.1611717381682 steps/sec)\n",
      "Step #12738\tEpoch   4 Batch  237/3125   Loss: 0.845542 mae: 0.705734 (2214.942650134133 steps/sec)\n",
      "Step #12739\tEpoch   4 Batch  238/3125   Loss: 0.666074 mae: 0.632772 (1876.0249402882266 steps/sec)\n",
      "Step #12740\tEpoch   4 Batch  239/3125   Loss: 0.772209 mae: 0.708711 (2088.984072277395 steps/sec)\n",
      "Step #12741\tEpoch   4 Batch  240/3125   Loss: 0.865899 mae: 0.716479 (2141.5243852626418 steps/sec)\n",
      "Step #12742\tEpoch   4 Batch  241/3125   Loss: 0.714032 mae: 0.673768 (2141.436915410693 steps/sec)\n",
      "Step #12743\tEpoch   4 Batch  242/3125   Loss: 0.756651 mae: 0.679124 (1931.096971426993 steps/sec)\n",
      "Step #12744\tEpoch   4 Batch  243/3125   Loss: 0.834966 mae: 0.724326 (1876.3102800393665 steps/sec)\n",
      "Step #12745\tEpoch   4 Batch  244/3125   Loss: 0.765499 mae: 0.688342 (2164.0425553870127 steps/sec)\n",
      "Step #12746\tEpoch   4 Batch  245/3125   Loss: 0.923819 mae: 0.763170 (2282.9871543653385 steps/sec)\n",
      "Step #12747\tEpoch   4 Batch  246/3125   Loss: 0.954461 mae: 0.784340 (2156.387977748758 steps/sec)\n",
      "Step #12748\tEpoch   4 Batch  247/3125   Loss: 0.789240 mae: 0.706004 (1784.4153634089478 steps/sec)\n",
      "Step #12749\tEpoch   4 Batch  248/3125   Loss: 0.729350 mae: 0.679910 (1960.010093740946 steps/sec)\n",
      "Step #12750\tEpoch   4 Batch  249/3125   Loss: 0.816179 mae: 0.683609 (2154.327861442689 steps/sec)\n",
      "Step #12751\tEpoch   4 Batch  250/3125   Loss: 0.758652 mae: 0.678979 (2045.5030480370642 steps/sec)\n",
      "Step #12752\tEpoch   4 Batch  251/3125   Loss: 0.868243 mae: 0.736758 (1990.8977852036796 steps/sec)\n",
      "Step #12753\tEpoch   4 Batch  252/3125   Loss: 0.807786 mae: 0.697393 (1992.3541706251187 steps/sec)\n",
      "Step #12754\tEpoch   4 Batch  253/3125   Loss: 0.885395 mae: 0.716061 (2141.1526877329115 steps/sec)\n",
      "Step #12755\tEpoch   4 Batch  254/3125   Loss: 0.763565 mae: 0.709981 (2015.484565409603 steps/sec)\n",
      "Step #12756\tEpoch   4 Batch  255/3125   Loss: 0.928844 mae: 0.777295 (2133.3116321652 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12757\tEpoch   4 Batch  256/3125   Loss: 0.777936 mae: 0.696616 (1851.6263464594738 steps/sec)\n",
      "Step #12758\tEpoch   4 Batch  257/3125   Loss: 0.842978 mae: 0.733609 (2075.9770342506436 steps/sec)\n",
      "Step #12759\tEpoch   4 Batch  258/3125   Loss: 0.932157 mae: 0.766589 (2193.8572265461544 steps/sec)\n",
      "Step #12760\tEpoch   4 Batch  259/3125   Loss: 0.739032 mae: 0.664584 (2077.704684156297 steps/sec)\n",
      "Step #12761\tEpoch   4 Batch  260/3125   Loss: 0.892569 mae: 0.734840 (2101.986569108951 steps/sec)\n",
      "Step #12762\tEpoch   4 Batch  261/3125   Loss: 0.998218 mae: 0.771200 (2120.262865231018 steps/sec)\n",
      "Step #12763\tEpoch   4 Batch  262/3125   Loss: 0.704789 mae: 0.654672 (1953.5649743828599 steps/sec)\n",
      "Step #12764\tEpoch   4 Batch  263/3125   Loss: 0.713269 mae: 0.677922 (2053.8969306407066 steps/sec)\n",
      "Step #12765\tEpoch   4 Batch  264/3125   Loss: 0.826638 mae: 0.737248 (2019.9300732978243 steps/sec)\n",
      "Step #12766\tEpoch   4 Batch  265/3125   Loss: 0.737850 mae: 0.671954 (1827.5515895147796 steps/sec)\n",
      "Step #12767\tEpoch   4 Batch  266/3125   Loss: 0.745941 mae: 0.683198 (1852.2805158099275 steps/sec)\n",
      "Step #12768\tEpoch   4 Batch  267/3125   Loss: 0.842959 mae: 0.727104 (2115.5146673122704 steps/sec)\n",
      "Step #12769\tEpoch   4 Batch  268/3125   Loss: 0.880071 mae: 0.718831 (2288.243188687274 steps/sec)\n",
      "Step #12770\tEpoch   4 Batch  269/3125   Loss: 0.817655 mae: 0.715499 (2185.899520533667 steps/sec)\n",
      "Step #12771\tEpoch   4 Batch  270/3125   Loss: 0.643220 mae: 0.643022 (2155.8337959250807 steps/sec)\n",
      "Step #12772\tEpoch   4 Batch  271/3125   Loss: 0.782026 mae: 0.699036 (2139.1652046187114 steps/sec)\n",
      "Step #12773\tEpoch   4 Batch  272/3125   Loss: 0.819709 mae: 0.729398 (2235.531393241659 steps/sec)\n",
      "Step #12774\tEpoch   4 Batch  273/3125   Loss: 0.683065 mae: 0.634839 (2241.1456051295754 steps/sec)\n",
      "Step #12775\tEpoch   4 Batch  274/3125   Loss: 0.817169 mae: 0.723913 (1857.1522187685414 steps/sec)\n",
      "Step #12776\tEpoch   4 Batch  275/3125   Loss: 0.717296 mae: 0.680238 (2231.1314431618703 steps/sec)\n",
      "Step #12777\tEpoch   4 Batch  276/3125   Loss: 0.819513 mae: 0.696059 (1773.2501310604907 steps/sec)\n",
      "Step #12778\tEpoch   4 Batch  277/3125   Loss: 0.764968 mae: 0.686456 (1960.6331161242672 steps/sec)\n",
      "Step #12779\tEpoch   4 Batch  278/3125   Loss: 0.729290 mae: 0.677249 (2037.8307469561078 steps/sec)\n",
      "Step #12780\tEpoch   4 Batch  279/3125   Loss: 0.801930 mae: 0.690418 (1848.443876426777 steps/sec)\n",
      "Step #12781\tEpoch   4 Batch  280/3125   Loss: 0.846545 mae: 0.724746 (1893.7961675305676 steps/sec)\n",
      "Step #12782\tEpoch   4 Batch  281/3125   Loss: 0.826592 mae: 0.715699 (1942.1310959233947 steps/sec)\n",
      "Step #12783\tEpoch   4 Batch  282/3125   Loss: 0.913347 mae: 0.756226 (1399.8931966250134 steps/sec)\n",
      "Step #12784\tEpoch   4 Batch  283/3125   Loss: 0.726047 mae: 0.672758 (1723.0162512118573 steps/sec)\n",
      "Step #12785\tEpoch   4 Batch  284/3125   Loss: 0.851729 mae: 0.716304 (2098.453040885349 steps/sec)\n",
      "Step #12786\tEpoch   4 Batch  285/3125   Loss: 0.784233 mae: 0.689481 (2128.0944939419155 steps/sec)\n",
      "Step #12787\tEpoch   4 Batch  286/3125   Loss: 0.841901 mae: 0.698072 (1939.5446053678115 steps/sec)\n",
      "Step #12788\tEpoch   4 Batch  287/3125   Loss: 0.739866 mae: 0.693656 (1904.9086218798823 steps/sec)\n",
      "Step #12789\tEpoch   4 Batch  288/3125   Loss: 0.851933 mae: 0.729551 (1894.6516334200635 steps/sec)\n",
      "Step #12790\tEpoch   4 Batch  289/3125   Loss: 0.765301 mae: 0.691339 (1961.3115612666704 steps/sec)\n",
      "Step #12791\tEpoch   4 Batch  290/3125   Loss: 0.913941 mae: 0.775513 (1864.0688331081562 steps/sec)\n",
      "Step #12792\tEpoch   4 Batch  291/3125   Loss: 0.690849 mae: 0.652837 (1888.2034105846974 steps/sec)\n",
      "Step #12793\tEpoch   4 Batch  292/3125   Loss: 0.897339 mae: 0.743501 (2014.2069574905395 steps/sec)\n",
      "Step #12794\tEpoch   4 Batch  293/3125   Loss: 0.949389 mae: 0.762870 (2183.2143080222368 steps/sec)\n",
      "Step #12795\tEpoch   4 Batch  294/3125   Loss: 0.792620 mae: 0.705055 (2121.871806546264 steps/sec)\n",
      "Step #12796\tEpoch   4 Batch  295/3125   Loss: 0.731620 mae: 0.652449 (2183.873621510169 steps/sec)\n",
      "Step #12797\tEpoch   4 Batch  296/3125   Loss: 0.896196 mae: 0.748418 (1982.6350022689458 steps/sec)\n",
      "Step #12798\tEpoch   4 Batch  297/3125   Loss: 0.770956 mae: 0.703821 (1885.4363520304958 steps/sec)\n",
      "Step #12799\tEpoch   4 Batch  298/3125   Loss: 0.790993 mae: 0.692100 (1920.0292973220417 steps/sec)\n",
      "Step #12800\tEpoch   4 Batch  299/3125   Loss: 0.780992 mae: 0.707719 (1775.8027367565371 steps/sec)\n",
      "Step #12801\tEpoch   4 Batch  300/3125   Loss: 0.952016 mae: 0.782249 (1950.675757378452 steps/sec)\n",
      "Step #12802\tEpoch   4 Batch  301/3125   Loss: 0.842423 mae: 0.734170 (2082.801497680978 steps/sec)\n",
      "Step #12803\tEpoch   4 Batch  302/3125   Loss: 0.751383 mae: 0.701226 (2011.0392972900404 steps/sec)\n",
      "Step #12804\tEpoch   4 Batch  303/3125   Loss: 0.801045 mae: 0.718329 (2052.3693018339827 steps/sec)\n",
      "Step #12805\tEpoch   4 Batch  304/3125   Loss: 0.850915 mae: 0.733740 (2158.6964353724693 steps/sec)\n",
      "Step #12806\tEpoch   4 Batch  305/3125   Loss: 0.697900 mae: 0.651488 (2262.666019312726 steps/sec)\n",
      "Step #12807\tEpoch   4 Batch  306/3125   Loss: 0.791541 mae: 0.699882 (2098.2850739399278 steps/sec)\n",
      "Step #12808\tEpoch   4 Batch  307/3125   Loss: 0.754031 mae: 0.698220 (1739.5089581950897 steps/sec)\n",
      "Step #12809\tEpoch   4 Batch  308/3125   Loss: 0.695242 mae: 0.674754 (1979.7339777779875 steps/sec)\n",
      "Step #12810\tEpoch   4 Batch  309/3125   Loss: 0.727804 mae: 0.683301 (2197.0979874490577 steps/sec)\n",
      "Step #12811\tEpoch   4 Batch  310/3125   Loss: 0.871276 mae: 0.736971 (2121.421056890831 steps/sec)\n",
      "Step #12812\tEpoch   4 Batch  311/3125   Loss: 0.895469 mae: 0.731298 (2083.6706508887496 steps/sec)\n",
      "Step #12813\tEpoch   4 Batch  312/3125   Loss: 0.752831 mae: 0.689442 (2071.199865682992 steps/sec)\n",
      "Step #12814\tEpoch   4 Batch  313/3125   Loss: 0.815735 mae: 0.692957 (2014.0908916292112 steps/sec)\n",
      "Step #12815\tEpoch   4 Batch  314/3125   Loss: 0.948534 mae: 0.772737 (2123.6767222610406 steps/sec)\n",
      "Step #12816\tEpoch   4 Batch  315/3125   Loss: 0.767101 mae: 0.684279 (2021.9164875001204 steps/sec)\n",
      "Step #12817\tEpoch   4 Batch  316/3125   Loss: 0.719802 mae: 0.681886 (1656.662111241893 steps/sec)\n",
      "Step #12818\tEpoch   4 Batch  317/3125   Loss: 0.928771 mae: 0.761760 (2001.9970788426108 steps/sec)\n",
      "Step #12819\tEpoch   4 Batch  318/3125   Loss: 0.777258 mae: 0.697770 (2080.487296753008 steps/sec)\n",
      "Step #12820\tEpoch   4 Batch  319/3125   Loss: 0.886961 mae: 0.753606 (2056.8581488637587 steps/sec)\n",
      "Step #12821\tEpoch   4 Batch  320/3125   Loss: 0.929886 mae: 0.760283 (2006.8248150735399 steps/sec)\n",
      "Step #12822\tEpoch   4 Batch  321/3125   Loss: 0.832243 mae: 0.710773 (2152.5809597125995 steps/sec)\n",
      "Step #12823\tEpoch   4 Batch  322/3125   Loss: 0.662481 mae: 0.641891 (2045.8223180403672 steps/sec)\n",
      "Step #12824\tEpoch   4 Batch  323/3125   Loss: 0.778920 mae: 0.683846 (2171.9316051658607 steps/sec)\n",
      "Step #12825\tEpoch   4 Batch  324/3125   Loss: 0.735445 mae: 0.689424 (2189.8254114109095 steps/sec)\n",
      "Step #12826\tEpoch   4 Batch  325/3125   Loss: 0.787311 mae: 0.697801 (1736.5992613570495 steps/sec)\n",
      "Step #12827\tEpoch   4 Batch  326/3125   Loss: 0.711503 mae: 0.643863 (1986.3344036219323 steps/sec)\n",
      "Step #12828\tEpoch   4 Batch  327/3125   Loss: 0.908964 mae: 0.770923 (2050.1823229805163 steps/sec)\n",
      "Step #12829\tEpoch   4 Batch  328/3125   Loss: 0.792790 mae: 0.698395 (2097.3197855828466 steps/sec)\n",
      "Step #12830\tEpoch   4 Batch  329/3125   Loss: 0.892277 mae: 0.753486 (1891.4390851040803 steps/sec)\n",
      "Step #12831\tEpoch   4 Batch  330/3125   Loss: 0.842768 mae: 0.725122 (2059.1402706046383 steps/sec)\n",
      "Step #12832\tEpoch   4 Batch  331/3125   Loss: 0.802049 mae: 0.709947 (2156.387977748758 steps/sec)\n",
      "Step #12833\tEpoch   4 Batch  332/3125   Loss: 0.816517 mae: 0.701482 (2093.2794330488596 steps/sec)\n",
      "Step #12834\tEpoch   4 Batch  333/3125   Loss: 0.868380 mae: 0.748256 (1982.87870049072 steps/sec)\n",
      "Step #12835\tEpoch   4 Batch  334/3125   Loss: 0.806634 mae: 0.700369 (1600.9282726190113 steps/sec)\n",
      "Step #12836\tEpoch   4 Batch  335/3125   Loss: 0.717783 mae: 0.673774 (1994.9886321477154 steps/sec)\n",
      "Step #12837\tEpoch   4 Batch  336/3125   Loss: 0.655488 mae: 0.650733 (1910.722778501599 steps/sec)\n",
      "Step #12838\tEpoch   4 Batch  337/3125   Loss: 0.708153 mae: 0.648697 (1893.1465299342817 steps/sec)\n",
      "Step #12839\tEpoch   4 Batch  338/3125   Loss: 0.895651 mae: 0.764136 (2052.128304988551 steps/sec)\n",
      "Step #12840\tEpoch   4 Batch  339/3125   Loss: 0.771583 mae: 0.676493 (2146.7637093224416 steps/sec)\n",
      "Step #12841\tEpoch   4 Batch  340/3125   Loss: 0.654278 mae: 0.645254 (1871.187408544202 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12842\tEpoch   4 Batch  341/3125   Loss: 0.783875 mae: 0.668152 (1867.1225071225072 steps/sec)\n",
      "Step #12843\tEpoch   4 Batch  342/3125   Loss: 0.860103 mae: 0.740689 (2016.8801692633199 steps/sec)\n",
      "Step #12844\tEpoch   4 Batch  343/3125   Loss: 0.881634 mae: 0.737681 (2122.7738807405385 steps/sec)\n",
      "Step #12845\tEpoch   4 Batch  344/3125   Loss: 0.922742 mae: 0.745048 (2070.9135256302648 steps/sec)\n",
      "Step #12846\tEpoch   4 Batch  345/3125   Loss: 0.766897 mae: 0.707204 (2061.346412809499 steps/sec)\n",
      "Step #12847\tEpoch   4 Batch  346/3125   Loss: 0.886009 mae: 0.729614 (2115.7707828894268 steps/sec)\n",
      "Step #12848\tEpoch   4 Batch  347/3125   Loss: 0.707759 mae: 0.669186 (1973.6415140507067 steps/sec)\n",
      "Step #12849\tEpoch   4 Batch  348/3125   Loss: 0.937835 mae: 0.770989 (2129.1315559706795 steps/sec)\n",
      "Step #12850\tEpoch   4 Batch  349/3125   Loss: 0.890689 mae: 0.759623 (2093.1122932739813 steps/sec)\n",
      "Step #12851\tEpoch   4 Batch  350/3125   Loss: 0.769067 mae: 0.668997 (1697.8650712047734 steps/sec)\n",
      "Step #12852\tEpoch   4 Batch  351/3125   Loss: 0.993370 mae: 0.790953 (1943.174826729921 steps/sec)\n",
      "Step #12853\tEpoch   4 Batch  352/3125   Loss: 0.704744 mae: 0.684980 (2169.8416968442834 steps/sec)\n",
      "Step #12854\tEpoch   4 Batch  353/3125   Loss: 0.832794 mae: 0.730478 (2082.822183378357 steps/sec)\n",
      "Step #12855\tEpoch   4 Batch  354/3125   Loss: 0.767447 mae: 0.705578 (2051.124760377137 steps/sec)\n",
      "Step #12856\tEpoch   4 Batch  355/3125   Loss: 0.749110 mae: 0.710810 (2047.0004880429478 steps/sec)\n",
      "Step #12857\tEpoch   4 Batch  356/3125   Loss: 0.826986 mae: 0.707696 (2007.208966223524 steps/sec)\n",
      "Step #12858\tEpoch   4 Batch  357/3125   Loss: 0.842797 mae: 0.710653 (2105.3417795223418 steps/sec)\n",
      "Step #12859\tEpoch   4 Batch  358/3125   Loss: 0.855368 mae: 0.731434 (1902.2131920760467 steps/sec)\n",
      "Step #12860\tEpoch   4 Batch  359/3125   Loss: 0.732978 mae: 0.671154 (1531.673471176389 steps/sec)\n",
      "Step #12861\tEpoch   4 Batch  360/3125   Loss: 0.770594 mae: 0.677131 (1826.4692562271382 steps/sec)\n",
      "Step #12862\tEpoch   4 Batch  361/3125   Loss: 0.812516 mae: 0.722742 (1808.139053662574 steps/sec)\n",
      "Step #12863\tEpoch   4 Batch  362/3125   Loss: 0.699562 mae: 0.656879 (1955.4958785573085 steps/sec)\n",
      "Step #12864\tEpoch   4 Batch  363/3125   Loss: 0.869511 mae: 0.734009 (1869.236048594832 steps/sec)\n",
      "Step #12865\tEpoch   4 Batch  364/3125   Loss: 0.707317 mae: 0.681714 (2110.5964997031087 steps/sec)\n",
      "Step #12866\tEpoch   4 Batch  365/3125   Loss: 0.766270 mae: 0.709209 (1922.8636396977922 steps/sec)\n",
      "Step #12867\tEpoch   4 Batch  366/3125   Loss: 0.897777 mae: 0.738730 (1827.4719624946626 steps/sec)\n",
      "Step #12868\tEpoch   4 Batch  367/3125   Loss: 0.827153 mae: 0.709363 (1875.0353164172166 steps/sec)\n",
      "Step #12869\tEpoch   4 Batch  368/3125   Loss: 0.840107 mae: 0.713760 (2175.8074389168437 steps/sec)\n",
      "Step #12870\tEpoch   4 Batch  369/3125   Loss: 0.864397 mae: 0.724050 (2027.7619848774923 steps/sec)\n",
      "Step #12871\tEpoch   4 Batch  370/3125   Loss: 0.948093 mae: 0.786138 (2020.2997957689493 steps/sec)\n",
      "Step #12872\tEpoch   4 Batch  371/3125   Loss: 0.748643 mae: 0.698539 (2021.3513253012047 steps/sec)\n",
      "Step #12873\tEpoch   4 Batch  372/3125   Loss: 0.753465 mae: 0.706788 (2261.4216701173223 steps/sec)\n",
      "Step #12874\tEpoch   4 Batch  373/3125   Loss: 0.829189 mae: 0.719312 (2230.443291074619 steps/sec)\n",
      "Step #12875\tEpoch   4 Batch  374/3125   Loss: 0.773943 mae: 0.692889 (2001.6149198743951 steps/sec)\n",
      "Step #12876\tEpoch   4 Batch  375/3125   Loss: 0.773666 mae: 0.711274 (1966.7929624488877 steps/sec)\n",
      "Step #12877\tEpoch   4 Batch  376/3125   Loss: 0.751312 mae: 0.684203 (2017.5010582214184 steps/sec)\n",
      "Step #12878\tEpoch   4 Batch  377/3125   Loss: 0.746019 mae: 0.692601 (1890.5694736177848 steps/sec)\n",
      "Step #12879\tEpoch   4 Batch  378/3125   Loss: 0.850239 mae: 0.739947 (1988.0667760008341 steps/sec)\n",
      "Step #12880\tEpoch   4 Batch  379/3125   Loss: 0.791762 mae: 0.701663 (1873.762084308715 steps/sec)\n",
      "Step #12881\tEpoch   4 Batch  380/3125   Loss: 0.795036 mae: 0.710340 (2251.926938481858 steps/sec)\n",
      "Step #12882\tEpoch   4 Batch  381/3125   Loss: 0.750444 mae: 0.697256 (2030.137171953805 steps/sec)\n",
      "Step #12883\tEpoch   4 Batch  382/3125   Loss: 0.764593 mae: 0.693888 (1888.1524097633003 steps/sec)\n",
      "Step #12884\tEpoch   4 Batch  383/3125   Loss: 0.822947 mae: 0.700511 (1944.3278323753013 steps/sec)\n",
      "Step #12885\tEpoch   4 Batch  384/3125   Loss: 0.780788 mae: 0.704163 (1674.319383013716 steps/sec)\n",
      "Step #12886\tEpoch   4 Batch  385/3125   Loss: 0.783238 mae: 0.707026 (1975.7984586685761 steps/sec)\n",
      "Step #12887\tEpoch   4 Batch  386/3125   Loss: 0.722041 mae: 0.684127 (2003.9292130107403 steps/sec)\n",
      "Step #12888\tEpoch   4 Batch  387/3125   Loss: 0.812813 mae: 0.729739 (2090.3791714843906 steps/sec)\n",
      "Step #12889\tEpoch   4 Batch  388/3125   Loss: 0.802656 mae: 0.708485 (2140.7811192094896 steps/sec)\n",
      "Step #12890\tEpoch   4 Batch  389/3125   Loss: 0.644642 mae: 0.642682 (2005.1362954039143 steps/sec)\n",
      "Step #12891\tEpoch   4 Batch  390/3125   Loss: 0.897507 mae: 0.750426 (2141.983719243772 steps/sec)\n",
      "Step #12892\tEpoch   4 Batch  391/3125   Loss: 0.855825 mae: 0.707428 (2007.0744965929102 steps/sec)\n",
      "Step #12893\tEpoch   4 Batch  392/3125   Loss: 0.678079 mae: 0.653297 (1648.484086246335 steps/sec)\n",
      "Step #12894\tEpoch   4 Batch  393/3125   Loss: 0.869051 mae: 0.721308 (1929.2494227390227 steps/sec)\n",
      "Step #12895\tEpoch   4 Batch  394/3125   Loss: 0.816317 mae: 0.717693 (1962.4861971514663 steps/sec)\n",
      "Step #12896\tEpoch   4 Batch  395/3125   Loss: 0.793119 mae: 0.700953 (2056.23296401608 steps/sec)\n",
      "Step #12897\tEpoch   4 Batch  396/3125   Loss: 0.808459 mae: 0.696058 (2199.8405572105903 steps/sec)\n",
      "Step #12898\tEpoch   4 Batch  397/3125   Loss: 0.761068 mae: 0.689090 (1923.022328183027 steps/sec)\n",
      "Step #12899\tEpoch   4 Batch  398/3125   Loss: 0.847210 mae: 0.739442 (2037.9891742709153 steps/sec)\n",
      "Step #12900\tEpoch   4 Batch  399/3125   Loss: 0.894266 mae: 0.747043 (2186.059020357958 steps/sec)\n",
      "Step #12901\tEpoch   4 Batch  400/3125   Loss: 0.836233 mae: 0.709570 (2166.1884251081983 steps/sec)\n",
      "Step #12902\tEpoch   4 Batch  401/3125   Loss: 0.846444 mae: 0.740062 (1749.7263382725937 steps/sec)\n",
      "Step #12903\tEpoch   4 Batch  402/3125   Loss: 0.853652 mae: 0.723352 (2057.080080040805 steps/sec)\n",
      "Step #12904\tEpoch   4 Batch  403/3125   Loss: 0.902094 mae: 0.757102 (2074.0881398844845 steps/sec)\n",
      "Step #12905\tEpoch   4 Batch  404/3125   Loss: 0.752069 mae: 0.682529 (1976.2080663399925 steps/sec)\n",
      "Step #12906\tEpoch   4 Batch  405/3125   Loss: 0.766745 mae: 0.679616 (2158.540903289555 steps/sec)\n",
      "Step #12907\tEpoch   4 Batch  406/3125   Loss: 0.943673 mae: 0.763443 (2225.77981554006 steps/sec)\n",
      "Step #12908\tEpoch   4 Batch  407/3125   Loss: 0.740580 mae: 0.669668 (2135.396959545459 steps/sec)\n",
      "Step #12909\tEpoch   4 Batch  408/3125   Loss: 0.816153 mae: 0.698555 (2250.114804403339 steps/sec)\n",
      "Step #12910\tEpoch   4 Batch  409/3125   Loss: 0.720622 mae: 0.662277 (2053.7159085344956 steps/sec)\n",
      "Step #12911\tEpoch   4 Batch  410/3125   Loss: 0.817725 mae: 0.731311 (1719.272989612966 steps/sec)\n",
      "Step #12912\tEpoch   4 Batch  411/3125   Loss: 0.869366 mae: 0.761576 (2039.6541495248932 steps/sec)\n",
      "Step #12913\tEpoch   4 Batch  412/3125   Loss: 0.658733 mae: 0.646918 (2156.831528390567 steps/sec)\n",
      "Step #12914\tEpoch   4 Batch  413/3125   Loss: 0.740469 mae: 0.682689 (2021.0201701889812 steps/sec)\n",
      "Step #12915\tEpoch   4 Batch  414/3125   Loss: 0.746397 mae: 0.670559 (2129.109940202439 steps/sec)\n",
      "Step #12916\tEpoch   4 Batch  415/3125   Loss: 0.952144 mae: 0.757141 (2048.3200500078137 steps/sec)\n",
      "Step #12917\tEpoch   4 Batch  416/3125   Loss: 0.855998 mae: 0.711131 (1906.4324933638777 steps/sec)\n",
      "Step #12918\tEpoch   4 Batch  417/3125   Loss: 0.921250 mae: 0.748168 (2229.8503971334094 steps/sec)\n",
      "Step #12919\tEpoch   4 Batch  418/3125   Loss: 0.779208 mae: 0.695432 (2086.3032232391565 steps/sec)\n",
      "Step #12920\tEpoch   4 Batch  419/3125   Loss: 0.853295 mae: 0.736705 (1927.3699785863303 steps/sec)\n",
      "Step #12921\tEpoch   4 Batch  420/3125   Loss: 0.730264 mae: 0.682642 (1860.9921022273493 steps/sec)\n",
      "Step #12922\tEpoch   4 Batch  421/3125   Loss: 0.813085 mae: 0.713007 (2034.9240233654834 steps/sec)\n",
      "Step #12923\tEpoch   4 Batch  422/3125   Loss: 0.898014 mae: 0.765025 (2116.753134021035 steps/sec)\n",
      "Step #12924\tEpoch   4 Batch  423/3125   Loss: 0.824685 mae: 0.722907 (2218.269515548974 steps/sec)\n",
      "Step #12925\tEpoch   4 Batch  424/3125   Loss: 0.872398 mae: 0.765395 (2064.9186203365466 steps/sec)\n",
      "Step #12926\tEpoch   4 Batch  425/3125   Loss: 0.852062 mae: 0.734178 (2057.705780193687 steps/sec)\n",
      "Step #12927\tEpoch   4 Batch  426/3125   Loss: 0.714270 mae: 0.676030 (2250.138947007006 steps/sec)\n",
      "Step #12928\tEpoch   4 Batch  427/3125   Loss: 0.704419 mae: 0.680796 (1956.2437618350232 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #12929\tEpoch   4 Batch  428/3125   Loss: 0.731772 mae: 0.682917 (1683.8508169737845 steps/sec)\n",
      "Step #12930\tEpoch   4 Batch  429/3125   Loss: 0.732246 mae: 0.691738 (2138.205546492659 steps/sec)\n",
      "Step #12931\tEpoch   4 Batch  430/3125   Loss: 0.925705 mae: 0.762821 (2170.245881281563 steps/sec)\n",
      "Step #12932\tEpoch   4 Batch  431/3125   Loss: 0.752025 mae: 0.693454 (1988.5569072927433 steps/sec)\n",
      "Step #12933\tEpoch   4 Batch  432/3125   Loss: 0.844856 mae: 0.720887 (1937.1618064087052 steps/sec)\n",
      "Step #12934\tEpoch   4 Batch  433/3125   Loss: 0.844217 mae: 0.730087 (2153.7526188226593 steps/sec)\n",
      "Step #12935\tEpoch   4 Batch  434/3125   Loss: 0.783628 mae: 0.714954 (2073.493439851297 steps/sec)\n",
      "Step #12936\tEpoch   4 Batch  435/3125   Loss: 0.825636 mae: 0.713230 (2079.352740540969 steps/sec)\n",
      "Step #12937\tEpoch   4 Batch  436/3125   Loss: 0.739005 mae: 0.680332 (2021.3318425846498 steps/sec)\n",
      "Step #12938\tEpoch   4 Batch  437/3125   Loss: 0.791708 mae: 0.712349 (1705.7921150451834 steps/sec)\n",
      "Step #12939\tEpoch   4 Batch  438/3125   Loss: 0.883106 mae: 0.759307 (2169.5723241811675 steps/sec)\n",
      "Step #12940\tEpoch   4 Batch  439/3125   Loss: 0.870757 mae: 0.707206 (2208.5745879627193 steps/sec)\n",
      "Step #12941\tEpoch   4 Batch  440/3125   Loss: 0.770859 mae: 0.692933 (2220.641895826936 steps/sec)\n",
      "Step #12942\tEpoch   4 Batch  441/3125   Loss: 0.776017 mae: 0.691906 (2123.6982278481014 steps/sec)\n",
      "Step #12943\tEpoch   4 Batch  442/3125   Loss: 0.903561 mae: 0.747805 (2127.5763416861114 steps/sec)\n",
      "Step #12944\tEpoch   4 Batch  443/3125   Loss: 0.747386 mae: 0.698921 (2200.5099524673933 steps/sec)\n",
      "Step #12945\tEpoch   4 Batch  444/3125   Loss: 0.805954 mae: 0.704400 (2250.4286986661514 steps/sec)\n",
      "Step #12946\tEpoch   4 Batch  445/3125   Loss: 0.878014 mae: 0.731939 (1822.374389544483 steps/sec)\n",
      "Step #12947\tEpoch   4 Batch  446/3125   Loss: 0.874396 mae: 0.725783 (1902.1614316423434 steps/sec)\n",
      "Step #12948\tEpoch   4 Batch  447/3125   Loss: 0.776118 mae: 0.680726 (1969.360215609124 steps/sec)\n",
      "Step #12949\tEpoch   4 Batch  448/3125   Loss: 0.771004 mae: 0.685323 (2028.8801818797465 steps/sec)\n",
      "Step #12950\tEpoch   4 Batch  449/3125   Loss: 0.784738 mae: 0.709749 (1921.2253909506492 steps/sec)\n",
      "Step #12951\tEpoch   4 Batch  450/3125   Loss: 0.738890 mae: 0.666533 (2104.7290244881574 steps/sec)\n",
      "Step #12952\tEpoch   4 Batch  451/3125   Loss: 0.744661 mae: 0.678115 (1974.1988929472454 steps/sec)\n",
      "Step #12953\tEpoch   4 Batch  452/3125   Loss: 0.770905 mae: 0.705155 (1915.4871944759052 steps/sec)\n",
      "Step #12954\tEpoch   4 Batch  453/3125   Loss: 0.725684 mae: 0.678281 (1971.2298379516487 steps/sec)\n",
      "Step #12955\tEpoch   4 Batch  454/3125   Loss: 0.832165 mae: 0.730748 (1757.4095799953072 steps/sec)\n",
      "Step #12956\tEpoch   4 Batch  455/3125   Loss: 0.712511 mae: 0.659367 (2141.3275881433983 steps/sec)\n",
      "Step #12957\tEpoch   4 Batch  456/3125   Loss: 0.860166 mae: 0.746944 (2157.5859833948907 steps/sec)\n",
      "Step #12958\tEpoch   4 Batch  457/3125   Loss: 0.878115 mae: 0.762464 (1987.991392630651 steps/sec)\n",
      "Step #12959\tEpoch   4 Batch  458/3125   Loss: 0.852427 mae: 0.724236 (2206.413601548691 steps/sec)\n",
      "Step #12960\tEpoch   4 Batch  459/3125   Loss: 0.822766 mae: 0.717482 (2048.500122100122 steps/sec)\n",
      "Step #12961\tEpoch   4 Batch  460/3125   Loss: 0.771689 mae: 0.698881 (2392.479693346719 steps/sec)\n",
      "Step #12962\tEpoch   4 Batch  461/3125   Loss: 0.749575 mae: 0.689749 (2244.5757342238207 steps/sec)\n",
      "Step #12963\tEpoch   4 Batch  462/3125   Loss: 0.731490 mae: 0.698200 (1953.5285788806916 steps/sec)\n",
      "Step #12964\tEpoch   4 Batch  463/3125   Loss: 0.745864 mae: 0.660285 (1978.072061875118 steps/sec)\n",
      "Step #12965\tEpoch   4 Batch  464/3125   Loss: 0.721225 mae: 0.674872 (2206.5296760413707 steps/sec)\n",
      "Step #12966\tEpoch   4 Batch  465/3125   Loss: 0.795097 mae: 0.717664 (2082.1191000972976 steps/sec)\n",
      "Step #12967\tEpoch   4 Batch  466/3125   Loss: 0.848209 mae: 0.739253 (2141.96184172897 steps/sec)\n",
      "Step #12968\tEpoch   4 Batch  467/3125   Loss: 0.850968 mae: 0.733671 (2140.234929123251 steps/sec)\n",
      "Step #12969\tEpoch   4 Batch  468/3125   Loss: 0.650136 mae: 0.663325 (2218.0114435595606 steps/sec)\n",
      "Step #12970\tEpoch   4 Batch  469/3125   Loss: 0.755969 mae: 0.679608 (2053.2333388813286 steps/sec)\n",
      "Step #12971\tEpoch   4 Batch  470/3125   Loss: 0.759346 mae: 0.693123 (2093.718301984745 steps/sec)\n",
      "Step #12972\tEpoch   4 Batch  471/3125   Loss: 0.822266 mae: 0.719431 (2035.8524817737912 steps/sec)\n",
      "Step #12973\tEpoch   4 Batch  472/3125   Loss: 0.833233 mae: 0.724073 (2009.478455008001 steps/sec)\n",
      "Step #12974\tEpoch   4 Batch  473/3125   Loss: 0.791858 mae: 0.714438 (1885.8771795724936 steps/sec)\n",
      "Step #12975\tEpoch   4 Batch  474/3125   Loss: 0.790909 mae: 0.687673 (1160.0639454803932 steps/sec)\n",
      "Step #12976\tEpoch   4 Batch  475/3125   Loss: 0.744299 mae: 0.668747 (1648.0565815324164 steps/sec)\n",
      "Step #12977\tEpoch   4 Batch  476/3125   Loss: 0.684930 mae: 0.661360 (1858.9301068120374 steps/sec)\n",
      "Step #12978\tEpoch   4 Batch  477/3125   Loss: 0.866381 mae: 0.744443 (1867.1391305122017 steps/sec)\n",
      "Step #12979\tEpoch   4 Batch  478/3125   Loss: 0.857419 mae: 0.720466 (1791.5954038699756 steps/sec)\n",
      "Step #12980\tEpoch   4 Batch  479/3125   Loss: 0.866962 mae: 0.718757 (1880.4153291609132 steps/sec)\n",
      "Step #12981\tEpoch   4 Batch  480/3125   Loss: 0.907618 mae: 0.745950 (2114.5334650829823 steps/sec)\n",
      "Step #12982\tEpoch   4 Batch  481/3125   Loss: 0.751430 mae: 0.663086 (1944.832702722754 steps/sec)\n",
      "Step #12983\tEpoch   4 Batch  482/3125   Loss: 0.870763 mae: 0.711041 (1917.6065031134845 steps/sec)\n",
      "Step #12984\tEpoch   4 Batch  483/3125   Loss: 0.804822 mae: 0.709770 (1780.8695652173913 steps/sec)\n",
      "Step #12985\tEpoch   4 Batch  484/3125   Loss: 0.701388 mae: 0.676408 (1813.894270689178 steps/sec)\n",
      "Step #12986\tEpoch   4 Batch  485/3125   Loss: 0.830844 mae: 0.746486 (2006.0761431031185 steps/sec)\n",
      "Step #12987\tEpoch   4 Batch  486/3125   Loss: 0.831587 mae: 0.729803 (1295.2338601594683 steps/sec)\n",
      "Step #12988\tEpoch   4 Batch  487/3125   Loss: 0.740225 mae: 0.689127 (1189.5427654155724 steps/sec)\n",
      "Step #12989\tEpoch   4 Batch  488/3125   Loss: 0.742944 mae: 0.674040 (1239.3416659279615 steps/sec)\n",
      "Step #12990\tEpoch   4 Batch  489/3125   Loss: 0.725528 mae: 0.686755 (1446.8904803300632 steps/sec)\n",
      "Step #12991\tEpoch   4 Batch  490/3125   Loss: 0.785958 mae: 0.698666 (1788.3869867394362 steps/sec)\n",
      "Step #12992\tEpoch   4 Batch  491/3125   Loss: 0.753841 mae: 0.699851 (1688.8409286743922 steps/sec)\n",
      "Step #12993\tEpoch   4 Batch  492/3125   Loss: 0.664149 mae: 0.651117 (2141.983719243772 steps/sec)\n",
      "Step #12994\tEpoch   4 Batch  493/3125   Loss: 0.682956 mae: 0.661951 (1829.16154242004 steps/sec)\n",
      "Step #12995\tEpoch   4 Batch  494/3125   Loss: 0.764525 mae: 0.714475 (2004.3505686705535 steps/sec)\n",
      "Step #12996\tEpoch   4 Batch  495/3125   Loss: 0.753062 mae: 0.692157 (2193.926079360596 steps/sec)\n",
      "Step #12997\tEpoch   4 Batch  496/3125   Loss: 0.678677 mae: 0.669350 (2168.921616282798 steps/sec)\n",
      "Step #12998\tEpoch   4 Batch  497/3125   Loss: 0.937095 mae: 0.752862 (2135.00565017765 steps/sec)\n",
      "Step #12999\tEpoch   4 Batch  498/3125   Loss: 0.741972 mae: 0.678436 (2143.056265200597 steps/sec)\n",
      "Step #13000\tEpoch   4 Batch  499/3125   Loss: 0.779929 mae: 0.686077 (2111.574050766737 steps/sec)\n",
      "Step #13001\tEpoch   4 Batch  500/3125   Loss: 0.655929 mae: 0.663297 (2077.601767369057 steps/sec)\n",
      "Step #13002\tEpoch   4 Batch  501/3125   Loss: 0.861043 mae: 0.725278 (1607.5180708115192 steps/sec)\n",
      "Step #13003\tEpoch   4 Batch  502/3125   Loss: 0.873261 mae: 0.734506 (2022.09194692996 steps/sec)\n",
      "Step #13004\tEpoch   4 Batch  503/3125   Loss: 0.766095 mae: 0.666731 (2151.2781584670306 steps/sec)\n",
      "Step #13005\tEpoch   4 Batch  504/3125   Loss: 0.912385 mae: 0.717200 (2212.652458324541 steps/sec)\n",
      "Step #13006\tEpoch   4 Batch  505/3125   Loss: 0.708980 mae: 0.669968 (2168.8991850411617 steps/sec)\n",
      "Step #13007\tEpoch   4 Batch  506/3125   Loss: 0.788261 mae: 0.669602 (1921.4366210087499 steps/sec)\n",
      "Step #13008\tEpoch   4 Batch  507/3125   Loss: 0.764184 mae: 0.701787 (2233.4600679467926 steps/sec)\n",
      "Step #13009\tEpoch   4 Batch  508/3125   Loss: 0.820116 mae: 0.711461 (2078.219421074016 steps/sec)\n",
      "Step #13010\tEpoch   4 Batch  509/3125   Loss: 0.704606 mae: 0.662812 (2247.4863628082435 steps/sec)\n",
      "Step #13011\tEpoch   4 Batch  510/3125   Loss: 0.875427 mae: 0.729427 (1776.3743244845755 steps/sec)\n",
      "Step #13012\tEpoch   4 Batch  511/3125   Loss: 0.906441 mae: 0.757712 (1903.819163905406 steps/sec)\n",
      "Step #13013\tEpoch   4 Batch  512/3125   Loss: 0.752956 mae: 0.681413 (1987.6710770747243 steps/sec)\n",
      "Step #13014\tEpoch   4 Batch  513/3125   Loss: 0.751752 mae: 0.679812 (2085.5148272638676 steps/sec)\n",
      "Step #13015\tEpoch   4 Batch  514/3125   Loss: 0.714586 mae: 0.676892 (2251.660976185875 steps/sec)\n",
      "Step #13016\tEpoch   4 Batch  515/3125   Loss: 0.853393 mae: 0.735071 (2109.704743222172 steps/sec)\n",
      "Step #13017\tEpoch   4 Batch  516/3125   Loss: 0.919425 mae: 0.783144 (2020.3581853739367 steps/sec)\n",
      "Step #13018\tEpoch   4 Batch  517/3125   Loss: 0.768484 mae: 0.692978 (2007.7662466970473 steps/sec)\n",
      "Step #13019\tEpoch   4 Batch  518/3125   Loss: 0.785320 mae: 0.690268 (2083.6913538541025 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13020\tEpoch   4 Batch  519/3125   Loss: 0.772171 mae: 0.716580 (1766.9154941444099 steps/sec)\n",
      "Step #13021\tEpoch   4 Batch  520/3125   Loss: 0.866467 mae: 0.725024 (1920.644747687517 steps/sec)\n",
      "Step #13022\tEpoch   4 Batch  521/3125   Loss: 0.854141 mae: 0.718217 (2015.8139087806987 steps/sec)\n",
      "Step #13023\tEpoch   4 Batch  522/3125   Loss: 0.807331 mae: 0.712066 (2091.2755158006003 steps/sec)\n",
      "Step #13024\tEpoch   4 Batch  523/3125   Loss: 0.653906 mae: 0.612415 (2306.818756806107 steps/sec)\n",
      "Step #13025\tEpoch   4 Batch  524/3125   Loss: 0.845152 mae: 0.708700 (2298.52584969147 steps/sec)\n",
      "Step #13026\tEpoch   4 Batch  525/3125   Loss: 0.907234 mae: 0.726871 (2088.796812749004 steps/sec)\n",
      "Step #13027\tEpoch   4 Batch  526/3125   Loss: 0.693950 mae: 0.668493 (1999.0391581194951 steps/sec)\n",
      "Step #13028\tEpoch   4 Batch  527/3125   Loss: 0.806484 mae: 0.695471 (2383.3439403582142 steps/sec)\n",
      "Step #13029\tEpoch   4 Batch  528/3125   Loss: 0.821347 mae: 0.710915 (1653.4489691331257 steps/sec)\n",
      "Step #13030\tEpoch   4 Batch  529/3125   Loss: 0.712445 mae: 0.679592 (2180.4676696576175 steps/sec)\n",
      "Step #13031\tEpoch   4 Batch  530/3125   Loss: 0.705807 mae: 0.669087 (2074.3548403050477 steps/sec)\n",
      "Step #13032\tEpoch   4 Batch  531/3125   Loss: 0.861825 mae: 0.747791 (2218.269515548974 steps/sec)\n",
      "Step #13033\tEpoch   4 Batch  532/3125   Loss: 0.770454 mae: 0.697897 (1963.0924187252524 steps/sec)\n",
      "Step #13034\tEpoch   4 Batch  533/3125   Loss: 0.779281 mae: 0.687139 (2069.442169352372 steps/sec)\n",
      "Step #13035\tEpoch   4 Batch  534/3125   Loss: 0.759129 mae: 0.689208 (2060.0707269155205 steps/sec)\n",
      "Step #13036\tEpoch   4 Batch  535/3125   Loss: 0.853927 mae: 0.722843 (2007.458743347245 steps/sec)\n",
      "Step #13037\tEpoch   4 Batch  536/3125   Loss: 0.912702 mae: 0.751178 (1782.8983388026456 steps/sec)\n",
      "Step #13038\tEpoch   4 Batch  537/3125   Loss: 0.760621 mae: 0.690015 (2117.009549575014 steps/sec)\n",
      "Step #13039\tEpoch   4 Batch  538/3125   Loss: 0.753160 mae: 0.695116 (1792.422286989 steps/sec)\n",
      "Step #13040\tEpoch   4 Batch  539/3125   Loss: 0.859890 mae: 0.706686 (1798.6174719978044 steps/sec)\n",
      "Step #13041\tEpoch   4 Batch  540/3125   Loss: 0.706852 mae: 0.688407 (1958.0885511008198 steps/sec)\n",
      "Step #13042\tEpoch   4 Batch  541/3125   Loss: 0.598666 mae: 0.611982 (1969.5081751674009 steps/sec)\n",
      "Step #13043\tEpoch   4 Batch  542/3125   Loss: 0.707149 mae: 0.654008 (1873.0090115837702 steps/sec)\n",
      "Step #13044\tEpoch   4 Batch  543/3125   Loss: 0.865383 mae: 0.748101 (1913.8964179785535 steps/sec)\n",
      "Step #13045\tEpoch   4 Batch  544/3125   Loss: 0.785993 mae: 0.714284 (1848.9164741769964 steps/sec)\n",
      "Step #13046\tEpoch   4 Batch  545/3125   Loss: 0.685806 mae: 0.663386 (1815.134545643387 steps/sec)\n",
      "Step #13047\tEpoch   4 Batch  546/3125   Loss: 0.927106 mae: 0.773522 (2173.800194871157 steps/sec)\n",
      "Step #13048\tEpoch   4 Batch  547/3125   Loss: 0.890054 mae: 0.763452 (2048.080003125122 steps/sec)\n",
      "Step #13049\tEpoch   4 Batch  548/3125   Loss: 0.705208 mae: 0.669684 (2187.016508327163 steps/sec)\n",
      "Step #13050\tEpoch   4 Batch  549/3125   Loss: 0.771770 mae: 0.674996 (1992.6759974535123 steps/sec)\n",
      "Step #13051\tEpoch   4 Batch  550/3125   Loss: 0.837811 mae: 0.712934 (1902.0061672410666 steps/sec)\n",
      "Step #13052\tEpoch   4 Batch  551/3125   Loss: 0.779007 mae: 0.709441 (1792.7593841629694 steps/sec)\n",
      "Step #13053\tEpoch   4 Batch  552/3125   Loss: 0.838202 mae: 0.722602 (1793.3879491696455 steps/sec)\n",
      "Step #13054\tEpoch   4 Batch  553/3125   Loss: 0.790371 mae: 0.693923 (1837.0770079802378 steps/sec)\n",
      "Step #13055\tEpoch   4 Batch  554/3125   Loss: 0.816351 mae: 0.718890 (2042.7335774955193 steps/sec)\n",
      "Step #13056\tEpoch   4 Batch  555/3125   Loss: 0.899366 mae: 0.747130 (2000.9655843598232 steps/sec)\n",
      "Step #13057\tEpoch   4 Batch  556/3125   Loss: 0.752280 mae: 0.670722 (2036.1489766592877 steps/sec)\n",
      "Step #13058\tEpoch   4 Batch  557/3125   Loss: 0.912891 mae: 0.740475 (2091.880461237681 steps/sec)\n",
      "Step #13059\tEpoch   4 Batch  558/3125   Loss: 0.727527 mae: 0.684150 (2073.2269608715424 steps/sec)\n",
      "Step #13060\tEpoch   4 Batch  559/3125   Loss: 0.912626 mae: 0.762963 (2211.4858167246653 steps/sec)\n",
      "Step #13061\tEpoch   4 Batch  560/3125   Loss: 0.944473 mae: 0.767227 (1958.582302124679 steps/sec)\n",
      "Step #13062\tEpoch   4 Batch  561/3125   Loss: 0.888970 mae: 0.771653 (1851.5119143263262 steps/sec)\n",
      "Step #13063\tEpoch   4 Batch  562/3125   Loss: 0.600333 mae: 0.634980 (1707.3057948125113 steps/sec)\n",
      "Step #13064\tEpoch   4 Batch  563/3125   Loss: 0.744244 mae: 0.710798 (1834.9873563922406 steps/sec)\n",
      "Step #13065\tEpoch   4 Batch  564/3125   Loss: 0.912446 mae: 0.725731 (1812.5146926640393 steps/sec)\n",
      "Step #13066\tEpoch   4 Batch  565/3125   Loss: 0.909425 mae: 0.744687 (1840.9473564086134 steps/sec)\n",
      "Step #13067\tEpoch   4 Batch  566/3125   Loss: 0.679192 mae: 0.644667 (1901.9371689762752 steps/sec)\n",
      "Step #13068\tEpoch   4 Batch  567/3125   Loss: 0.757919 mae: 0.696559 (2132.552369330893 steps/sec)\n",
      "Step #13069\tEpoch   4 Batch  568/3125   Loss: 1.085874 mae: 0.841889 (1885.0126737016196 steps/sec)\n",
      "Step #13070\tEpoch   4 Batch  569/3125   Loss: 0.805414 mae: 0.710574 (2060.0707269155205 steps/sec)\n",
      "Step #13071\tEpoch   4 Batch  570/3125   Loss: 0.849561 mae: 0.718257 (2158.985340141657 steps/sec)\n",
      "Step #13072\tEpoch   4 Batch  571/3125   Loss: 0.851860 mae: 0.706356 (2081.953737714683 steps/sec)\n",
      "Step #13073\tEpoch   4 Batch  572/3125   Loss: 0.857902 mae: 0.730779 (2027.0172047167987 steps/sec)\n",
      "Step #13074\tEpoch   4 Batch  573/3125   Loss: 0.747580 mae: 0.683151 (2190.1227089969193 steps/sec)\n",
      "Step #13075\tEpoch   4 Batch  574/3125   Loss: 0.868464 mae: 0.717191 (1821.0925763509583 steps/sec)\n",
      "Step #13076\tEpoch   4 Batch  575/3125   Loss: 0.877685 mae: 0.762338 (2090.1291660022325 steps/sec)\n",
      "Step #13077\tEpoch   4 Batch  576/3125   Loss: 0.795336 mae: 0.722495 (2113.0846583237612 steps/sec)\n",
      "Step #13078\tEpoch   4 Batch  577/3125   Loss: 0.800404 mae: 0.737393 (1736.9588444304563 steps/sec)\n",
      "Step #13079\tEpoch   4 Batch  578/3125   Loss: 0.820414 mae: 0.716987 (1777.006507592191 steps/sec)\n",
      "Step #13080\tEpoch   4 Batch  579/3125   Loss: 0.818247 mae: 0.698019 (1980.2761043228645 steps/sec)\n",
      "Step #13081\tEpoch   4 Batch  580/3125   Loss: 0.906579 mae: 0.745248 (2080.549217246374 steps/sec)\n",
      "Step #13082\tEpoch   4 Batch  581/3125   Loss: 0.740505 mae: 0.692674 (1991.162424161864 steps/sec)\n",
      "Step #13083\tEpoch   4 Batch  582/3125   Loss: 0.830369 mae: 0.718937 (2063.6588173937002 steps/sec)\n",
      "Step #13084\tEpoch   4 Batch  583/3125   Loss: 0.830758 mae: 0.715751 (1948.1750536475704 steps/sec)\n",
      "Step #13085\tEpoch   4 Batch  584/3125   Loss: 0.646855 mae: 0.635159 (1856.2481191028342 steps/sec)\n",
      "Step #13086\tEpoch   4 Batch  585/3125   Loss: 0.728797 mae: 0.662845 (1741.733800641164 steps/sec)\n",
      "Step #13087\tEpoch   4 Batch  586/3125   Loss: 0.880852 mae: 0.720882 (2076.922772198784 steps/sec)\n",
      "Step #13088\tEpoch   4 Batch  587/3125   Loss: 0.787560 mae: 0.693005 (1784.3090876604867 steps/sec)\n",
      "Step #13089\tEpoch   4 Batch  588/3125   Loss: 0.828947 mae: 0.713761 (1981.6795335783875 steps/sec)\n",
      "Step #13090\tEpoch   4 Batch  589/3125   Loss: 0.730176 mae: 0.678680 (2043.6691776216417 steps/sec)\n",
      "Step #13091\tEpoch   4 Batch  590/3125   Loss: 0.928177 mae: 0.723559 (2087.445378987707 steps/sec)\n",
      "Step #13092\tEpoch   4 Batch  591/3125   Loss: 0.773828 mae: 0.683200 (1919.6777884571377 steps/sec)\n",
      "Step #13093\tEpoch   4 Batch  592/3125   Loss: 0.762132 mae: 0.696695 (2052.469733893146 steps/sec)\n",
      "Step #13094\tEpoch   4 Batch  593/3125   Loss: 0.770772 mae: 0.669430 (1829.7041451093642 steps/sec)\n",
      "Step #13095\tEpoch   4 Batch  594/3125   Loss: 0.822070 mae: 0.688921 (2069.1767306022575 steps/sec)\n",
      "Step #13096\tEpoch   4 Batch  595/3125   Loss: 0.932583 mae: 0.753622 (1803.288161243723 steps/sec)\n",
      "Step #13097\tEpoch   4 Batch  596/3125   Loss: 0.763864 mae: 0.698709 (2058.372266499156 steps/sec)\n",
      "Step #13098\tEpoch   4 Batch  597/3125   Loss: 0.814656 mae: 0.730415 (2032.5179298313626 steps/sec)\n",
      "Step #13099\tEpoch   4 Batch  598/3125   Loss: 0.848554 mae: 0.740175 (2149.9769332499513 steps/sec)\n",
      "Step #13100\tEpoch   4 Batch  599/3125   Loss: 0.712778 mae: 0.674457 (1740.7073549308166 steps/sec)\n",
      "Step #13101\tEpoch   4 Batch  600/3125   Loss: 0.765870 mae: 0.693716 (1613.7152002954801 steps/sec)\n",
      "Step #13102\tEpoch   4 Batch  601/3125   Loss: 0.734255 mae: 0.674779 (1351.75418807165 steps/sec)\n",
      "Step #13103\tEpoch   4 Batch  602/3125   Loss: 0.901223 mae: 0.753835 (2046.9205692310695 steps/sec)\n",
      "Step #13104\tEpoch   4 Batch  603/3125   Loss: 0.773842 mae: 0.685064 (2098.180108253044 steps/sec)\n",
      "Step #13105\tEpoch   4 Batch  604/3125   Loss: 0.835620 mae: 0.714612 (2051.5260604163404 steps/sec)\n",
      "Step #13106\tEpoch   4 Batch  605/3125   Loss: 0.822262 mae: 0.727723 (1963.625468164794 steps/sec)\n",
      "Step #13107\tEpoch   4 Batch  606/3125   Loss: 0.718652 mae: 0.674267 (2021.6630998515434 steps/sec)\n",
      "Step #13108\tEpoch   4 Batch  607/3125   Loss: 0.878818 mae: 0.744925 (1962.0088316742758 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13109\tEpoch   4 Batch  608/3125   Loss: 0.811241 mae: 0.719131 (1843.731153017715 steps/sec)\n",
      "Step #13110\tEpoch   4 Batch  609/3125   Loss: 0.727175 mae: 0.665745 (1877.4357000259618 steps/sec)\n",
      "Step #13111\tEpoch   4 Batch  610/3125   Loss: 0.750555 mae: 0.666080 (2159.474432110715 steps/sec)\n",
      "Step #13112\tEpoch   4 Batch  611/3125   Loss: 0.804617 mae: 0.716362 (2198.3647113086504 steps/sec)\n",
      "Step #13113\tEpoch   4 Batch  612/3125   Loss: 0.854640 mae: 0.722666 (2099.9659543788675 steps/sec)\n",
      "Step #13114\tEpoch   4 Batch  613/3125   Loss: 0.868251 mae: 0.721057 (2127.4900075070504 steps/sec)\n",
      "Step #13115\tEpoch   4 Batch  614/3125   Loss: 0.875653 mae: 0.739561 (1853.6393929483722 steps/sec)\n",
      "Step #13116\tEpoch   4 Batch  615/3125   Loss: 0.828344 mae: 0.713764 (1993.2062918785343 steps/sec)\n",
      "Step #13117\tEpoch   4 Batch  616/3125   Loss: 0.796997 mae: 0.707018 (1928.4865650230813 steps/sec)\n",
      "Step #13118\tEpoch   4 Batch  617/3125   Loss: 0.888280 mae: 0.736325 (1871.4044778383588 steps/sec)\n",
      "Step #13119\tEpoch   4 Batch  618/3125   Loss: 0.686127 mae: 0.695172 (1900.7132822767028 steps/sec)\n",
      "Step #13120\tEpoch   4 Batch  619/3125   Loss: 0.783952 mae: 0.695622 (1934.981223646211 steps/sec)\n",
      "Step #13121\tEpoch   4 Batch  620/3125   Loss: 0.844655 mae: 0.717897 (1838.3814299239104 steps/sec)\n",
      "Step #13122\tEpoch   4 Batch  621/3125   Loss: 0.619946 mae: 0.627243 (2254.4688353292768 steps/sec)\n",
      "Step #13123\tEpoch   4 Batch  622/3125   Loss: 0.741061 mae: 0.694081 (1956.6453009395323 steps/sec)\n",
      "Step #13124\tEpoch   4 Batch  623/3125   Loss: 0.763157 mae: 0.688995 (1976.7296308864004 steps/sec)\n",
      "Step #13125\tEpoch   4 Batch  624/3125   Loss: 0.656810 mae: 0.628549 (1960.5414703462718 steps/sec)\n",
      "Step #13126\tEpoch   4 Batch  625/3125   Loss: 0.784183 mae: 0.703672 (1795.4607330291174 steps/sec)\n",
      "Step #13127\tEpoch   4 Batch  626/3125   Loss: 0.753454 mae: 0.691310 (1842.2410991154018 steps/sec)\n",
      "Step #13128\tEpoch   4 Batch  627/3125   Loss: 0.872316 mae: 0.751805 (1817.9668333954594 steps/sec)\n",
      "Step #13129\tEpoch   4 Batch  628/3125   Loss: 0.763700 mae: 0.691988 (2012.3321978601928 steps/sec)\n",
      "Step #13130\tEpoch   4 Batch  629/3125   Loss: 0.917295 mae: 0.749512 (1903.0935506411245 steps/sec)\n",
      "Step #13131\tEpoch   4 Batch  630/3125   Loss: 0.834801 mae: 0.742317 (1704.045697941805 steps/sec)\n",
      "Step #13132\tEpoch   4 Batch  631/3125   Loss: 0.830285 mae: 0.716986 (1960.871435250117 steps/sec)\n",
      "Step #13133\tEpoch   4 Batch  632/3125   Loss: 0.807748 mae: 0.670829 (1726.9180411564653 steps/sec)\n",
      "Step #13134\tEpoch   4 Batch  633/3125   Loss: 0.765159 mae: 0.694806 (1825.6265615070556 steps/sec)\n",
      "Step #13135\tEpoch   4 Batch  634/3125   Loss: 0.689464 mae: 0.661528 (1909.2963337248154 steps/sec)\n",
      "Step #13136\tEpoch   4 Batch  635/3125   Loss: 0.778436 mae: 0.723248 (2130.472591328376 steps/sec)\n",
      "Step #13137\tEpoch   4 Batch  636/3125   Loss: 0.847760 mae: 0.734733 (2195.2810635402493 steps/sec)\n",
      "Step #13138\tEpoch   4 Batch  637/3125   Loss: 0.810819 mae: 0.714656 (2064.8576267181284 steps/sec)\n",
      "Step #13139\tEpoch   4 Batch  638/3125   Loss: 0.853035 mae: 0.722237 (2138.031155696925 steps/sec)\n",
      "Step #13140\tEpoch   4 Batch  639/3125   Loss: 0.824239 mae: 0.719476 (2079.847667407172 steps/sec)\n",
      "Step #13141\tEpoch   4 Batch  640/3125   Loss: 0.762614 mae: 0.701316 (1989.5756448812697 steps/sec)\n",
      "Step #13142\tEpoch   4 Batch  641/3125   Loss: 0.938788 mae: 0.768716 (1949.3883621490984 steps/sec)\n",
      "Step #13143\tEpoch   4 Batch  642/3125   Loss: 0.780832 mae: 0.688160 (1905.4971015282852 steps/sec)\n",
      "Step #13144\tEpoch   4 Batch  643/3125   Loss: 0.704167 mae: 0.672293 (2160.5645701334156 steps/sec)\n",
      "Step #13145\tEpoch   4 Batch  644/3125   Loss: 0.946406 mae: 0.768138 (2230.2535307129488 steps/sec)\n",
      "Step #13146\tEpoch   4 Batch  645/3125   Loss: 0.830863 mae: 0.720043 (2096.1877536333286 steps/sec)\n",
      "Step #13147\tEpoch   4 Batch  646/3125   Loss: 0.872716 mae: 0.750244 (2036.346687899326 steps/sec)\n",
      "Step #13148\tEpoch   4 Batch  647/3125   Loss: 0.757593 mae: 0.686381 (2161.299365157886 steps/sec)\n",
      "Step #13149\tEpoch   4 Batch  648/3125   Loss: 0.844735 mae: 0.744543 (2102.8718113268087 steps/sec)\n",
      "Step #13150\tEpoch   4 Batch  649/3125   Loss: 0.776715 mae: 0.691557 (1748.9821278157237 steps/sec)\n",
      "Step #13151\tEpoch   4 Batch  650/3125   Loss: 0.779260 mae: 0.716266 (1970.433426351345 steps/sec)\n",
      "Step #13152\tEpoch   4 Batch  651/3125   Loss: 0.772437 mae: 0.676453 (2026.8408895417951 steps/sec)\n",
      "Step #13153\tEpoch   4 Batch  652/3125   Loss: 0.826892 mae: 0.720693 (2191.541701064864 steps/sec)\n",
      "Step #13154\tEpoch   4 Batch  653/3125   Loss: 0.758078 mae: 0.701052 (1972.5276999191105 steps/sec)\n",
      "Step #13155\tEpoch   4 Batch  654/3125   Loss: 0.770169 mae: 0.688268 (2260.00819018471 steps/sec)\n",
      "Step #13156\tEpoch   4 Batch  655/3125   Loss: 0.714257 mae: 0.667616 (2079.2702756295857 steps/sec)\n",
      "Step #13157\tEpoch   4 Batch  656/3125   Loss: 0.830358 mae: 0.701569 (2193.0771965783365 steps/sec)\n",
      "Step #13158\tEpoch   4 Batch  657/3125   Loss: 0.825969 mae: 0.723725 (2068.9930051992383 steps/sec)\n",
      "Step #13159\tEpoch   4 Batch  658/3125   Loss: 0.783735 mae: 0.701047 (1911.1755110224094 steps/sec)\n",
      "Step #13160\tEpoch   4 Batch  659/3125   Loss: 0.870118 mae: 0.735129 (1922.63446922816 steps/sec)\n",
      "Step #13161\tEpoch   4 Batch  660/3125   Loss: 0.778873 mae: 0.679864 (2058.0490677134444 steps/sec)\n",
      "Step #13162\tEpoch   4 Batch  661/3125   Loss: 0.795432 mae: 0.719073 (1958.1068337363797 steps/sec)\n",
      "Step #13163\tEpoch   4 Batch  662/3125   Loss: 0.836921 mae: 0.716736 (2142.9029785929597 steps/sec)\n",
      "Step #13164\tEpoch   4 Batch  663/3125   Loss: 0.775696 mae: 0.697843 (1931.9864761536264 steps/sec)\n",
      "Step #13165\tEpoch   4 Batch  664/3125   Loss: 0.748949 mae: 0.689517 (2089.0881198573506 steps/sec)\n",
      "Step #13166\tEpoch   4 Batch  665/3125   Loss: 0.921609 mae: 0.732890 (2158.874213771734 steps/sec)\n",
      "Step #13167\tEpoch   4 Batch  666/3125   Loss: 0.705532 mae: 0.683170 (2283.708115995688 steps/sec)\n",
      "Step #13168\tEpoch   4 Batch  667/3125   Loss: 0.738556 mae: 0.663400 (1489.3804995490282 steps/sec)\n",
      "Step #13169\tEpoch   4 Batch  668/3125   Loss: 0.773707 mae: 0.706187 (2143.340998518064 steps/sec)\n",
      "Step #13170\tEpoch   4 Batch  669/3125   Loss: 0.807706 mae: 0.701748 (2070.7294916860856 steps/sec)\n",
      "Step #13171\tEpoch   4 Batch  670/3125   Loss: 0.855157 mae: 0.722036 (2282.167303276637 steps/sec)\n",
      "Step #13172\tEpoch   4 Batch  671/3125   Loss: 0.756356 mae: 0.681760 (2044.4063170208617 steps/sec)\n",
      "Step #13173\tEpoch   4 Batch  672/3125   Loss: 0.839287 mae: 0.714998 (2244.9842102446073 steps/sec)\n",
      "Step #13174\tEpoch   4 Batch  673/3125   Loss: 0.842721 mae: 0.728442 (2290.5174861836213 steps/sec)\n",
      "Step #13175\tEpoch   4 Batch  674/3125   Loss: 0.800221 mae: 0.721630 (2118.9988784366824 steps/sec)\n",
      "Step #13176\tEpoch   4 Batch  675/3125   Loss: 0.696881 mae: 0.641174 (2099.524462642786 steps/sec)\n",
      "Step #13177\tEpoch   4 Batch  676/3125   Loss: 0.943682 mae: 0.764986 (1959.387467182405 steps/sec)\n",
      "Step #13178\tEpoch   4 Batch  677/3125   Loss: 0.820096 mae: 0.705361 (1880.1287395892168 steps/sec)\n",
      "Step #13179\tEpoch   4 Batch  678/3125   Loss: 0.768786 mae: 0.661146 (2039.912067389063 steps/sec)\n",
      "Step #13180\tEpoch   4 Batch  679/3125   Loss: 0.839966 mae: 0.719097 (2027.1543599508955 steps/sec)\n",
      "Step #13181\tEpoch   4 Batch  680/3125   Loss: 0.873774 mae: 0.745707 (1966.1844535490948 steps/sec)\n",
      "Step #13182\tEpoch   4 Batch  681/3125   Loss: 0.747575 mae: 0.665348 (2068.4420246972027 steps/sec)\n",
      "Step #13183\tEpoch   4 Batch  682/3125   Loss: 0.831942 mae: 0.701116 (2069.973251211592 steps/sec)\n",
      "Step #13184\tEpoch   4 Batch  683/3125   Loss: 0.808914 mae: 0.713551 (2093.174967561633 steps/sec)\n",
      "Step #13185\tEpoch   4 Batch  684/3125   Loss: 0.842536 mae: 0.733165 (2152.47049163502 steps/sec)\n",
      "Step #13186\tEpoch   4 Batch  685/3125   Loss: 0.816156 mae: 0.730788 (1849.1121025622938 steps/sec)\n",
      "Step #13187\tEpoch   4 Batch  686/3125   Loss: 0.864162 mae: 0.739686 (1982.2601988733034 steps/sec)\n",
      "Step #13188\tEpoch   4 Batch  687/3125   Loss: 0.824778 mae: 0.724785 (2011.270739426489 steps/sec)\n",
      "Step #13189\tEpoch   4 Batch  688/3125   Loss: 0.847562 mae: 0.730678 (2059.969549629193 steps/sec)\n",
      "Step #13190\tEpoch   4 Batch  689/3125   Loss: 0.843900 mae: 0.727569 (2228.1446223478288 steps/sec)\n",
      "Step #13191\tEpoch   4 Batch  690/3125   Loss: 0.672721 mae: 0.656840 (1956.6453009395323 steps/sec)\n",
      "Step #13192\tEpoch   4 Batch  691/3125   Loss: 0.801539 mae: 0.738832 (2158.141066540433 steps/sec)\n",
      "Step #13193\tEpoch   4 Batch  692/3125   Loss: 0.907032 mae: 0.769207 (2345.9125687950245 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13194\tEpoch   4 Batch  693/3125   Loss: 0.817358 mae: 0.732112 (2091.796999680817 steps/sec)\n",
      "Step #13195\tEpoch   4 Batch  694/3125   Loss: 0.778718 mae: 0.691065 (2054.9837337827776 steps/sec)\n",
      "Step #13196\tEpoch   4 Batch  695/3125   Loss: 0.735459 mae: 0.689759 (2131.6419670264886 steps/sec)\n",
      "Step #13197\tEpoch   4 Batch  696/3125   Loss: 0.704148 mae: 0.682302 (2119.812798819379 steps/sec)\n",
      "Step #13198\tEpoch   4 Batch  697/3125   Loss: 0.871968 mae: 0.722016 (2139.7545123407035 steps/sec)\n",
      "Step #13199\tEpoch   4 Batch  698/3125   Loss: 0.834948 mae: 0.727361 (2120.6916776216 steps/sec)\n",
      "Step #13200\tEpoch   4 Batch  699/3125   Loss: 0.736214 mae: 0.680634 (2034.6673652142697 steps/sec)\n",
      "Step #13201\tEpoch   4 Batch  700/3125   Loss: 0.817375 mae: 0.692487 (2097.928233446375 steps/sec)\n",
      "Step #13202\tEpoch   4 Batch  701/3125   Loss: 0.760003 mae: 0.686054 (2044.92462507557 steps/sec)\n",
      "Step #13203\tEpoch   4 Batch  702/3125   Loss: 0.735995 mae: 0.696715 (1699.3371687869703 steps/sec)\n",
      "Step #13204\tEpoch   4 Batch  703/3125   Loss: 0.778104 mae: 0.670586 (1818.8340184905726 steps/sec)\n",
      "Step #13205\tEpoch   4 Batch  704/3125   Loss: 0.763805 mae: 0.679650 (1907.4555459547955 steps/sec)\n",
      "Step #13206\tEpoch   4 Batch  705/3125   Loss: 0.891152 mae: 0.766309 (2075.1142862797096 steps/sec)\n",
      "Step #13207\tEpoch   4 Batch  706/3125   Loss: 0.778844 mae: 0.712442 (2098.2850739399278 steps/sec)\n",
      "Step #13208\tEpoch   4 Batch  707/3125   Loss: 0.847716 mae: 0.730266 (2200.1174989509022 steps/sec)\n",
      "Step #13209\tEpoch   4 Batch  708/3125   Loss: 0.763027 mae: 0.670507 (2237.2485011414797 steps/sec)\n",
      "Step #13210\tEpoch   4 Batch  709/3125   Loss: 0.747994 mae: 0.684197 (2057.4433434710095 steps/sec)\n",
      "Step #13211\tEpoch   4 Batch  710/3125   Loss: 0.742580 mae: 0.669901 (2017.3263945669844 steps/sec)\n",
      "Step #13212\tEpoch   4 Batch  711/3125   Loss: 0.850485 mae: 0.737548 (1871.6215975011155 steps/sec)\n",
      "Step #13213\tEpoch   4 Batch  712/3125   Loss: 0.683541 mae: 0.655327 (1827.7586522455313 steps/sec)\n",
      "Step #13214\tEpoch   4 Batch  713/3125   Loss: 0.742097 mae: 0.687840 (2175.3334854677096 steps/sec)\n",
      "Step #13215\tEpoch   4 Batch  714/3125   Loss: 0.750043 mae: 0.699726 (1908.6186497752053 steps/sec)\n",
      "Step #13216\tEpoch   4 Batch  715/3125   Loss: 0.749604 mae: 0.681148 (1744.602688672967 steps/sec)\n",
      "Step #13217\tEpoch   4 Batch  716/3125   Loss: 0.834945 mae: 0.729242 (1791.5954038699756 steps/sec)\n",
      "Step #13218\tEpoch   4 Batch  717/3125   Loss: 0.709874 mae: 0.689784 (1863.2407555484479 steps/sec)\n",
      "Step #13219\tEpoch   4 Batch  718/3125   Loss: 0.778974 mae: 0.673774 (1673.023749311932 steps/sec)\n",
      "Step #13220\tEpoch   4 Batch  719/3125   Loss: 0.831018 mae: 0.711121 (1805.3061997486357 steps/sec)\n",
      "Step #13221\tEpoch   4 Batch  720/3125   Loss: 0.732530 mae: 0.676858 (1869.5026609732834 steps/sec)\n",
      "Step #13222\tEpoch   4 Batch  721/3125   Loss: 0.676303 mae: 0.642944 (2148.2370777078936 steps/sec)\n",
      "Step #13223\tEpoch   4 Batch  722/3125   Loss: 0.802417 mae: 0.718875 (1918.659140188284 steps/sec)\n",
      "Step #13224\tEpoch   4 Batch  723/3125   Loss: 0.983718 mae: 0.787290 (2146.104646998025 steps/sec)\n",
      "Step #13225\tEpoch   4 Batch  724/3125   Loss: 0.656811 mae: 0.651609 (1936.9292153095907 steps/sec)\n",
      "Step #13226\tEpoch   4 Batch  725/3125   Loss: 0.744497 mae: 0.691951 (2146.01680259509 steps/sec)\n",
      "Step #13227\tEpoch   4 Batch  726/3125   Loss: 0.708569 mae: 0.682507 (1979.0053788808152 steps/sec)\n",
      "Step #13228\tEpoch   4 Batch  727/3125   Loss: 0.712589 mae: 0.683245 (1901.1268141889748 steps/sec)\n",
      "Step #13229\tEpoch   4 Batch  728/3125   Loss: 0.818386 mae: 0.683619 (2036.2280565480814 steps/sec)\n",
      "Step #13230\tEpoch   4 Batch  729/3125   Loss: 0.706779 mae: 0.659418 (2358.2583663188198 steps/sec)\n",
      "Step #13231\tEpoch   4 Batch  730/3125   Loss: 0.679540 mae: 0.658773 (1995.2923267208982 steps/sec)\n",
      "Step #13232\tEpoch   4 Batch  731/3125   Loss: 0.848126 mae: 0.730477 (2166.568866482086 steps/sec)\n",
      "Step #13233\tEpoch   4 Batch  732/3125   Loss: 0.874772 mae: 0.739976 (1970.9149006155726 steps/sec)\n",
      "Step #13234\tEpoch   4 Batch  733/3125   Loss: 0.775064 mae: 0.694268 (2084.706303368888 steps/sec)\n",
      "Step #13235\tEpoch   4 Batch  734/3125   Loss: 0.880549 mae: 0.741246 (2016.8607726411556 steps/sec)\n",
      "Step #13236\tEpoch   4 Batch  735/3125   Loss: 0.770202 mae: 0.686822 (1761.05470882143 steps/sec)\n",
      "Step #13237\tEpoch   4 Batch  736/3125   Loss: 0.819401 mae: 0.701110 (2031.0022565056122 steps/sec)\n",
      "Step #13238\tEpoch   4 Batch  737/3125   Loss: 0.649069 mae: 0.645438 (2287.1700911748026 steps/sec)\n",
      "Step #13239\tEpoch   4 Batch  738/3125   Loss: 0.850902 mae: 0.739015 (2207.389007010084 steps/sec)\n",
      "Step #13240\tEpoch   4 Batch  739/3125   Loss: 0.782830 mae: 0.704201 (2233.8407132433613 steps/sec)\n",
      "Step #13241\tEpoch   4 Batch  740/3125   Loss: 0.836832 mae: 0.725014 (2014.4197796497833 steps/sec)\n",
      "Step #13242\tEpoch   4 Batch  741/3125   Loss: 0.860370 mae: 0.710465 (2034.9240233654834 steps/sec)\n",
      "Step #13243\tEpoch   4 Batch  742/3125   Loss: 0.772814 mae: 0.700699 (1981.8668077908087 steps/sec)\n",
      "Step #13244\tEpoch   4 Batch  743/3125   Loss: 0.709703 mae: 0.675925 (2207.4819477484684 steps/sec)\n",
      "Step #13245\tEpoch   4 Batch  744/3125   Loss: 0.760828 mae: 0.697108 (1821.6778721703931 steps/sec)\n",
      "Step #13246\tEpoch   4 Batch  745/3125   Loss: 0.741148 mae: 0.687744 (1930.1194617777533 steps/sec)\n",
      "Step #13247\tEpoch   4 Batch  746/3125   Loss: 0.728433 mae: 0.679486 (2073.063007848797 steps/sec)\n",
      "Step #13248\tEpoch   4 Batch  747/3125   Loss: 0.709682 mae: 0.679745 (2039.2971401343875 steps/sec)\n",
      "Step #13249\tEpoch   4 Batch  748/3125   Loss: 0.706701 mae: 0.671177 (2232.627858450794 steps/sec)\n",
      "Step #13250\tEpoch   4 Batch  749/3125   Loss: 0.753236 mae: 0.677911 (2088.0481102393564 steps/sec)\n",
      "Step #13251\tEpoch   4 Batch  750/3125   Loss: 0.778812 mae: 0.697621 (2014.5939403254627 steps/sec)\n",
      "Step #13252\tEpoch   4 Batch  751/3125   Loss: 0.733424 mae: 0.674857 (2242.5595620000854 steps/sec)\n",
      "Step #13253\tEpoch   4 Batch  752/3125   Loss: 0.806479 mae: 0.725740 (2165.159665080168 steps/sec)\n",
      "Step #13254\tEpoch   4 Batch  753/3125   Loss: 0.878762 mae: 0.756578 (1959.7902980123167 steps/sec)\n",
      "Step #13255\tEpoch   4 Batch  754/3125   Loss: 0.805568 mae: 0.706579 (2015.465195621462 steps/sec)\n",
      "Step #13256\tEpoch   4 Batch  755/3125   Loss: 0.881964 mae: 0.745125 (2045.3235024479684 steps/sec)\n",
      "Step #13257\tEpoch   4 Batch  756/3125   Loss: 0.794924 mae: 0.701560 (1570.347518102241 steps/sec)\n",
      "Step #13258\tEpoch   4 Batch  757/3125   Loss: 0.805057 mae: 0.701409 (1929.0897049083817 steps/sec)\n",
      "Step #13259\tEpoch   4 Batch  758/3125   Loss: 0.723988 mae: 0.666247 (2091.7135447835626 steps/sec)\n",
      "Step #13260\tEpoch   4 Batch  759/3125   Loss: 0.868581 mae: 0.729133 (2219.9601981623405 steps/sec)\n",
      "Step #13261\tEpoch   4 Batch  760/3125   Loss: 0.730713 mae: 0.689142 (2272.7442183063486 steps/sec)\n",
      "Step #13262\tEpoch   4 Batch  761/3125   Loss: 0.757130 mae: 0.678900 (1953.3648159014913 steps/sec)\n",
      "Step #13263\tEpoch   4 Batch  762/3125   Loss: 0.811408 mae: 0.699766 (1750.6465319342533 steps/sec)\n",
      "Step #13264\tEpoch   4 Batch  763/3125   Loss: 0.832081 mae: 0.732230 (1911.7330148861886 steps/sec)\n",
      "Step #13265\tEpoch   4 Batch  764/3125   Loss: 0.830110 mae: 0.731279 (2064.8576267181284 steps/sec)\n",
      "Step #13266\tEpoch   4 Batch  765/3125   Loss: 0.761923 mae: 0.704467 (2060.0707269155205 steps/sec)\n",
      "Step #13267\tEpoch   4 Batch  766/3125   Loss: 0.805211 mae: 0.701940 (2405.7909167039497 steps/sec)\n",
      "Step #13268\tEpoch   4 Batch  767/3125   Loss: 0.821657 mae: 0.704956 (2198.295579618235 steps/sec)\n",
      "Step #13269\tEpoch   4 Batch  768/3125   Loss: 0.691774 mae: 0.670065 (2139.296133836581 steps/sec)\n",
      "Step #13270\tEpoch   4 Batch  769/3125   Loss: 0.875609 mae: 0.713828 (2088.734400366523 steps/sec)\n",
      "Step #13271\tEpoch   4 Batch  770/3125   Loss: 0.738180 mae: 0.667323 (1836.819563294299 steps/sec)\n",
      "Step #13272\tEpoch   4 Batch  771/3125   Loss: 0.745321 mae: 0.686611 (2144.634201214898 steps/sec)\n",
      "Step #13273\tEpoch   4 Batch  772/3125   Loss: 0.779434 mae: 0.692573 (2104.285527939716 steps/sec)\n",
      "Step #13274\tEpoch   4 Batch  773/3125   Loss: 0.693971 mae: 0.673304 (2044.306672515475 steps/sec)\n",
      "Step #13275\tEpoch   4 Batch  774/3125   Loss: 0.748347 mae: 0.694842 (2019.1327120080105 steps/sec)\n",
      "Step #13276\tEpoch   4 Batch  775/3125   Loss: 0.756258 mae: 0.687409 (2153.951706500416 steps/sec)\n",
      "Step #13277\tEpoch   4 Batch  776/3125   Loss: 0.790142 mae: 0.679628 (2144.5464771449024 steps/sec)\n",
      "Step #13278\tEpoch   4 Batch  777/3125   Loss: 0.697690 mae: 0.674491 (2415.0999021132034 steps/sec)\n",
      "Step #13279\tEpoch   4 Batch  778/3125   Loss: 0.678174 mae: 0.635667 (2214.054054054054 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13280\tEpoch   4 Batch  779/3125   Loss: 0.740409 mae: 0.660282 (2121.6142118627777 steps/sec)\n",
      "Step #13281\tEpoch   4 Batch  780/3125   Loss: 0.816090 mae: 0.730637 (1935.892181297886 steps/sec)\n",
      "Step #13282\tEpoch   4 Batch  781/3125   Loss: 0.760065 mae: 0.678396 (1896.3820340546358 steps/sec)\n",
      "Step #13283\tEpoch   4 Batch  782/3125   Loss: 0.907516 mae: 0.771669 (2111.829213030562 steps/sec)\n",
      "Step #13284\tEpoch   4 Batch  783/3125   Loss: 0.747590 mae: 0.673599 (2320.602848258844 steps/sec)\n",
      "Step #13285\tEpoch   4 Batch  784/3125   Loss: 0.888141 mae: 0.747886 (1960.010093740946 steps/sec)\n",
      "Step #13286\tEpoch   4 Batch  785/3125   Loss: 0.922491 mae: 0.756375 (1933.0371462807632 steps/sec)\n",
      "Step #13287\tEpoch   4 Batch  786/3125   Loss: 0.858777 mae: 0.716140 (2232.9131175468483 steps/sec)\n",
      "Step #13288\tEpoch   4 Batch  787/3125   Loss: 0.834755 mae: 0.719466 (2255.9967297410685 steps/sec)\n",
      "Step #13289\tEpoch   4 Batch  788/3125   Loss: 0.781761 mae: 0.704364 (1842.9048472705542 steps/sec)\n",
      "Step #13290\tEpoch   4 Batch  789/3125   Loss: 0.825393 mae: 0.713809 (2339.1877572418102 steps/sec)\n",
      "Step #13291\tEpoch   4 Batch  790/3125   Loss: 0.712429 mae: 0.674431 (2108.1141938078003 steps/sec)\n",
      "Step #13292\tEpoch   4 Batch  791/3125   Loss: 0.827522 mae: 0.725189 (2222.8308566340916 steps/sec)\n",
      "Step #13293\tEpoch   4 Batch  792/3125   Loss: 0.895590 mae: 0.732270 (1960.9447758681952 steps/sec)\n",
      "Step #13294\tEpoch   4 Batch  793/3125   Loss: 0.761742 mae: 0.710057 (2251.540104999839 steps/sec)\n",
      "Step #13295\tEpoch   4 Batch  794/3125   Loss: 0.828777 mae: 0.728049 (2061.184333382476 steps/sec)\n",
      "Step #13296\tEpoch   4 Batch  795/3125   Loss: 0.776480 mae: 0.695767 (2267.0688070915085 steps/sec)\n",
      "Step #13297\tEpoch   4 Batch  796/3125   Loss: 0.902722 mae: 0.737465 (1965.4105320375247 steps/sec)\n",
      "Step #13298\tEpoch   4 Batch  797/3125   Loss: 0.773532 mae: 0.694307 (1627.9076266252669 steps/sec)\n",
      "Step #13299\tEpoch   4 Batch  798/3125   Loss: 0.709277 mae: 0.649749 (2059.7267646856617 steps/sec)\n",
      "Step #13300\tEpoch   4 Batch  799/3125   Loss: 0.866609 mae: 0.742706 (1977.9414677393495 steps/sec)\n",
      "Step #13301\tEpoch   4 Batch  800/3125   Loss: 0.681399 mae: 0.652689 (1615.9284943750963 steps/sec)\n",
      "Step #13302\tEpoch   4 Batch  801/3125   Loss: 0.942013 mae: 0.758788 (1976.915970664203 steps/sec)\n",
      "Step #13303\tEpoch   4 Batch  802/3125   Loss: 0.710539 mae: 0.673340 (1857.8267571446288 steps/sec)\n",
      "Step #13304\tEpoch   4 Batch  803/3125   Loss: 0.816309 mae: 0.714201 (2111.722887926694 steps/sec)\n",
      "Step #13305\tEpoch   4 Batch  804/3125   Loss: 0.800049 mae: 0.718962 (2140.0383689130167 steps/sec)\n",
      "Step #13306\tEpoch   4 Batch  805/3125   Loss: 0.721395 mae: 0.661870 (1679.9929504690342 steps/sec)\n",
      "Step #13307\tEpoch   4 Batch  806/3125   Loss: 0.842963 mae: 0.714568 (1699.4748784440842 steps/sec)\n",
      "Step #13308\tEpoch   4 Batch  807/3125   Loss: 0.718517 mae: 0.694898 (1896.4506298436468 steps/sec)\n",
      "Step #13309\tEpoch   4 Batch  808/3125   Loss: 0.795461 mae: 0.714688 (1944.4540253864056 steps/sec)\n",
      "Step #13310\tEpoch   4 Batch  809/3125   Loss: 0.829647 mae: 0.705403 (1824.8799164636268 steps/sec)\n",
      "Step #13311\tEpoch   4 Batch  810/3125   Loss: 0.755634 mae: 0.681773 (1885.5041582378062 steps/sec)\n",
      "Step #13312\tEpoch   4 Batch  811/3125   Loss: 0.790681 mae: 0.715564 (1883.5905081822918 steps/sec)\n",
      "Step #13313\tEpoch   4 Batch  812/3125   Loss: 0.832154 mae: 0.712860 (1884.5214453240837 steps/sec)\n",
      "Step #13314\tEpoch   4 Batch  813/3125   Loss: 0.929787 mae: 0.761332 (1829.7680018846029 steps/sec)\n",
      "Step #13315\tEpoch   4 Batch  814/3125   Loss: 0.898290 mae: 0.741959 (2132.660802359282 steps/sec)\n",
      "Step #13316\tEpoch   4 Batch  815/3125   Loss: 0.749539 mae: 0.671778 (1842.4353173731606 steps/sec)\n",
      "Step #13317\tEpoch   4 Batch  816/3125   Loss: 0.843760 mae: 0.724790 (1954.420659254634 steps/sec)\n",
      "Step #13318\tEpoch   4 Batch  817/3125   Loss: 0.685067 mae: 0.641045 (2101.9022991961833 steps/sec)\n",
      "Step #13319\tEpoch   4 Batch  818/3125   Loss: 0.643406 mae: 0.638530 (2212.8625845459055 steps/sec)\n",
      "Step #13320\tEpoch   4 Batch  819/3125   Loss: 0.687025 mae: 0.658668 (1724.6741284735642 steps/sec)\n",
      "Step #13321\tEpoch   4 Batch  820/3125   Loss: 0.789933 mae: 0.703640 (1927.9724201333026 steps/sec)\n",
      "Step #13322\tEpoch   4 Batch  821/3125   Loss: 0.946126 mae: 0.775499 (1853.8524097450586 steps/sec)\n",
      "Step #13323\tEpoch   4 Batch  822/3125   Loss: 0.739139 mae: 0.681053 (1782.534636634084 steps/sec)\n",
      "Step #13324\tEpoch   4 Batch  823/3125   Loss: 0.749187 mae: 0.682970 (2103.293617362699 steps/sec)\n",
      "Step #13325\tEpoch   4 Batch  824/3125   Loss: 0.811533 mae: 0.700374 (2218.175664240988 steps/sec)\n",
      "Step #13326\tEpoch   4 Batch  825/3125   Loss: 0.833399 mae: 0.739520 (2266.5542658279833 steps/sec)\n",
      "Step #13327\tEpoch   4 Batch  826/3125   Loss: 0.845086 mae: 0.724941 (2237.8453362927235 steps/sec)\n",
      "Step #13328\tEpoch   4 Batch  827/3125   Loss: 0.655428 mae: 0.641317 (1908.0108813334182 steps/sec)\n",
      "Step #13329\tEpoch   4 Batch  828/3125   Loss: 0.968064 mae: 0.780521 (2069.27881437043 steps/sec)\n",
      "Step #13330\tEpoch   4 Batch  829/3125   Loss: 0.941857 mae: 0.757736 (2152.846129366717 steps/sec)\n",
      "Step #13331\tEpoch   4 Batch  830/3125   Loss: 0.687481 mae: 0.654586 (1626.01434386509 steps/sec)\n",
      "Step #13332\tEpoch   4 Batch  831/3125   Loss: 0.884503 mae: 0.757704 (1732.324467206344 steps/sec)\n",
      "Step #13333\tEpoch   4 Batch  832/3125   Loss: 0.843106 mae: 0.730101 (1781.5654892366244 steps/sec)\n",
      "Step #13334\tEpoch   4 Batch  833/3125   Loss: 0.915790 mae: 0.753260 (1975.1099558293072 steps/sec)\n",
      "Step #13335\tEpoch   4 Batch  834/3125   Loss: 0.823955 mae: 0.720436 (2169.303018391708 steps/sec)\n",
      "Step #13336\tEpoch   4 Batch  835/3125   Loss: 0.726884 mae: 0.677449 (2054.6615981502528 steps/sec)\n",
      "Step #13337\tEpoch   4 Batch  836/3125   Loss: 0.780436 mae: 0.660538 (1976.1335795861446 steps/sec)\n",
      "Step #13338\tEpoch   4 Batch  837/3125   Loss: 0.825975 mae: 0.730358 (1897.239838244208 steps/sec)\n",
      "Step #13339\tEpoch   4 Batch  838/3125   Loss: 0.859615 mae: 0.714696 (1830.4227909087735 steps/sec)\n",
      "Step #13340\tEpoch   4 Batch  839/3125   Loss: 0.795890 mae: 0.718531 (1696.5327551895416 steps/sec)\n",
      "Step #13341\tEpoch   4 Batch  840/3125   Loss: 0.716597 mae: 0.680724 (1940.2982865178935 steps/sec)\n",
      "Step #13342\tEpoch   4 Batch  841/3125   Loss: 0.892075 mae: 0.744231 (1951.9830226085985 steps/sec)\n",
      "Step #13343\tEpoch   4 Batch  842/3125   Loss: 0.685662 mae: 0.651230 (1950.458050055338 steps/sec)\n",
      "Step #13344\tEpoch   4 Batch  843/3125   Loss: 0.709663 mae: 0.683981 (2008.7470426528482 steps/sec)\n",
      "Step #13345\tEpoch   4 Batch  844/3125   Loss: 0.892097 mae: 0.717330 (1996.0899650685778 steps/sec)\n",
      "Step #13346\tEpoch   4 Batch  845/3125   Loss: 0.708290 mae: 0.664523 (2151.2781584670306 steps/sec)\n",
      "Step #13347\tEpoch   4 Batch  846/3125   Loss: 0.632184 mae: 0.634584 (1860.4472911473258 steps/sec)\n",
      "Step #13348\tEpoch   4 Batch  847/3125   Loss: 0.811411 mae: 0.693685 (1603.7440925011088 steps/sec)\n",
      "Step #13349\tEpoch   4 Batch  848/3125   Loss: 0.936939 mae: 0.751104 (2194.89047275163 steps/sec)\n",
      "Step #13350\tEpoch   4 Batch  849/3125   Loss: 0.908543 mae: 0.767932 (2144.349124224174 steps/sec)\n",
      "Step #13351\tEpoch   4 Batch  850/3125   Loss: 0.865725 mae: 0.740422 (1998.0868537891347 steps/sec)\n",
      "Step #13352\tEpoch   4 Batch  851/3125   Loss: 0.800800 mae: 0.705417 (2102.3447916353393 steps/sec)\n",
      "Step #13353\tEpoch   4 Batch  852/3125   Loss: 0.824042 mae: 0.727249 (2204.5812440211507 steps/sec)\n",
      "Step #13354\tEpoch   4 Batch  853/3125   Loss: 0.836037 mae: 0.712657 (1984.5299266619352 steps/sec)\n",
      "Step #13355\tEpoch   4 Batch  854/3125   Loss: 0.723812 mae: 0.652778 (2039.4954632538147 steps/sec)\n",
      "Step #13356\tEpoch   4 Batch  855/3125   Loss: 0.940435 mae: 0.784353 (1723.9652436969263 steps/sec)\n",
      "Step #13357\tEpoch   4 Batch  856/3125   Loss: 0.679246 mae: 0.643161 (1762.6681011296396 steps/sec)\n",
      "Step #13358\tEpoch   4 Batch  857/3125   Loss: 0.678259 mae: 0.662581 (1815.605980589921 steps/sec)\n",
      "Step #13359\tEpoch   4 Batch  858/3125   Loss: 0.840531 mae: 0.724572 (1928.8590480570247 steps/sec)\n",
      "Step #13360\tEpoch   4 Batch  859/3125   Loss: 0.909847 mae: 0.715217 (2066.953804910261 steps/sec)\n",
      "Step #13361\tEpoch   4 Batch  860/3125   Loss: 0.797923 mae: 0.693239 (2038.7222211420683 steps/sec)\n",
      "Step #13362\tEpoch   4 Batch  861/3125   Loss: 0.675098 mae: 0.667354 (1947.8674394412246 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13363\tEpoch   4 Batch  862/3125   Loss: 0.786952 mae: 0.698391 (1922.1762921276224 steps/sec)\n",
      "Step #13364\tEpoch   4 Batch  863/3125   Loss: 0.672128 mae: 0.637883 (1737.9808728225019 steps/sec)\n",
      "Step #13365\tEpoch   4 Batch  864/3125   Loss: 0.789612 mae: 0.706454 (2046.1616516411036 steps/sec)\n",
      "Step #13366\tEpoch   4 Batch  865/3125   Loss: 0.870829 mae: 0.701319 (1903.9574386955615 steps/sec)\n",
      "Step #13367\tEpoch   4 Batch  866/3125   Loss: 0.805441 mae: 0.704034 (2210.9961940306375 steps/sec)\n",
      "Step #13368\tEpoch   4 Batch  867/3125   Loss: 0.689948 mae: 0.655980 (2022.1114443019544 steps/sec)\n",
      "Step #13369\tEpoch   4 Batch  868/3125   Loss: 0.845016 mae: 0.722234 (2030.4713217923395 steps/sec)\n",
      "Step #13370\tEpoch   4 Batch  869/3125   Loss: 0.899956 mae: 0.761823 (2038.5636798413593 steps/sec)\n",
      "Step #13371\tEpoch   4 Batch  870/3125   Loss: 0.735214 mae: 0.668331 (1990.9166856535278 steps/sec)\n",
      "Step #13372\tEpoch   4 Batch  871/3125   Loss: 0.693536 mae: 0.663125 (1957.2479187665658 steps/sec)\n",
      "Step #13373\tEpoch   4 Batch  872/3125   Loss: 0.665842 mae: 0.659370 (2035.7536693329191 steps/sec)\n",
      "Step #13374\tEpoch   4 Batch  873/3125   Loss: 0.795442 mae: 0.722685 (2122.6879358684982 steps/sec)\n",
      "Step #13375\tEpoch   4 Batch  874/3125   Loss: 0.739005 mae: 0.687344 (2181.306816999854 steps/sec)\n",
      "Step #13376\tEpoch   4 Batch  875/3125   Loss: 0.847234 mae: 0.754931 (2062.785985481872 steps/sec)\n",
      "Step #13377\tEpoch   4 Batch  876/3125   Loss: 0.896068 mae: 0.772292 (1930.6347525891829 steps/sec)\n",
      "Step #13378\tEpoch   4 Batch  877/3125   Loss: 0.752214 mae: 0.695826 (1974.4033440974608 steps/sec)\n",
      "Step #13379\tEpoch   4 Batch  878/3125   Loss: 0.825173 mae: 0.705041 (2195.487903183593 steps/sec)\n",
      "Step #13380\tEpoch   4 Batch  879/3125   Loss: 0.716290 mae: 0.674799 (1873.293434569004 steps/sec)\n",
      "Step #13381\tEpoch   4 Batch  880/3125   Loss: 0.742872 mae: 0.698479 (2060.49578007251 steps/sec)\n",
      "Step #13382\tEpoch   4 Batch  881/3125   Loss: 0.728701 mae: 0.693468 (2002.8765985082182 steps/sec)\n",
      "Step #13383\tEpoch   4 Batch  882/3125   Loss: 0.738082 mae: 0.680161 (2081.0860159568133 steps/sec)\n",
      "Step #13384\tEpoch   4 Batch  883/3125   Loss: 0.829673 mae: 0.704574 (2002.9531150014805 steps/sec)\n",
      "Step #13385\tEpoch   4 Batch  884/3125   Loss: 0.939049 mae: 0.778353 (2003.0487688399014 steps/sec)\n",
      "Step #13386\tEpoch   4 Batch  885/3125   Loss: 0.820248 mae: 0.726140 (1940.4060030718556 steps/sec)\n",
      "Step #13387\tEpoch   4 Batch  886/3125   Loss: 0.828648 mae: 0.725297 (1958.5457194355463 steps/sec)\n",
      "Step #13388\tEpoch   4 Batch  887/3125   Loss: 0.783816 mae: 0.702423 (1954.7303469231772 steps/sec)\n",
      "Step #13389\tEpoch   4 Batch  888/3125   Loss: 0.812528 mae: 0.728779 (1633.4104415418526 steps/sec)\n",
      "Step #13390\tEpoch   4 Batch  889/3125   Loss: 0.940139 mae: 0.760222 (2083.6913538541025 steps/sec)\n",
      "Step #13391\tEpoch   4 Batch  890/3125   Loss: 0.753776 mae: 0.661021 (2115.9415610622327 steps/sec)\n",
      "Step #13392\tEpoch   4 Batch  891/3125   Loss: 0.736106 mae: 0.673502 (2195.6947818075214 steps/sec)\n",
      "Step #13393\tEpoch   4 Batch  892/3125   Loss: 0.758895 mae: 0.682874 (2288.243188687274 steps/sec)\n",
      "Step #13394\tEpoch   4 Batch  893/3125   Loss: 0.779007 mae: 0.695894 (2114.341597185115 steps/sec)\n",
      "Step #13395\tEpoch   4 Batch  894/3125   Loss: 0.974499 mae: 0.768704 (2155.567889813958 steps/sec)\n",
      "Step #13396\tEpoch   4 Batch  895/3125   Loss: 0.720522 mae: 0.678583 (2050.5832542949615 steps/sec)\n",
      "Step #13397\tEpoch   4 Batch  896/3125   Loss: 0.833534 mae: 0.732410 (1796.3527345924879 steps/sec)\n",
      "Step #13398\tEpoch   4 Batch  897/3125   Loss: 0.812159 mae: 0.692176 (1927.9015251105452 steps/sec)\n",
      "Step #13399\tEpoch   4 Batch  898/3125   Loss: 0.676802 mae: 0.644602 (2020.6112460014645 steps/sec)\n",
      "Step #13400\tEpoch   4 Batch  899/3125   Loss: 0.896444 mae: 0.752879 (2180.3543208849705 steps/sec)\n",
      "Step #13401\tEpoch   4 Batch  900/3125   Loss: 0.794836 mae: 0.706641 (2191.358502001024 steps/sec)\n",
      "Step #13402\tEpoch   4 Batch  901/3125   Loss: 0.720620 mae: 0.680449 (2065.427040655531 steps/sec)\n",
      "Step #13403\tEpoch   4 Batch  902/3125   Loss: 0.753243 mae: 0.705676 (2063.5572873617507 steps/sec)\n",
      "Step #13404\tEpoch   4 Batch  903/3125   Loss: 0.832562 mae: 0.716509 (2067.3817034700314 steps/sec)\n",
      "Step #13405\tEpoch   4 Batch  904/3125   Loss: 0.891825 mae: 0.736952 (1969.0643631754378 steps/sec)\n",
      "Step #13406\tEpoch   4 Batch  905/3125   Loss: 0.843519 mae: 0.701973 (1694.44920252735 steps/sec)\n",
      "Step #13407\tEpoch   4 Batch  906/3125   Loss: 0.857585 mae: 0.742752 (2006.8248150735399 steps/sec)\n",
      "Step #13408\tEpoch   4 Batch  907/3125   Loss: 0.748638 mae: 0.687622 (2055.4066901235897 steps/sec)\n",
      "Step #13409\tEpoch   4 Batch  908/3125   Loss: 0.775032 mae: 0.713401 (1897.7033752601574 steps/sec)\n",
      "Step #13410\tEpoch   4 Batch  909/3125   Loss: 0.779514 mae: 0.689190 (1954.8761162586923 steps/sec)\n",
      "Step #13411\tEpoch   4 Batch  910/3125   Loss: 0.875839 mae: 0.740528 (1993.8506003936072 steps/sec)\n",
      "Step #13412\tEpoch   4 Batch  911/3125   Loss: 0.752360 mae: 0.686331 (1739.4368183137726 steps/sec)\n",
      "Step #13413\tEpoch   4 Batch  912/3125   Loss: 0.920336 mae: 0.747205 (1818.3924390878349 steps/sec)\n",
      "Step #13414\tEpoch   4 Batch  913/3125   Loss: 0.781316 mae: 0.708594 (1577.3397014027303 steps/sec)\n",
      "Step #13415\tEpoch   4 Batch  914/3125   Loss: 0.852888 mae: 0.737572 (1609.2203098502928 steps/sec)\n",
      "Step #13416\tEpoch   4 Batch  915/3125   Loss: 0.836460 mae: 0.718164 (1684.9737269206666 steps/sec)\n",
      "Step #13417\tEpoch   4 Batch  916/3125   Loss: 0.859614 mae: 0.731697 (1840.5918956634691 steps/sec)\n",
      "Step #13418\tEpoch   4 Batch  917/3125   Loss: 0.631581 mae: 0.644491 (1757.9104427567938 steps/sec)\n",
      "Step #13419\tEpoch   4 Batch  918/3125   Loss: 0.813333 mae: 0.701447 (1798.355271620289 steps/sec)\n",
      "Step #13420\tEpoch   4 Batch  919/3125   Loss: 0.781877 mae: 0.694443 (1858.880675069581 steps/sec)\n",
      "Step #13421\tEpoch   4 Batch  920/3125   Loss: 0.780977 mae: 0.678784 (1606.3114195332307 steps/sec)\n",
      "Step #13422\tEpoch   4 Batch  921/3125   Loss: 0.872019 mae: 0.729017 (1561.0313821235038 steps/sec)\n",
      "Step #13423\tEpoch   4 Batch  922/3125   Loss: 0.772219 mae: 0.692503 (1742.095513411585 steps/sec)\n",
      "Step #13424\tEpoch   4 Batch  923/3125   Loss: 0.770896 mae: 0.681906 (1839.5908807817475 steps/sec)\n",
      "Step #13425\tEpoch   4 Batch  924/3125   Loss: 0.911620 mae: 0.726502 (1903.0935506411245 steps/sec)\n",
      "Step #13426\tEpoch   4 Batch  925/3125   Loss: 0.817607 mae: 0.734568 (1773.4450711609854 steps/sec)\n",
      "Step #13427\tEpoch   4 Batch  926/3125   Loss: 0.770977 mae: 0.708769 (1882.5421903052065 steps/sec)\n",
      "Step #13428\tEpoch   4 Batch  927/3125   Loss: 0.684375 mae: 0.643220 (1694.9285142769395 steps/sec)\n",
      "Step #13429\tEpoch   4 Batch  928/3125   Loss: 0.832542 mae: 0.730078 (1714.03164639728 steps/sec)\n",
      "Step #13430\tEpoch   4 Batch  929/3125   Loss: 0.773736 mae: 0.701071 (1683.8913780090252 steps/sec)\n",
      "Step #13431\tEpoch   4 Batch  930/3125   Loss: 0.753577 mae: 0.666852 (1805.9124923575052 steps/sec)\n",
      "Step #13432\tEpoch   4 Batch  931/3125   Loss: 0.890035 mae: 0.755397 (1853.7049313639698 steps/sec)\n",
      "Step #13433\tEpoch   4 Batch  932/3125   Loss: 0.821385 mae: 0.720791 (2022.3064387035804 steps/sec)\n",
      "Step #13434\tEpoch   4 Batch  933/3125   Loss: 0.737709 mae: 0.693646 (2142.7935015837334 steps/sec)\n",
      "Step #13435\tEpoch   4 Batch  934/3125   Loss: 0.792295 mae: 0.721594 (1980.9869266228368 steps/sec)\n",
      "Step #13436\tEpoch   4 Batch  935/3125   Loss: 0.860605 mae: 0.741991 (1688.922534247127 steps/sec)\n",
      "Step #13437\tEpoch   4 Batch  936/3125   Loss: 0.782732 mae: 0.718222 (1964.1036207316388 steps/sec)\n",
      "Step #13438\tEpoch   4 Batch  937/3125   Loss: 0.731749 mae: 0.662872 (2169.6621076372367 steps/sec)\n",
      "Step #13439\tEpoch   4 Batch  938/3125   Loss: 0.813968 mae: 0.736834 (1987.1248945867326 steps/sec)\n",
      "Step #13440\tEpoch   4 Batch  939/3125   Loss: 0.872872 mae: 0.723070 (2242.3917111298824 steps/sec)\n",
      "Step #13441\tEpoch   4 Batch  940/3125   Loss: 0.800616 mae: 0.702091 (1981.3799684438272 steps/sec)\n",
      "Step #13442\tEpoch   4 Batch  941/3125   Loss: 0.734857 mae: 0.686351 (1921.8944455136136 steps/sec)\n",
      "Step #13443\tEpoch   4 Batch  942/3125   Loss: 0.754930 mae: 0.692428 (2081.519786404105 steps/sec)\n",
      "Step #13444\tEpoch   4 Batch  943/3125   Loss: 0.791092 mae: 0.697092 (1976.2080663399925 steps/sec)\n",
      "Step #13445\tEpoch   4 Batch  944/3125   Loss: 0.773098 mae: 0.686121 (1894.2922436296958 steps/sec)\n",
      "Step #13446\tEpoch   4 Batch  945/3125   Loss: 0.824147 mae: 0.724267 (1835.4691616268587 steps/sec)\n",
      "Step #13447\tEpoch   4 Batch  946/3125   Loss: 0.891661 mae: 0.738516 (2117.308779581617 steps/sec)\n",
      "Step #13448\tEpoch   4 Batch  947/3125   Loss: 0.694785 mae: 0.649359 (1935.4276643656096 steps/sec)\n",
      "Step #13449\tEpoch   4 Batch  948/3125   Loss: 0.640505 mae: 0.647782 (2171.481822794247 steps/sec)\n",
      "Step #13450\tEpoch   4 Batch  949/3125   Loss: 0.686771 mae: 0.657451 (2104.454456965671 steps/sec)\n",
      "Step #13451\tEpoch   4 Batch  950/3125   Loss: 0.808865 mae: 0.701454 (2083.9398215314905 steps/sec)\n",
      "Step #13452\tEpoch   4 Batch  951/3125   Loss: 0.874629 mae: 0.724165 (2142.618361633871 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13453\tEpoch   4 Batch  952/3125   Loss: 0.908341 mae: 0.738293 (1747.2044255971473 steps/sec)\n",
      "Step #13454\tEpoch   4 Batch  953/3125   Loss: 0.811232 mae: 0.718922 (1700.7290627610312 steps/sec)\n",
      "Step #13455\tEpoch   4 Batch  954/3125   Loss: 0.849817 mae: 0.739237 (2172.2015640374952 steps/sec)\n",
      "Step #13456\tEpoch   4 Batch  955/3125   Loss: 0.857061 mae: 0.727605 (1959.7902980123167 steps/sec)\n",
      "Step #13457\tEpoch   4 Batch  956/3125   Loss: 0.731623 mae: 0.670346 (2188.5913464548853 steps/sec)\n",
      "Step #13458\tEpoch   4 Batch  957/3125   Loss: 0.859853 mae: 0.744618 (2133.1814344274803 steps/sec)\n",
      "Step #13459\tEpoch   4 Batch  958/3125   Loss: 0.893594 mae: 0.721905 (2207.3193065920073 steps/sec)\n",
      "Step #13460\tEpoch   4 Batch  959/3125   Loss: 0.892448 mae: 0.732737 (1939.2755754061827 steps/sec)\n",
      "Step #13461\tEpoch   4 Batch  960/3125   Loss: 0.872635 mae: 0.721383 (2134.0062886041947 steps/sec)\n",
      "Step #13462\tEpoch   4 Batch  961/3125   Loss: 0.822705 mae: 0.718181 (1774.6906998392146 steps/sec)\n",
      "Step #13463\tEpoch   4 Batch  962/3125   Loss: 0.699974 mae: 0.655579 (1844.8178187512096 steps/sec)\n",
      "Step #13464\tEpoch   4 Batch  963/3125   Loss: 0.766323 mae: 0.677745 (1958.4725581569091 steps/sec)\n",
      "Step #13465\tEpoch   4 Batch  964/3125   Loss: 0.733881 mae: 0.676492 (2100.765316344112 steps/sec)\n",
      "Step #13466\tEpoch   4 Batch  965/3125   Loss: 0.685886 mae: 0.664162 (1999.267846247712 steps/sec)\n",
      "Step #13467\tEpoch   4 Batch  966/3125   Loss: 0.795299 mae: 0.706960 (2303.955000878889 steps/sec)\n",
      "Step #13468\tEpoch   4 Batch  967/3125   Loss: 0.772961 mae: 0.686726 (2240.858238857961 steps/sec)\n",
      "Step #13469\tEpoch   4 Batch  968/3125   Loss: 0.947331 mae: 0.756723 (2206.343962714753 steps/sec)\n",
      "Step #13470\tEpoch   4 Batch  969/3125   Loss: 0.753505 mae: 0.688266 (1684.6759422897721 steps/sec)\n",
      "Step #13471\tEpoch   4 Batch  970/3125   Loss: 0.884693 mae: 0.744734 (1713.6814924373043 steps/sec)\n",
      "Step #13472\tEpoch   4 Batch  971/3125   Loss: 0.797454 mae: 0.705082 (1774.1802307874523 steps/sec)\n",
      "Step #13473\tEpoch   4 Batch  972/3125   Loss: 0.796032 mae: 0.729957 (1928.64618299199 steps/sec)\n",
      "Step #13474\tEpoch   4 Batch  973/3125   Loss: 0.859728 mae: 0.727909 (2050.122196805287 steps/sec)\n",
      "Step #13475\tEpoch   4 Batch  974/3125   Loss: 0.747874 mae: 0.674400 (1843.1964000070313 steps/sec)\n",
      "Step #13476\tEpoch   4 Batch  975/3125   Loss: 0.751768 mae: 0.676017 (1815.2916634206722 steps/sec)\n",
      "Step #13477\tEpoch   4 Batch  976/3125   Loss: 0.791952 mae: 0.698301 (1993.8695569499905 steps/sec)\n",
      "Step #13478\tEpoch   4 Batch  977/3125   Loss: 0.687510 mae: 0.669306 (1800.5786848228315 steps/sec)\n",
      "Step #13479\tEpoch   4 Batch  978/3125   Loss: 0.630727 mae: 0.637069 (1632.0757066367826 steps/sec)\n",
      "Step #13480\tEpoch   4 Batch  979/3125   Loss: 0.845670 mae: 0.718711 (1786.3457099293862 steps/sec)\n",
      "Step #13481\tEpoch   4 Batch  980/3125   Loss: 0.829510 mae: 0.709133 (1997.8584357435457 steps/sec)\n",
      "Step #13482\tEpoch   4 Batch  981/3125   Loss: 0.809038 mae: 0.749219 (2070.565933414952 steps/sec)\n",
      "Step #13483\tEpoch   4 Batch  982/3125   Loss: 0.851906 mae: 0.709503 (2221.0417063819873 steps/sec)\n",
      "Step #13484\tEpoch   4 Batch  983/3125   Loss: 0.806309 mae: 0.707158 (2233.935894841122 steps/sec)\n",
      "Step #13485\tEpoch   4 Batch  984/3125   Loss: 0.821622 mae: 0.721009 (1974.6450227863356 steps/sec)\n",
      "Step #13486\tEpoch   4 Batch  985/3125   Loss: 0.858152 mae: 0.732107 (1978.7066216292717 steps/sec)\n",
      "Step #13487\tEpoch   4 Batch  986/3125   Loss: 0.918424 mae: 0.775554 (1804.9954383488543 steps/sec)\n",
      "Step #13488\tEpoch   4 Batch  987/3125   Loss: 0.697975 mae: 0.663147 (1926.413933108586 steps/sec)\n",
      "Step #13489\tEpoch   4 Batch  988/3125   Loss: 0.696953 mae: 0.648135 (2138.29275255924 steps/sec)\n",
      "Step #13490\tEpoch   4 Batch  989/3125   Loss: 0.792520 mae: 0.717511 (2082.925618028863 steps/sec)\n",
      "Step #13491\tEpoch   4 Batch  990/3125   Loss: 0.658670 mae: 0.626891 (1838.429777423229 steps/sec)\n",
      "Step #13492\tEpoch   4 Batch  991/3125   Loss: 0.813023 mae: 0.696532 (2256.506488196432 steps/sec)\n",
      "Step #13493\tEpoch   4 Batch  992/3125   Loss: 0.820141 mae: 0.686557 (2285.923568267533 steps/sec)\n",
      "Step #13494\tEpoch   4 Batch  993/3125   Loss: 0.806599 mae: 0.707512 (2157.4971965885825 steps/sec)\n",
      "Step #13495\tEpoch   4 Batch  994/3125   Loss: 0.709266 mae: 0.651882 (2122.0435710888723 steps/sec)\n",
      "Step #13496\tEpoch   4 Batch  995/3125   Loss: 0.701180 mae: 0.653939 (1782.3376932425656 steps/sec)\n",
      "Step #13497\tEpoch   4 Batch  996/3125   Loss: 0.803885 mae: 0.725151 (2115.8561685298037 steps/sec)\n",
      "Step #13498\tEpoch   4 Batch  997/3125   Loss: 0.765002 mae: 0.705077 (2170.785028155018 steps/sec)\n",
      "Step #13499\tEpoch   4 Batch  998/3125   Loss: 0.718106 mae: 0.699714 (1980.0330453665674 steps/sec)\n",
      "Step #13500\tEpoch   4 Batch  999/3125   Loss: 0.870637 mae: 0.725783 (1978.3333018885724 steps/sec)\n",
      "Step #13501\tEpoch   4 Batch 1000/3125   Loss: 0.891098 mae: 0.745227 (2220.8535423064704 steps/sec)\n",
      "Step #13502\tEpoch   4 Batch 1001/3125   Loss: 0.762292 mae: 0.701555 (2071.158955113328 steps/sec)\n",
      "Step #13503\tEpoch   4 Batch 1002/3125   Loss: 0.781647 mae: 0.692703 (1859.3421402606614 steps/sec)\n",
      "Step #13504\tEpoch   4 Batch 1003/3125   Loss: 1.007232 mae: 0.812322 (1733.2694183182637 steps/sec)\n",
      "Step #13505\tEpoch   4 Batch 1004/3125   Loss: 0.756539 mae: 0.680646 (2173.4397346875326 steps/sec)\n",
      "Step #13506\tEpoch   4 Batch 1005/3125   Loss: 0.657220 mae: 0.655896 (2203.515702982989 steps/sec)\n",
      "Step #13507\tEpoch   4 Batch 1006/3125   Loss: 0.815673 mae: 0.682580 (1954.347805828138 steps/sec)\n",
      "Step #13508\tEpoch   4 Batch 1007/3125   Loss: 0.836971 mae: 0.737687 (1746.782388512219 steps/sec)\n",
      "Step #13509\tEpoch   4 Batch 1008/3125   Loss: 0.746775 mae: 0.671904 (2107.5633630132857 steps/sec)\n",
      "Step #13510\tEpoch   4 Batch 1009/3125   Loss: 0.853590 mae: 0.737417 (1794.876798384129 steps/sec)\n",
      "Step #13511\tEpoch   4 Batch 1010/3125   Loss: 0.937615 mae: 0.742802 (1842.7105300154647 steps/sec)\n",
      "Step #13512\tEpoch   4 Batch 1011/3125   Loss: 0.802817 mae: 0.712009 (1984.4360333081 steps/sec)\n",
      "Step #13513\tEpoch   4 Batch 1012/3125   Loss: 0.775055 mae: 0.697359 (2026.5079334402722 steps/sec)\n",
      "Step #13514\tEpoch   4 Batch 1013/3125   Loss: 0.802394 mae: 0.698511 (2243.279207581884 steps/sec)\n",
      "Step #13515\tEpoch   4 Batch 1014/3125   Loss: 0.800355 mae: 0.708956 (2193.2606831349744 steps/sec)\n",
      "Step #13516\tEpoch   4 Batch 1015/3125   Loss: 0.857976 mae: 0.725837 (2050.9642843171773 steps/sec)\n",
      "Step #13517\tEpoch   4 Batch 1016/3125   Loss: 0.891256 mae: 0.739833 (2107.83874242409 steps/sec)\n",
      "Step #13518\tEpoch   4 Batch 1017/3125   Loss: 0.818108 mae: 0.737076 (2087.507714359658 steps/sec)\n",
      "Step #13519\tEpoch   4 Batch 1018/3125   Loss: 0.734438 mae: 0.672288 (1534.9694419030193 steps/sec)\n",
      "Step #13520\tEpoch   4 Batch 1019/3125   Loss: 0.818805 mae: 0.722335 (2083.5050419750632 steps/sec)\n",
      "Step #13521\tEpoch   4 Batch 1020/3125   Loss: 0.792952 mae: 0.715878 (1978.7066216292717 steps/sec)\n",
      "Step #13522\tEpoch   4 Batch 1021/3125   Loss: 0.865927 mae: 0.703377 (2135.114332837857 steps/sec)\n",
      "Step #13523\tEpoch   4 Batch 1022/3125   Loss: 0.841247 mae: 0.722117 (2007.785468784406 steps/sec)\n",
      "Step #13524\tEpoch   4 Batch 1023/3125   Loss: 0.741329 mae: 0.685282 (1882.6773915541512 steps/sec)\n",
      "Step #13525\tEpoch   4 Batch 1024/3125   Loss: 0.802091 mae: 0.706034 (2114.7893431216344 steps/sec)\n",
      "Step #13526\tEpoch   4 Batch 1025/3125   Loss: 0.777204 mae: 0.694775 (2244.5757342238207 steps/sec)\n",
      "Step #13527\tEpoch   4 Batch 1026/3125   Loss: 0.804777 mae: 0.725219 (2162.5251348257834 steps/sec)\n",
      "Step #13528\tEpoch   4 Batch 1027/3125   Loss: 0.827331 mae: 0.728997 (2034.3318329970512 steps/sec)\n",
      "Step #13529\tEpoch   4 Batch 1028/3125   Loss: 0.891800 mae: 0.733065 (1710.2854346762356 steps/sec)\n",
      "Step #13530\tEpoch   4 Batch 1029/3125   Loss: 0.752562 mae: 0.691428 (1979.4163174387436 steps/sec)\n",
      "Step #13531\tEpoch   4 Batch 1030/3125   Loss: 0.721588 mae: 0.661750 (1887.778487906311 steps/sec)\n",
      "Step #13532\tEpoch   4 Batch 1031/3125   Loss: 0.764974 mae: 0.676079 (2212.3956915740946 steps/sec)\n",
      "Step #13533\tEpoch   4 Batch 1032/3125   Loss: 0.894410 mae: 0.729108 (2060.9412620261996 steps/sec)\n",
      "Step #13534\tEpoch   4 Batch 1033/3125   Loss: 0.818249 mae: 0.719055 (1931.3993111197067 steps/sec)\n",
      "Step #13535\tEpoch   4 Batch 1034/3125   Loss: 0.788186 mae: 0.692739 (1969.6006611818625 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13536\tEpoch   4 Batch 1035/3125   Loss: 0.829230 mae: 0.736447 (1911.541336250114 steps/sec)\n",
      "Step #13537\tEpoch   4 Batch 1036/3125   Loss: 0.909069 mae: 0.742473 (1925.158352764059 steps/sec)\n",
      "Step #13538\tEpoch   4 Batch 1037/3125   Loss: 0.680651 mae: 0.674159 (1953.5285788806916 steps/sec)\n",
      "Step #13539\tEpoch   4 Batch 1038/3125   Loss: 0.863381 mae: 0.734751 (1940.0828892835998 steps/sec)\n",
      "Step #13540\tEpoch   4 Batch 1039/3125   Loss: 0.792074 mae: 0.683886 (2143.691543407374 steps/sec)\n",
      "Step #13541\tEpoch   4 Batch 1040/3125   Loss: 0.695121 mae: 0.673997 (2105.1938404705975 steps/sec)\n",
      "Step #13542\tEpoch   4 Batch 1041/3125   Loss: 0.963193 mae: 0.756949 (2131.360333350272 steps/sec)\n",
      "Step #13543\tEpoch   4 Batch 1042/3125   Loss: 0.765599 mae: 0.669236 (1963.69901494438 steps/sec)\n",
      "Step #13544\tEpoch   4 Batch 1043/3125   Loss: 0.829172 mae: 0.718596 (1781.6411659261398 steps/sec)\n",
      "Step #13545\tEpoch   4 Batch 1044/3125   Loss: 0.825157 mae: 0.728417 (1837.22185233206 steps/sec)\n",
      "Step #13546\tEpoch   4 Batch 1045/3125   Loss: 0.956339 mae: 0.773194 (1695.7096883742743 steps/sec)\n",
      "Step #13547\tEpoch   4 Batch 1046/3125   Loss: 0.727985 mae: 0.675213 (1790.7692833172514 steps/sec)\n",
      "Step #13548\tEpoch   4 Batch 1047/3125   Loss: 0.766345 mae: 0.696292 (2044.0675653284209 steps/sec)\n",
      "Step #13549\tEpoch   4 Batch 1048/3125   Loss: 0.775359 mae: 0.697130 (2171.1895641370743 steps/sec)\n",
      "Step #13550\tEpoch   4 Batch 1049/3125   Loss: 0.874838 mae: 0.743905 (2220.547843672903 steps/sec)\n",
      "Step #13551\tEpoch   4 Batch 1050/3125   Loss: 0.762398 mae: 0.706834 (1874.3984841442923 steps/sec)\n",
      "Step #13552\tEpoch   4 Batch 1051/3125   Loss: 0.829583 mae: 0.735935 (2000.94649263415 steps/sec)\n",
      "Step #13553\tEpoch   4 Batch 1052/3125   Loss: 0.717314 mae: 0.651540 (1935.5884334591635 steps/sec)\n",
      "Step #13554\tEpoch   4 Batch 1053/3125   Loss: 0.858317 mae: 0.728457 (1831.3818639094593 steps/sec)\n",
      "Step #13555\tEpoch   4 Batch 1054/3125   Loss: 0.707896 mae: 0.689090 (1768.0923354494946 steps/sec)\n",
      "Step #13556\tEpoch   4 Batch 1055/3125   Loss: 0.750183 mae: 0.695782 (1949.6239553022767 steps/sec)\n",
      "Step #13557\tEpoch   4 Batch 1056/3125   Loss: 0.947906 mae: 0.792838 (1827.0261793788386 steps/sec)\n",
      "Step #13558\tEpoch   4 Batch 1057/3125   Loss: 0.829864 mae: 0.730212 (1848.557929624145 steps/sec)\n",
      "Step #13559\tEpoch   4 Batch 1058/3125   Loss: 0.812398 mae: 0.718268 (2027.8208066216073 steps/sec)\n",
      "Step #13560\tEpoch   4 Batch 1059/3125   Loss: 0.709240 mae: 0.673010 (1678.8632269943562 steps/sec)\n",
      "Step #13561\tEpoch   4 Batch 1060/3125   Loss: 0.859542 mae: 0.739217 (1504.4347838562965 steps/sec)\n",
      "Step #13562\tEpoch   4 Batch 1061/3125   Loss: 0.765282 mae: 0.663992 (1784.6431397911686 steps/sec)\n",
      "Step #13563\tEpoch   4 Batch 1062/3125   Loss: 0.741282 mae: 0.689242 (1918.4309707636576 steps/sec)\n",
      "Step #13564\tEpoch   4 Batch 1063/3125   Loss: 0.631616 mae: 0.626050 (1910.5138973662874 steps/sec)\n",
      "Step #13565\tEpoch   4 Batch 1064/3125   Loss: 0.663944 mae: 0.656894 (2053.6555749231284 steps/sec)\n",
      "Step #13566\tEpoch   4 Batch 1065/3125   Loss: 0.820070 mae: 0.718162 (1563.2305914800045 steps/sec)\n",
      "Step #13567\tEpoch   4 Batch 1066/3125   Loss: 0.733617 mae: 0.676248 (2144.7219324620073 steps/sec)\n",
      "Step #13568\tEpoch   4 Batch 1067/3125   Loss: 0.811332 mae: 0.710870 (1873.0926564370054 steps/sec)\n",
      "Step #13569\tEpoch   4 Batch 1068/3125   Loss: 0.778121 mae: 0.713392 (1764.2250843351196 steps/sec)\n",
      "Step #13570\tEpoch   4 Batch 1069/3125   Loss: 0.799257 mae: 0.700758 (1937.96736096993 steps/sec)\n",
      "Step #13571\tEpoch   4 Batch 1070/3125   Loss: 0.797155 mae: 0.710182 (1913.075842440386 steps/sec)\n",
      "Step #13572\tEpoch   4 Batch 1071/3125   Loss: 0.723083 mae: 0.670542 (1937.5197479651513 steps/sec)\n",
      "Step #13573\tEpoch   4 Batch 1072/3125   Loss: 0.750527 mae: 0.681525 (1945.6988050174423 steps/sec)\n",
      "Step #13574\tEpoch   4 Batch 1073/3125   Loss: 0.614979 mae: 0.618868 (1786.0870750153301 steps/sec)\n",
      "Step #13575\tEpoch   4 Batch 1074/3125   Loss: 0.807978 mae: 0.704130 (1880.7694722209767 steps/sec)\n",
      "Step #13576\tEpoch   4 Batch 1075/3125   Loss: 0.766692 mae: 0.704804 (1764.7743911675109 steps/sec)\n",
      "Step #13577\tEpoch   4 Batch 1076/3125   Loss: 0.758768 mae: 0.689536 (1650.7548684686953 steps/sec)\n",
      "Step #13578\tEpoch   4 Batch 1077/3125   Loss: 0.716272 mae: 0.654254 (2061.9544377476477 steps/sec)\n",
      "Step #13579\tEpoch   4 Batch 1078/3125   Loss: 0.836021 mae: 0.716668 (2055.5275667728497 steps/sec)\n",
      "Step #13580\tEpoch   4 Batch 1079/3125   Loss: 0.810540 mae: 0.704935 (2030.5499612703331 steps/sec)\n",
      "Step #13581\tEpoch   4 Batch 1080/3125   Loss: 0.833438 mae: 0.722806 (2119.5557037890503 steps/sec)\n",
      "Step #13582\tEpoch   4 Batch 1081/3125   Loss: 0.930366 mae: 0.766544 (1936.3390425188127 steps/sec)\n",
      "Step #13583\tEpoch   4 Batch 1082/3125   Loss: 0.908724 mae: 0.758532 (1664.7234393852798 steps/sec)\n",
      "Step #13584\tEpoch   4 Batch 1083/3125   Loss: 0.740379 mae: 0.679597 (1735.5644934372776 steps/sec)\n",
      "Step #13585\tEpoch   4 Batch 1084/3125   Loss: 0.790757 mae: 0.674638 (1745.9970694018916 steps/sec)\n",
      "Step #13586\tEpoch   4 Batch 1085/3125   Loss: 0.700931 mae: 0.658127 (1813.3610030263726 steps/sec)\n",
      "Step #13587\tEpoch   4 Batch 1086/3125   Loss: 0.881603 mae: 0.730328 (2098.64203584545 steps/sec)\n",
      "Step #13588\tEpoch   4 Batch 1087/3125   Loss: 0.773779 mae: 0.695454 (1834.6020942866392 steps/sec)\n",
      "Step #13589\tEpoch   4 Batch 1088/3125   Loss: 0.808567 mae: 0.729837 (2054.3194396826175 steps/sec)\n",
      "Step #13590\tEpoch   4 Batch 1089/3125   Loss: 0.819427 mae: 0.720673 (1877.603788957231 steps/sec)\n",
      "Step #13591\tEpoch   4 Batch 1090/3125   Loss: 0.855807 mae: 0.715509 (2151.0133749076886 steps/sec)\n",
      "Step #13592\tEpoch   4 Batch 1091/3125   Loss: 0.878656 mae: 0.741192 (1832.1018284745821 steps/sec)\n",
      "Step #13593\tEpoch   4 Batch 1092/3125   Loss: 0.771258 mae: 0.697625 (1709.630136874628 steps/sec)\n",
      "Step #13594\tEpoch   4 Batch 1093/3125   Loss: 0.789849 mae: 0.700712 (1939.8675398675398 steps/sec)\n",
      "Step #13595\tEpoch   4 Batch 1094/3125   Loss: 0.799969 mae: 0.702976 (2077.6223499108382 steps/sec)\n",
      "Step #13596\tEpoch   4 Batch 1095/3125   Loss: 0.731893 mae: 0.677907 (1890.126449938262 steps/sec)\n",
      "Step #13597\tEpoch   4 Batch 1096/3125   Loss: 0.696089 mae: 0.655212 (2091.358935747978 steps/sec)\n",
      "Step #13598\tEpoch   4 Batch 1097/3125   Loss: 0.939172 mae: 0.746785 (1988.9906864697737 steps/sec)\n",
      "Step #13599\tEpoch   4 Batch 1098/3125   Loss: 0.918544 mae: 0.761707 (2036.7620065070655 steps/sec)\n",
      "Step #13600\tEpoch   4 Batch 1099/3125   Loss: 0.762712 mae: 0.708892 (1887.3028015010934 steps/sec)\n",
      "Step #13601\tEpoch   4 Batch 1100/3125   Loss: 0.762917 mae: 0.685852 (1725.5113627013773 steps/sec)\n",
      "Step #13602\tEpoch   4 Batch 1101/3125   Loss: 0.836494 mae: 0.700842 (1777.3227679138947 steps/sec)\n",
      "Step #13603\tEpoch   4 Batch 1102/3125   Loss: 0.949257 mae: 0.762511 (2006.901633539719 steps/sec)\n",
      "Step #13604\tEpoch   4 Batch 1103/3125   Loss: 0.885452 mae: 0.737478 (1982.7099799568884 steps/sec)\n",
      "Step #13605\tEpoch   4 Batch 1104/3125   Loss: 0.863921 mae: 0.735609 (1963.349716800075 steps/sec)\n",
      "Step #13606\tEpoch   4 Batch 1105/3125   Loss: 0.679684 mae: 0.654726 (1924.7166365330079 steps/sec)\n",
      "Step #13607\tEpoch   4 Batch 1106/3125   Loss: 0.855009 mae: 0.719503 (2113.1698272908648 steps/sec)\n",
      "Step #13608\tEpoch   4 Batch 1107/3125   Loss: 0.752810 mae: 0.685331 (1583.234183904575 steps/sec)\n",
      "Step #13609\tEpoch   4 Batch 1108/3125   Loss: 0.911993 mae: 0.771606 (1871.6884136872357 steps/sec)\n",
      "Step #13610\tEpoch   4 Batch 1109/3125   Loss: 0.719618 mae: 0.673204 (1982.9536966121086 steps/sec)\n",
      "Step #13611\tEpoch   4 Batch 1110/3125   Loss: 0.918563 mae: 0.778784 (1824.9593177566028 steps/sec)\n",
      "Step #13612\tEpoch   4 Batch 1111/3125   Loss: 0.775166 mae: 0.706134 (2168.85432394978 steps/sec)\n",
      "Step #13613\tEpoch   4 Batch 1112/3125   Loss: 0.770883 mae: 0.690105 (2141.2401343666083 steps/sec)\n",
      "Step #13614\tEpoch   4 Batch 1113/3125   Loss: 0.843176 mae: 0.734204 (2196.91385830566 steps/sec)\n",
      "Step #13615\tEpoch   4 Batch 1114/3125   Loss: 0.787394 mae: 0.699338 (2132.032044243829 steps/sec)\n",
      "Step #13616\tEpoch   4 Batch 1115/3125   Loss: 0.844906 mae: 0.735395 (2061.1033032265673 steps/sec)\n",
      "Step #13617\tEpoch   4 Batch 1116/3125   Loss: 0.832176 mae: 0.711692 (1903.9574386955615 steps/sec)\n",
      "Step #13618\tEpoch   4 Batch 1117/3125   Loss: 0.725403 mae: 0.676914 (1739.5089581950897 steps/sec)\n",
      "Step #13619\tEpoch   4 Batch 1118/3125   Loss: 0.726502 mae: 0.665001 (2001.6913399956095 steps/sec)\n",
      "Step #13620\tEpoch   4 Batch 1119/3125   Loss: 0.770217 mae: 0.678765 (1516.8505030486701 steps/sec)\n",
      "Step #13621\tEpoch   4 Batch 1120/3125   Loss: 0.784423 mae: 0.712979 (1771.9765781446713 steps/sec)\n",
      "Step #13622\tEpoch   4 Batch 1121/3125   Loss: 0.821934 mae: 0.734354 (2172.2240636393767 steps/sec)\n",
      "Step #13623\tEpoch   4 Batch 1122/3125   Loss: 0.758735 mae: 0.675410 (2048.680225856241 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13624\tEpoch   4 Batch 1123/3125   Loss: 0.638947 mae: 0.634153 (1713.3734752734908 steps/sec)\n",
      "Step #13625\tEpoch   4 Batch 1124/3125   Loss: 0.802052 mae: 0.688849 (1845.1586791838602 steps/sec)\n",
      "Step #13626\tEpoch   4 Batch 1125/3125   Loss: 0.709583 mae: 0.648597 (1982.5412881330296 steps/sec)\n",
      "Step #13627\tEpoch   4 Batch 1126/3125   Loss: 0.863902 mae: 0.748382 (1906.4324933638777 steps/sec)\n",
      "Step #13628\tEpoch   4 Batch 1127/3125   Loss: 0.785114 mae: 0.675376 (1927.9901447036975 steps/sec)\n",
      "Step #13629\tEpoch   4 Batch 1128/3125   Loss: 0.838776 mae: 0.710152 (1993.547344506022 steps/sec)\n",
      "Step #13630\tEpoch   4 Batch 1129/3125   Loss: 0.840595 mae: 0.736409 (2157.031185715461 steps/sec)\n",
      "Step #13631\tEpoch   4 Batch 1130/3125   Loss: 0.748203 mae: 0.701660 (2105.067052115956 steps/sec)\n",
      "Step #13632\tEpoch   4 Batch 1131/3125   Loss: 0.719308 mae: 0.662775 (1756.3645804544274 steps/sec)\n",
      "Step #13633\tEpoch   4 Batch 1132/3125   Loss: 0.840779 mae: 0.720798 (1823.0872879956187 steps/sec)\n",
      "Step #13634\tEpoch   4 Batch 1133/3125   Loss: 0.916142 mae: 0.763290 (1889.7858037540664 steps/sec)\n",
      "Step #13635\tEpoch   4 Batch 1134/3125   Loss: 0.690133 mae: 0.664618 (1889.1389141616596 steps/sec)\n",
      "Step #13636\tEpoch   4 Batch 1135/3125   Loss: 0.739072 mae: 0.698479 (1892.2070539830913 steps/sec)\n",
      "Step #13637\tEpoch   4 Batch 1136/3125   Loss: 0.800993 mae: 0.704443 (2198.6873833637374 steps/sec)\n",
      "Step #13638\tEpoch   4 Batch 1137/3125   Loss: 0.801643 mae: 0.711355 (2091.0044469260374 steps/sec)\n",
      "Step #13639\tEpoch   4 Batch 1138/3125   Loss: 0.779387 mae: 0.696306 (1951.746859004188 steps/sec)\n",
      "Step #13640\tEpoch   4 Batch 1139/3125   Loss: 0.808565 mae: 0.732053 (1893.5909706546274 steps/sec)\n",
      "Step #13641\tEpoch   4 Batch 1140/3125   Loss: 0.783018 mae: 0.697511 (1941.6815576768172 steps/sec)\n",
      "Step #13642\tEpoch   4 Batch 1141/3125   Loss: 0.916736 mae: 0.754118 (1786.8632045328675 steps/sec)\n",
      "Step #13643\tEpoch   4 Batch 1142/3125   Loss: 0.619527 mae: 0.599971 (1839.5263365641858 steps/sec)\n",
      "Step #13644\tEpoch   4 Batch 1143/3125   Loss: 0.959230 mae: 0.785949 (2063.212159968518 steps/sec)\n",
      "Step #13645\tEpoch   4 Batch 1144/3125   Loss: 0.814107 mae: 0.708860 (1924.8579636717423 steps/sec)\n",
      "Step #13646\tEpoch   4 Batch 1145/3125   Loss: 0.740583 mae: 0.675944 (1923.6749894512832 steps/sec)\n",
      "Step #13647\tEpoch   4 Batch 1146/3125   Loss: 0.894791 mae: 0.740520 (1838.4942447115343 steps/sec)\n",
      "Step #13648\tEpoch   4 Batch 1147/3125   Loss: 0.872752 mae: 0.712461 (1719.569031961823 steps/sec)\n",
      "Step #13649\tEpoch   4 Batch 1148/3125   Loss: 0.943768 mae: 0.785914 (1647.6034096712103 steps/sec)\n",
      "Step #13650\tEpoch   4 Batch 1149/3125   Loss: 0.724913 mae: 0.678985 (1994.969654306425 steps/sec)\n",
      "Step #13651\tEpoch   4 Batch 1150/3125   Loss: 0.763857 mae: 0.720487 (2095.643136941402 steps/sec)\n",
      "Step #13652\tEpoch   4 Batch 1151/3125   Loss: 0.736480 mae: 0.667331 (2068.421623647536 steps/sec)\n",
      "Step #13653\tEpoch   4 Batch 1152/3125   Loss: 0.835588 mae: 0.747637 (2240.714583355593 steps/sec)\n",
      "Step #13654\tEpoch   4 Batch 1153/3125   Loss: 0.836608 mae: 0.722458 (2119.8985110232798 steps/sec)\n",
      "Step #13655\tEpoch   4 Batch 1154/3125   Loss: 0.720049 mae: 0.680970 (2115.322621317114 steps/sec)\n",
      "Step #13656\tEpoch   4 Batch 1155/3125   Loss: 0.832422 mae: 0.713021 (1964.5451990632318 steps/sec)\n",
      "Step #13657\tEpoch   4 Batch 1156/3125   Loss: 0.813920 mae: 0.726905 (1745.2560272296796 steps/sec)\n",
      "Step #13658\tEpoch   4 Batch 1157/3125   Loss: 0.771430 mae: 0.712478 (1971.4336745725109 steps/sec)\n",
      "Step #13659\tEpoch   4 Batch 1158/3125   Loss: 0.767927 mae: 0.696081 (2130.3860219423 steps/sec)\n",
      "Step #13660\tEpoch   4 Batch 1159/3125   Loss: 0.852451 mae: 0.726755 (2147.9290425662666 steps/sec)\n",
      "Step #13661\tEpoch   4 Batch 1160/3125   Loss: 0.724032 mae: 0.686065 (2079.5176899888943 steps/sec)\n",
      "Step #13662\tEpoch   4 Batch 1161/3125   Loss: 0.804224 mae: 0.715016 (1942.04117162251 steps/sec)\n",
      "Step #13663\tEpoch   4 Batch 1162/3125   Loss: 0.861092 mae: 0.751070 (1974.4033440974608 steps/sec)\n",
      "Step #13664\tEpoch   4 Batch 1163/3125   Loss: 0.723145 mae: 0.687245 (2029.7049059744684 steps/sec)\n",
      "Step #13665\tEpoch   4 Batch 1164/3125   Loss: 0.825732 mae: 0.703679 (1874.5660296404883 steps/sec)\n",
      "Step #13666\tEpoch   4 Batch 1165/3125   Loss: 0.899030 mae: 0.734026 (2049.080569831747 steps/sec)\n",
      "Step #13667\tEpoch   4 Batch 1166/3125   Loss: 0.796475 mae: 0.714867 (2084.3747825827677 steps/sec)\n",
      "Step #13668\tEpoch   4 Batch 1167/3125   Loss: 0.808379 mae: 0.706978 (2251.0325876948177 steps/sec)\n",
      "Step #13669\tEpoch   4 Batch 1168/3125   Loss: 0.771909 mae: 0.710178 (2081.3338626439063 steps/sec)\n",
      "Step #13670\tEpoch   4 Batch 1169/3125   Loss: 0.702395 mae: 0.654151 (2061.528782636049 steps/sec)\n",
      "Step #13671\tEpoch   4 Batch 1170/3125   Loss: 0.813051 mae: 0.684304 (2019.7160852899822 steps/sec)\n",
      "Step #13672\tEpoch   4 Batch 1171/3125   Loss: 0.790766 mae: 0.697522 (2073.9035413020047 steps/sec)\n",
      "Step #13673\tEpoch   4 Batch 1172/3125   Loss: 0.769241 mae: 0.685874 (1658.8111528574254 steps/sec)\n",
      "Step #13674\tEpoch   4 Batch 1173/3125   Loss: 0.769066 mae: 0.700282 (2088.110481614609 steps/sec)\n",
      "Step #13675\tEpoch   4 Batch 1174/3125   Loss: 0.729796 mae: 0.652560 (2082.387870001688 steps/sec)\n",
      "Step #13676\tEpoch   4 Batch 1175/3125   Loss: 0.733365 mae: 0.673747 (2244.6958587988483 steps/sec)\n",
      "Step #13677\tEpoch   4 Batch 1176/3125   Loss: 0.814216 mae: 0.682500 (1849.9929428369796 steps/sec)\n",
      "Step #13678\tEpoch   4 Batch 1177/3125   Loss: 0.832143 mae: 0.709824 (2047.2402819266288 steps/sec)\n",
      "Step #13679\tEpoch   4 Batch 1178/3125   Loss: 0.770686 mae: 0.678648 (2001.5194029280956 steps/sec)\n",
      "Step #13680\tEpoch   4 Batch 1179/3125   Loss: 0.733126 mae: 0.696682 (2069.544279313952 steps/sec)\n",
      "Step #13681\tEpoch   4 Batch 1180/3125   Loss: 0.792638 mae: 0.701580 (1922.1939102857875 steps/sec)\n",
      "Step #13682\tEpoch   4 Batch 1181/3125   Loss: 0.793381 mae: 0.722442 (2055.910436640982 steps/sec)\n",
      "Step #13683\tEpoch   4 Batch 1182/3125   Loss: 0.801535 mae: 0.705310 (1865.3289216209484 steps/sec)\n",
      "Step #13684\tEpoch   4 Batch 1183/3125   Loss: 0.706249 mae: 0.652529 (2076.347003029643 steps/sec)\n",
      "Step #13685\tEpoch   4 Batch 1184/3125   Loss: 0.818296 mae: 0.725825 (1992.978988282475 steps/sec)\n",
      "Step #13686\tEpoch   4 Batch 1185/3125   Loss: 0.758264 mae: 0.687065 (2079.78578866465 steps/sec)\n",
      "Step #13687\tEpoch   4 Batch 1186/3125   Loss: 0.852713 mae: 0.743827 (2174.859738454997 steps/sec)\n",
      "Step #13688\tEpoch   4 Batch 1187/3125   Loss: 0.653990 mae: 0.641732 (2008.9202237719364 steps/sec)\n",
      "Step #13689\tEpoch   4 Batch 1188/3125   Loss: 0.811301 mae: 0.717131 (2049.4209852534473 steps/sec)\n",
      "Step #13690\tEpoch   4 Batch 1189/3125   Loss: 0.634551 mae: 0.625071 (1890.3649753468121 steps/sec)\n",
      "Step #13691\tEpoch   4 Batch 1190/3125   Loss: 0.872820 mae: 0.719710 (1965.5763210677264 steps/sec)\n",
      "Step #13692\tEpoch   4 Batch 1191/3125   Loss: 0.794618 mae: 0.685512 (1993.774777772496 steps/sec)\n",
      "Step #13693\tEpoch   4 Batch 1192/3125   Loss: 0.864106 mae: 0.703280 (2064.1666174530997 steps/sec)\n",
      "Step #13694\tEpoch   4 Batch 1193/3125   Loss: 0.704117 mae: 0.648180 (2117.9074934356695 steps/sec)\n",
      "Step #13695\tEpoch   4 Batch 1194/3125   Loss: 0.876306 mae: 0.733367 (2232.841795939227 steps/sec)\n",
      "Step #13696\tEpoch   4 Batch 1195/3125   Loss: 0.946153 mae: 0.783445 (1880.4659128610242 steps/sec)\n",
      "Step #13697\tEpoch   4 Batch 1196/3125   Loss: 0.750770 mae: 0.678503 (2100.6601023709595 steps/sec)\n",
      "Step #13698\tEpoch   4 Batch 1197/3125   Loss: 0.866919 mae: 0.751600 (1891.797392990844 steps/sec)\n",
      "Step #13699\tEpoch   4 Batch 1198/3125   Loss: 0.763231 mae: 0.665155 (2006.0377647261387 steps/sec)\n",
      "Step #13700\tEpoch   4 Batch 1199/3125   Loss: 0.792863 mae: 0.699218 (2216.019273849275 steps/sec)\n",
      "Step #13701\tEpoch   4 Batch 1200/3125   Loss: 0.917541 mae: 0.752373 (2166.255552112385 steps/sec)\n",
      "Step #13702\tEpoch   4 Batch 1201/3125   Loss: 0.843211 mae: 0.703560 (2233.341142893655 steps/sec)\n",
      "Step #13703\tEpoch   4 Batch 1202/3125   Loss: 0.868967 mae: 0.767262 (2008.4392388212648 steps/sec)\n",
      "Step #13704\tEpoch   4 Batch 1203/3125   Loss: 0.745629 mae: 0.689321 (2106.4202490960224 steps/sec)\n",
      "Step #13705\tEpoch   4 Batch 1204/3125   Loss: 0.869880 mae: 0.749897 (2039.6541495248932 steps/sec)\n",
      "Step #13706\tEpoch   4 Batch 1205/3125   Loss: 0.761333 mae: 0.697045 (1792.2384692299147 steps/sec)\n",
      "Step #13707\tEpoch   4 Batch 1206/3125   Loss: 0.785355 mae: 0.711060 (2110.5115380357665 steps/sec)\n",
      "Step #13708\tEpoch   4 Batch 1207/3125   Loss: 0.852767 mae: 0.709647 (2255.293156106164 steps/sec)\n",
      "Step #13709\tEpoch   4 Batch 1208/3125   Loss: 0.817110 mae: 0.698360 (2094.7430454976775 steps/sec)\n",
      "Step #13710\tEpoch   4 Batch 1209/3125   Loss: 0.824769 mae: 0.704954 (2162.9042904290427 steps/sec)\n",
      "Step #13711\tEpoch   4 Batch 1210/3125   Loss: 0.747618 mae: 0.679509 (2246.4752070099516 steps/sec)\n",
      "Step #13712\tEpoch   4 Batch 1211/3125   Loss: 0.695116 mae: 0.656936 (2185.717263517739 steps/sec)\n",
      "Step #13713\tEpoch   4 Batch 1212/3125   Loss: 0.839559 mae: 0.705851 (1992.0513697328927 steps/sec)\n",
      "Step #13714\tEpoch   4 Batch 1213/3125   Loss: 0.849787 mae: 0.709077 (1980.0517400910173 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13715\tEpoch   4 Batch 1214/3125   Loss: 0.868074 mae: 0.757267 (1729.5671034943466 steps/sec)\n",
      "Step #13716\tEpoch   4 Batch 1215/3125   Loss: 0.650697 mae: 0.639229 (2042.0572942024187 steps/sec)\n",
      "Step #13717\tEpoch   4 Batch 1216/3125   Loss: 0.799059 mae: 0.675770 (1892.44610483951 steps/sec)\n",
      "Step #13718\tEpoch   4 Batch 1217/3125   Loss: 0.748493 mae: 0.665826 (2028.8016716810648 steps/sec)\n",
      "Step #13719\tEpoch   4 Batch 1218/3125   Loss: 0.773136 mae: 0.669553 (2127.66268287239 steps/sec)\n",
      "Step #13720\tEpoch   4 Batch 1219/3125   Loss: 0.912948 mae: 0.771446 (2337.3367215014937 steps/sec)\n",
      "Step #13721\tEpoch   4 Batch 1220/3125   Loss: 0.852198 mae: 0.721607 (2171.279481498354 steps/sec)\n",
      "Step #13722\tEpoch   4 Batch 1221/3125   Loss: 0.809768 mae: 0.718926 (2179.130904632266 steps/sec)\n",
      "Step #13723\tEpoch   4 Batch 1222/3125   Loss: 0.708089 mae: 0.643559 (2154.7927048548677 steps/sec)\n",
      "Step #13724\tEpoch   4 Batch 1223/3125   Loss: 0.583119 mae: 0.633443 (1984.8304451111594 steps/sec)\n",
      "Step #13725\tEpoch   4 Batch 1224/3125   Loss: 0.750489 mae: 0.694398 (1698.0850357486984 steps/sec)\n",
      "Step #13726\tEpoch   4 Batch 1225/3125   Loss: 0.815046 mae: 0.707239 (1901.6439821909485 steps/sec)\n",
      "Step #13727\tEpoch   4 Batch 1226/3125   Loss: 0.810233 mae: 0.717247 (2103.0616031047243 steps/sec)\n",
      "Step #13728\tEpoch   4 Batch 1227/3125   Loss: 0.875427 mae: 0.746477 (2286.6463860085264 steps/sec)\n",
      "Step #13729\tEpoch   4 Batch 1228/3125   Loss: 0.858966 mae: 0.738825 (2161.321639476044 steps/sec)\n",
      "Step #13730\tEpoch   4 Batch 1229/3125   Loss: 0.717488 mae: 0.681760 (2216.253460993807 steps/sec)\n",
      "Step #13731\tEpoch   4 Batch 1230/3125   Loss: 0.683571 mae: 0.650254 (2006.5943949556515 steps/sec)\n",
      "Step #13732\tEpoch   4 Batch 1231/3125   Loss: 0.728438 mae: 0.673716 (2043.151505704237 steps/sec)\n",
      "Step #13733\tEpoch   4 Batch 1232/3125   Loss: 0.806161 mae: 0.688777 (1829.368970149514 steps/sec)\n",
      "Step #13734\tEpoch   4 Batch 1233/3125   Loss: 0.733710 mae: 0.670875 (1910.722778501599 steps/sec)\n",
      "Step #13735\tEpoch   4 Batch 1234/3125   Loss: 0.776441 mae: 0.681680 (1851.1687027752278 steps/sec)\n",
      "Step #13736\tEpoch   4 Batch 1235/3125   Loss: 0.892949 mae: 0.748183 (1998.7152728139147 steps/sec)\n",
      "Step #13737\tEpoch   4 Batch 1236/3125   Loss: 0.753519 mae: 0.693594 (2069.8710989162832 steps/sec)\n",
      "Step #13738\tEpoch   4 Batch 1237/3125   Loss: 0.904216 mae: 0.774958 (2010.518747183848 steps/sec)\n",
      "Step #13739\tEpoch   4 Batch 1238/3125   Loss: 0.913955 mae: 0.731186 (1942.8867889568278 steps/sec)\n",
      "Step #13740\tEpoch   4 Batch 1239/3125   Loss: 0.825251 mae: 0.717754 (1938.7377393201505 steps/sec)\n",
      "Step #13741\tEpoch   4 Batch 1240/3125   Loss: 0.736022 mae: 0.669185 (1941.6635804755203 steps/sec)\n",
      "Step #13742\tEpoch   4 Batch 1241/3125   Loss: 0.815879 mae: 0.719251 (1790.7539919733583 steps/sec)\n",
      "Step #13743\tEpoch   4 Batch 1242/3125   Loss: 0.782831 mae: 0.723178 (2239.2312209705833 steps/sec)\n",
      "Step #13744\tEpoch   4 Batch 1243/3125   Loss: 0.833180 mae: 0.707843 (2211.1127511966765 steps/sec)\n",
      "Step #13745\tEpoch   4 Batch 1244/3125   Loss: 0.907528 mae: 0.733079 (1979.7339777779875 steps/sec)\n",
      "Step #13746\tEpoch   4 Batch 1245/3125   Loss: 0.949740 mae: 0.780710 (2099.524462642786 steps/sec)\n",
      "Step #13747\tEpoch   4 Batch 1246/3125   Loss: 0.791953 mae: 0.703293 (1993.3578564163981 steps/sec)\n",
      "Step #13748\tEpoch   4 Batch 1247/3125   Loss: 0.930063 mae: 0.784800 (2302.437310614378 steps/sec)\n",
      "Step #13749\tEpoch   4 Batch 1248/3125   Loss: 0.789631 mae: 0.699154 (2101.712716595011 steps/sec)\n",
      "Step #13750\tEpoch   4 Batch 1249/3125   Loss: 0.902859 mae: 0.751212 (1766.662454615145 steps/sec)\n",
      "Step #13751\tEpoch   4 Batch 1250/3125   Loss: 0.800687 mae: 0.696338 (2070.647709320695 steps/sec)\n",
      "Step #13752\tEpoch   4 Batch 1251/3125   Loss: 0.844348 mae: 0.737667 (2279.2652972502988 steps/sec)\n",
      "Step #13753\tEpoch   4 Batch 1252/3125   Loss: 0.748448 mae: 0.679271 (2168.562772084751 steps/sec)\n",
      "Step #13754\tEpoch   4 Batch 1253/3125   Loss: 0.938443 mae: 0.769164 (2093.99007498677 steps/sec)\n",
      "Step #13755\tEpoch   4 Batch 1254/3125   Loss: 0.929175 mae: 0.756946 (2142.618361633871 steps/sec)\n",
      "Step #13756\tEpoch   4 Batch 1255/3125   Loss: 0.830682 mae: 0.709566 (2044.984446763074 steps/sec)\n",
      "Step #13757\tEpoch   4 Batch 1256/3125   Loss: 0.776900 mae: 0.702371 (2087.6739602205985 steps/sec)\n",
      "Step #13758\tEpoch   4 Batch 1257/3125   Loss: 1.046528 mae: 0.778271 (2229.0445670312383 steps/sec)\n",
      "Step #13759\tEpoch   4 Batch 1258/3125   Loss: 0.795154 mae: 0.711207 (1829.5604836599027 steps/sec)\n",
      "Step #13760\tEpoch   4 Batch 1259/3125   Loss: 0.792095 mae: 0.712888 (1944.832702722754 steps/sec)\n",
      "Step #13761\tEpoch   4 Batch 1260/3125   Loss: 0.815551 mae: 0.737166 (2100.9968241882643 steps/sec)\n",
      "Step #13762\tEpoch   4 Batch 1261/3125   Loss: 0.744006 mae: 0.679637 (2019.7938938649716 steps/sec)\n",
      "Step #13763\tEpoch   4 Batch 1262/3125   Loss: 0.801595 mae: 0.700015 (2220.336255452505 steps/sec)\n",
      "Step #13764\tEpoch   4 Batch 1263/3125   Loss: 0.703366 mae: 0.664668 (2115.685404140269 steps/sec)\n",
      "Step #13765\tEpoch   4 Batch 1264/3125   Loss: 0.807720 mae: 0.697876 (2250.0423797006597 steps/sec)\n",
      "Step #13766\tEpoch   4 Batch 1265/3125   Loss: 0.768186 mae: 0.691946 (2107.0339894103345 steps/sec)\n",
      "Step #13767\tEpoch   4 Batch 1266/3125   Loss: 0.782662 mae: 0.704768 (2153.022945434013 steps/sec)\n",
      "Step #13768\tEpoch   4 Batch 1267/3125   Loss: 0.770865 mae: 0.698926 (1857.004214925796 steps/sec)\n",
      "Step #13769\tEpoch   4 Batch 1268/3125   Loss: 0.944922 mae: 0.741517 (2069.442169352372 steps/sec)\n",
      "Step #13770\tEpoch   4 Batch 1269/3125   Loss: 0.785790 mae: 0.673971 (2001.6913399956095 steps/sec)\n",
      "Step #13771\tEpoch   4 Batch 1270/3125   Loss: 0.801636 mae: 0.704760 (2173.552365652692 steps/sec)\n",
      "Step #13772\tEpoch   4 Batch 1271/3125   Loss: 0.944544 mae: 0.773131 (2278.200601827208 steps/sec)\n",
      "Step #13773\tEpoch   4 Batch 1272/3125   Loss: 0.874461 mae: 0.730260 (2323.508165481176 steps/sec)\n",
      "Step #13774\tEpoch   4 Batch 1273/3125   Loss: 0.749645 mae: 0.698111 (2231.748768210793 steps/sec)\n",
      "Step #13775\tEpoch   4 Batch 1274/3125   Loss: 0.873394 mae: 0.716728 (2084.209061726677 steps/sec)\n",
      "Step #13776\tEpoch   4 Batch 1275/3125   Loss: 0.801837 mae: 0.716417 (2022.3259402121505 steps/sec)\n",
      "Step #13777\tEpoch   4 Batch 1276/3125   Loss: 0.834589 mae: 0.716732 (1821.361449340814 steps/sec)\n",
      "Step #13778\tEpoch   4 Batch 1277/3125   Loss: 0.880496 mae: 0.757422 (1949.3158834027365 steps/sec)\n",
      "Step #13779\tEpoch   4 Batch 1278/3125   Loss: 0.856145 mae: 0.724562 (2256.919318560928 steps/sec)\n",
      "Step #13780\tEpoch   4 Batch 1279/3125   Loss: 0.953993 mae: 0.765104 (2102.59772811582 steps/sec)\n",
      "Step #13781\tEpoch   4 Batch 1280/3125   Loss: 0.825731 mae: 0.739061 (1870.419721374931 steps/sec)\n",
      "Step #13782\tEpoch   4 Batch 1281/3125   Loss: 0.875909 mae: 0.721355 (2170.223422640299 steps/sec)\n",
      "Step #13783\tEpoch   4 Batch 1282/3125   Loss: 0.739485 mae: 0.698018 (2213.7502243146528 steps/sec)\n",
      "Step #13784\tEpoch   4 Batch 1283/3125   Loss: 0.658170 mae: 0.644768 (1962.688229403562 steps/sec)\n",
      "Step #13785\tEpoch   4 Batch 1284/3125   Loss: 0.860809 mae: 0.738137 (2179.28941817086 steps/sec)\n",
      "Step #13786\tEpoch   4 Batch 1285/3125   Loss: 0.807729 mae: 0.722447 (2034.0753241966615 steps/sec)\n",
      "Step #13787\tEpoch   4 Batch 1286/3125   Loss: 0.741549 mae: 0.688717 (1735.9236480725774 steps/sec)\n",
      "Step #13788\tEpoch   4 Batch 1287/3125   Loss: 0.748549 mae: 0.685832 (2021.3513253012047 steps/sec)\n",
      "Step #13789\tEpoch   4 Batch 1288/3125   Loss: 0.917954 mae: 0.743317 (2118.378149053516 steps/sec)\n",
      "Step #13790\tEpoch   4 Batch 1289/3125   Loss: 0.818712 mae: 0.710523 (2069.360488637597 steps/sec)\n",
      "Step #13791\tEpoch   4 Batch 1290/3125   Loss: 0.703695 mae: 0.685650 (2267.583582024999 steps/sec)\n",
      "Step #13792\tEpoch   4 Batch 1291/3125   Loss: 0.873804 mae: 0.742503 (2153.420887798166 steps/sec)\n",
      "Step #13793\tEpoch   4 Batch 1292/3125   Loss: 0.810042 mae: 0.682291 (2195.9936753264433 steps/sec)\n",
      "Step #13794\tEpoch   4 Batch 1293/3125   Loss: 0.817708 mae: 0.717731 (2088.9008416753823 steps/sec)\n",
      "Step #13795\tEpoch   4 Batch 1294/3125   Loss: 0.748148 mae: 0.686787 (1705.320507086691 steps/sec)\n",
      "Step #13796\tEpoch   4 Batch 1295/3125   Loss: 0.746893 mae: 0.703869 (1758.514804162439 steps/sec)\n",
      "Step #13797\tEpoch   4 Batch 1296/3125   Loss: 0.699775 mae: 0.665394 (2208.388530269684 steps/sec)\n",
      "Step #13798\tEpoch   4 Batch 1297/3125   Loss: 0.853218 mae: 0.707354 (2097.6764191047764 steps/sec)\n",
      "Step #13799\tEpoch   4 Batch 1298/3125   Loss: 0.704942 mae: 0.659839 (2064.247888654842 steps/sec)\n",
      "Step #13800\tEpoch   4 Batch 1299/3125   Loss: 0.769386 mae: 0.696704 (2228.57082133407 steps/sec)\n",
      "Step #13801\tEpoch   4 Batch 1300/3125   Loss: 0.731046 mae: 0.657327 (2310.4275688836497 steps/sec)\n",
      "Step #13802\tEpoch   4 Batch 1301/3125   Loss: 0.745045 mae: 0.681468 (2112.3822762114846 steps/sec)\n",
      "Step #13803\tEpoch   4 Batch 1302/3125   Loss: 0.731605 mae: 0.671334 (2111.042660707455 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13804\tEpoch   4 Batch 1303/3125   Loss: 0.700256 mae: 0.661465 (1999.1344384812637 steps/sec)\n",
      "Step #13805\tEpoch   4 Batch 1304/3125   Loss: 0.700241 mae: 0.670068 (2069.70767621341 steps/sec)\n",
      "Step #13806\tEpoch   4 Batch 1305/3125   Loss: 0.715809 mae: 0.675002 (2171.729180042665 steps/sec)\n",
      "Step #13807\tEpoch   4 Batch 1306/3125   Loss: 0.698642 mae: 0.653659 (2160.8539751885587 steps/sec)\n",
      "Step #13808\tEpoch   4 Batch 1307/3125   Loss: 0.978492 mae: 0.774317 (2218.386840852594 steps/sec)\n",
      "Step #13809\tEpoch   4 Batch 1308/3125   Loss: 0.818370 mae: 0.690193 (2124.946297572245 steps/sec)\n",
      "Step #13810\tEpoch   4 Batch 1309/3125   Loss: 0.801164 mae: 0.712895 (1966.7007399210377 steps/sec)\n",
      "Step #13811\tEpoch   4 Batch 1310/3125   Loss: 0.762696 mae: 0.682927 (2173.4172098952236 steps/sec)\n",
      "Step #13812\tEpoch   4 Batch 1311/3125   Loss: 0.779181 mae: 0.708355 (2199.448342405269 steps/sec)\n",
      "Step #13813\tEpoch   4 Batch 1312/3125   Loss: 0.774627 mae: 0.697202 (1938.7377393201505 steps/sec)\n",
      "Step #13814\tEpoch   4 Batch 1313/3125   Loss: 0.774386 mae: 0.704440 (1993.6231498293612 steps/sec)\n",
      "Step #13815\tEpoch   4 Batch 1314/3125   Loss: 0.837928 mae: 0.745060 (1942.1310959233947 steps/sec)\n",
      "Step #13816\tEpoch   4 Batch 1315/3125   Loss: 0.889717 mae: 0.752803 (2209.5286259139853 steps/sec)\n",
      "Step #13817\tEpoch   4 Batch 1316/3125   Loss: 0.806102 mae: 0.703131 (2231.6537728922135 steps/sec)\n",
      "Step #13818\tEpoch   4 Batch 1317/3125   Loss: 0.677019 mae: 0.658989 (2094.7639690752544 steps/sec)\n",
      "Step #13819\tEpoch   4 Batch 1318/3125   Loss: 0.816930 mae: 0.699946 (2191.9311007985284 steps/sec)\n",
      "Step #13820\tEpoch   4 Batch 1319/3125   Loss: 0.812411 mae: 0.699689 (2134.288622023204 steps/sec)\n",
      "Step #13821\tEpoch   4 Batch 1320/3125   Loss: 0.730390 mae: 0.647463 (2165.159665080168 steps/sec)\n",
      "Step #13822\tEpoch   4 Batch 1321/3125   Loss: 0.825589 mae: 0.736409 (1763.4091788170795 steps/sec)\n",
      "Step #13823\tEpoch   4 Batch 1322/3125   Loss: 0.670527 mae: 0.635429 (1858.3865021976465 steps/sec)\n",
      "Step #13824\tEpoch   4 Batch 1323/3125   Loss: 0.808971 mae: 0.687771 (2084.105499572675 steps/sec)\n",
      "Step #13825\tEpoch   4 Batch 1324/3125   Loss: 0.729117 mae: 0.685238 (2101.1020718951627 steps/sec)\n",
      "Step #13826\tEpoch   4 Batch 1325/3125   Loss: 0.757812 mae: 0.676566 (2080.9208176225443 steps/sec)\n",
      "Step #13827\tEpoch   4 Batch 1326/3125   Loss: 0.746401 mae: 0.666470 (1829.0817750488418 steps/sec)\n",
      "Step #13828\tEpoch   4 Batch 1327/3125   Loss: 0.697915 mae: 0.658617 (1953.3284278568967 steps/sec)\n",
      "Step #13829\tEpoch   4 Batch 1328/3125   Loss: 0.849666 mae: 0.727896 (2009.4014391522223 steps/sec)\n",
      "Step #13830\tEpoch   4 Batch 1329/3125   Loss: 0.723639 mae: 0.693203 (1455.9713408961525 steps/sec)\n",
      "Step #13831\tEpoch   4 Batch 1330/3125   Loss: 0.886497 mae: 0.740601 (1589.7752340522306 steps/sec)\n",
      "Step #13832\tEpoch   4 Batch 1331/3125   Loss: 0.628107 mae: 0.610666 (2084.62341328615 steps/sec)\n",
      "Step #13833\tEpoch   4 Batch 1332/3125   Loss: 0.816280 mae: 0.723708 (2186.2869176318504 steps/sec)\n",
      "Step #13834\tEpoch   4 Batch 1333/3125   Loss: 0.700819 mae: 0.640859 (2187.016508327163 steps/sec)\n",
      "Step #13835\tEpoch   4 Batch 1334/3125   Loss: 0.667540 mae: 0.643449 (2203.909369876834 steps/sec)\n",
      "Step #13836\tEpoch   4 Batch 1335/3125   Loss: 0.815955 mae: 0.728475 (2034.2726329165494 steps/sec)\n",
      "Step #13837\tEpoch   4 Batch 1336/3125   Loss: 0.760694 mae: 0.699737 (2131.317011697511 steps/sec)\n",
      "Step #13838\tEpoch   4 Batch 1337/3125   Loss: 0.675820 mae: 0.640924 (2074.8473905515707 steps/sec)\n",
      "Step #13839\tEpoch   4 Batch 1338/3125   Loss: 0.741498 mae: 0.704755 (1911.3671162960263 steps/sec)\n",
      "Step #13840\tEpoch   4 Batch 1339/3125   Loss: 0.860859 mae: 0.741521 (1975.1843654344243 steps/sec)\n",
      "Step #13841\tEpoch   4 Batch 1340/3125   Loss: 0.701419 mae: 0.679182 (2211.019504480759 steps/sec)\n",
      "Step #13842\tEpoch   4 Batch 1341/3125   Loss: 0.835202 mae: 0.702886 (2245.9940239683847 steps/sec)\n",
      "Step #13843\tEpoch   4 Batch 1342/3125   Loss: 0.826743 mae: 0.722436 (2114.149764103391 steps/sec)\n",
      "Step #13844\tEpoch   4 Batch 1343/3125   Loss: 0.695948 mae: 0.648620 (2043.3306701482938 steps/sec)\n",
      "Step #13845\tEpoch   4 Batch 1344/3125   Loss: 0.728386 mae: 0.667607 (2373.2566824344203 steps/sec)\n",
      "Step #13846\tEpoch   4 Batch 1345/3125   Loss: 0.804455 mae: 0.715623 (2089.775093918468 steps/sec)\n",
      "Step #13847\tEpoch   4 Batch 1346/3125   Loss: 0.752294 mae: 0.696273 (1881.3768850532435 steps/sec)\n",
      "Step #13848\tEpoch   4 Batch 1347/3125   Loss: 0.748454 mae: 0.686537 (2087.861018467818 steps/sec)\n",
      "Step #13849\tEpoch   4 Batch 1348/3125   Loss: 0.763454 mae: 0.716171 (1935.3562200073827 steps/sec)\n",
      "Step #13850\tEpoch   4 Batch 1349/3125   Loss: 0.826562 mae: 0.745190 (2315.8620080392243 steps/sec)\n",
      "Step #13851\tEpoch   4 Batch 1350/3125   Loss: 0.760393 mae: 0.663532 (2080.569858229908 steps/sec)\n",
      "Step #13852\tEpoch   4 Batch 1351/3125   Loss: 0.829330 mae: 0.730864 (1981.0617797090497 steps/sec)\n",
      "Step #13853\tEpoch   4 Batch 1352/3125   Loss: 0.809238 mae: 0.714086 (2274.0503789809263 steps/sec)\n",
      "Step #13854\tEpoch   4 Batch 1353/3125   Loss: 0.660856 mae: 0.637846 (1987.1248945867326 steps/sec)\n",
      "Step #13855\tEpoch   4 Batch 1354/3125   Loss: 0.683986 mae: 0.649095 (2192.297721095547 steps/sec)\n",
      "Step #13856\tEpoch   4 Batch 1355/3125   Loss: 0.795747 mae: 0.696461 (1600.8671689529087 steps/sec)\n",
      "Step #13857\tEpoch   4 Batch 1356/3125   Loss: 0.867677 mae: 0.733256 (2027.3307296701596 steps/sec)\n",
      "Step #13858\tEpoch   4 Batch 1357/3125   Loss: 0.713921 mae: 0.673908 (2408.2220410413056 steps/sec)\n",
      "Step #13859\tEpoch   4 Batch 1358/3125   Loss: 0.985227 mae: 0.772952 (2298.52584969147 steps/sec)\n",
      "Step #13860\tEpoch   4 Batch 1359/3125   Loss: 0.954715 mae: 0.772807 (1952.4918768445848 steps/sec)\n",
      "Step #13861\tEpoch   4 Batch 1360/3125   Loss: 0.705316 mae: 0.672900 (2302.032930845225 steps/sec)\n",
      "Step #13862\tEpoch   4 Batch 1361/3125   Loss: 0.772218 mae: 0.703612 (2234.8167092924127 steps/sec)\n",
      "Step #13863\tEpoch   4 Batch 1362/3125   Loss: 0.764221 mae: 0.686382 (2195.510887772194 steps/sec)\n",
      "Step #13864\tEpoch   4 Batch 1363/3125   Loss: 0.964614 mae: 0.768086 (2192.3893953332777 steps/sec)\n",
      "Step #13865\tEpoch   4 Batch 1364/3125   Loss: 0.801515 mae: 0.712994 (1929.5156779037245 steps/sec)\n",
      "Step #13866\tEpoch   4 Batch 1365/3125   Loss: 0.723964 mae: 0.689456 (1645.7028062025238 steps/sec)\n",
      "Step #13867\tEpoch   4 Batch 1366/3125   Loss: 0.861656 mae: 0.733485 (2129.282878638657 steps/sec)\n",
      "Step #13868\tEpoch   4 Batch 1367/3125   Loss: 0.787969 mae: 0.703510 (2321.8103715513043 steps/sec)\n",
      "Step #13869\tEpoch   4 Batch 1368/3125   Loss: 0.863281 mae: 0.730429 (2195.487903183593 steps/sec)\n",
      "Step #13870\tEpoch   4 Batch 1369/3125   Loss: 0.717258 mae: 0.683153 (2164.1318817398483 steps/sec)\n",
      "Step #13871\tEpoch   4 Batch 1370/3125   Loss: 0.751220 mae: 0.705879 (2196.2696492716286 steps/sec)\n",
      "Step #13872\tEpoch   4 Batch 1371/3125   Loss: 0.788194 mae: 0.717719 (2311.828383711445 steps/sec)\n",
      "Step #13873\tEpoch   4 Batch 1372/3125   Loss: 0.899670 mae: 0.738058 (2091.2755158006003 steps/sec)\n",
      "Step #13874\tEpoch   4 Batch 1373/3125   Loss: 0.922596 mae: 0.746595 (1836.0958868129367 steps/sec)\n",
      "Step #13875\tEpoch   4 Batch 1374/3125   Loss: 0.736869 mae: 0.660877 (1952.0556998315228 steps/sec)\n",
      "Step #13876\tEpoch   4 Batch 1375/3125   Loss: 0.849037 mae: 0.720773 (2076.141448540767 steps/sec)\n",
      "Step #13877\tEpoch   4 Batch 1376/3125   Loss: 0.782014 mae: 0.680698 (2119.1915925626517 steps/sec)\n",
      "Step #13878\tEpoch   4 Batch 1377/3125   Loss: 0.855147 mae: 0.738045 (2076.573159984553 steps/sec)\n",
      "Step #13879\tEpoch   4 Batch 1378/3125   Loss: 0.764089 mae: 0.682596 (2082.822183378357 steps/sec)\n",
      "Step #13880\tEpoch   4 Batch 1379/3125   Loss: 0.823775 mae: 0.720590 (2114.6187508822877 steps/sec)\n",
      "Step #13881\tEpoch   4 Batch 1380/3125   Loss: 0.749162 mae: 0.678206 (1942.8147928555547 steps/sec)\n",
      "Step #13882\tEpoch   4 Batch 1381/3125   Loss: 0.851334 mae: 0.757513 (2094.680277272818 steps/sec)\n",
      "Step #13883\tEpoch   4 Batch 1382/3125   Loss: 0.611653 mae: 0.626192 (2234.3405071382913 steps/sec)\n",
      "Step #13884\tEpoch   4 Batch 1383/3125   Loss: 0.827042 mae: 0.716246 (1987.2943673716927 steps/sec)\n",
      "Step #13885\tEpoch   4 Batch 1384/3125   Loss: 0.865921 mae: 0.727655 (2095.391870828504 steps/sec)\n",
      "Step #13886\tEpoch   4 Batch 1385/3125   Loss: 0.694705 mae: 0.644639 (2152.8682297868845 steps/sec)\n",
      "Step #13887\tEpoch   4 Batch 1386/3125   Loss: 0.855205 mae: 0.737807 (2288.9674743505784 steps/sec)\n",
      "Step #13888\tEpoch   4 Batch 1387/3125   Loss: 0.741221 mae: 0.678099 (2196.637722449749 steps/sec)\n",
      "Step #13889\tEpoch   4 Batch 1388/3125   Loss: 0.733610 mae: 0.691405 (2355.397816613505 steps/sec)\n",
      "Step #13890\tEpoch   4 Batch 1389/3125   Loss: 0.786706 mae: 0.707205 (2194.316326957686 steps/sec)\n",
      "Step #13891\tEpoch   4 Batch 1390/3125   Loss: 0.816850 mae: 0.710967 (2143.056265200597 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13892\tEpoch   4 Batch 1391/3125   Loss: 0.770589 mae: 0.676464 (1987.3508647239992 steps/sec)\n",
      "Step #13893\tEpoch   4 Batch 1392/3125   Loss: 0.729295 mae: 0.697429 (2080.838228290205 steps/sec)\n",
      "Step #13894\tEpoch   4 Batch 1393/3125   Loss: 0.742828 mae: 0.673867 (2061.346412809499 steps/sec)\n",
      "Step #13895\tEpoch   4 Batch 1394/3125   Loss: 0.834692 mae: 0.722737 (2073.4729390362063 steps/sec)\n",
      "Step #13896\tEpoch   4 Batch 1395/3125   Loss: 0.776222 mae: 0.694510 (2097.6764191047764 steps/sec)\n",
      "Step #13897\tEpoch   4 Batch 1396/3125   Loss: 0.745105 mae: 0.697644 (2112.616351694403 steps/sec)\n",
      "Step #13898\tEpoch   4 Batch 1397/3125   Loss: 0.934678 mae: 0.763258 (2107.012819997589 steps/sec)\n",
      "Step #13899\tEpoch   4 Batch 1398/3125   Loss: 0.838950 mae: 0.730793 (2102.8718113268087 steps/sec)\n",
      "Step #13900\tEpoch   4 Batch 1399/3125   Loss: 0.825085 mae: 0.723155 (2149.8667322753927 steps/sec)\n",
      "Step #13901\tEpoch   4 Batch 1400/3125   Loss: 0.804578 mae: 0.725794 (1769.2856721026567 steps/sec)\n",
      "Step #13902\tEpoch   4 Batch 1401/3125   Loss: 0.780737 mae: 0.707215 (1937.5555493962324 steps/sec)\n",
      "Step #13903\tEpoch   4 Batch 1402/3125   Loss: 0.684474 mae: 0.678342 (2230.2535307129488 steps/sec)\n",
      "Step #13904\tEpoch   4 Batch 1403/3125   Loss: 0.730143 mae: 0.693728 (2110.1505272478466 steps/sec)\n",
      "Step #13905\tEpoch   4 Batch 1404/3125   Loss: 0.837644 mae: 0.731333 (2098.1171338815857 steps/sec)\n",
      "Step #13906\tEpoch   4 Batch 1405/3125   Loss: 0.805046 mae: 0.717416 (2373.9551731944757 steps/sec)\n",
      "Step #13907\tEpoch   4 Batch 1406/3125   Loss: 0.897442 mae: 0.758891 (2246.667737961326 steps/sec)\n",
      "Step #13908\tEpoch   4 Batch 1407/3125   Loss: 0.902091 mae: 0.768628 (2012.7184605787227 steps/sec)\n",
      "Step #13909\tEpoch   4 Batch 1408/3125   Loss: 0.876401 mae: 0.736241 (2106.5048816746353 steps/sec)\n",
      "Step #13910\tEpoch   4 Batch 1409/3125   Loss: 0.764456 mae: 0.664252 (1896.2448573624486 steps/sec)\n",
      "Step #13911\tEpoch   4 Batch 1410/3125   Loss: 0.842328 mae: 0.709040 (2000.4120721889428 steps/sec)\n",
      "Step #13912\tEpoch   4 Batch 1411/3125   Loss: 0.601405 mae: 0.623206 (2129.9965467508987 steps/sec)\n",
      "Step #13913\tEpoch   4 Batch 1412/3125   Loss: 0.895175 mae: 0.743840 (1924.9286350243697 steps/sec)\n",
      "Step #13914\tEpoch   4 Batch 1413/3125   Loss: 0.720468 mae: 0.658703 (1802.776607724643 steps/sec)\n",
      "Step #13915\tEpoch   4 Batch 1414/3125   Loss: 0.712253 mae: 0.659449 (2097.5715143028606 steps/sec)\n",
      "Step #13916\tEpoch   4 Batch 1415/3125   Loss: 0.795162 mae: 0.707046 (1965.4842125980563 steps/sec)\n",
      "Step #13917\tEpoch   4 Batch 1416/3125   Loss: 0.772756 mae: 0.678178 (1998.5628925123651 steps/sec)\n",
      "Step #13918\tEpoch   4 Batch 1417/3125   Loss: 0.782721 mae: 0.697879 (1805.6326144044083 steps/sec)\n",
      "Step #13919\tEpoch   4 Batch 1418/3125   Loss: 0.893949 mae: 0.718489 (1831.0300871356976 steps/sec)\n",
      "Step #13920\tEpoch   4 Batch 1419/3125   Loss: 0.818481 mae: 0.676535 (2138.9470248658795 steps/sec)\n",
      "Step #13921\tEpoch   4 Batch 1420/3125   Loss: 0.797872 mae: 0.716620 (1468.4190257462346 steps/sec)\n",
      "Step #13922\tEpoch   4 Batch 1421/3125   Loss: 0.727305 mae: 0.690867 (1846.9299327156798 steps/sec)\n",
      "Step #13923\tEpoch   4 Batch 1422/3125   Loss: 0.793556 mae: 0.690710 (2067.198296681091 steps/sec)\n",
      "Step #13924\tEpoch   4 Batch 1423/3125   Loss: 0.718852 mae: 0.679678 (2089.421141775431 steps/sec)\n",
      "Step #13925\tEpoch   4 Batch 1424/3125   Loss: 0.783544 mae: 0.689726 (1949.9321245932124 steps/sec)\n",
      "Step #13926\tEpoch   4 Batch 1425/3125   Loss: 0.976054 mae: 0.775737 (1950.2222552867 steps/sec)\n",
      "Step #13927\tEpoch   4 Batch 1426/3125   Loss: 0.836151 mae: 0.720836 (1983.1037058751217 steps/sec)\n",
      "Step #13928\tEpoch   4 Batch 1427/3125   Loss: 0.663942 mae: 0.638061 (2135.7449105333376 steps/sec)\n",
      "Step #13929\tEpoch   4 Batch 1428/3125   Loss: 0.753577 mae: 0.680833 (2062.6236796034386 steps/sec)\n",
      "Step #13930\tEpoch   4 Batch 1429/3125   Loss: 0.885771 mae: 0.739918 (2027.8404146280145 steps/sec)\n",
      "Step #13931\tEpoch   4 Batch 1430/3125   Loss: 0.738919 mae: 0.672214 (1965.6315902934643 steps/sec)\n",
      "Step #13932\tEpoch   4 Batch 1431/3125   Loss: 0.814865 mae: 0.716805 (2155.4349613550403 steps/sec)\n",
      "Step #13933\tEpoch   4 Batch 1432/3125   Loss: 0.869393 mae: 0.724586 (2001.118331281787 steps/sec)\n",
      "Step #13934\tEpoch   4 Batch 1433/3125   Loss: 0.780049 mae: 0.704015 (1855.4269738471883 steps/sec)\n",
      "Step #13935\tEpoch   4 Batch 1434/3125   Loss: 0.758493 mae: 0.689057 (1877.1500179018976 steps/sec)\n",
      "Step #13936\tEpoch   4 Batch 1435/3125   Loss: 0.797272 mae: 0.716722 (1812.5773552290407 steps/sec)\n",
      "Step #13937\tEpoch   4 Batch 1436/3125   Loss: 0.817725 mae: 0.694167 (2209.9477322542575 steps/sec)\n",
      "Step #13938\tEpoch   4 Batch 1437/3125   Loss: 0.795959 mae: 0.710010 (2123.3971892592444 steps/sec)\n",
      "Step #13939\tEpoch   4 Batch 1438/3125   Loss: 0.878522 mae: 0.747759 (2005.3088544654809 steps/sec)\n",
      "Step #13940\tEpoch   4 Batch 1439/3125   Loss: 0.901406 mae: 0.739336 (2024.1216894448305 steps/sec)\n",
      "Step #13941\tEpoch   4 Batch 1440/3125   Loss: 0.705728 mae: 0.667030 (2056.2732870533787 steps/sec)\n",
      "Step #13942\tEpoch   4 Batch 1441/3125   Loss: 0.713289 mae: 0.668947 (2009.0741876149602 steps/sec)\n",
      "Step #13943\tEpoch   4 Batch 1442/3125   Loss: 0.833496 mae: 0.746461 (1885.5211105516794 steps/sec)\n",
      "Step #13944\tEpoch   4 Batch 1443/3125   Loss: 0.754833 mae: 0.699898 (2013.7814480507009 steps/sec)\n",
      "Step #13945\tEpoch   4 Batch 1444/3125   Loss: 0.775030 mae: 0.695423 (1968.9719275185428 steps/sec)\n",
      "Step #13946\tEpoch   4 Batch 1445/3125   Loss: 0.837514 mae: 0.730391 (2035.6548664835325 steps/sec)\n",
      "Step #13947\tEpoch   4 Batch 1446/3125   Loss: 0.678436 mae: 0.642371 (2036.5048845384451 steps/sec)\n",
      "Step #13948\tEpoch   4 Batch 1447/3125   Loss: 0.775704 mae: 0.706665 (2081.4371495211158 steps/sec)\n",
      "Step #13949\tEpoch   4 Batch 1448/3125   Loss: 0.837171 mae: 0.734542 (1983.3475193402562 steps/sec)\n",
      "Step #13950\tEpoch   4 Batch 1449/3125   Loss: 0.773255 mae: 0.689789 (2161.588967109535 steps/sec)\n",
      "Step #13951\tEpoch   4 Batch 1450/3125   Loss: 0.793999 mae: 0.717607 (1804.0637957435094 steps/sec)\n",
      "Step #13952\tEpoch   4 Batch 1451/3125   Loss: 0.795145 mae: 0.697765 (2157.5193926050906 steps/sec)\n",
      "Step #13953\tEpoch   4 Batch 1452/3125   Loss: 0.770761 mae: 0.720360 (2147.6650827461904 steps/sec)\n",
      "Step #13954\tEpoch   4 Batch 1453/3125   Loss: 0.804013 mae: 0.704293 (2108.177769735718 steps/sec)\n",
      "Step #13955\tEpoch   4 Batch 1454/3125   Loss: 0.834253 mae: 0.720416 (2079.6001745269923 steps/sec)\n",
      "Step #13956\tEpoch   4 Batch 1455/3125   Loss: 0.752105 mae: 0.694474 (1869.6359956850824 steps/sec)\n",
      "Step #13957\tEpoch   4 Batch 1456/3125   Loss: 0.833329 mae: 0.710296 (2141.3275881433983 steps/sec)\n",
      "Step #13958\tEpoch   4 Batch 1457/3125   Loss: 0.778102 mae: 0.721850 (2275.432105463028 steps/sec)\n",
      "Step #13959\tEpoch   4 Batch 1458/3125   Loss: 0.795730 mae: 0.682756 (1724.8017896502945 steps/sec)\n",
      "Step #13960\tEpoch   4 Batch 1459/3125   Loss: 0.815183 mae: 0.685682 (1816.2034831859633 steps/sec)\n",
      "Step #13961\tEpoch   4 Batch 1460/3125   Loss: 0.828641 mae: 0.728244 (2084.892830158667 steps/sec)\n",
      "Step #13962\tEpoch   4 Batch 1461/3125   Loss: 0.746744 mae: 0.682042 (2043.390398612505 steps/sec)\n",
      "Step #13963\tEpoch   4 Batch 1462/3125   Loss: 0.888129 mae: 0.716549 (2175.0401891743327 steps/sec)\n",
      "Step #13964\tEpoch   4 Batch 1463/3125   Loss: 0.785403 mae: 0.724488 (2122.1509380502316 steps/sec)\n",
      "Step #13965\tEpoch   4 Batch 1464/3125   Loss: 0.797631 mae: 0.712774 (2066.7908425233322 steps/sec)\n",
      "Step #13966\tEpoch   4 Batch 1465/3125   Loss: 0.792620 mae: 0.699536 (2210.7398115156752 steps/sec)\n",
      "Step #13967\tEpoch   4 Batch 1466/3125   Loss: 0.693353 mae: 0.648050 (2000.8892196429765 steps/sec)\n",
      "Step #13968\tEpoch   4 Batch 1467/3125   Loss: 0.726249 mae: 0.659049 (1507.755354408265 steps/sec)\n",
      "Step #13969\tEpoch   4 Batch 1468/3125   Loss: 0.909187 mae: 0.750630 (1849.4545518682812 steps/sec)\n",
      "Step #13970\tEpoch   4 Batch 1469/3125   Loss: 0.848578 mae: 0.720229 (2109.4500940483017 steps/sec)\n",
      "Step #13971\tEpoch   4 Batch 1470/3125   Loss: 0.857464 mae: 0.718826 (2142.8591864469126 steps/sec)\n",
      "Step #13972\tEpoch   4 Batch 1471/3125   Loss: 0.749718 mae: 0.678037 (2036.4060087587272 steps/sec)\n",
      "Step #13973\tEpoch   4 Batch 1472/3125   Loss: 0.758451 mae: 0.695053 (2161.678091016853 steps/sec)\n",
      "Step #13974\tEpoch   4 Batch 1473/3125   Loss: 0.860700 mae: 0.723047 (2183.8963635606283 steps/sec)\n",
      "Step #13975\tEpoch   4 Batch 1474/3125   Loss: 0.693401 mae: 0.646653 (2228.0736050316605 steps/sec)\n",
      "Step #13976\tEpoch   4 Batch 1475/3125   Loss: 0.866824 mae: 0.733009 (2112.2758954111437 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #13977\tEpoch   4 Batch 1476/3125   Loss: 0.689627 mae: 0.664264 (1977.2328289256589 steps/sec)\n",
      "Step #13978\tEpoch   4 Batch 1477/3125   Loss: 0.820888 mae: 0.737813 (1936.4820816827798 steps/sec)\n",
      "Step #13979\tEpoch   4 Batch 1478/3125   Loss: 0.774328 mae: 0.689363 (2060.0504906631563 steps/sec)\n",
      "Step #13980\tEpoch   4 Batch 1479/3125   Loss: 0.656933 mae: 0.655466 (2056.091845838603 steps/sec)\n",
      "Step #13981\tEpoch   4 Batch 1480/3125   Loss: 0.732121 mae: 0.655044 (2099.9659543788675 steps/sec)\n",
      "Step #13982\tEpoch   4 Batch 1481/3125   Loss: 0.735930 mae: 0.691505 (2009.9599378941516 steps/sec)\n",
      "Step #13983\tEpoch   4 Batch 1482/3125   Loss: 0.814875 mae: 0.696561 (1976.2080663399925 steps/sec)\n",
      "Step #13984\tEpoch   4 Batch 1483/3125   Loss: 0.889864 mae: 0.729025 (2107.012819997589 steps/sec)\n",
      "Step #13985\tEpoch   4 Batch 1484/3125   Loss: 0.823853 mae: 0.729325 (1937.6092540236893 steps/sec)\n",
      "Step #13986\tEpoch   4 Batch 1485/3125   Loss: 0.847405 mae: 0.712529 (1867.7710387331783 steps/sec)\n",
      "Step #13987\tEpoch   4 Batch 1486/3125   Loss: 0.813648 mae: 0.704781 (1920.0644552887213 steps/sec)\n",
      "Step #13988\tEpoch   4 Batch 1487/3125   Loss: 0.786363 mae: 0.696641 (2151.653380118399 steps/sec)\n",
      "Step #13989\tEpoch   4 Batch 1488/3125   Loss: 0.875081 mae: 0.723032 (2066.6075405506613 steps/sec)\n",
      "Step #13990\tEpoch   4 Batch 1489/3125   Loss: 0.721865 mae: 0.652573 (2024.7861432405816 steps/sec)\n",
      "Step #13991\tEpoch   4 Batch 1490/3125   Loss: 0.867319 mae: 0.737030 (2154.2393425783257 steps/sec)\n",
      "Step #13992\tEpoch   4 Batch 1491/3125   Loss: 0.804259 mae: 0.694867 (2066.2206764731964 steps/sec)\n",
      "Step #13993\tEpoch   4 Batch 1492/3125   Loss: 0.752914 mae: 0.692636 (2035.674626286158 steps/sec)\n",
      "Step #13994\tEpoch   4 Batch 1493/3125   Loss: 0.845926 mae: 0.750348 (1741.8639999335533 steps/sec)\n",
      "Step #13995\tEpoch   4 Batch 1494/3125   Loss: 0.713862 mae: 0.675422 (2123.4186891852214 steps/sec)\n",
      "Step #13996\tEpoch   4 Batch 1495/3125   Loss: 0.832129 mae: 0.719434 (1886.3521475151788 steps/sec)\n",
      "Step #13997\tEpoch   4 Batch 1496/3125   Loss: 0.719606 mae: 0.682086 (2058.9583231063766 steps/sec)\n",
      "Step #13998\tEpoch   4 Batch 1497/3125   Loss: 0.885383 mae: 0.737037 (2096.5020843538505 steps/sec)\n",
      "Step #13999\tEpoch   4 Batch 1498/3125   Loss: 0.805579 mae: 0.712568 (1914.9799567174673 steps/sec)\n",
      "Step #14000\tEpoch   4 Batch 1499/3125   Loss: 0.842639 mae: 0.724166 (1966.3319362793359 steps/sec)\n",
      "Step #14001\tEpoch   4 Batch 1500/3125   Loss: 0.838264 mae: 0.722217 (1550.4254674227245 steps/sec)\n",
      "Step #14002\tEpoch   4 Batch 1501/3125   Loss: 0.677575 mae: 0.663967 (1829.5604836599027 steps/sec)\n",
      "Step #14003\tEpoch   4 Batch 1502/3125   Loss: 0.738608 mae: 0.667635 (1684.9872650870554 steps/sec)\n",
      "Step #14004\tEpoch   4 Batch 1503/3125   Loss: 0.890458 mae: 0.747576 (2088.8176177053556 steps/sec)\n",
      "Step #14005\tEpoch   4 Batch 1504/3125   Loss: 0.879963 mae: 0.736840 (2224.1038476222798 steps/sec)\n",
      "Step #14006\tEpoch   4 Batch 1505/3125   Loss: 0.594212 mae: 0.593476 (2059.3020287122686 steps/sec)\n",
      "Step #14007\tEpoch   4 Batch 1506/3125   Loss: 0.732115 mae: 0.666717 (1926.785615846824 steps/sec)\n",
      "Step #14008\tEpoch   4 Batch 1507/3125   Loss: 0.835529 mae: 0.725836 (1995.918988883813 steps/sec)\n",
      "Step #14009\tEpoch   4 Batch 1508/3125   Loss: 0.800967 mae: 0.703999 (1973.3072377582898 steps/sec)\n",
      "Step #14010\tEpoch   4 Batch 1509/3125   Loss: 0.719585 mae: 0.660631 (1971.063093884226 steps/sec)\n",
      "Step #14011\tEpoch   4 Batch 1510/3125   Loss: 0.860892 mae: 0.719444 (1834.6020942866392 steps/sec)\n",
      "Step #14012\tEpoch   4 Batch 1511/3125   Loss: 0.823251 mae: 0.732652 (2049.0205082609505 steps/sec)\n",
      "Step #14013\tEpoch   4 Batch 1512/3125   Loss: 0.810067 mae: 0.695246 (2126.3898605830163 steps/sec)\n",
      "Step #14014\tEpoch   4 Batch 1513/3125   Loss: 0.751158 mae: 0.680632 (2223.3021648326016 steps/sec)\n",
      "Step #14015\tEpoch   4 Batch 1514/3125   Loss: 0.636806 mae: 0.661835 (2204.002017824113 steps/sec)\n",
      "Step #14016\tEpoch   4 Batch 1515/3125   Loss: 0.689323 mae: 0.670382 (2063.130970299757 steps/sec)\n",
      "Step #14017\tEpoch   4 Batch 1516/3125   Loss: 0.755006 mae: 0.716403 (2229.1630349284637 steps/sec)\n",
      "Step #14018\tEpoch   4 Batch 1517/3125   Loss: 0.798273 mae: 0.691890 (1767.4068955055327 steps/sec)\n",
      "Step #14019\tEpoch   4 Batch 1518/3125   Loss: 0.862380 mae: 0.746405 (2203.909369876834 steps/sec)\n",
      "Step #14020\tEpoch   4 Batch 1519/3125   Loss: 0.722176 mae: 0.690778 (2190.694662070406 steps/sec)\n",
      "Step #14021\tEpoch   4 Batch 1520/3125   Loss: 0.743754 mae: 0.679714 (2251.370907139023 steps/sec)\n",
      "Step #14022\tEpoch   4 Batch 1521/3125   Loss: 0.807284 mae: 0.702444 (2200.6484989034284 steps/sec)\n",
      "Step #14023\tEpoch   4 Batch 1522/3125   Loss: 0.848756 mae: 0.724959 (2220.7359559485362 steps/sec)\n",
      "Step #14024\tEpoch   4 Batch 1523/3125   Loss: 0.725191 mae: 0.682839 (2113.361482571322 steps/sec)\n",
      "Step #14025\tEpoch   4 Batch 1524/3125   Loss: 0.714547 mae: 0.670348 (2302.9682747109146 steps/sec)\n",
      "Step #14026\tEpoch   4 Batch 1525/3125   Loss: 0.750693 mae: 0.664050 (2034.9832613652904 steps/sec)\n",
      "Step #14027\tEpoch   4 Batch 1526/3125   Loss: 0.873479 mae: 0.744248 (1941.3041063428 steps/sec)\n",
      "Step #14028\tEpoch   4 Batch 1527/3125   Loss: 0.661155 mae: 0.627505 (1988.783203254654 steps/sec)\n",
      "Step #14029\tEpoch   4 Batch 1528/3125   Loss: 0.823136 mae: 0.712849 (1912.3257192358546 steps/sec)\n",
      "Step #14030\tEpoch   4 Batch 1529/3125   Loss: 0.883962 mae: 0.711445 (2152.117069966956 steps/sec)\n",
      "Step #14031\tEpoch   4 Batch 1530/3125   Loss: 0.839511 mae: 0.724301 (1975.0169517064717 steps/sec)\n",
      "Step #14032\tEpoch   4 Batch 1531/3125   Loss: 0.678855 mae: 0.665918 (2034.9832613652904 steps/sec)\n",
      "Step #14033\tEpoch   4 Batch 1532/3125   Loss: 0.754894 mae: 0.669279 (2051.9475944933124 steps/sec)\n",
      "Step #14034\tEpoch   4 Batch 1533/3125   Loss: 0.919036 mae: 0.746902 (1901.350885782154 steps/sec)\n",
      "Step #14035\tEpoch   4 Batch 1534/3125   Loss: 0.863607 mae: 0.729445 (2110.3203992915796 steps/sec)\n",
      "Step #14036\tEpoch   4 Batch 1535/3125   Loss: 0.632238 mae: 0.637332 (1748.6758721899807 steps/sec)\n",
      "Step #14037\tEpoch   4 Batch 1536/3125   Loss: 0.786058 mae: 0.713809 (2151.7637643389216 steps/sec)\n",
      "Step #14038\tEpoch   4 Batch 1537/3125   Loss: 0.966613 mae: 0.766606 (2197.0059190194333 steps/sec)\n",
      "Step #14039\tEpoch   4 Batch 1538/3125   Loss: 0.784616 mae: 0.685319 (2235.745887570495 steps/sec)\n",
      "Step #14040\tEpoch   4 Batch 1539/3125   Loss: 0.765821 mae: 0.671626 (2236.1511558474795 steps/sec)\n",
      "Step #14041\tEpoch   4 Batch 1540/3125   Loss: 0.858942 mae: 0.733685 (2235.531393241659 steps/sec)\n",
      "Step #14042\tEpoch   4 Batch 1541/3125   Loss: 0.766419 mae: 0.702863 (2058.897681085433 steps/sec)\n",
      "Step #14043\tEpoch   4 Batch 1542/3125   Loss: 0.795561 mae: 0.710778 (2159.074249474941 steps/sec)\n",
      "Step #14044\tEpoch   4 Batch 1543/3125   Loss: 0.832441 mae: 0.731561 (2114.277648956548 steps/sec)\n",
      "Step #14045\tEpoch   4 Batch 1544/3125   Loss: 0.899853 mae: 0.770481 (1978.949355024393 steps/sec)\n",
      "Step #14046\tEpoch   4 Batch 1545/3125   Loss: 0.768861 mae: 0.704338 (2067.6466818500003 steps/sec)\n",
      "Step #14047\tEpoch   4 Batch 1546/3125   Loss: 0.857196 mae: 0.745066 (2167.4197482378718 steps/sec)\n",
      "Step #14048\tEpoch   4 Batch 1547/3125   Loss: 0.713976 mae: 0.662909 (1986.3344036219323 steps/sec)\n",
      "Step #14049\tEpoch   4 Batch 1548/3125   Loss: 0.880599 mae: 0.771541 (2029.2924601331476 steps/sec)\n",
      "Step #14050\tEpoch   4 Batch 1549/3125   Loss: 0.899016 mae: 0.726420 (2114.341597185115 steps/sec)\n",
      "Step #14051\tEpoch   4 Batch 1550/3125   Loss: 0.795671 mae: 0.714594 (1876.3102800393665 steps/sec)\n",
      "Step #14052\tEpoch   4 Batch 1551/3125   Loss: 0.899029 mae: 0.755925 (2063.9634674435083 steps/sec)\n",
      "Step #14053\tEpoch   4 Batch 1552/3125   Loss: 0.700035 mae: 0.657838 (2059.7267646856617 steps/sec)\n",
      "Step #14054\tEpoch   4 Batch 1553/3125   Loss: 0.913831 mae: 0.747211 (1780.582276976371 steps/sec)\n",
      "Step #14055\tEpoch   4 Batch 1554/3125   Loss: 0.698836 mae: 0.665860 (2249.511407639418 steps/sec)\n",
      "Step #14056\tEpoch   4 Batch 1555/3125   Loss: 0.773194 mae: 0.711402 (2085.4940880478125 steps/sec)\n",
      "Step #14057\tEpoch   4 Batch 1556/3125   Loss: 0.866414 mae: 0.737337 (2136.027704216745 steps/sec)\n",
      "Step #14058\tEpoch   4 Batch 1557/3125   Loss: 1.037025 mae: 0.790061 (2167.3301502656 steps/sec)\n",
      "Step #14059\tEpoch   4 Batch 1558/3125   Loss: 0.715333 mae: 0.672414 (1999.3440872516494 steps/sec)\n",
      "Step #14060\tEpoch   4 Batch 1559/3125   Loss: 0.743116 mae: 0.699075 (2209.575185434929 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14061\tEpoch   4 Batch 1560/3125   Loss: 0.730671 mae: 0.685193 (1830.7583521750137 steps/sec)\n",
      "Step #14062\tEpoch   4 Batch 1561/3125   Loss: 0.937394 mae: 0.728454 (2239.6377539033297 steps/sec)\n",
      "Step #14063\tEpoch   4 Batch 1562/3125   Loss: 0.694942 mae: 0.650202 (2166.0989289071135 steps/sec)\n",
      "Step #14064\tEpoch   4 Batch 1563/3125   Loss: 0.837202 mae: 0.726342 (2119.5557037890503 steps/sec)\n",
      "Step #14065\tEpoch   4 Batch 1564/3125   Loss: 0.741115 mae: 0.684288 (2145.0071085927034 steps/sec)\n",
      "Step #14066\tEpoch   4 Batch 1565/3125   Loss: 0.856369 mae: 0.729154 (2098.8100599473582 steps/sec)\n",
      "Step #14067\tEpoch   4 Batch 1566/3125   Loss: 0.846187 mae: 0.733174 (2446.4286130910023 steps/sec)\n",
      "Step #14068\tEpoch   4 Batch 1567/3125   Loss: 0.740506 mae: 0.683696 (2299.15583134168 steps/sec)\n",
      "Step #14069\tEpoch   4 Batch 1568/3125   Loss: 0.735289 mae: 0.664322 (2183.8963635606283 steps/sec)\n",
      "Step #14070\tEpoch   4 Batch 1569/3125   Loss: 0.760325 mae: 0.690775 (2081.602429849028 steps/sec)\n",
      "Step #14071\tEpoch   4 Batch 1570/3125   Loss: 0.748664 mae: 0.652492 (1946.4211464211464 steps/sec)\n",
      "Step #14072\tEpoch   4 Batch 1571/3125   Loss: 0.942782 mae: 0.760448 (2082.6153448926493 steps/sec)\n",
      "Step #14073\tEpoch   4 Batch 1572/3125   Loss: 0.874392 mae: 0.730910 (1871.6215975011155 steps/sec)\n",
      "Step #14074\tEpoch   4 Batch 1573/3125   Loss: 0.690651 mae: 0.655064 (2134.4624028009607 steps/sec)\n",
      "Step #14075\tEpoch   4 Batch 1574/3125   Loss: 0.772171 mae: 0.709174 (2056.8581488637587 steps/sec)\n",
      "Step #14076\tEpoch   4 Batch 1575/3125   Loss: 0.752360 mae: 0.680846 (2201.2721738217697 steps/sec)\n",
      "Step #14077\tEpoch   4 Batch 1576/3125   Loss: 0.817914 mae: 0.702889 (1985.939393939394 steps/sec)\n",
      "Step #14078\tEpoch   4 Batch 1577/3125   Loss: 0.652614 mae: 0.632312 (2032.4588352732524 steps/sec)\n",
      "Step #14079\tEpoch   4 Batch 1578/3125   Loss: 0.685105 mae: 0.655457 (1990.3876086708933 steps/sec)\n",
      "Step #14080\tEpoch   4 Batch 1579/3125   Loss: 0.733193 mae: 0.685392 (2072.223155440056 steps/sec)\n",
      "Step #14081\tEpoch   4 Batch 1580/3125   Loss: 0.873896 mae: 0.741310 (2179.674475648035 steps/sec)\n",
      "Step #14082\tEpoch   4 Batch 1581/3125   Loss: 0.925978 mae: 0.756891 (2077.1902021572687 steps/sec)\n",
      "Step #14083\tEpoch   4 Batch 1582/3125   Loss: 0.788100 mae: 0.695865 (2065.996768727588 steps/sec)\n",
      "Step #14084\tEpoch   4 Batch 1583/3125   Loss: 0.693662 mae: 0.662138 (2179.4253052740974 steps/sec)\n",
      "Step #14085\tEpoch   4 Batch 1584/3125   Loss: 0.673136 mae: 0.671683 (2084.9757416686552 steps/sec)\n",
      "Step #14086\tEpoch   4 Batch 1585/3125   Loss: 0.725237 mae: 0.712227 (1934.9098122433916 steps/sec)\n",
      "Step #14087\tEpoch   4 Batch 1586/3125   Loss: 0.788439 mae: 0.684667 (2043.908191608596 steps/sec)\n",
      "Step #14088\tEpoch   4 Batch 1587/3125   Loss: 0.728342 mae: 0.698637 (1877.3684728799449 steps/sec)\n",
      "Step #14089\tEpoch   4 Batch 1588/3125   Loss: 0.708370 mae: 0.686506 (2254.9051653692313 steps/sec)\n",
      "Step #14090\tEpoch   4 Batch 1589/3125   Loss: 0.715564 mae: 0.674419 (2078.4253872607806 steps/sec)\n",
      "Step #14091\tEpoch   4 Batch 1590/3125   Loss: 0.872515 mae: 0.749588 (1986.6542884750195 steps/sec)\n",
      "Step #14092\tEpoch   4 Batch 1591/3125   Loss: 0.739130 mae: 0.692423 (2075.7920992982213 steps/sec)\n",
      "Step #14093\tEpoch   4 Batch 1592/3125   Loss: 0.739547 mae: 0.664393 (2077.4371217149255 steps/sec)\n",
      "Step #14094\tEpoch   4 Batch 1593/3125   Loss: 0.883445 mae: 0.739860 (2114.0005846597383 steps/sec)\n",
      "Step #14095\tEpoch   4 Batch 1594/3125   Loss: 0.645756 mae: 0.642894 (2115.9415610622327 steps/sec)\n",
      "Step #14096\tEpoch   4 Batch 1595/3125   Loss: 0.932119 mae: 0.761193 (1668.4583194106322 steps/sec)\n",
      "Step #14097\tEpoch   4 Batch 1596/3125   Loss: 0.887145 mae: 0.731797 (2203.863048824061 steps/sec)\n",
      "Step #14098\tEpoch   4 Batch 1597/3125   Loss: 0.711011 mae: 0.691387 (2017.1905659651418 steps/sec)\n",
      "Step #14099\tEpoch   4 Batch 1598/3125   Loss: 0.803851 mae: 0.719986 (2235.316940065445 steps/sec)\n",
      "Step #14100\tEpoch   4 Batch 1599/3125   Loss: 0.789402 mae: 0.693642 (2216.7922793146095 steps/sec)\n",
      "Step #14101\tEpoch   4 Batch 1600/3125   Loss: 0.728602 mae: 0.670936 (2236.938272658425 steps/sec)\n",
      "Step #14102\tEpoch   4 Batch 1601/3125   Loss: 0.710679 mae: 0.659564 (2147.7530621441156 steps/sec)\n",
      "Step #14103\tEpoch   4 Batch 1602/3125   Loss: 0.717052 mae: 0.671605 (2131.750307490572 steps/sec)\n",
      "Step #14104\tEpoch   4 Batch 1603/3125   Loss: 0.688168 mae: 0.682253 (1949.6058307303288 steps/sec)\n",
      "Step #14105\tEpoch   4 Batch 1604/3125   Loss: 0.779387 mae: 0.690023 (1998.0107086374117 steps/sec)\n",
      "Step #14106\tEpoch   4 Batch 1605/3125   Loss: 0.762721 mae: 0.677150 (1786.1631363330523 steps/sec)\n",
      "Step #14107\tEpoch   4 Batch 1606/3125   Loss: 0.719689 mae: 0.660024 (2132.205457724999 steps/sec)\n",
      "Step #14108\tEpoch   4 Batch 1607/3125   Loss: 0.934516 mae: 0.755129 (2187.427117123695 steps/sec)\n",
      "Step #14109\tEpoch   4 Batch 1608/3125   Loss: 0.775353 mae: 0.701960 (2288.7426469785764 steps/sec)\n",
      "Step #14110\tEpoch   4 Batch 1609/3125   Loss: 0.832787 mae: 0.745427 (2187.723763822241 steps/sec)\n",
      "Step #14111\tEpoch   4 Batch 1610/3125   Loss: 0.733648 mae: 0.661952 (2310.0203778157183 steps/sec)\n",
      "Step #14112\tEpoch   4 Batch 1611/3125   Loss: 0.705701 mae: 0.663788 (2239.0638679507165 steps/sec)\n",
      "Step #14113\tEpoch   4 Batch 1612/3125   Loss: 0.819178 mae: 0.722342 (2278.720445062587 steps/sec)\n",
      "Step #14114\tEpoch   4 Batch 1613/3125   Loss: 0.689232 mae: 0.665910 (1795.7067139322014 steps/sec)\n",
      "Step #14115\tEpoch   4 Batch 1614/3125   Loss: 0.776918 mae: 0.685220 (2119.791371851373 steps/sec)\n",
      "Step #14116\tEpoch   4 Batch 1615/3125   Loss: 0.799460 mae: 0.714229 (2063.0497870205504 steps/sec)\n",
      "Step #14117\tEpoch   4 Batch 1616/3125   Loss: 0.876224 mae: 0.730219 (2077.787024927674 steps/sec)\n",
      "Step #14118\tEpoch   4 Batch 1617/3125   Loss: 0.842419 mae: 0.719703 (2200.9025460193525 steps/sec)\n",
      "Step #14119\tEpoch   4 Batch 1618/3125   Loss: 0.696100 mae: 0.639862 (2184.4650687999333 steps/sec)\n",
      "Step #14120\tEpoch   4 Batch 1619/3125   Loss: 0.880893 mae: 0.734707 (2055.144839481008 steps/sec)\n",
      "Step #14121\tEpoch   4 Batch 1620/3125   Loss: 0.743957 mae: 0.681192 (2290.2173200829966 steps/sec)\n",
      "Step #14122\tEpoch   4 Batch 1621/3125   Loss: 0.797279 mae: 0.703459 (2171.1670859603896 steps/sec)\n",
      "Step #14123\tEpoch   4 Batch 1622/3125   Loss: 0.742590 mae: 0.667132 (2058.109659754458 steps/sec)\n",
      "Step #14124\tEpoch   4 Batch 1623/3125   Loss: 0.839364 mae: 0.733633 (1913.163104263025 steps/sec)\n",
      "Step #14125\tEpoch   4 Batch 1624/3125   Loss: 0.844480 mae: 0.732930 (1827.2808859535241 steps/sec)\n",
      "Step #14126\tEpoch   4 Batch 1625/3125   Loss: 0.828910 mae: 0.739115 (2201.480144025362 steps/sec)\n",
      "Step #14127\tEpoch   4 Batch 1626/3125   Loss: 0.735129 mae: 0.683939 (2230.6805369413064 steps/sec)\n",
      "Step #14128\tEpoch   4 Batch 1627/3125   Loss: 0.835807 mae: 0.735773 (2128.742539282959 steps/sec)\n",
      "Step #14129\tEpoch   4 Batch 1628/3125   Loss: 0.808466 mae: 0.734522 (1967.8818418114085 steps/sec)\n",
      "Step #14130\tEpoch   4 Batch 1629/3125   Loss: 0.731346 mae: 0.673565 (2035.1807462758989 steps/sec)\n",
      "Step #14131\tEpoch   4 Batch 1630/3125   Loss: 0.923933 mae: 0.759690 (2081.168624960305 steps/sec)\n",
      "Step #14132\tEpoch   4 Batch 1631/3125   Loss: 0.884846 mae: 0.739869 (1794.8153536736702 steps/sec)\n",
      "Step #14133\tEpoch   4 Batch 1632/3125   Loss: 0.777564 mae: 0.673678 (1961.8069392604234 steps/sec)\n",
      "Step #14134\tEpoch   4 Batch 1633/3125   Loss: 0.942984 mae: 0.736469 (2227.978922318545 steps/sec)\n",
      "Step #14135\tEpoch   4 Batch 1634/3125   Loss: 0.938529 mae: 0.777393 (2201.6650394213307 steps/sec)\n",
      "Step #14136\tEpoch   4 Batch 1635/3125   Loss: 0.829430 mae: 0.730399 (2135.657912156176 steps/sec)\n",
      "Step #14137\tEpoch   4 Batch 1636/3125   Loss: 0.814860 mae: 0.721085 (2089.421141775431 steps/sec)\n",
      "Step #14138\tEpoch   4 Batch 1637/3125   Loss: 0.785310 mae: 0.698888 (2087.0714449210313 steps/sec)\n",
      "Step #14139\tEpoch   4 Batch 1638/3125   Loss: 0.790223 mae: 0.680271 (2189.6882243615178 steps/sec)\n",
      "Step #14140\tEpoch   4 Batch 1639/3125   Loss: 0.751135 mae: 0.664815 (2069.442169352372 steps/sec)\n",
      "Step #14141\tEpoch   4 Batch 1640/3125   Loss: 0.892072 mae: 0.769279 (2077.9723155276797 steps/sec)\n",
      "Step #14142\tEpoch   4 Batch 1641/3125   Loss: 0.613757 mae: 0.607558 (2020.4749747097644 steps/sec)\n",
      "Step #14143\tEpoch   4 Batch 1642/3125   Loss: 0.786266 mae: 0.714289 (2074.929505001435 steps/sec)\n",
      "Step #14144\tEpoch   4 Batch 1643/3125   Loss: 0.820638 mae: 0.693273 (2078.1370460288363 steps/sec)\n",
      "Step #14145\tEpoch   4 Batch 1644/3125   Loss: 0.724559 mae: 0.668245 (2252.7736003093714 steps/sec)\n",
      "Step #14146\tEpoch   4 Batch 1645/3125   Loss: 0.828184 mae: 0.711635 (2363.3876148081367 steps/sec)\n",
      "Step #14147\tEpoch   4 Batch 1646/3125   Loss: 0.715740 mae: 0.663587 (2055.8499740219 steps/sec)\n",
      "Step #14148\tEpoch   4 Batch 1647/3125   Loss: 0.809947 mae: 0.711661 (1817.2736804707065 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14149\tEpoch   4 Batch 1648/3125   Loss: 0.824524 mae: 0.694155 (1733.2837436876514 steps/sec)\n",
      "Step #14150\tEpoch   4 Batch 1649/3125   Loss: 0.843101 mae: 0.740928 (1588.402547925077 steps/sec)\n",
      "Step #14151\tEpoch   4 Batch 1650/3125   Loss: 0.692927 mae: 0.655866 (1913.075842440386 steps/sec)\n",
      "Step #14152\tEpoch   4 Batch 1651/3125   Loss: 0.798010 mae: 0.685861 (1566.1140484511754 steps/sec)\n",
      "Step #14153\tEpoch   4 Batch 1652/3125   Loss: 0.701967 mae: 0.659776 (1879.5558224365236 steps/sec)\n",
      "Step #14154\tEpoch   4 Batch 1653/3125   Loss: 0.795906 mae: 0.719060 (2187.7922321791834 steps/sec)\n",
      "Step #14155\tEpoch   4 Batch 1654/3125   Loss: 0.679935 mae: 0.654598 (2141.699346405229 steps/sec)\n",
      "Step #14156\tEpoch   4 Batch 1655/3125   Loss: 0.862008 mae: 0.713806 (2161.9677944784644 steps/sec)\n",
      "Step #14157\tEpoch   4 Batch 1656/3125   Loss: 0.802115 mae: 0.692223 (1877.7046567639923 steps/sec)\n",
      "Step #14158\tEpoch   4 Batch 1657/3125   Loss: 0.751238 mae: 0.688981 (2112.4673885671114 steps/sec)\n",
      "Step #14159\tEpoch   4 Batch 1658/3125   Loss: 0.651063 mae: 0.635903 (2253.572464780408 steps/sec)\n",
      "Step #14160\tEpoch   4 Batch 1659/3125   Loss: 0.693901 mae: 0.671363 (2399.1305641037375 steps/sec)\n",
      "Step #14161\tEpoch   4 Batch 1660/3125   Loss: 0.857672 mae: 0.722491 (2206.8315268862466 steps/sec)\n",
      "Step #14162\tEpoch   4 Batch 1661/3125   Loss: 0.937984 mae: 0.764213 (2097.382712098331 steps/sec)\n",
      "Step #14163\tEpoch   4 Batch 1662/3125   Loss: 0.825237 mae: 0.714976 (2092.110014864177 steps/sec)\n",
      "Step #14164\tEpoch   4 Batch 1663/3125   Loss: 0.810993 mae: 0.733804 (2162.9042904290427 steps/sec)\n",
      "Step #14165\tEpoch   4 Batch 1664/3125   Loss: 0.818893 mae: 0.720234 (2158.6964353724693 steps/sec)\n",
      "Step #14166\tEpoch   4 Batch 1665/3125   Loss: 0.801347 mae: 0.704044 (2163.2835790103463 steps/sec)\n",
      "Step #14167\tEpoch   4 Batch 1666/3125   Loss: 0.758421 mae: 0.693843 (1930.1372258474225 steps/sec)\n",
      "Step #14168\tEpoch   4 Batch 1667/3125   Loss: 0.737189 mae: 0.693835 (2157.4971965885825 steps/sec)\n",
      "Step #14169\tEpoch   4 Batch 1668/3125   Loss: 0.754961 mae: 0.668821 (2100.5969790456347 steps/sec)\n",
      "Step #14170\tEpoch   4 Batch 1669/3125   Loss: 0.815570 mae: 0.712939 (2103.1459660031087 steps/sec)\n",
      "Step #14171\tEpoch   4 Batch 1670/3125   Loss: 0.704077 mae: 0.630665 (2006.4984021890966 steps/sec)\n",
      "Step #14172\tEpoch   4 Batch 1671/3125   Loss: 0.776635 mae: 0.714535 (2003.6611698163683 steps/sec)\n",
      "Step #14173\tEpoch   4 Batch 1672/3125   Loss: 0.816957 mae: 0.709929 (2156.9424445632944 steps/sec)\n",
      "Step #14174\tEpoch   4 Batch 1673/3125   Loss: 0.690810 mae: 0.678563 (2029.9603136192043 steps/sec)\n",
      "Step #14175\tEpoch   4 Batch 1674/3125   Loss: 0.737691 mae: 0.672771 (2126.3036226667614 steps/sec)\n",
      "Step #14176\tEpoch   4 Batch 1675/3125   Loss: 0.819976 mae: 0.721830 (1895.6792132190765 steps/sec)\n",
      "Step #14177\tEpoch   4 Batch 1676/3125   Loss: 0.711711 mae: 0.655623 (2179.47060474107 steps/sec)\n",
      "Step #14178\tEpoch   4 Batch 1677/3125   Loss: 0.700328 mae: 0.660188 (2261.5192166673855 steps/sec)\n",
      "Step #14179\tEpoch   4 Batch 1678/3125   Loss: 0.870731 mae: 0.722307 (2105.722289719157 steps/sec)\n",
      "Step #14180\tEpoch   4 Batch 1679/3125   Loss: 0.782693 mae: 0.667767 (2194.7756195579373 steps/sec)\n",
      "Step #14181\tEpoch   4 Batch 1680/3125   Loss: 0.767640 mae: 0.700134 (2245.0322760215386 steps/sec)\n",
      "Step #14182\tEpoch   4 Batch 1681/3125   Loss: 0.835023 mae: 0.712819 (2007.6317023904116 steps/sec)\n",
      "Step #14183\tEpoch   4 Batch 1682/3125   Loss: 0.824083 mae: 0.734603 (1548.033541986536 steps/sec)\n",
      "Step #14184\tEpoch   4 Batch 1683/3125   Loss: 0.712070 mae: 0.673649 (1933.9284396901512 steps/sec)\n",
      "Step #14185\tEpoch   4 Batch 1684/3125   Loss: 0.653657 mae: 0.642792 (2083.753465218644 steps/sec)\n",
      "Step #14186\tEpoch   4 Batch 1685/3125   Loss: 0.816597 mae: 0.705394 (2050.8639995305944 steps/sec)\n",
      "Step #14187\tEpoch   4 Batch 1686/3125   Loss: 0.653555 mae: 0.642974 (2003.7568912966626 steps/sec)\n",
      "Step #14188\tEpoch   4 Batch 1687/3125   Loss: 0.911434 mae: 0.758438 (2237.5349422785566 steps/sec)\n",
      "Step #14189\tEpoch   4 Batch 1688/3125   Loss: 0.836312 mae: 0.727178 (2091.108695868939 steps/sec)\n",
      "Step #14190\tEpoch   4 Batch 1689/3125   Loss: 0.823273 mae: 0.736614 (2258.7667618073133 steps/sec)\n",
      "Step #14191\tEpoch   4 Batch 1690/3125   Loss: 0.767655 mae: 0.689422 (2021.7410585173045 steps/sec)\n",
      "Step #14192\tEpoch   4 Batch 1691/3125   Loss: 0.885136 mae: 0.749671 (1745.5174996878773 steps/sec)\n",
      "Step #14193\tEpoch   4 Batch 1692/3125   Loss: 0.784126 mae: 0.709269 (2126.842724433086 steps/sec)\n",
      "Step #14194\tEpoch   4 Batch 1693/3125   Loss: 0.971230 mae: 0.765182 (2200.1174989509022 steps/sec)\n",
      "Step #14195\tEpoch   4 Batch 1694/3125   Loss: 0.816689 mae: 0.691250 (2234.721452623503 steps/sec)\n",
      "Step #14196\tEpoch   4 Batch 1695/3125   Loss: 0.823888 mae: 0.685649 (2318.0122026704394 steps/sec)\n",
      "Step #14197\tEpoch   4 Batch 1696/3125   Loss: 0.804114 mae: 0.703152 (2083.587843141151 steps/sec)\n",
      "Step #14198\tEpoch   4 Batch 1697/3125   Loss: 0.737849 mae: 0.695717 (2257.2108191886687 steps/sec)\n",
      "Step #14199\tEpoch   4 Batch 1698/3125   Loss: 0.735437 mae: 0.683726 (2120.8846997906576 steps/sec)\n",
      "Step #14200\tEpoch   4 Batch 1699/3125   Loss: 0.846597 mae: 0.711112 (2106.8434800080367 steps/sec)\n",
      "Step #14201\tEpoch   4 Batch 1700/3125   Loss: 0.750562 mae: 0.688951 (1979.6405376831294 steps/sec)\n",
      "Step #14202\tEpoch   4 Batch 1701/3125   Loss: 0.815235 mae: 0.721190 (1974.5520624429191 steps/sec)\n",
      "Step #14203\tEpoch   4 Batch 1702/3125   Loss: 0.757526 mae: 0.681484 (1993.774777772496 steps/sec)\n",
      "Step #14204\tEpoch   4 Batch 1703/3125   Loss: 0.874942 mae: 0.734370 (2037.3358203155358 steps/sec)\n",
      "Step #14205\tEpoch   4 Batch 1704/3125   Loss: 0.798219 mae: 0.712726 (2095.3290637145683 steps/sec)\n",
      "Step #14206\tEpoch   4 Batch 1705/3125   Loss: 0.777093 mae: 0.701842 (2027.487528520051 steps/sec)\n",
      "Step #14207\tEpoch   4 Batch 1706/3125   Loss: 0.748537 mae: 0.691158 (2206.5296760413707 steps/sec)\n",
      "Step #14208\tEpoch   4 Batch 1707/3125   Loss: 0.806333 mae: 0.716336 (1928.9300135208468 steps/sec)\n",
      "Step #14209\tEpoch   4 Batch 1708/3125   Loss: 0.826560 mae: 0.704259 (2236.0557853883224 steps/sec)\n",
      "Step #14210\tEpoch   4 Batch 1709/3125   Loss: 0.654908 mae: 0.644499 (1727.6008929821814 steps/sec)\n",
      "Step #14211\tEpoch   4 Batch 1710/3125   Loss: 0.770130 mae: 0.673929 (1874.130473637176 steps/sec)\n",
      "Step #14212\tEpoch   4 Batch 1711/3125   Loss: 0.720839 mae: 0.667026 (2132.032044243829 steps/sec)\n",
      "Step #14213\tEpoch   4 Batch 1712/3125   Loss: 0.642410 mae: 0.642667 (2234.435731333106 steps/sec)\n",
      "Step #14214\tEpoch   4 Batch 1713/3125   Loss: 0.748650 mae: 0.701443 (2255.608496907771 steps/sec)\n",
      "Step #14215\tEpoch   4 Batch 1714/3125   Loss: 0.828319 mae: 0.740447 (2271.5871795149533 steps/sec)\n",
      "Step #14216\tEpoch   4 Batch 1715/3125   Loss: 0.661331 mae: 0.628254 (2224.599293526111 steps/sec)\n",
      "Step #14217\tEpoch   4 Batch 1716/3125   Loss: 0.950991 mae: 0.751198 (2292.946720460087 steps/sec)\n",
      "Step #14218\tEpoch   4 Batch 1717/3125   Loss: 0.754931 mae: 0.670654 (2053.6354645070946 steps/sec)\n",
      "Step #14219\tEpoch   4 Batch 1718/3125   Loss: 0.794739 mae: 0.693522 (1989.7455359684245 steps/sec)\n",
      "Step #14220\tEpoch   4 Batch 1719/3125   Loss: 0.785837 mae: 0.708678 (1894.874181161057 steps/sec)\n",
      "Step #14221\tEpoch   4 Batch 1720/3125   Loss: 0.798556 mae: 0.701097 (2080.219017199992 steps/sec)\n",
      "Step #14222\tEpoch   4 Batch 1721/3125   Loss: 0.830010 mae: 0.706418 (1985.0558936836824 steps/sec)\n",
      "Step #14223\tEpoch   4 Batch 1722/3125   Loss: 0.684985 mae: 0.660542 (2141.699346405229 steps/sec)\n",
      "Step #14224\tEpoch   4 Batch 1723/3125   Loss: 0.861493 mae: 0.715421 (2157.874590990472 steps/sec)\n",
      "Step #14225\tEpoch   4 Batch 1724/3125   Loss: 0.886133 mae: 0.758766 (2082.305164179401 steps/sec)\n",
      "Step #14226\tEpoch   4 Batch 1725/3125   Loss: 0.746160 mae: 0.675459 (2198.6873833637374 steps/sec)\n",
      "Step #14227\tEpoch   4 Batch 1726/3125   Loss: 0.833406 mae: 0.740485 (2146.01680259509 steps/sec)\n",
      "Step #14228\tEpoch   4 Batch 1727/3125   Loss: 0.713234 mae: 0.669162 (1722.591667761861 steps/sec)\n",
      "Step #14229\tEpoch   4 Batch 1728/3125   Loss: 0.913713 mae: 0.744738 (1856.1824007364005 steps/sec)\n",
      "Step #14230\tEpoch   4 Batch 1729/3125   Loss: 0.705460 mae: 0.672746 (2035.1017477122534 steps/sec)\n",
      "Step #14231\tEpoch   4 Batch 1730/3125   Loss: 0.800841 mae: 0.710870 (1961.6417854603958 steps/sec)\n",
      "Step #14232\tEpoch   4 Batch 1731/3125   Loss: 0.765972 mae: 0.687891 (2130.9271960575115 steps/sec)\n",
      "Step #14233\tEpoch   4 Batch 1732/3125   Loss: 0.885144 mae: 0.714796 (2143.3629042148727 steps/sec)\n",
      "Step #14234\tEpoch   4 Batch 1733/3125   Loss: 0.789522 mae: 0.712672 (2100.4076318293355 steps/sec)\n",
      "Step #14235\tEpoch   4 Batch 1734/3125   Loss: 0.827962 mae: 0.725690 (2211.5091375001316 steps/sec)\n",
      "Step #14236\tEpoch   4 Batch 1735/3125   Loss: 0.846431 mae: 0.731082 (2138.663457714233 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14237\tEpoch   4 Batch 1736/3125   Loss: 0.675648 mae: 0.646649 (1854.672161593293 steps/sec)\n",
      "Step #14238\tEpoch   4 Batch 1737/3125   Loss: 0.740864 mae: 0.683588 (1981.3799684438272 steps/sec)\n",
      "Step #14239\tEpoch   4 Batch 1738/3125   Loss: 0.815605 mae: 0.703095 (2084.3747825827677 steps/sec)\n",
      "Step #14240\tEpoch   4 Batch 1739/3125   Loss: 0.784423 mae: 0.694791 (1868.4366675279086 steps/sec)\n",
      "Step #14241\tEpoch   4 Batch 1740/3125   Loss: 0.904844 mae: 0.747030 (2202.728790950245 steps/sec)\n",
      "Step #14242\tEpoch   4 Batch 1741/3125   Loss: 0.803085 mae: 0.694124 (2209.2725836186464 steps/sec)\n",
      "Step #14243\tEpoch   4 Batch 1742/3125   Loss: 0.853115 mae: 0.733748 (2102.6398901131956 steps/sec)\n",
      "Step #14244\tEpoch   4 Batch 1743/3125   Loss: 0.780250 mae: 0.709183 (1916.6075671723634 steps/sec)\n",
      "Step #14245\tEpoch   4 Batch 1744/3125   Loss: 0.781720 mae: 0.704584 (2200.9025460193525 steps/sec)\n",
      "Step #14246\tEpoch   4 Batch 1745/3125   Loss: 0.639707 mae: 0.636103 (1758.3378748878586 steps/sec)\n",
      "Step #14247\tEpoch   4 Batch 1746/3125   Loss: 0.761234 mae: 0.698557 (2085.701498771743 steps/sec)\n",
      "Step #14248\tEpoch   4 Batch 1747/3125   Loss: 0.793182 mae: 0.720351 (2100.9336806251254 steps/sec)\n",
      "Step #14249\tEpoch   4 Batch 1748/3125   Loss: 0.700204 mae: 0.650234 (2068.5032302608865 steps/sec)\n",
      "Step #14250\tEpoch   4 Batch 1749/3125   Loss: 0.812607 mae: 0.720711 (1926.9449523582002 steps/sec)\n",
      "Step #14251\tEpoch   4 Batch 1750/3125   Loss: 0.699886 mae: 0.667126 (2086.739171534045 steps/sec)\n",
      "Step #14252\tEpoch   4 Batch 1751/3125   Loss: 0.827449 mae: 0.715855 (2189.9854846962753 steps/sec)\n",
      "Step #14253\tEpoch   4 Batch 1752/3125   Loss: 0.782133 mae: 0.692919 (2081.602429849028 steps/sec)\n",
      "Step #14254\tEpoch   4 Batch 1753/3125   Loss: 0.619628 mae: 0.624117 (2080.1983831771067 steps/sec)\n",
      "Step #14255\tEpoch   4 Batch 1754/3125   Loss: 0.857401 mae: 0.724534 (2058.9583231063766 steps/sec)\n",
      "Step #14256\tEpoch   4 Batch 1755/3125   Loss: 0.740574 mae: 0.690894 (1892.497337881495 steps/sec)\n",
      "Step #14257\tEpoch   4 Batch 1756/3125   Loss: 0.852182 mae: 0.739782 (2053.4544884850384 steps/sec)\n",
      "Step #14258\tEpoch   4 Batch 1757/3125   Loss: 0.717047 mae: 0.693784 (2161.4107413400393 steps/sec)\n",
      "Step #14259\tEpoch   4 Batch 1758/3125   Loss: 0.844758 mae: 0.745941 (1933.0371462807632 steps/sec)\n",
      "Step #14260\tEpoch   4 Batch 1759/3125   Loss: 0.977341 mae: 0.774302 (1933.6431363871063 steps/sec)\n",
      "Step #14261\tEpoch   4 Batch 1760/3125   Loss: 0.800600 mae: 0.724530 (2105.701146655421 steps/sec)\n",
      "Step #14262\tEpoch   4 Batch 1761/3125   Loss: 0.734174 mae: 0.699933 (1958.5457194355463 steps/sec)\n",
      "Step #14263\tEpoch   4 Batch 1762/3125   Loss: 0.681656 mae: 0.667908 (1771.407816604583 steps/sec)\n",
      "Step #14264\tEpoch   4 Batch 1763/3125   Loss: 0.723934 mae: 0.699563 (1918.8698062969504 steps/sec)\n",
      "Step #14265\tEpoch   4 Batch 1764/3125   Loss: 0.935699 mae: 0.765272 (1907.3167626167544 steps/sec)\n",
      "Step #14266\tEpoch   4 Batch 1765/3125   Loss: 0.818891 mae: 0.734380 (2121.2493931056806 steps/sec)\n",
      "Step #14267\tEpoch   4 Batch 1766/3125   Loss: 0.700514 mae: 0.685823 (2146.1924985928467 steps/sec)\n",
      "Step #14268\tEpoch   4 Batch 1767/3125   Loss: 0.678955 mae: 0.645244 (2101.9022991961833 steps/sec)\n",
      "Step #14269\tEpoch   4 Batch 1768/3125   Loss: 0.790184 mae: 0.667775 (2103.209242618742 steps/sec)\n",
      "Step #14270\tEpoch   4 Batch 1769/3125   Loss: 0.842591 mae: 0.705640 (1973.6229401744793 steps/sec)\n",
      "Step #14271\tEpoch   4 Batch 1770/3125   Loss: 0.904200 mae: 0.726749 (1870.8368645012802 steps/sec)\n",
      "Step #14272\tEpoch   4 Batch 1771/3125   Loss: 0.833153 mae: 0.724915 (1855.0659000442283 steps/sec)\n",
      "Step #14273\tEpoch   4 Batch 1772/3125   Loss: 0.865836 mae: 0.722548 (2180.172987358616 steps/sec)\n",
      "Step #14274\tEpoch   4 Batch 1773/3125   Loss: 0.758289 mae: 0.677716 (1975.817073515418 steps/sec)\n",
      "Step #14275\tEpoch   4 Batch 1774/3125   Loss: 0.763118 mae: 0.680382 (2027.4091260634184 steps/sec)\n",
      "Step #14276\tEpoch   4 Batch 1775/3125   Loss: 0.836673 mae: 0.719314 (2041.878352984704 steps/sec)\n",
      "Step #14277\tEpoch   4 Batch 1776/3125   Loss: 0.833091 mae: 0.707681 (2059.2413664437704 steps/sec)\n",
      "Step #14278\tEpoch   4 Batch 1777/3125   Loss: 0.786343 mae: 0.717180 (2281.6458863720436 steps/sec)\n",
      "Step #14279\tEpoch   4 Batch 1778/3125   Loss: 0.826519 mae: 0.705838 (2234.031084550403 steps/sec)\n",
      "Step #14280\tEpoch   4 Batch 1779/3125   Loss: 0.756802 mae: 0.673680 (1682.9319573399243 steps/sec)\n",
      "Step #14281\tEpoch   4 Batch 1780/3125   Loss: 0.817117 mae: 0.725972 (2011.7530816825747 steps/sec)\n",
      "Step #14282\tEpoch   4 Batch 1781/3125   Loss: 0.702900 mae: 0.660615 (2084.188348472501 steps/sec)\n",
      "Step #14283\tEpoch   4 Batch 1782/3125   Loss: 0.797864 mae: 0.732191 (2087.424601357673 steps/sec)\n",
      "Step #14284\tEpoch   4 Batch 1783/3125   Loss: 0.888975 mae: 0.732829 (2120.8846997906576 steps/sec)\n",
      "Step #14285\tEpoch   4 Batch 1784/3125   Loss: 0.926377 mae: 0.748813 (2067.993294546889 steps/sec)\n",
      "Step #14286\tEpoch   4 Batch 1785/3125   Loss: 0.880855 mae: 0.724089 (1997.9155353587319 steps/sec)\n",
      "Step #14287\tEpoch   4 Batch 1786/3125   Loss: 0.716121 mae: 0.657814 (2128.0944939419155 steps/sec)\n",
      "Step #14288\tEpoch   4 Batch 1787/3125   Loss: 0.711137 mae: 0.658064 (2134.8535130402915 steps/sec)\n",
      "Step #14289\tEpoch   4 Batch 1788/3125   Loss: 0.786702 mae: 0.695411 (1804.6071370180102 steps/sec)\n",
      "Step #14290\tEpoch   4 Batch 1789/3125   Loss: 0.703875 mae: 0.673581 (1777.4583425152136 steps/sec)\n",
      "Step #14291\tEpoch   4 Batch 1790/3125   Loss: 0.828075 mae: 0.717126 (2222.8308566340916 steps/sec)\n",
      "Step #14292\tEpoch   4 Batch 1791/3125   Loss: 0.880839 mae: 0.731095 (2273.853126456971 steps/sec)\n",
      "Step #14293\tEpoch   4 Batch 1792/3125   Loss: 0.786176 mae: 0.710880 (2309.587894539768 steps/sec)\n",
      "Step #14294\tEpoch   4 Batch 1793/3125   Loss: 0.704409 mae: 0.639001 (2127.597926325721 steps/sec)\n",
      "Step #14295\tEpoch   4 Batch 1794/3125   Loss: 0.734374 mae: 0.654355 (2332.8646435881465 steps/sec)\n",
      "Step #14296\tEpoch   4 Batch 1795/3125   Loss: 0.801970 mae: 0.699054 (2136.3105727994134 steps/sec)\n",
      "Step #14297\tEpoch   4 Batch 1796/3125   Loss: 0.785766 mae: 0.704303 (2111.552790027991 steps/sec)\n",
      "Step #14298\tEpoch   4 Batch 1797/3125   Loss: 0.751178 mae: 0.676389 (1754.997656825333 steps/sec)\n",
      "Step #14299\tEpoch   4 Batch 1798/3125   Loss: 0.969461 mae: 0.762906 (2142.618361633871 steps/sec)\n",
      "Step #14300\tEpoch   4 Batch 1799/3125   Loss: 0.899294 mae: 0.740733 (2215.1532115809155 steps/sec)\n",
      "Step #14301\tEpoch   4 Batch 1800/3125   Loss: 0.746806 mae: 0.685246 (2288.5178637683057 steps/sec)\n",
      "Step #14302\tEpoch   4 Batch 1801/3125   Loss: 0.761211 mae: 0.692220 (2014.9036336734498 steps/sec)\n",
      "Step #14303\tEpoch   4 Batch 1802/3125   Loss: 0.792245 mae: 0.706870 (2179.6971303254236 steps/sec)\n",
      "Step #14304\tEpoch   4 Batch 1803/3125   Loss: 0.723444 mae: 0.677378 (1926.0423937401272 steps/sec)\n",
      "Step #14305\tEpoch   4 Batch 1804/3125   Loss: 0.833165 mae: 0.721889 (2045.582856195316 steps/sec)\n",
      "Step #14306\tEpoch   4 Batch 1805/3125   Loss: 0.862972 mae: 0.726928 (1913.4423955985803 steps/sec)\n",
      "Step #14307\tEpoch   4 Batch 1806/3125   Loss: 0.770355 mae: 0.698284 (1879.3368581414106 steps/sec)\n",
      "Step #14308\tEpoch   4 Batch 1807/3125   Loss: 0.846518 mae: 0.738317 (2060.5767624662244 steps/sec)\n",
      "Step #14309\tEpoch   4 Batch 1808/3125   Loss: 0.756392 mae: 0.662256 (2140.6937100626747 steps/sec)\n",
      "Step #14310\tEpoch   4 Batch 1809/3125   Loss: 0.752251 mae: 0.682530 (2005.1171240080314 steps/sec)\n",
      "Step #14311\tEpoch   4 Batch 1810/3125   Loss: 0.876044 mae: 0.741060 (2200.1174989509022 steps/sec)\n",
      "Step #14312\tEpoch   4 Batch 1811/3125   Loss: 0.817562 mae: 0.723682 (2076.6554110925167 steps/sec)\n",
      "Step #14313\tEpoch   4 Batch 1812/3125   Loss: 0.743194 mae: 0.690104 (2108.177769735718 steps/sec)\n",
      "Step #14314\tEpoch   4 Batch 1813/3125   Loss: 0.754528 mae: 0.691719 (2157.785780430085 steps/sec)\n",
      "Step #14315\tEpoch   4 Batch 1814/3125   Loss: 0.770117 mae: 0.687616 (1954.8032288734364 steps/sec)\n",
      "Step #14316\tEpoch   4 Batch 1815/3125   Loss: 0.760615 mae: 0.680939 (1997.287619047619 steps/sec)\n",
      "Step #14317\tEpoch   4 Batch 1816/3125   Loss: 0.805302 mae: 0.700580 (1896.5878363101967 steps/sec)\n",
      "Step #14318\tEpoch   4 Batch 1817/3125   Loss: 0.770945 mae: 0.698355 (2093.0078444679534 steps/sec)\n",
      "Step #14319\tEpoch   4 Batch 1818/3125   Loss: 0.806829 mae: 0.738752 (2317.6279465558587 steps/sec)\n",
      "Step #14320\tEpoch   4 Batch 1819/3125   Loss: 0.733926 mae: 0.681352 (2354.8159626310944 steps/sec)\n",
      "Step #14321\tEpoch   4 Batch 1820/3125   Loss: 0.768581 mae: 0.682577 (2186.195753020526 steps/sec)\n",
      "Step #14322\tEpoch   4 Batch 1821/3125   Loss: 0.791093 mae: 0.699626 (2086.6353577967047 steps/sec)\n",
      "Step #14323\tEpoch   4 Batch 1822/3125   Loss: 0.795634 mae: 0.705169 (2144.743866395312 steps/sec)\n",
      "Step #14324\tEpoch   4 Batch 1823/3125   Loss: 0.826648 mae: 0.734723 (2331.3345561669726 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14325\tEpoch   4 Batch 1824/3125   Loss: 0.697957 mae: 0.680447 (1985.8077589553723 steps/sec)\n",
      "Step #14326\tEpoch   4 Batch 1825/3125   Loss: 0.903143 mae: 0.769100 (1886.3521475151788 steps/sec)\n",
      "Step #14327\tEpoch   4 Batch 1826/3125   Loss: 0.837322 mae: 0.726732 (2209.575185434929 steps/sec)\n",
      "Step #14328\tEpoch   4 Batch 1827/3125   Loss: 0.801968 mae: 0.712133 (2339.1616659602473 steps/sec)\n",
      "Step #14329\tEpoch   4 Batch 1828/3125   Loss: 0.724680 mae: 0.666030 (2205.949425674254 steps/sec)\n",
      "Step #14330\tEpoch   4 Batch 1829/3125   Loss: 0.775581 mae: 0.715445 (2109.810865191147 steps/sec)\n",
      "Step #14331\tEpoch   4 Batch 1830/3125   Loss: 0.825030 mae: 0.735550 (2236.866693687736 steps/sec)\n",
      "Step #14332\tEpoch   4 Batch 1831/3125   Loss: 0.674093 mae: 0.652156 (2130.818939239992 steps/sec)\n",
      "Step #14333\tEpoch   4 Batch 1832/3125   Loss: 0.704491 mae: 0.666195 (2123.3111939089586 steps/sec)\n",
      "Step #14334\tEpoch   4 Batch 1833/3125   Loss: 0.722688 mae: 0.670480 (2037.9693695094456 steps/sec)\n",
      "Step #14335\tEpoch   4 Batch 1834/3125   Loss: 0.818380 mae: 0.723551 (2099.50344385712 steps/sec)\n",
      "Step #14336\tEpoch   4 Batch 1835/3125   Loss: 0.875272 mae: 0.737178 (2329.0819839630394 steps/sec)\n",
      "Step #14337\tEpoch   4 Batch 1836/3125   Loss: 0.798842 mae: 0.722798 (2319.217030688416 steps/sec)\n",
      "Step #14338\tEpoch   4 Batch 1837/3125   Loss: 0.765702 mae: 0.722194 (2179.90104361565 steps/sec)\n",
      "Step #14339\tEpoch   4 Batch 1838/3125   Loss: 0.779950 mae: 0.703799 (2419.1952750092287 steps/sec)\n",
      "Step #14340\tEpoch   4 Batch 1839/3125   Loss: 0.695859 mae: 0.652458 (2111.403976843695 steps/sec)\n",
      "Step #14341\tEpoch   4 Batch 1840/3125   Loss: 0.820048 mae: 0.681226 (2174.476380075484 steps/sec)\n",
      "Step #14342\tEpoch   4 Batch 1841/3125   Loss: 0.806634 mae: 0.687802 (2013.6267619157352 steps/sec)\n",
      "Step #14343\tEpoch   4 Batch 1842/3125   Loss: 0.745125 mae: 0.666039 (1905.4105376011921 steps/sec)\n",
      "Step #14344\tEpoch   4 Batch 1843/3125   Loss: 0.580356 mae: 0.614150 (1982.0728503109465 steps/sec)\n",
      "Step #14345\tEpoch   4 Batch 1844/3125   Loss: 0.800284 mae: 0.711327 (1979.9769633112408 steps/sec)\n",
      "Step #14346\tEpoch   4 Batch 1845/3125   Loss: 0.793671 mae: 0.720765 (2153.3324434496 steps/sec)\n",
      "Step #14347\tEpoch   4 Batch 1846/3125   Loss: 1.079716 mae: 0.807416 (2018.977202711029 steps/sec)\n",
      "Step #14348\tEpoch   4 Batch 1847/3125   Loss: 0.800457 mae: 0.709881 (2116.3259127696933 steps/sec)\n",
      "Step #14349\tEpoch   4 Batch 1848/3125   Loss: 0.950498 mae: 0.772606 (1933.99977867129 steps/sec)\n",
      "Step #14350\tEpoch   4 Batch 1849/3125   Loss: 0.740352 mae: 0.671253 (2172.021584000497 steps/sec)\n",
      "Step #14351\tEpoch   4 Batch 1850/3125   Loss: 0.761840 mae: 0.691591 (2053.293647685439 steps/sec)\n",
      "Step #14352\tEpoch   4 Batch 1851/3125   Loss: 0.833807 mae: 0.717523 (1948.0302819190933 steps/sec)\n",
      "Step #14353\tEpoch   4 Batch 1852/3125   Loss: 0.857391 mae: 0.744660 (1893.0098209127673 steps/sec)\n",
      "Step #14354\tEpoch   4 Batch 1853/3125   Loss: 0.782109 mae: 0.713048 (2131.750307490572 steps/sec)\n",
      "Step #14355\tEpoch   4 Batch 1854/3125   Loss: 0.901739 mae: 0.760994 (1928.5575030806865 steps/sec)\n",
      "Step #14356\tEpoch   4 Batch 1855/3125   Loss: 0.721065 mae: 0.669936 (2224.9769243010983 steps/sec)\n",
      "Step #14357\tEpoch   4 Batch 1856/3125   Loss: 0.896780 mae: 0.737703 (2285.9983213246273 steps/sec)\n",
      "Step #14358\tEpoch   4 Batch 1857/3125   Loss: 0.833047 mae: 0.700119 (2211.299268225817 steps/sec)\n",
      "Step #14359\tEpoch   4 Batch 1858/3125   Loss: 0.698950 mae: 0.633581 (2334.4746977758978 steps/sec)\n",
      "Step #14360\tEpoch   4 Batch 1859/3125   Loss: 0.858641 mae: 0.723041 (1903.4562881208249 steps/sec)\n",
      "Step #14361\tEpoch   4 Batch 1860/3125   Loss: 0.849281 mae: 0.730845 (2064.5933626706833 steps/sec)\n",
      "Step #14362\tEpoch   4 Batch 1861/3125   Loss: 0.821967 mae: 0.700932 (2161.7895062364705 steps/sec)\n",
      "Step #14363\tEpoch   4 Batch 1862/3125   Loss: 0.706254 mae: 0.649447 (1969.5821632840896 steps/sec)\n",
      "Step #14364\tEpoch   4 Batch 1863/3125   Loss: 0.743039 mae: 0.666873 (2050.8639995305944 steps/sec)\n",
      "Step #14365\tEpoch   4 Batch 1864/3125   Loss: 0.701893 mae: 0.654576 (2094.157354982375 steps/sec)\n",
      "Step #14366\tEpoch   4 Batch 1865/3125   Loss: 0.651207 mae: 0.658195 (2273.5326640792696 steps/sec)\n",
      "Step #14367\tEpoch   4 Batch 1866/3125   Loss: 0.809142 mae: 0.731003 (2096.1668016032463 steps/sec)\n",
      "Step #14368\tEpoch   4 Batch 1867/3125   Loss: 0.662857 mae: 0.659829 (2135.1795477453447 steps/sec)\n",
      "Step #14369\tEpoch   4 Batch 1868/3125   Loss: 0.890344 mae: 0.753611 (1867.421773432352 steps/sec)\n",
      "Step #14370\tEpoch   4 Batch 1869/3125   Loss: 0.863280 mae: 0.734693 (1811.3249265849024 steps/sec)\n",
      "Step #14371\tEpoch   4 Batch 1870/3125   Loss: 0.841774 mae: 0.721739 (2091.5257956098094 steps/sec)\n",
      "Step #14372\tEpoch   4 Batch 1871/3125   Loss: 0.689403 mae: 0.660440 (2072.3664966994743 steps/sec)\n",
      "Step #14373\tEpoch   4 Batch 1872/3125   Loss: 0.810614 mae: 0.720059 (2088.734400366523 steps/sec)\n",
      "Step #14374\tEpoch   4 Batch 1873/3125   Loss: 0.670074 mae: 0.656573 (2176.5752301481043 steps/sec)\n",
      "Step #14375\tEpoch   4 Batch 1874/3125   Loss: 0.872954 mae: 0.738807 (2059.3020287122686 steps/sec)\n",
      "Step #14376\tEpoch   4 Batch 1875/3125   Loss: 0.766799 mae: 0.681277 (2264.05838407393 steps/sec)\n",
      "Step #14377\tEpoch   4 Batch 1876/3125   Loss: 0.825856 mae: 0.718628 (2049.841654611565 steps/sec)\n",
      "Step #14378\tEpoch   4 Batch 1877/3125   Loss: 0.815215 mae: 0.705539 (1950.004649173377 steps/sec)\n",
      "Step #14379\tEpoch   4 Batch 1878/3125   Loss: 0.765113 mae: 0.707325 (2130.840589723529 steps/sec)\n",
      "Step #14380\tEpoch   4 Batch 1879/3125   Loss: 0.706615 mae: 0.654589 (2240.6427625112183 steps/sec)\n",
      "Step #14381\tEpoch   4 Batch 1880/3125   Loss: 0.807840 mae: 0.720742 (2183.9645925540226 steps/sec)\n",
      "Step #14382\tEpoch   4 Batch 1881/3125   Loss: 0.728897 mae: 0.669698 (2322.453183313215 steps/sec)\n",
      "Step #14383\tEpoch   4 Batch 1882/3125   Loss: 0.738824 mae: 0.683279 (2033.187908401683 steps/sec)\n",
      "Step #14384\tEpoch   4 Batch 1883/3125   Loss: 0.796012 mae: 0.707459 (2181.397574320248 steps/sec)\n",
      "Step #14385\tEpoch   4 Batch 1884/3125   Loss: 0.862204 mae: 0.740709 (2205.068029356718 steps/sec)\n",
      "Step #14386\tEpoch   4 Batch 1885/3125   Loss: 0.799420 mae: 0.693423 (2052.5500866177954 steps/sec)\n",
      "Step #14387\tEpoch   4 Batch 1886/3125   Loss: 0.773633 mae: 0.702818 (1856.0509779626516 steps/sec)\n",
      "Step #14388\tEpoch   4 Batch 1887/3125   Loss: 0.708008 mae: 0.643752 (1828.683042526661 steps/sec)\n",
      "Step #14389\tEpoch   4 Batch 1888/3125   Loss: 0.759195 mae: 0.674633 (2092.8407480589985 steps/sec)\n",
      "Step #14390\tEpoch   4 Batch 1889/3125   Loss: 0.852669 mae: 0.719118 (2054.1383430955784 steps/sec)\n",
      "Step #14391\tEpoch   4 Batch 1890/3125   Loss: 0.705384 mae: 0.654676 (2075.278563936113 steps/sec)\n",
      "Step #14392\tEpoch   4 Batch 1891/3125   Loss: 0.771550 mae: 0.702098 (2329.2889352911125 steps/sec)\n",
      "Step #14393\tEpoch   4 Batch 1892/3125   Loss: 0.780198 mae: 0.678413 (2212.3956915740946 steps/sec)\n",
      "Step #14394\tEpoch   4 Batch 1893/3125   Loss: 0.788394 mae: 0.716581 (2365.5202752241835 steps/sec)\n",
      "Step #14395\tEpoch   4 Batch 1894/3125   Loss: 0.651666 mae: 0.635809 (2114.149764103391 steps/sec)\n",
      "Step #14396\tEpoch   4 Batch 1895/3125   Loss: 0.733388 mae: 0.691759 (1635.5888316955234 steps/sec)\n",
      "Step #14397\tEpoch   4 Batch 1896/3125   Loss: 0.880707 mae: 0.748341 (2185.8767367444575 steps/sec)\n",
      "Step #14398\tEpoch   4 Batch 1897/3125   Loss: 0.782410 mae: 0.716929 (2070.7499382868427 steps/sec)\n",
      "Step #14399\tEpoch   4 Batch 1898/3125   Loss: 0.740342 mae: 0.703682 (2192.5956904032537 steps/sec)\n",
      "Step #14400\tEpoch   4 Batch 1899/3125   Loss: 0.867222 mae: 0.723346 (2343.108052244059 steps/sec)\n",
      "Step #14401\tEpoch   4 Batch 1900/3125   Loss: 0.674271 mae: 0.639518 (2343.108052244059 steps/sec)\n",
      "Step #14402\tEpoch   4 Batch 1901/3125   Loss: 0.830339 mae: 0.721877 (2232.1525885558585 steps/sec)\n",
      "Step #14403\tEpoch   4 Batch 1902/3125   Loss: 0.809482 mae: 0.714734 (2182.555392508872 steps/sec)\n",
      "Step #14404\tEpoch   4 Batch 1903/3125   Loss: 0.815046 mae: 0.730951 (2090.650078256622 steps/sec)\n",
      "Step #14405\tEpoch   4 Batch 1904/3125   Loss: 0.823170 mae: 0.715711 (1991.3325863607877 steps/sec)\n",
      "Step #14406\tEpoch   4 Batch 1905/3125   Loss: 0.737328 mae: 0.690218 (1981.3799684438272 steps/sec)\n",
      "Step #14407\tEpoch   4 Batch 1906/3125   Loss: 0.885472 mae: 0.774422 (1884.6061216053488 steps/sec)\n",
      "Step #14408\tEpoch   4 Batch 1907/3125   Loss: 0.872467 mae: 0.732904 (2230.229812938011 steps/sec)\n",
      "Step #14409\tEpoch   4 Batch 1908/3125   Loss: 0.773203 mae: 0.703824 (2226.063327282956 steps/sec)\n",
      "Step #14410\tEpoch   4 Batch 1909/3125   Loss: 0.710027 mae: 0.643932 (2067.6466818500003 steps/sec)\n",
      "Step #14411\tEpoch   4 Batch 1910/3125   Loss: 0.708306 mae: 0.689951 (1928.3447045625908 steps/sec)\n",
      "Step #14412\tEpoch   4 Batch 1911/3125   Loss: 0.799980 mae: 0.711218 (1978.2959776620633 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14413\tEpoch   4 Batch 1912/3125   Loss: 0.842626 mae: 0.736348 (2019.9495290015604 steps/sec)\n",
      "Step #14414\tEpoch   4 Batch 1913/3125   Loss: 0.761762 mae: 0.678739 (1809.9492525977837 steps/sec)\n",
      "Step #14415\tEpoch   4 Batch 1914/3125   Loss: 0.838174 mae: 0.699424 (2180.6490522090858 steps/sec)\n",
      "Step #14416\tEpoch   4 Batch 1915/3125   Loss: 0.710383 mae: 0.666862 (2200.2329119236215 steps/sec)\n",
      "Step #14417\tEpoch   4 Batch 1916/3125   Loss: 0.717171 mae: 0.664058 (2288.3430629057775 steps/sec)\n",
      "Step #14418\tEpoch   4 Batch 1917/3125   Loss: 0.751898 mae: 0.690914 (2097.9492207038675 steps/sec)\n",
      "Step #14419\tEpoch   4 Batch 1918/3125   Loss: 0.693445 mae: 0.656241 (2196.4306661080855 steps/sec)\n",
      "Step #14420\tEpoch   4 Batch 1919/3125   Loss: 0.722380 mae: 0.665640 (2324.8217987517596 steps/sec)\n",
      "Step #14421\tEpoch   4 Batch 1920/3125   Loss: 0.848484 mae: 0.683191 (2359.9302312496484 steps/sec)\n",
      "Step #14422\tEpoch   4 Batch 1921/3125   Loss: 0.679658 mae: 0.650672 (1914.9100139704337 steps/sec)\n",
      "Step #14423\tEpoch   4 Batch 1922/3125   Loss: 0.905996 mae: 0.751852 (2087.2583952067203 steps/sec)\n",
      "Step #14424\tEpoch   4 Batch 1923/3125   Loss: 0.806350 mae: 0.709070 (2062.5222513990107 steps/sec)\n",
      "Step #14425\tEpoch   4 Batch 1924/3125   Loss: 0.965098 mae: 0.775697 (2089.44195917066 steps/sec)\n",
      "Step #14426\tEpoch   4 Batch 1925/3125   Loss: 0.750403 mae: 0.685253 (1960.010093740946 steps/sec)\n",
      "Step #14427\tEpoch   4 Batch 1926/3125   Loss: 0.783207 mae: 0.695468 (2084.6441351888666 steps/sec)\n",
      "Step #14428\tEpoch   4 Batch 1927/3125   Loss: 0.835042 mae: 0.732630 (2102.7874704207275 steps/sec)\n",
      "Step #14429\tEpoch   4 Batch 1928/3125   Loss: 0.820112 mae: 0.720823 (2160.8539751885587 steps/sec)\n",
      "Step #14430\tEpoch   4 Batch 1929/3125   Loss: 0.761456 mae: 0.694796 (2129.109940202439 steps/sec)\n",
      "Step #14431\tEpoch   4 Batch 1930/3125   Loss: 0.882588 mae: 0.731613 (1679.7103770864703 steps/sec)\n",
      "Step #14432\tEpoch   4 Batch 1931/3125   Loss: 0.642710 mae: 0.644539 (1986.014621765976 steps/sec)\n",
      "Step #14433\tEpoch   4 Batch 1932/3125   Loss: 0.836639 mae: 0.732662 (2284.4544177078683 steps/sec)\n",
      "Step #14434\tEpoch   4 Batch 1933/3125   Loss: 0.619837 mae: 0.612113 (2006.4216145883163 steps/sec)\n",
      "Step #14435\tEpoch   4 Batch 1934/3125   Loss: 0.694413 mae: 0.667084 (2079.0023098351394 steps/sec)\n",
      "Step #14436\tEpoch   4 Batch 1935/3125   Loss: 0.720367 mae: 0.662477 (2066.6482715124757 steps/sec)\n",
      "Step #14437\tEpoch   4 Batch 1936/3125   Loss: 0.750684 mae: 0.681788 (1984.9055889451517 steps/sec)\n",
      "Step #14438\tEpoch   4 Batch 1937/3125   Loss: 0.934285 mae: 0.732903 (1855.1643607798733 steps/sec)\n",
      "Step #14439\tEpoch   4 Batch 1938/3125   Loss: 0.844579 mae: 0.722733 (1764.7892420455598 steps/sec)\n",
      "Step #14440\tEpoch   4 Batch 1939/3125   Loss: 0.772173 mae: 0.675528 (1969.7486568734269 steps/sec)\n",
      "Step #14441\tEpoch   4 Batch 1940/3125   Loss: 0.748910 mae: 0.684569 (1903.3871846070067 steps/sec)\n",
      "Step #14442\tEpoch   4 Batch 1941/3125   Loss: 0.679507 mae: 0.668273 (1943.643070307142 steps/sec)\n",
      "Step #14443\tEpoch   4 Batch 1942/3125   Loss: 0.673032 mae: 0.653793 (2007.228177641654 steps/sec)\n",
      "Step #14444\tEpoch   4 Batch 1943/3125   Loss: 0.754854 mae: 0.690276 (2130.2778201025953 steps/sec)\n",
      "Step #14445\tEpoch   4 Batch 1944/3125   Loss: 0.833230 mae: 0.732058 (2271.6856050348256 steps/sec)\n",
      "Step #14446\tEpoch   4 Batch 1945/3125   Loss: 0.823779 mae: 0.715909 (2158.874213771734 steps/sec)\n",
      "Step #14447\tEpoch   4 Batch 1946/3125   Loss: 0.822610 mae: 0.738922 (2069.8915285687494 steps/sec)\n",
      "Step #14448\tEpoch   4 Batch 1947/3125   Loss: 0.733687 mae: 0.691437 (1956.4080079108905 steps/sec)\n",
      "Step #14449\tEpoch   4 Batch 1948/3125   Loss: 0.746922 mae: 0.700324 (1883.1000206524375 steps/sec)\n",
      "Step #14450\tEpoch   4 Batch 1949/3125   Loss: 0.748769 mae: 0.671866 (2054.0578659719095 steps/sec)\n",
      "Step #14451\tEpoch   4 Batch 1950/3125   Loss: 0.764219 mae: 0.691844 (1992.3541706251187 steps/sec)\n",
      "Step #14452\tEpoch   4 Batch 1951/3125   Loss: 0.771971 mae: 0.695547 (2047.2602672862345 steps/sec)\n",
      "Step #14453\tEpoch   4 Batch 1952/3125   Loss: 0.774454 mae: 0.680946 (2126.476105494773 steps/sec)\n",
      "Step #14454\tEpoch   4 Batch 1953/3125   Loss: 1.048792 mae: 0.801315 (2089.5044138453263 steps/sec)\n",
      "Step #14455\tEpoch   4 Batch 1954/3125   Loss: 0.677397 mae: 0.656570 (2196.2696492716286 steps/sec)\n",
      "Step #14456\tEpoch   4 Batch 1955/3125   Loss: 0.710514 mae: 0.672412 (2194.683745656997 steps/sec)\n",
      "Step #14457\tEpoch   4 Batch 1956/3125   Loss: 0.773137 mae: 0.716789 (1724.6173962385178 steps/sec)\n",
      "Step #14458\tEpoch   4 Batch 1957/3125   Loss: 0.799674 mae: 0.696364 (2185.967874751139 steps/sec)\n",
      "Step #14459\tEpoch   4 Batch 1958/3125   Loss: 0.851798 mae: 0.718085 (2325.2600066526225 steps/sec)\n",
      "Step #14460\tEpoch   4 Batch 1959/3125   Loss: 0.880463 mae: 0.730567 (2183.9873364992086 steps/sec)\n",
      "Step #14461\tEpoch   4 Batch 1960/3125   Loss: 0.664599 mae: 0.646106 (2096.7955447573913 steps/sec)\n",
      "Step #14462\tEpoch   4 Batch 1961/3125   Loss: 0.936400 mae: 0.750849 (2192.6873895632716 steps/sec)\n",
      "Step #14463\tEpoch   4 Batch 1962/3125   Loss: 0.771376 mae: 0.707409 (2177.524426585262 steps/sec)\n",
      "Step #14464\tEpoch   4 Batch 1963/3125   Loss: 0.730376 mae: 0.688843 (2202.6825195097103 steps/sec)\n",
      "Step #14465\tEpoch   4 Batch 1964/3125   Loss: 0.921852 mae: 0.736143 (2037.9891742709153 steps/sec)\n",
      "Step #14466\tEpoch   4 Batch 1965/3125   Loss: 0.714826 mae: 0.661611 (1990.6710077931448 steps/sec)\n",
      "Step #14467\tEpoch   4 Batch 1966/3125   Loss: 0.784605 mae: 0.710413 (2199.0562674328376 steps/sec)\n",
      "Step #14468\tEpoch   4 Batch 1967/3125   Loss: 0.806492 mae: 0.717427 (2089.2546175456773 steps/sec)\n",
      "Step #14469\tEpoch   4 Batch 1968/3125   Loss: 0.782087 mae: 0.691947 (2238.1558164354324 steps/sec)\n",
      "Step #14470\tEpoch   4 Batch 1969/3125   Loss: 0.632461 mae: 0.634899 (2197.8117795011526 steps/sec)\n",
      "Step #14471\tEpoch   4 Batch 1970/3125   Loss: 0.826805 mae: 0.709197 (2007.1321242283582 steps/sec)\n",
      "Step #14472\tEpoch   4 Batch 1971/3125   Loss: 0.727524 mae: 0.664744 (2067.9117281637646 steps/sec)\n",
      "Step #14473\tEpoch   4 Batch 1972/3125   Loss: 0.720494 mae: 0.672013 (1927.5294117647059 steps/sec)\n",
      "Step #14474\tEpoch   4 Batch 1973/3125   Loss: 0.781509 mae: 0.718162 (2201.9655606887864 steps/sec)\n",
      "Step #14475\tEpoch   4 Batch 1974/3125   Loss: 0.789531 mae: 0.704256 (1688.8001288452247 steps/sec)\n",
      "Step #14476\tEpoch   4 Batch 1975/3125   Loss: 0.852311 mae: 0.708214 (1938.881133104666 steps/sec)\n",
      "Step #14477\tEpoch   4 Batch 1976/3125   Loss: 0.762712 mae: 0.686942 (2081.065364730632 steps/sec)\n",
      "Step #14478\tEpoch   4 Batch 1977/3125   Loss: 0.867018 mae: 0.747933 (2192.710315551745 steps/sec)\n",
      "Step #14479\tEpoch   4 Batch 1978/3125   Loss: 0.872353 mae: 0.727765 (2102.1762011206783 steps/sec)\n",
      "Step #14480\tEpoch   4 Batch 1979/3125   Loss: 0.721905 mae: 0.686167 (2046.9005905031477 steps/sec)\n",
      "Step #14481\tEpoch   4 Batch 1980/3125   Loss: 0.758917 mae: 0.702816 (2054.802520061532 steps/sec)\n",
      "Step #14482\tEpoch   4 Batch 1981/3125   Loss: 0.840944 mae: 0.721118 (2299.0802153107425 steps/sec)\n",
      "Step #14483\tEpoch   4 Batch 1982/3125   Loss: 0.891742 mae: 0.762582 (2187.1533607967876 steps/sec)\n",
      "Step #14484\tEpoch   4 Batch 1983/3125   Loss: 0.750651 mae: 0.671589 (1979.9769633112408 steps/sec)\n",
      "Step #14485\tEpoch   4 Batch 1984/3125   Loss: 0.685282 mae: 0.677442 (2041.4808180906675 steps/sec)\n",
      "Step #14486\tEpoch   4 Batch 1985/3125   Loss: 0.792433 mae: 0.712535 (1999.8207252996654 steps/sec)\n",
      "Step #14487\tEpoch   4 Batch 1986/3125   Loss: 0.821692 mae: 0.707760 (2197.719651240778 steps/sec)\n",
      "Step #14488\tEpoch   4 Batch 1987/3125   Loss: 0.751564 mae: 0.724485 (2295.4816112084063 steps/sec)\n",
      "Step #14489\tEpoch   4 Batch 1988/3125   Loss: 0.679907 mae: 0.665262 (2144.5464771449024 steps/sec)\n",
      "Step #14490\tEpoch   4 Batch 1989/3125   Loss: 0.779124 mae: 0.680499 (1902.6619005280252 steps/sec)\n",
      "Step #14491\tEpoch   4 Batch 1990/3125   Loss: 0.723122 mae: 0.655078 (2103.4201921726744 steps/sec)\n",
      "Step #14492\tEpoch   4 Batch 1991/3125   Loss: 0.702723 mae: 0.667186 (2230.0638026371757 steps/sec)\n",
      "Step #14493\tEpoch   4 Batch 1992/3125   Loss: 0.884009 mae: 0.763785 (1872.2231149677718 steps/sec)\n",
      "Step #14494\tEpoch   4 Batch 1993/3125   Loss: 0.773511 mae: 0.689438 (2090.650078256622 steps/sec)\n",
      "Step #14495\tEpoch   4 Batch 1994/3125   Loss: 0.823458 mae: 0.723141 (2073.4114390231844 steps/sec)\n",
      "Step #14496\tEpoch   4 Batch 1995/3125   Loss: 0.858639 mae: 0.738305 (2069.8710989162832 steps/sec)\n",
      "Step #14497\tEpoch   4 Batch 1996/3125   Loss: 0.799044 mae: 0.688782 (2064.1463006525655 steps/sec)\n",
      "Step #14498\tEpoch   4 Batch 1997/3125   Loss: 0.888998 mae: 0.717031 (2236.1511558474795 steps/sec)\n",
      "Step #14499\tEpoch   4 Batch 1998/3125   Loss: 0.902650 mae: 0.731822 (2121.077757100089 steps/sec)\n",
      "Step #14500\tEpoch   4 Batch 1999/3125   Loss: 0.859990 mae: 0.737120 (2110.851425753138 steps/sec)\n",
      "Step #14501\tEpoch   4 Batch 2000/3125   Loss: 0.915541 mae: 0.770716 (2217.378248641333 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14502\tEpoch   4 Batch 2001/3125   Loss: 0.705608 mae: 0.657703 (1783.1560509824928 steps/sec)\n",
      "Step #14503\tEpoch   4 Batch 2002/3125   Loss: 0.746114 mae: 0.700799 (2065.264318915938 steps/sec)\n",
      "Step #14504\tEpoch   4 Batch 2003/3125   Loss: 0.678443 mae: 0.653051 (2003.7568912966626 steps/sec)\n",
      "Step #14505\tEpoch   4 Batch 2004/3125   Loss: 0.862142 mae: 0.739976 (1748.8654463578368 steps/sec)\n",
      "Step #14506\tEpoch   4 Batch 2005/3125   Loss: 0.782226 mae: 0.705451 (2120.4343693757455 steps/sec)\n",
      "Step #14507\tEpoch   4 Batch 2006/3125   Loss: 0.861678 mae: 0.738080 (1922.6168429930874 steps/sec)\n",
      "Step #14508\tEpoch   4 Batch 2007/3125   Loss: 0.736954 mae: 0.675752 (2287.095261464638 steps/sec)\n",
      "Step #14509\tEpoch   4 Batch 2008/3125   Loss: 0.810390 mae: 0.729931 (1968.4914019674102 steps/sec)\n",
      "Step #14510\tEpoch   4 Batch 2009/3125   Loss: 0.774191 mae: 0.704179 (2009.9214107724747 steps/sec)\n",
      "Step #14511\tEpoch   4 Batch 2010/3125   Loss: 0.701740 mae: 0.635551 (1859.704880817253 steps/sec)\n",
      "Step #14512\tEpoch   4 Batch 2011/3125   Loss: 0.722104 mae: 0.658805 (2086.5523142436423 steps/sec)\n",
      "Step #14513\tEpoch   4 Batch 2012/3125   Loss: 0.788109 mae: 0.722188 (2160.008239777526 steps/sec)\n",
      "Step #14514\tEpoch   4 Batch 2013/3125   Loss: 0.705309 mae: 0.681418 (2218.551117129316 steps/sec)\n",
      "Step #14515\tEpoch   4 Batch 2014/3125   Loss: 0.825760 mae: 0.691849 (1962.8535594616349 steps/sec)\n",
      "Step #14516\tEpoch   4 Batch 2015/3125   Loss: 0.914682 mae: 0.759448 (2148.1490586524083 steps/sec)\n",
      "Step #14517\tEpoch   4 Batch 2016/3125   Loss: 0.727894 mae: 0.687670 (2058.291457286432 steps/sec)\n",
      "Step #14518\tEpoch   4 Batch 2017/3125   Loss: 0.761796 mae: 0.701741 (2083.0083731463365 steps/sec)\n",
      "Step #14519\tEpoch   4 Batch 2018/3125   Loss: 0.718216 mae: 0.677199 (1575.8344479343561 steps/sec)\n",
      "Step #14520\tEpoch   4 Batch 2019/3125   Loss: 0.854324 mae: 0.696792 (1950.839069767442 steps/sec)\n",
      "Step #14521\tEpoch   4 Batch 2020/3125   Loss: 0.798066 mae: 0.705686 (1753.3396316330711 steps/sec)\n",
      "Step #14522\tEpoch   4 Batch 2021/3125   Loss: 0.717274 mae: 0.668956 (2020.3776493256262 steps/sec)\n",
      "Step #14523\tEpoch   4 Batch 2022/3125   Loss: 0.850037 mae: 0.752208 (1887.6425530382812 steps/sec)\n",
      "Step #14524\tEpoch   4 Batch 2023/3125   Loss: 1.033768 mae: 0.799212 (1862.1157500310774 steps/sec)\n",
      "Step #14525\tEpoch   4 Batch 2024/3125   Loss: 0.858234 mae: 0.712688 (2064.6746674805313 steps/sec)\n",
      "Step #14526\tEpoch   4 Batch 2025/3125   Loss: 0.783376 mae: 0.708951 (2131.902002643082 steps/sec)\n",
      "Step #14527\tEpoch   4 Batch 2026/3125   Loss: 0.849156 mae: 0.727567 (2042.972372676616 steps/sec)\n",
      "Step #14528\tEpoch   4 Batch 2027/3125   Loss: 0.780926 mae: 0.689504 (1865.129847029527 steps/sec)\n",
      "Step #14529\tEpoch   4 Batch 2028/3125   Loss: 0.744709 mae: 0.701065 (1905.1162790697674 steps/sec)\n",
      "Step #14530\tEpoch   4 Batch 2029/3125   Loss: 0.930684 mae: 0.770571 (2220.359763263491 steps/sec)\n",
      "Step #14531\tEpoch   4 Batch 2030/3125   Loss: 0.762185 mae: 0.690163 (2201.757498766391 steps/sec)\n",
      "Step #14532\tEpoch   4 Batch 2031/3125   Loss: 0.865909 mae: 0.727073 (2174.2960228922157 steps/sec)\n",
      "Step #14533\tEpoch   4 Batch 2032/3125   Loss: 0.724790 mae: 0.693339 (2031.100608220664 steps/sec)\n",
      "Step #14534\tEpoch   4 Batch 2033/3125   Loss: 0.859836 mae: 0.730445 (2185.8767367444575 steps/sec)\n",
      "Step #14535\tEpoch   4 Batch 2034/3125   Loss: 0.820254 mae: 0.701355 (2189.139648009353 steps/sec)\n",
      "Step #14536\tEpoch   4 Batch 2035/3125   Loss: 0.956025 mae: 0.768669 (1906.571148041747 steps/sec)\n",
      "Step #14537\tEpoch   4 Batch 2036/3125   Loss: 0.655408 mae: 0.647665 (2020.3581853739367 steps/sec)\n",
      "Step #14538\tEpoch   4 Batch 2037/3125   Loss: 0.804286 mae: 0.689384 (1920.9262278564493 steps/sec)\n",
      "Step #14539\tEpoch   4 Batch 2038/3125   Loss: 0.875343 mae: 0.742245 (2178.180307436643 steps/sec)\n",
      "Step #14540\tEpoch   4 Batch 2039/3125   Loss: 0.897983 mae: 0.744682 (2085.411135308215 steps/sec)\n",
      "Step #14541\tEpoch   4 Batch 2040/3125   Loss: 0.761707 mae: 0.710259 (2135.3752163730783 steps/sec)\n",
      "Step #14542\tEpoch   4 Batch 2041/3125   Loss: 0.775723 mae: 0.698078 (2113.0846583237612 steps/sec)\n",
      "Step #14543\tEpoch   4 Batch 2042/3125   Loss: 0.729626 mae: 0.680080 (2072.632754514098 steps/sec)\n",
      "Step #14544\tEpoch   4 Batch 2043/3125   Loss: 0.772627 mae: 0.685082 (2039.4756292060529 steps/sec)\n",
      "Step #14545\tEpoch   4 Batch 2044/3125   Loss: 0.922736 mae: 0.760816 (1931.185885039689 steps/sec)\n",
      "Step #14546\tEpoch   4 Batch 2045/3125   Loss: 0.623166 mae: 0.639741 (2032.0255801560002 steps/sec)\n",
      "Step #14547\tEpoch   4 Batch 2046/3125   Loss: 0.719299 mae: 0.669756 (2142.443250311587 steps/sec)\n",
      "Step #14548\tEpoch   4 Batch 2047/3125   Loss: 0.883146 mae: 0.736233 (1902.7482148851811 steps/sec)\n",
      "Step #14549\tEpoch   4 Batch 2048/3125   Loss: 0.843641 mae: 0.740499 (1915.329747107121 steps/sec)\n",
      "Step #14550\tEpoch   4 Batch 2049/3125   Loss: 0.751072 mae: 0.663390 (2062.3599870189896 steps/sec)\n",
      "Step #14551\tEpoch   4 Batch 2050/3125   Loss: 0.835255 mae: 0.722697 (2048.360063292376 steps/sec)\n",
      "Step #14552\tEpoch   4 Batch 2051/3125   Loss: 0.763339 mae: 0.702949 (2030.3730310100784 steps/sec)\n",
      "Step #14553\tEpoch   4 Batch 2052/3125   Loss: 0.810995 mae: 0.714460 (1786.5587596370917 steps/sec)\n",
      "Step #14554\tEpoch   4 Batch 2053/3125   Loss: 0.887502 mae: 0.727266 (1961.2565347099478 steps/sec)\n",
      "Step #14555\tEpoch   4 Batch 2054/3125   Loss: 0.635785 mae: 0.647127 (2221.441660928976 steps/sec)\n",
      "Step #14556\tEpoch   4 Batch 2055/3125   Loss: 0.737524 mae: 0.669082 (2052.148385897273 steps/sec)\n",
      "Step #14557\tEpoch   4 Batch 2056/3125   Loss: 0.801985 mae: 0.696378 (2095.391870828504 steps/sec)\n",
      "Step #14558\tEpoch   4 Batch 2057/3125   Loss: 0.725360 mae: 0.686280 (2144.2833479887936 steps/sec)\n",
      "Step #14559\tEpoch   4 Batch 2058/3125   Loss: 0.729585 mae: 0.667521 (2164.0425553870127 steps/sec)\n",
      "Step #14560\tEpoch   4 Batch 2059/3125   Loss: 0.863348 mae: 0.719884 (2115.0452830949835 steps/sec)\n",
      "Step #14561\tEpoch   4 Batch 2060/3125   Loss: 0.722567 mae: 0.670642 (1968.269700041296 steps/sec)\n",
      "Step #14562\tEpoch   4 Batch 2061/3125   Loss: 0.690948 mae: 0.668367 (2068.1768424374513 steps/sec)\n",
      "Step #14563\tEpoch   4 Batch 2062/3125   Loss: 0.746159 mae: 0.694334 (2190.214201418262 steps/sec)\n",
      "Step #14564\tEpoch   4 Batch 2063/3125   Loss: 0.723333 mae: 0.671856 (2158.141066540433 steps/sec)\n",
      "Step #14565\tEpoch   4 Batch 2064/3125   Loss: 0.665562 mae: 0.640184 (1932.5918075842048 steps/sec)\n",
      "Step #14566\tEpoch   4 Batch 2065/3125   Loss: 0.605212 mae: 0.618209 (2132.205457724999 steps/sec)\n",
      "Step #14567\tEpoch   4 Batch 2066/3125   Loss: 0.722195 mae: 0.666563 (2233.864507882403 steps/sec)\n",
      "Step #14568\tEpoch   4 Batch 2067/3125   Loss: 0.824319 mae: 0.705099 (1912.7617657789128 steps/sec)\n",
      "Step #14569\tEpoch   4 Batch 2068/3125   Loss: 0.706167 mae: 0.654656 (2061.265369909869 steps/sec)\n",
      "Step #14570\tEpoch   4 Batch 2069/3125   Loss: 0.829650 mae: 0.699956 (1781.7774001699236 steps/sec)\n",
      "Step #14571\tEpoch   4 Batch 2070/3125   Loss: 0.783263 mae: 0.701733 (1917.0105213122847 steps/sec)\n",
      "Step #14572\tEpoch   4 Batch 2071/3125   Loss: 0.902458 mae: 0.739020 (1907.5249451978789 steps/sec)\n",
      "Step #14573\tEpoch   4 Batch 2072/3125   Loss: 0.721364 mae: 0.667617 (2052.5500866177954 steps/sec)\n",
      "Step #14574\tEpoch   4 Batch 2073/3125   Loss: 0.681069 mae: 0.650735 (1907.0219150677458 steps/sec)\n",
      "Step #14575\tEpoch   4 Batch 2074/3125   Loss: 0.838895 mae: 0.699558 (2002.8000878608743 steps/sec)\n",
      "Step #14576\tEpoch   4 Batch 2075/3125   Loss: 0.856289 mae: 0.746458 (1816.8013791789035 steps/sec)\n",
      "Step #14577\tEpoch   4 Batch 2076/3125   Loss: 0.915609 mae: 0.751977 (2019.7160852899822 steps/sec)\n",
      "Step #14578\tEpoch   4 Batch 2077/3125   Loss: 0.752697 mae: 0.690593 (1987.200212255882 steps/sec)\n",
      "Step #14579\tEpoch   4 Batch 2078/3125   Loss: 0.810073 mae: 0.701885 (1767.5409614995617 steps/sec)\n",
      "Step #14580\tEpoch   4 Batch 2079/3125   Loss: 0.808808 mae: 0.697669 (2179.2214809734605 steps/sec)\n",
      "Step #14581\tEpoch   4 Batch 2080/3125   Loss: 0.929404 mae: 0.767190 (1965.9632709307884 steps/sec)\n",
      "Step #14582\tEpoch   4 Batch 2081/3125   Loss: 0.791229 mae: 0.724980 (1903.9747244566304 steps/sec)\n",
      "Step #14583\tEpoch   4 Batch 2082/3125   Loss: 0.807992 mae: 0.705554 (1960.761434608624 steps/sec)\n",
      "Step #14584\tEpoch   4 Batch 2083/3125   Loss: 0.715678 mae: 0.681534 (2042.2362667861212 steps/sec)\n",
      "Step #14585\tEpoch   4 Batch 2084/3125   Loss: 0.872122 mae: 0.755016 (2110.766443560968 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14586\tEpoch   4 Batch 2085/3125   Loss: 0.806603 mae: 0.707174 (1809.0437002915653 steps/sec)\n",
      "Step #14587\tEpoch   4 Batch 2086/3125   Loss: 0.873843 mae: 0.714792 (1888.084412953643 steps/sec)\n",
      "Step #14588\tEpoch   4 Batch 2087/3125   Loss: 0.777304 mae: 0.710328 (2080.8175819814455 steps/sec)\n",
      "Step #14589\tEpoch   4 Batch 2088/3125   Loss: 0.790802 mae: 0.707375 (2028.6250459478806 steps/sec)\n",
      "Step #14590\tEpoch   4 Batch 2089/3125   Loss: 0.788850 mae: 0.687244 (2186.5604570904275 steps/sec)\n",
      "Step #14591\tEpoch   4 Batch 2090/3125   Loss: 0.704351 mae: 0.645430 (2034.6673652142697 steps/sec)\n",
      "Step #14592\tEpoch   4 Batch 2091/3125   Loss: 0.657644 mae: 0.646837 (2020.3581853739367 steps/sec)\n",
      "Step #14593\tEpoch   4 Batch 2092/3125   Loss: 0.962772 mae: 0.776308 (2085.5148272638676 steps/sec)\n",
      "Step #14594\tEpoch   4 Batch 2093/3125   Loss: 0.809169 mae: 0.701549 (1986.1839052156042 steps/sec)\n",
      "Step #14595\tEpoch   4 Batch 2094/3125   Loss: 0.752913 mae: 0.703446 (1617.0872947943897 steps/sec)\n",
      "Step #14596\tEpoch   4 Batch 2095/3125   Loss: 0.797500 mae: 0.693026 (1986.0898552920676 steps/sec)\n",
      "Step #14597\tEpoch   4 Batch 2096/3125   Loss: 0.581205 mae: 0.582680 (2196.246648793566 steps/sec)\n",
      "Step #14598\tEpoch   4 Batch 2097/3125   Loss: 0.714280 mae: 0.657120 (1941.5197748481708 steps/sec)\n",
      "Step #14599\tEpoch   4 Batch 2098/3125   Loss: 0.741897 mae: 0.693691 (2026.2729714583857 steps/sec)\n",
      "Step #14600\tEpoch   4 Batch 2099/3125   Loss: 0.842813 mae: 0.705438 (2180.3769896967237 steps/sec)\n",
      "Step #14601\tEpoch   4 Batch 2100/3125   Loss: 0.770827 mae: 0.693371 (2097.5715143028606 steps/sec)\n",
      "Step #14602\tEpoch   4 Batch 2101/3125   Loss: 0.722833 mae: 0.657158 (2004.0058099533676 steps/sec)\n",
      "Step #14603\tEpoch   4 Batch 2102/3125   Loss: 0.860289 mae: 0.720775 (1710.634202047392 steps/sec)\n",
      "Step #14604\tEpoch   4 Batch 2103/3125   Loss: 0.914165 mae: 0.759142 (2131.273691805811 steps/sec)\n",
      "Step #14605\tEpoch   4 Batch 2104/3125   Loss: 0.744298 mae: 0.648182 (2107.372757875697 steps/sec)\n",
      "Step #14606\tEpoch   4 Batch 2105/3125   Loss: 0.760625 mae: 0.694911 (1969.8966748074395 steps/sec)\n",
      "Step #14607\tEpoch   4 Batch 2106/3125   Loss: 0.735429 mae: 0.672216 (1896.896623461201 steps/sec)\n",
      "Step #14608\tEpoch   4 Batch 2107/3125   Loss: 0.847079 mae: 0.724986 (2025.7640740311426 steps/sec)\n",
      "Step #14609\tEpoch   4 Batch 2108/3125   Loss: 0.825594 mae: 0.708825 (2047.1603443899962 steps/sec)\n",
      "Step #14610\tEpoch   4 Batch 2109/3125   Loss: 0.745137 mae: 0.697672 (2051.8070638880736 steps/sec)\n",
      "Step #14611\tEpoch   4 Batch 2110/3125   Loss: 0.817063 mae: 0.707818 (1860.1172578341893 steps/sec)\n",
      "Step #14612\tEpoch   4 Batch 2111/3125   Loss: 0.766293 mae: 0.682979 (2123.053249645677 steps/sec)\n",
      "Step #14613\tEpoch   4 Batch 2112/3125   Loss: 0.819605 mae: 0.696090 (2094.8685932333756 steps/sec)\n",
      "Step #14614\tEpoch   4 Batch 2113/3125   Loss: 0.840119 mae: 0.715849 (2208.970064673787 steps/sec)\n",
      "Step #14615\tEpoch   4 Batch 2114/3125   Loss: 0.856962 mae: 0.725421 (2130.1263559907366 steps/sec)\n",
      "Step #14616\tEpoch   4 Batch 2115/3125   Loss: 0.960512 mae: 0.769662 (2253.160857793631 steps/sec)\n",
      "Step #14617\tEpoch   4 Batch 2116/3125   Loss: 0.773923 mae: 0.684463 (2049.501099438065 steps/sec)\n",
      "Step #14618\tEpoch   4 Batch 2117/3125   Loss: 0.693290 mae: 0.658261 (2084.4783715012722 steps/sec)\n",
      "Step #14619\tEpoch   4 Batch 2118/3125   Loss: 0.773936 mae: 0.690642 (2098.3900501295766 steps/sec)\n",
      "Step #14620\tEpoch   4 Batch 2119/3125   Loss: 0.815888 mae: 0.692710 (1867.9041265486806 steps/sec)\n",
      "Step #14621\tEpoch   4 Batch 2120/3125   Loss: 0.734131 mae: 0.680104 (2214.942650134133 steps/sec)\n",
      "Step #14622\tEpoch   4 Batch 2121/3125   Loss: 0.766555 mae: 0.685729 (1973.177272000226 steps/sec)\n",
      "Step #14623\tEpoch   4 Batch 2122/3125   Loss: 0.806825 mae: 0.702520 (2059.46381223608 steps/sec)\n",
      "Step #14624\tEpoch   4 Batch 2123/3125   Loss: 0.698412 mae: 0.673488 (2041.1629016088687 steps/sec)\n",
      "Step #14625\tEpoch   4 Batch 2124/3125   Loss: 0.657748 mae: 0.618952 (2158.7853209120385 steps/sec)\n",
      "Step #14626\tEpoch   4 Batch 2125/3125   Loss: 0.875074 mae: 0.733575 (2243.6392036032566 steps/sec)\n",
      "Step #14627\tEpoch   4 Batch 2126/3125   Loss: 0.852926 mae: 0.725745 (2050.082114647689 steps/sec)\n",
      "Step #14628\tEpoch   4 Batch 2127/3125   Loss: 0.795506 mae: 0.698880 (2020.1051881249157 steps/sec)\n",
      "Step #14629\tEpoch   4 Batch 2128/3125   Loss: 0.721527 mae: 0.658346 (2013.7621109841466 steps/sec)\n",
      "Step #14630\tEpoch   4 Batch 2129/3125   Loss: 0.694053 mae: 0.688101 (2039.4756292060529 steps/sec)\n",
      "Step #14631\tEpoch   4 Batch 2130/3125   Loss: 0.788396 mae: 0.700563 (2085.0793903299896 steps/sec)\n",
      "Step #14632\tEpoch   4 Batch 2131/3125   Loss: 0.840854 mae: 0.722370 (2169.7518985246343 steps/sec)\n",
      "Step #14633\tEpoch   4 Batch 2132/3125   Loss: 0.723127 mae: 0.679779 (2337.988160403126 steps/sec)\n",
      "Step #14634\tEpoch   4 Batch 2133/3125   Loss: 0.718366 mae: 0.674534 (2139.9728568658866 steps/sec)\n",
      "Step #14635\tEpoch   4 Batch 2134/3125   Loss: 0.696908 mae: 0.654967 (2184.8519575771465 steps/sec)\n",
      "Step #14636\tEpoch   4 Batch 2135/3125   Loss: 0.785058 mae: 0.690426 (2354.4987088806556 steps/sec)\n",
      "Step #14637\tEpoch   4 Batch 2136/3125   Loss: 0.917182 mae: 0.755389 (2190.580247558364 steps/sec)\n",
      "Step #14638\tEpoch   4 Batch 2137/3125   Loss: 0.783209 mae: 0.697306 (1995.4442086834067 steps/sec)\n",
      "Step #14639\tEpoch   4 Batch 2138/3125   Loss: 0.676863 mae: 0.660866 (2042.0572942024187 steps/sec)\n",
      "Step #14640\tEpoch   4 Batch 2139/3125   Loss: 0.828354 mae: 0.711018 (2048.8603612845236 steps/sec)\n",
      "Step #14641\tEpoch   4 Batch 2140/3125   Loss: 0.789078 mae: 0.686278 (2085.5148272638676 steps/sec)\n",
      "Step #14642\tEpoch   4 Batch 2141/3125   Loss: 0.827722 mae: 0.728090 (2131.5553025837007 steps/sec)\n",
      "Step #14643\tEpoch   4 Batch 2142/3125   Loss: 0.638832 mae: 0.642084 (2233.745539756085 steps/sec)\n",
      "Step #14644\tEpoch   4 Batch 2143/3125   Loss: 0.717598 mae: 0.667724 (2181.1253250130007 steps/sec)\n",
      "Step #14645\tEpoch   4 Batch 2144/3125   Loss: 0.654192 mae: 0.634367 (2102.618808903148 steps/sec)\n",
      "Step #14646\tEpoch   4 Batch 2145/3125   Loss: 0.897498 mae: 0.750819 (2020.53337444119 steps/sec)\n",
      "Step #14647\tEpoch   4 Batch 2146/3125   Loss: 0.624171 mae: 0.643014 (1728.0564276238267 steps/sec)\n",
      "Step #14648\tEpoch   4 Batch 2147/3125   Loss: 0.762861 mae: 0.692155 (1738.6436743491959 steps/sec)\n",
      "Step #14649\tEpoch   4 Batch 2148/3125   Loss: 0.744979 mae: 0.676035 (2136.4846830142933 steps/sec)\n",
      "Step #14650\tEpoch   4 Batch 2149/3125   Loss: 0.737364 mae: 0.676269 (1993.376803604357 steps/sec)\n",
      "Step #14651\tEpoch   4 Batch 2150/3125   Loss: 0.795375 mae: 0.691139 (2218.7858397342306 steps/sec)\n",
      "Step #14652\tEpoch   4 Batch 2151/3125   Loss: 0.824699 mae: 0.716074 (2184.5333333333333 steps/sec)\n",
      "Step #14653\tEpoch   4 Batch 2152/3125   Loss: 0.894727 mae: 0.735105 (2159.652338681441 steps/sec)\n",
      "Step #14654\tEpoch   4 Batch 2153/3125   Loss: 0.910552 mae: 0.780980 (2001.5194029280956 steps/sec)\n",
      "Step #14655\tEpoch   4 Batch 2154/3125   Loss: 0.745130 mae: 0.686697 (2276.98855616599 steps/sec)\n",
      "Step #14656\tEpoch   4 Batch 2155/3125   Loss: 0.783297 mae: 0.698993 (1840.5434343788945 steps/sec)\n",
      "Step #14657\tEpoch   4 Batch 2156/3125   Loss: 0.841370 mae: 0.708372 (1949.4789681617476 steps/sec)\n",
      "Step #14658\tEpoch   4 Batch 2157/3125   Loss: 0.762999 mae: 0.697284 (2184.4650687999333 steps/sec)\n",
      "Step #14659\tEpoch   4 Batch 2158/3125   Loss: 0.694196 mae: 0.642371 (2181.783377201652 steps/sec)\n",
      "Step #14660\tEpoch   4 Batch 2159/3125   Loss: 0.740721 mae: 0.684115 (2072.202679735979 steps/sec)\n",
      "Step #14661\tEpoch   4 Batch 2160/3125   Loss: 0.728448 mae: 0.679286 (2034.0950533462658 steps/sec)\n",
      "Step #14662\tEpoch   4 Batch 2161/3125   Loss: 0.862208 mae: 0.742835 (2150.528107631412 steps/sec)\n",
      "Step #14663\tEpoch   4 Batch 2162/3125   Loss: 0.876236 mae: 0.727562 (2227.7659156335976 steps/sec)\n",
      "Step #14664\tEpoch   4 Batch 2163/3125   Loss: 0.898646 mae: 0.743601 (2169.7518985246343 steps/sec)\n",
      "Step #14665\tEpoch   4 Batch 2164/3125   Loss: 0.710507 mae: 0.662998 (1897.6862031833934 steps/sec)\n",
      "Step #14666\tEpoch   4 Batch 2165/3125   Loss: 0.744060 mae: 0.685311 (1808.1078750883728 steps/sec)\n",
      "Step #14667\tEpoch   4 Batch 2166/3125   Loss: 0.727832 mae: 0.685810 (2270.9722131981894 steps/sec)\n",
      "Step #14668\tEpoch   4 Batch 2167/3125   Loss: 0.795771 mae: 0.738774 (2260.0812578806135 steps/sec)\n",
      "Step #14669\tEpoch   4 Batch 2168/3125   Loss: 0.855519 mae: 0.735135 (2064.816966307623 steps/sec)\n",
      "Step #14670\tEpoch   4 Batch 2169/3125   Loss: 0.655248 mae: 0.620807 (2169.9539551968546 steps/sec)\n",
      "Step #14671\tEpoch   4 Batch 2170/3125   Loss: 0.780971 mae: 0.691870 (2263.27649471185 steps/sec)\n",
      "Step #14672\tEpoch   4 Batch 2171/3125   Loss: 0.791652 mae: 0.702660 (2087.861018467818 steps/sec)\n",
      "Step #14673\tEpoch   4 Batch 2172/3125   Loss: 0.766046 mae: 0.669761 (2260.178688824943 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14674\tEpoch   4 Batch 2173/3125   Loss: 0.754742 mae: 0.667205 (2139.885513708764 steps/sec)\n",
      "Step #14675\tEpoch   4 Batch 2174/3125   Loss: 0.710562 mae: 0.648600 (1834.184909521852 steps/sec)\n",
      "Step #14676\tEpoch   4 Batch 2175/3125   Loss: 0.774850 mae: 0.695402 (2300.013160780873 steps/sec)\n",
      "Step #14677\tEpoch   4 Batch 2176/3125   Loss: 0.748757 mae: 0.692562 (2190.694662070406 steps/sec)\n",
      "Step #14678\tEpoch   4 Batch 2177/3125   Loss: 0.835726 mae: 0.733002 (2161.321639476044 steps/sec)\n",
      "Step #14679\tEpoch   4 Batch 2178/3125   Loss: 0.745823 mae: 0.691537 (2282.266647803328 steps/sec)\n",
      "Step #14680\tEpoch   4 Batch 2179/3125   Loss: 0.820998 mae: 0.709155 (2116.603586964201 steps/sec)\n",
      "Step #14681\tEpoch   4 Batch 2180/3125   Loss: 0.851659 mae: 0.712725 (1987.5957237091516 steps/sec)\n",
      "Step #14682\tEpoch   4 Batch 2181/3125   Loss: 0.914895 mae: 0.748977 (2234.721452623503 steps/sec)\n",
      "Step #14683\tEpoch   4 Batch 2182/3125   Loss: 0.671704 mae: 0.650134 (2026.7429499197867 steps/sec)\n",
      "Step #14684\tEpoch   4 Batch 2183/3125   Loss: 0.667536 mae: 0.627655 (1934.017614238945 steps/sec)\n",
      "Step #14685\tEpoch   4 Batch 2184/3125   Loss: 0.839512 mae: 0.718847 (2099.50344385712 steps/sec)\n",
      "Step #14686\tEpoch   4 Batch 2185/3125   Loss: 0.831758 mae: 0.721650 (2106.1240886175106 steps/sec)\n",
      "Step #14687\tEpoch   4 Batch 2186/3125   Loss: 0.809028 mae: 0.706631 (1928.3447045625908 steps/sec)\n",
      "Step #14688\tEpoch   4 Batch 2187/3125   Loss: 0.831690 mae: 0.708413 (2090.900208376953 steps/sec)\n",
      "Step #14689\tEpoch   4 Batch 2188/3125   Loss: 0.736936 mae: 0.669919 (2182.7371225762136 steps/sec)\n",
      "Step #14690\tEpoch   4 Batch 2189/3125   Loss: 0.862760 mae: 0.694080 (2172.5841206696505 steps/sec)\n",
      "Step #14691\tEpoch   4 Batch 2190/3125   Loss: 0.808837 mae: 0.681297 (2019.2882452626714 steps/sec)\n",
      "Step #14692\tEpoch   4 Batch 2191/3125   Loss: 0.861674 mae: 0.723962 (1803.6758951071204 steps/sec)\n",
      "Step #14693\tEpoch   4 Batch 2192/3125   Loss: 0.785705 mae: 0.695420 (1989.9154560722657 steps/sec)\n",
      "Step #14694\tEpoch   4 Batch 2193/3125   Loss: 0.842196 mae: 0.733722 (1975.742616232512 steps/sec)\n",
      "Step #14695\tEpoch   4 Batch 2194/3125   Loss: 0.766600 mae: 0.683534 (2025.5292845000772 steps/sec)\n",
      "Step #14696\tEpoch   4 Batch 2195/3125   Loss: 0.836401 mae: 0.705098 (2198.088210632232 steps/sec)\n",
      "Step #14697\tEpoch   4 Batch 2196/3125   Loss: 0.689340 mae: 0.655732 (2130.5591677503253 steps/sec)\n",
      "Step #14698\tEpoch   4 Batch 2197/3125   Loss: 0.757573 mae: 0.671155 (2021.2539154739532 steps/sec)\n",
      "Step #14699\tEpoch   4 Batch 2198/3125   Loss: 0.787026 mae: 0.698153 (2140.890390681626 steps/sec)\n",
      "Step #14700\tEpoch   4 Batch 2199/3125   Loss: 0.717319 mae: 0.656850 (2062.278864402946 steps/sec)\n",
      "Step #14701\tEpoch   4 Batch 2200/3125   Loss: 0.752554 mae: 0.678114 (1966.5716429107276 steps/sec)\n",
      "Step #14702\tEpoch   4 Batch 2201/3125   Loss: 0.888109 mae: 0.749718 (2270.3575797598814 steps/sec)\n",
      "Step #14703\tEpoch   4 Batch 2202/3125   Loss: 0.842540 mae: 0.722527 (2245.272635782577 steps/sec)\n",
      "Step #14704\tEpoch   4 Batch 2203/3125   Loss: 0.798292 mae: 0.722788 (2072.284584980237 steps/sec)\n",
      "Step #14705\tEpoch   4 Batch 2204/3125   Loss: 0.794213 mae: 0.710848 (2037.3358203155358 steps/sec)\n",
      "Step #14706\tEpoch   4 Batch 2205/3125   Loss: 0.848127 mae: 0.705409 (2180.6263777398826 steps/sec)\n",
      "Step #14707\tEpoch   4 Batch 2206/3125   Loss: 0.649909 mae: 0.631213 (2272.8181118661337 steps/sec)\n",
      "Step #14708\tEpoch   4 Batch 2207/3125   Loss: 0.779193 mae: 0.668429 (1779.6756591620772 steps/sec)\n",
      "Step #14709\tEpoch   4 Batch 2208/3125   Loss: 0.805262 mae: 0.727743 (2174.2960228922157 steps/sec)\n",
      "Step #14710\tEpoch   4 Batch 2209/3125   Loss: 0.719849 mae: 0.654472 (2132.205457724999 steps/sec)\n",
      "Step #14711\tEpoch   4 Batch 2210/3125   Loss: 0.849709 mae: 0.728676 (2088.3808006373233 steps/sec)\n",
      "Step #14712\tEpoch   4 Batch 2211/3125   Loss: 0.700082 mae: 0.666315 (2125.8509883426254 steps/sec)\n",
      "Step #14713\tEpoch   4 Batch 2212/3125   Loss: 0.779662 mae: 0.703481 (2258.061459611947 steps/sec)\n",
      "Step #14714\tEpoch   4 Batch 2213/3125   Loss: 0.720106 mae: 0.670492 (2079.0023098351394 steps/sec)\n",
      "Step #14715\tEpoch   4 Batch 2214/3125   Loss: 0.810241 mae: 0.698835 (2011.4443559912143 steps/sec)\n",
      "Step #14716\tEpoch   4 Batch 2215/3125   Loss: 0.802866 mae: 0.707249 (2178.655502342638 steps/sec)\n",
      "Step #14717\tEpoch   4 Batch 2216/3125   Loss: 0.802579 mae: 0.701172 (1888.084412953643 steps/sec)\n",
      "Step #14718\tEpoch   4 Batch 2217/3125   Loss: 0.786126 mae: 0.722529 (2003.0487688399014 steps/sec)\n",
      "Step #14719\tEpoch   4 Batch 2218/3125   Loss: 0.857145 mae: 0.753828 (2084.0226572592665 steps/sec)\n",
      "Step #14720\tEpoch   4 Batch 2219/3125   Loss: 0.898692 mae: 0.739747 (2117.479806138934 steps/sec)\n",
      "Step #14721\tEpoch   4 Batch 2220/3125   Loss: 0.772177 mae: 0.669602 (2081.519786404105 steps/sec)\n",
      "Step #14722\tEpoch   4 Batch 2221/3125   Loss: 0.781867 mae: 0.689143 (2061.1033032265673 steps/sec)\n",
      "Step #14723\tEpoch   4 Batch 2222/3125   Loss: 0.910377 mae: 0.762791 (2046.1616516411036 steps/sec)\n",
      "Step #14724\tEpoch   4 Batch 2223/3125   Loss: 0.771822 mae: 0.685672 (2153.686264441592 steps/sec)\n",
      "Step #14725\tEpoch   4 Batch 2224/3125   Loss: 0.683807 mae: 0.630100 (1966.1844535490948 steps/sec)\n",
      "Step #14726\tEpoch   4 Batch 2225/3125   Loss: 0.674264 mae: 0.659504 (2135.8101639678175 steps/sec)\n",
      "Step #14727\tEpoch   4 Batch 2226/3125   Loss: 0.753468 mae: 0.699865 (1918.3432277421539 steps/sec)\n",
      "Step #14728\tEpoch   4 Batch 2227/3125   Loss: 0.859946 mae: 0.691532 (2178.3613096227355 steps/sec)\n",
      "Step #14729\tEpoch   4 Batch 2228/3125   Loss: 0.753718 mae: 0.688130 (2179.04033582012 steps/sec)\n",
      "Step #14730\tEpoch   4 Batch 2229/3125   Loss: 0.813072 mae: 0.713778 (2143.056265200597 steps/sec)\n",
      "Step #14731\tEpoch   4 Batch 2230/3125   Loss: 0.704664 mae: 0.663926 (2322.221730079284 steps/sec)\n",
      "Step #14732\tEpoch   4 Batch 2231/3125   Loss: 0.844860 mae: 0.715045 (2296.939826071718 steps/sec)\n",
      "Step #14733\tEpoch   4 Batch 2232/3125   Loss: 0.692679 mae: 0.660160 (2041.1629016088687 steps/sec)\n",
      "Step #14734\tEpoch   4 Batch 2233/3125   Loss: 0.722318 mae: 0.674459 (2274.592999924077 steps/sec)\n",
      "Step #14735\tEpoch   4 Batch 2234/3125   Loss: 0.780651 mae: 0.703217 (1790.585804424484 steps/sec)\n",
      "Step #14736\tEpoch   4 Batch 2235/3125   Loss: 0.902132 mae: 0.771709 (2134.288622023204 steps/sec)\n",
      "Step #14737\tEpoch   4 Batch 2236/3125   Loss: 0.752736 mae: 0.688017 (2061.6909162406605 steps/sec)\n",
      "Step #14738\tEpoch   4 Batch 2237/3125   Loss: 0.720147 mae: 0.688477 (2146.3901909811066 steps/sec)\n",
      "Step #14739\tEpoch   4 Batch 2238/3125   Loss: 0.820176 mae: 0.713671 (2181.5791116196815 steps/sec)\n",
      "Step #14740\tEpoch   4 Batch 2239/3125   Loss: 0.587034 mae: 0.610127 (2017.442833642774 steps/sec)\n",
      "Step #14741\tEpoch   4 Batch 2240/3125   Loss: 0.783260 mae: 0.700317 (2005.0596120199248 steps/sec)\n",
      "Step #14742\tEpoch   4 Batch 2241/3125   Loss: 0.855862 mae: 0.738911 (2069.360488637597 steps/sec)\n",
      "Step #14743\tEpoch   4 Batch 2242/3125   Loss: 0.697175 mae: 0.691547 (2002.8957270834528 steps/sec)\n",
      "Step #14744\tEpoch   4 Batch 2243/3125   Loss: 0.651087 mae: 0.634211 (1747.2044255971473 steps/sec)\n",
      "Step #14745\tEpoch   4 Batch 2244/3125   Loss: 0.798888 mae: 0.708120 (1870.8869341802415 steps/sec)\n",
      "Step #14746\tEpoch   4 Batch 2245/3125   Loss: 0.693148 mae: 0.649773 (2244.7919677167292 steps/sec)\n",
      "Step #14747\tEpoch   4 Batch 2246/3125   Loss: 0.709204 mae: 0.662186 (2182.7371225762136 steps/sec)\n",
      "Step #14748\tEpoch   4 Batch 2247/3125   Loss: 0.827258 mae: 0.715645 (2185.5122607001053 steps/sec)\n",
      "Step #14749\tEpoch   4 Batch 2248/3125   Loss: 0.828204 mae: 0.708239 (2073.9035413020047 steps/sec)\n",
      "Step #14750\tEpoch   4 Batch 2249/3125   Loss: 0.743244 mae: 0.681766 (2186.195753020526 steps/sec)\n",
      "Step #14751\tEpoch   4 Batch 2250/3125   Loss: 0.883856 mae: 0.739831 (1945.4461121727675 steps/sec)\n",
      "Step #14752\tEpoch   4 Batch 2251/3125   Loss: 0.809646 mae: 0.700734 (2057.5240861016814 steps/sec)\n",
      "Step #14753\tEpoch   4 Batch 2252/3125   Loss: 0.862359 mae: 0.733580 (1903.9747244566304 steps/sec)\n",
      "Step #14754\tEpoch   4 Batch 2253/3125   Loss: 0.685277 mae: 0.639693 (2038.0683971661533 steps/sec)\n",
      "Step #14755\tEpoch   4 Batch 2254/3125   Loss: 0.870106 mae: 0.740115 (2055.245543370672 steps/sec)\n",
      "Step #14756\tEpoch   4 Batch 2255/3125   Loss: 0.903898 mae: 0.747101 (2193.5589142827257 steps/sec)\n",
      "Step #14757\tEpoch   4 Batch 2256/3125   Loss: 0.694812 mae: 0.669953 (1795.9066230496514 steps/sec)\n",
      "Step #14758\tEpoch   4 Batch 2257/3125   Loss: 0.780528 mae: 0.704099 (2126.842724433086 steps/sec)\n",
      "Step #14759\tEpoch   4 Batch 2258/3125   Loss: 0.830367 mae: 0.740336 (2029.7049059744684 steps/sec)\n",
      "Step #14760\tEpoch   4 Batch 2259/3125   Loss: 0.788858 mae: 0.691137 (1954.7303469231772 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14761\tEpoch   4 Batch 2260/3125   Loss: 0.806831 mae: 0.690086 (2103.293617362699 steps/sec)\n",
      "Step #14762\tEpoch   4 Batch 2261/3125   Loss: 0.796686 mae: 0.668397 (2312.1342418028266 steps/sec)\n",
      "Step #14763\tEpoch   4 Batch 2262/3125   Loss: 0.885085 mae: 0.741464 (1946.1140858009855 steps/sec)\n",
      "Step #14764\tEpoch   4 Batch 2263/3125   Loss: 0.754261 mae: 0.695490 (2070.8930757988705 steps/sec)\n",
      "Step #14765\tEpoch   4 Batch 2264/3125   Loss: 0.719084 mae: 0.669635 (2134.1800234060956 steps/sec)\n",
      "Step #14766\tEpoch   4 Batch 2265/3125   Loss: 0.818420 mae: 0.711242 (2217.8941579593047 steps/sec)\n",
      "Step #14767\tEpoch   4 Batch 2266/3125   Loss: 0.806969 mae: 0.712155 (1746.2296829203303 steps/sec)\n",
      "Step #14768\tEpoch   4 Batch 2267/3125   Loss: 0.707987 mae: 0.669521 (1815.668857085963 steps/sec)\n",
      "Step #14769\tEpoch   4 Batch 2268/3125   Loss: 0.860287 mae: 0.728596 (1865.826794071069 steps/sec)\n",
      "Step #14770\tEpoch   4 Batch 2269/3125   Loss: 0.807323 mae: 0.705459 (1870.8869341802415 steps/sec)\n",
      "Step #14771\tEpoch   4 Batch 2270/3125   Loss: 0.849117 mae: 0.693960 (1857.4318459603564 steps/sec)\n",
      "Step #14772\tEpoch   4 Batch 2271/3125   Loss: 0.862432 mae: 0.725716 (1954.3295933201625 steps/sec)\n",
      "Step #14773\tEpoch   4 Batch 2272/3125   Loss: 0.801762 mae: 0.719325 (2021.5072005552236 steps/sec)\n",
      "Step #14774\tEpoch   4 Batch 2273/3125   Loss: 0.799188 mae: 0.705533 (1923.533835965733 steps/sec)\n",
      "Step #14775\tEpoch   4 Batch 2274/3125   Loss: 0.834731 mae: 0.714212 (2074.683181148165 steps/sec)\n",
      "Step #14776\tEpoch   4 Batch 2275/3125   Loss: 0.682176 mae: 0.652991 (2070.565933414952 steps/sec)\n",
      "Step #14777\tEpoch   4 Batch 2276/3125   Loss: 0.785495 mae: 0.690997 (1921.6831147886485 steps/sec)\n",
      "Step #14778\tEpoch   4 Batch 2277/3125   Loss: 0.756684 mae: 0.693512 (1728.4837095830346 steps/sec)\n",
      "Step #14779\tEpoch   4 Batch 2278/3125   Loss: 0.840721 mae: 0.727671 (2096.1668016032463 steps/sec)\n",
      "Step #14780\tEpoch   4 Batch 2279/3125   Loss: 0.737881 mae: 0.703104 (1898.596751706531 steps/sec)\n",
      "Step #14781\tEpoch   4 Batch 2280/3125   Loss: 0.795624 mae: 0.709957 (1869.8026908228496 steps/sec)\n",
      "Step #14782\tEpoch   4 Batch 2281/3125   Loss: 0.852597 mae: 0.737671 (1568.3276123811875 steps/sec)\n",
      "Step #14783\tEpoch   4 Batch 2282/3125   Loss: 0.792009 mae: 0.719206 (2110.256694070176 steps/sec)\n",
      "Step #14784\tEpoch   4 Batch 2283/3125   Loss: 0.718988 mae: 0.698089 (1822.8812822697007 steps/sec)\n",
      "Step #14785\tEpoch   4 Batch 2284/3125   Loss: 0.803524 mae: 0.723162 (1619.5849776425432 steps/sec)\n",
      "Step #14786\tEpoch   4 Batch 2285/3125   Loss: 0.920520 mae: 0.751087 (1869.169407381659 steps/sec)\n",
      "Step #14787\tEpoch   4 Batch 2286/3125   Loss: 0.791428 mae: 0.728948 (2027.6639562203293 steps/sec)\n",
      "Step #14788\tEpoch   4 Batch 2287/3125   Loss: 0.733114 mae: 0.686474 (2188.659869128252 steps/sec)\n",
      "Step #14789\tEpoch   4 Batch 2288/3125   Loss: 0.795363 mae: 0.681812 (2045.922110356669 steps/sec)\n",
      "Step #14790\tEpoch   4 Batch 2289/3125   Loss: 0.960269 mae: 0.749316 (2032.7740459643103 steps/sec)\n",
      "Step #14791\tEpoch   4 Batch 2290/3125   Loss: 0.715725 mae: 0.671691 (2068.3400234730207 steps/sec)\n",
      "Step #14792\tEpoch   4 Batch 2291/3125   Loss: 0.622203 mae: 0.629412 (2054.1383430955784 steps/sec)\n",
      "Step #14793\tEpoch   4 Batch 2292/3125   Loss: 0.772373 mae: 0.723547 (2115.2372787331688 steps/sec)\n",
      "Step #14794\tEpoch   4 Batch 2293/3125   Loss: 0.789750 mae: 0.707606 (1830.5506092664361 steps/sec)\n",
      "Step #14795\tEpoch   4 Batch 2294/3125   Loss: 0.707731 mae: 0.652278 (2024.3756938076162 steps/sec)\n",
      "Step #14796\tEpoch   4 Batch 2295/3125   Loss: 1.007301 mae: 0.777416 (2181.692587776333 steps/sec)\n",
      "Step #14797\tEpoch   4 Batch 2296/3125   Loss: 0.820142 mae: 0.727587 (2037.7515425351019 steps/sec)\n",
      "Step #14798\tEpoch   4 Batch 2297/3125   Loss: 0.905662 mae: 0.787745 (2122.344225962171 steps/sec)\n",
      "Step #14799\tEpoch   4 Batch 2298/3125   Loss: 0.820516 mae: 0.717172 (2070.9135256302648 steps/sec)\n",
      "Step #14800\tEpoch   4 Batch 2299/3125   Loss: 0.844640 mae: 0.707754 (1903.767316036965 steps/sec)\n",
      "Step #14801\tEpoch   4 Batch 2300/3125   Loss: 0.786243 mae: 0.695168 (2001.1374262867612 steps/sec)\n",
      "Step #14802\tEpoch   4 Batch 2301/3125   Loss: 0.840734 mae: 0.712813 (2012.4866851555078 steps/sec)\n",
      "Step #14803\tEpoch   4 Batch 2302/3125   Loss: 0.796419 mae: 0.702370 (2024.6493082708218 steps/sec)\n",
      "Step #14804\tEpoch   4 Batch 2303/3125   Loss: 0.815071 mae: 0.713489 (1864.0854021670534 steps/sec)\n",
      "Step #14805\tEpoch   4 Batch 2304/3125   Loss: 0.724072 mae: 0.668644 (2015.697657654194 steps/sec)\n",
      "Step #14806\tEpoch   4 Batch 2305/3125   Loss: 0.756694 mae: 0.683473 (1979.3229073268335 steps/sec)\n",
      "Step #14807\tEpoch   4 Batch 2306/3125   Loss: 0.740782 mae: 0.684058 (1886.7764282501125 steps/sec)\n",
      "Step #14808\tEpoch   4 Batch 2307/3125   Loss: 0.946591 mae: 0.772355 (2065.264318915938 steps/sec)\n",
      "Step #14809\tEpoch   4 Batch 2308/3125   Loss: 0.880414 mae: 0.746989 (1975.295990355 steps/sec)\n",
      "Step #14810\tEpoch   4 Batch 2309/3125   Loss: 0.866162 mae: 0.725021 (1891.2173434695958 steps/sec)\n",
      "Step #14811\tEpoch   4 Batch 2310/3125   Loss: 0.786599 mae: 0.713116 (1973.548648165401 steps/sec)\n",
      "Step #14812\tEpoch   4 Batch 2311/3125   Loss: 0.832683 mae: 0.714232 (1983.2724934273988 steps/sec)\n",
      "Step #14813\tEpoch   4 Batch 2312/3125   Loss: 0.893344 mae: 0.714050 (1793.203933304831 steps/sec)\n",
      "Step #14814\tEpoch   4 Batch 2313/3125   Loss: 0.726008 mae: 0.673117 (2089.358692078547 steps/sec)\n",
      "Step #14815\tEpoch   4 Batch 2314/3125   Loss: 0.781062 mae: 0.693031 (2008.2084478449474 steps/sec)\n",
      "Step #14816\tEpoch   4 Batch 2315/3125   Loss: 0.880130 mae: 0.740628 (2151.7416865887567 steps/sec)\n",
      "Step #14817\tEpoch   4 Batch 2316/3125   Loss: 0.948061 mae: 0.781053 (2130.5808129552684 steps/sec)\n",
      "Step #14818\tEpoch   4 Batch 2317/3125   Loss: 0.817440 mae: 0.718397 (2016.4535297398127 steps/sec)\n",
      "Step #14819\tEpoch   4 Batch 2318/3125   Loss: 0.816851 mae: 0.700926 (2098.2850739399278 steps/sec)\n",
      "Step #14820\tEpoch   4 Batch 2319/3125   Loss: 0.822523 mae: 0.712483 (2156.676264911559 steps/sec)\n",
      "Step #14821\tEpoch   4 Batch 2320/3125   Loss: 0.787163 mae: 0.708591 (1975.5564975743018 steps/sec)\n",
      "Step #14822\tEpoch   4 Batch 2321/3125   Loss: 0.744938 mae: 0.672136 (2167.688586608232 steps/sec)\n",
      "Step #14823\tEpoch   4 Batch 2322/3125   Loss: 0.778661 mae: 0.687623 (2113.6383793590003 steps/sec)\n",
      "Step #14824\tEpoch   4 Batch 2323/3125   Loss: 0.717930 mae: 0.669558 (2162.6366373798623 steps/sec)\n",
      "Step #14825\tEpoch   4 Batch 2324/3125   Loss: 0.642617 mae: 0.624318 (1973.1030135388148 steps/sec)\n",
      "Step #14826\tEpoch   4 Batch 2325/3125   Loss: 0.742535 mae: 0.681800 (1643.1754787350737 steps/sec)\n",
      "Step #14827\tEpoch   4 Batch 2326/3125   Loss: 0.713143 mae: 0.665623 (2066.627905830878 steps/sec)\n",
      "Step #14828\tEpoch   4 Batch 2327/3125   Loss: 0.792945 mae: 0.681316 (2188.7740831193773 steps/sec)\n",
      "Step #14829\tEpoch   4 Batch 2328/3125   Loss: 0.835742 mae: 0.716989 (2105.3417795223418 steps/sec)\n",
      "Step #14830\tEpoch   4 Batch 2329/3125   Loss: 0.779279 mae: 0.698333 (2161.588967109535 steps/sec)\n",
      "Step #14831\tEpoch   4 Batch 2330/3125   Loss: 0.884379 mae: 0.757630 (1932.9658782974175 steps/sec)\n",
      "Step #14832\tEpoch   4 Batch 2331/3125   Loss: 0.689931 mae: 0.670420 (2042.8330687031823 steps/sec)\n",
      "Step #14833\tEpoch   4 Batch 2332/3125   Loss: 0.805946 mae: 0.699945 (2103.5678820402227 steps/sec)\n",
      "Step #14834\tEpoch   4 Batch 2333/3125   Loss: 0.711788 mae: 0.667359 (2106.8646460181435 steps/sec)\n",
      "Step #14835\tEpoch   4 Batch 2334/3125   Loss: 0.727239 mae: 0.664219 (1716.2619790004337 steps/sec)\n",
      "Step #14836\tEpoch   4 Batch 2335/3125   Loss: 0.820794 mae: 0.725471 (2117.0309203420115 steps/sec)\n",
      "Step #14837\tEpoch   4 Batch 2336/3125   Loss: 0.686374 mae: 0.645675 (2070.116281365368 steps/sec)\n",
      "Step #14838\tEpoch   4 Batch 2337/3125   Loss: 0.853234 mae: 0.726850 (2229.1630349284637 steps/sec)\n",
      "Step #14839\tEpoch   4 Batch 2338/3125   Loss: 0.785703 mae: 0.699036 (2252.5558264680294 steps/sec)\n",
      "Step #14840\tEpoch   4 Batch 2339/3125   Loss: 0.691667 mae: 0.653430 (2070.054980307771 steps/sec)\n",
      "Step #14841\tEpoch   4 Batch 2340/3125   Loss: 0.772540 mae: 0.688132 (2240.116216966823 steps/sec)\n",
      "Step #14842\tEpoch   4 Batch 2341/3125   Loss: 0.748290 mae: 0.705177 (2111.574050766737 steps/sec)\n",
      "Step #14843\tEpoch   4 Batch 2342/3125   Loss: 0.770204 mae: 0.698764 (1970.9149006155726 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14844\tEpoch   4 Batch 2343/3125   Loss: 0.698349 mae: 0.655415 (1924.999311566599 steps/sec)\n",
      "Step #14845\tEpoch   4 Batch 2344/3125   Loss: 0.853513 mae: 0.737624 (1829.4806814910453 steps/sec)\n",
      "Step #14846\tEpoch   4 Batch 2345/3125   Loss: 0.815696 mae: 0.678573 (2185.147906181947 steps/sec)\n",
      "Step #14847\tEpoch   4 Batch 2346/3125   Loss: 0.679561 mae: 0.646003 (2054.3999373046895 steps/sec)\n",
      "Step #14848\tEpoch   4 Batch 2347/3125   Loss: 0.867972 mae: 0.725581 (2201.1797552322773 steps/sec)\n",
      "Step #14849\tEpoch   4 Batch 2348/3125   Loss: 0.980708 mae: 0.768364 (2208.481555197506 steps/sec)\n",
      "Step #14850\tEpoch   4 Batch 2349/3125   Loss: 0.776528 mae: 0.687185 (2104.7290244881574 steps/sec)\n",
      "Step #14851\tEpoch   4 Batch 2350/3125   Loss: 0.873994 mae: 0.738005 (2046.5809838881244 steps/sec)\n",
      "Step #14852\tEpoch   4 Batch 2351/3125   Loss: 0.768267 mae: 0.691208 (2016.7056130936926 steps/sec)\n",
      "Step #14853\tEpoch   4 Batch 2352/3125   Loss: 0.782956 mae: 0.701673 (1665.8871377732587 steps/sec)\n",
      "Step #14854\tEpoch   4 Batch 2353/3125   Loss: 0.807777 mae: 0.683723 (2056.1724824252647 steps/sec)\n",
      "Step #14855\tEpoch   4 Batch 2354/3125   Loss: 0.777420 mae: 0.687229 (2086.469277300223 steps/sec)\n",
      "Step #14856\tEpoch   4 Batch 2355/3125   Loss: 0.826933 mae: 0.734233 (2107.49982413651 steps/sec)\n",
      "Step #14857\tEpoch   4 Batch 2356/3125   Loss: 0.834376 mae: 0.748048 (2144.7219324620073 steps/sec)\n",
      "Step #14858\tEpoch   4 Batch 2357/3125   Loss: 0.731272 mae: 0.680658 (2033.0302266514143 steps/sec)\n",
      "Step #14859\tEpoch   4 Batch 2358/3125   Loss: 0.739318 mae: 0.689685 (1953.8015782069556 steps/sec)\n",
      "Step #14860\tEpoch   4 Batch 2359/3125   Loss: 0.809027 mae: 0.693267 (2257.8426622740435 steps/sec)\n",
      "Step #14861\tEpoch   4 Batch 2360/3125   Loss: 0.758083 mae: 0.687634 (2005.5198003232315 steps/sec)\n",
      "Step #14862\tEpoch   4 Batch 2361/3125   Loss: 0.680902 mae: 0.646532 (2040.825223822499 steps/sec)\n",
      "Step #14863\tEpoch   4 Batch 2362/3125   Loss: 0.861739 mae: 0.738788 (1830.2949903997207 steps/sec)\n",
      "Step #14864\tEpoch   4 Batch 2363/3125   Loss: 0.704461 mae: 0.674418 (1909.0530072005315 steps/sec)\n",
      "Step #14865\tEpoch   4 Batch 2364/3125   Loss: 0.810610 mae: 0.703058 (2038.325914118539 steps/sec)\n",
      "Step #14866\tEpoch   4 Batch 2365/3125   Loss: 0.627872 mae: 0.611406 (1943.9313323816764 steps/sec)\n",
      "Step #14867\tEpoch   4 Batch 2366/3125   Loss: 0.754973 mae: 0.698857 (1839.5747443027315 steps/sec)\n",
      "Step #14868\tEpoch   4 Batch 2367/3125   Loss: 0.850426 mae: 0.717966 (2056.5959282940416 steps/sec)\n",
      "Step #14869\tEpoch   4 Batch 2368/3125   Loss: 0.871567 mae: 0.735659 (1984.9055889451517 steps/sec)\n",
      "Step #14870\tEpoch   4 Batch 2369/3125   Loss: 0.774719 mae: 0.710879 (1933.2688035251713 steps/sec)\n",
      "Step #14871\tEpoch   4 Batch 2370/3125   Loss: 0.817142 mae: 0.706986 (1918.3783239875959 steps/sec)\n",
      "Step #14872\tEpoch   4 Batch 2371/3125   Loss: 0.795365 mae: 0.704406 (1944.7605623353982 steps/sec)\n",
      "Step #14873\tEpoch   4 Batch 2372/3125   Loss: 0.753077 mae: 0.699408 (2162.8373709559314 steps/sec)\n",
      "Step #14874\tEpoch   4 Batch 2373/3125   Loss: 0.712357 mae: 0.673563 (2021.5851471977483 steps/sec)\n",
      "Step #14875\tEpoch   4 Batch 2374/3125   Loss: 0.743727 mae: 0.684654 (2128.2240714430686 steps/sec)\n",
      "Step #14876\tEpoch   4 Batch 2375/3125   Loss: 0.824320 mae: 0.719392 (2067.7486146989804 steps/sec)\n",
      "Step #14877\tEpoch   4 Batch 2376/3125   Loss: 0.777553 mae: 0.685189 (2219.654745398546 steps/sec)\n",
      "Step #14878\tEpoch   4 Batch 2377/3125   Loss: 0.734538 mae: 0.666278 (1873.6448998918956 steps/sec)\n",
      "Step #14879\tEpoch   4 Batch 2378/3125   Loss: 0.900133 mae: 0.756284 (1878.2764457739604 steps/sec)\n",
      "Step #14880\tEpoch   4 Batch 2379/3125   Loss: 0.851473 mae: 0.742095 (2092.3187437020483 steps/sec)\n",
      "Step #14881\tEpoch   4 Batch 2380/3125   Loss: 0.792901 mae: 0.712702 (2069.952819945911 steps/sec)\n",
      "Step #14882\tEpoch   4 Batch 2381/3125   Loss: 0.741647 mae: 0.684621 (2052.2889632630695 steps/sec)\n",
      "Step #14883\tEpoch   4 Batch 2382/3125   Loss: 0.759334 mae: 0.688815 (1927.0688989763476 steps/sec)\n",
      "Step #14884\tEpoch   4 Batch 2383/3125   Loss: 0.764426 mae: 0.692961 (2033.9175047764986 steps/sec)\n",
      "Step #14885\tEpoch   4 Batch 2384/3125   Loss: 0.870420 mae: 0.750105 (2128.9370298557465 steps/sec)\n",
      "Step #14886\tEpoch   4 Batch 2385/3125   Loss: 0.827602 mae: 0.714159 (1766.2904692921875 steps/sec)\n",
      "Step #14887\tEpoch   4 Batch 2386/3125   Loss: 0.763810 mae: 0.675904 (2057.1002579771844 steps/sec)\n",
      "Step #14888\tEpoch   4 Batch 2387/3125   Loss: 0.792383 mae: 0.719791 (1915.1023688199734 steps/sec)\n",
      "Step #14889\tEpoch   4 Batch 2388/3125   Loss: 0.778940 mae: 0.679634 (2088.6719917136425 steps/sec)\n",
      "Step #14890\tEpoch   4 Batch 2389/3125   Loss: 0.786591 mae: 0.727482 (2116.069662785301 steps/sec)\n",
      "Step #14891\tEpoch   4 Batch 2390/3125   Loss: 0.826334 mae: 0.726399 (2068.15644661841 steps/sec)\n",
      "Step #14892\tEpoch   4 Batch 2391/3125   Loss: 0.822668 mae: 0.708769 (1974.2732339207712 steps/sec)\n",
      "Step #14893\tEpoch   4 Batch 2392/3125   Loss: 0.765280 mae: 0.689911 (2050.2023658226612 steps/sec)\n",
      "Step #14894\tEpoch   4 Batch 2393/3125   Loss: 0.785017 mae: 0.692291 (1829.3530125001091 steps/sec)\n",
      "Step #14895\tEpoch   4 Batch 2394/3125   Loss: 0.729640 mae: 0.658700 (1838.8972677212305 steps/sec)\n",
      "Step #14896\tEpoch   4 Batch 2395/3125   Loss: 0.749820 mae: 0.704401 (2083.5050419750632 steps/sec)\n",
      "Step #14897\tEpoch   4 Batch 2396/3125   Loss: 0.861749 mae: 0.746852 (1932.5027644673794 steps/sec)\n",
      "Step #14898\tEpoch   4 Batch 2397/3125   Loss: 0.690426 mae: 0.655962 (2059.8279181236003 steps/sec)\n",
      "Step #14899\tEpoch   4 Batch 2398/3125   Loss: 0.724348 mae: 0.678280 (2033.2864719170843 steps/sec)\n",
      "Step #14900\tEpoch   4 Batch 2399/3125   Loss: 0.704845 mae: 0.661115 (2139.2088459106026 steps/sec)\n",
      "Step #14901\tEpoch   4 Batch 2400/3125   Loss: 0.831587 mae: 0.707590 (2259.399476400306 steps/sec)\n",
      "Step #14902\tEpoch   4 Batch 2401/3125   Loss: 0.798419 mae: 0.682866 (2194.982363961776 steps/sec)\n",
      "Step #14903\tEpoch   4 Batch 2402/3125   Loss: 0.719514 mae: 0.670482 (1820.0968565030985 steps/sec)\n",
      "Step #14904\tEpoch   4 Batch 2403/3125   Loss: 0.776395 mae: 0.702582 (2035.5955893772325 steps/sec)\n",
      "Step #14905\tEpoch   4 Batch 2404/3125   Loss: 0.814531 mae: 0.706548 (2093.363012946567 steps/sec)\n",
      "Step #14906\tEpoch   4 Batch 2405/3125   Loss: 0.782743 mae: 0.691737 (2235.1263495582293 steps/sec)\n",
      "Step #14907\tEpoch   4 Batch 2406/3125   Loss: 0.796810 mae: 0.699879 (2240.7624664764776 steps/sec)\n",
      "Step #14908\tEpoch   4 Batch 2407/3125   Loss: 0.688443 mae: 0.683207 (2100.2393518472154 steps/sec)\n",
      "Step #14909\tEpoch   4 Batch 2408/3125   Loss: 0.900379 mae: 0.750274 (2110.9364147885694 steps/sec)\n",
      "Step #14910\tEpoch   4 Batch 2409/3125   Loss: 0.674942 mae: 0.661614 (2197.5123908920395 steps/sec)\n",
      "Step #14911\tEpoch   4 Batch 2410/3125   Loss: 0.839074 mae: 0.705691 (2151.0795646866954 steps/sec)\n",
      "Step #14912\tEpoch   4 Batch 2411/3125   Loss: 0.862223 mae: 0.740465 (1947.8855317054142 steps/sec)\n",
      "Step #14913\tEpoch   4 Batch 2412/3125   Loss: 0.858122 mae: 0.759139 (2026.1946629050647 steps/sec)\n",
      "Step #14914\tEpoch   4 Batch 2413/3125   Loss: 0.802667 mae: 0.705596 (2164.8914535825998 steps/sec)\n",
      "Step #14915\tEpoch   4 Batch 2414/3125   Loss: 0.784509 mae: 0.706225 (2272.424068395332 steps/sec)\n",
      "Step #14916\tEpoch   4 Batch 2415/3125   Loss: 0.812528 mae: 0.717957 (2310.9622250628113 steps/sec)\n",
      "Step #14917\tEpoch   4 Batch 2416/3125   Loss: 0.766717 mae: 0.690790 (2113.0207861036383 steps/sec)\n",
      "Step #14918\tEpoch   4 Batch 2417/3125   Loss: 0.733388 mae: 0.675603 (1965.0790378650875 steps/sec)\n",
      "Step #14919\tEpoch   4 Batch 2418/3125   Loss: 0.697712 mae: 0.645741 (2048.500122100122 steps/sec)\n",
      "Step #14920\tEpoch   4 Batch 2419/3125   Loss: 0.736969 mae: 0.685454 (2107.2033600273303 steps/sec)\n",
      "Step #14921\tEpoch   4 Batch 2420/3125   Loss: 0.727417 mae: 0.662079 (1665.0538701558542 steps/sec)\n",
      "Step #14922\tEpoch   4 Batch 2421/3125   Loss: 0.701105 mae: 0.660721 (2039.1583368986037 steps/sec)\n",
      "Step #14923\tEpoch   4 Batch 2422/3125   Loss: 0.771814 mae: 0.683567 (2106.9704824482087 steps/sec)\n",
      "Step #14924\tEpoch   4 Batch 2423/3125   Loss: 0.671688 mae: 0.632098 (2212.5123963454516 steps/sec)\n",
      "Step #14925\tEpoch   4 Batch 2424/3125   Loss: 0.749716 mae: 0.686077 (2290.8677794284717 steps/sec)\n",
      "Step #14926\tEpoch   4 Batch 2425/3125   Loss: 0.697829 mae: 0.664130 (2091.3380801372186 steps/sec)\n",
      "Step #14927\tEpoch   4 Batch 2426/3125   Loss: 0.938691 mae: 0.771747 (2144.45876025114 steps/sec)\n",
      "Step #14928\tEpoch   4 Batch 2427/3125   Loss: 0.838245 mae: 0.710526 (2054.5609514759044 steps/sec)\n",
      "Step #14929\tEpoch   4 Batch 2428/3125   Loss: 0.733173 mae: 0.667160 (1986.1650945183164 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #14930\tEpoch   4 Batch 2429/3125   Loss: 0.842894 mae: 0.694292 (1874.98502445261 steps/sec)\n",
      "Step #14931\tEpoch   4 Batch 2430/3125   Loss: 0.762079 mae: 0.696765 (1981.136638452237 steps/sec)\n",
      "Step #14932\tEpoch   4 Batch 2431/3125   Loss: 0.756840 mae: 0.704848 (2214.451495728752 steps/sec)\n",
      "Step #14933\tEpoch   4 Batch 2432/3125   Loss: 0.801412 mae: 0.705087 (2321.450552369988 steps/sec)\n",
      "Step #14934\tEpoch   4 Batch 2433/3125   Loss: 0.658411 mae: 0.644348 (2222.6188331301996 steps/sec)\n",
      "Step #14935\tEpoch   4 Batch 2434/3125   Loss: 0.917179 mae: 0.739930 (2017.442833642774 steps/sec)\n",
      "Step #14936\tEpoch   4 Batch 2435/3125   Loss: 0.731981 mae: 0.651636 (2089.421141775431 steps/sec)\n",
      "Step #14937\tEpoch   4 Batch 2436/3125   Loss: 0.768059 mae: 0.691891 (2304.2840975266727 steps/sec)\n",
      "Step #14938\tEpoch   4 Batch 2437/3125   Loss: 0.730751 mae: 0.672464 (1960.99978493216 steps/sec)\n",
      "Step #14939\tEpoch   4 Batch 2438/3125   Loss: 0.772317 mae: 0.709473 (1776.7505697559156 steps/sec)\n",
      "Step #14940\tEpoch   4 Batch 2439/3125   Loss: 0.761349 mae: 0.684965 (2128.0297111081795 steps/sec)\n",
      "Step #14941\tEpoch   4 Batch 2440/3125   Loss: 0.743863 mae: 0.705009 (2349.618508767016 steps/sec)\n",
      "Step #14942\tEpoch   4 Batch 2441/3125   Loss: 0.712902 mae: 0.666473 (2215.317009274713 steps/sec)\n",
      "Step #14943\tEpoch   4 Batch 2442/3125   Loss: 0.709649 mae: 0.649276 (2141.043389484431 steps/sec)\n",
      "Step #14944\tEpoch   4 Batch 2443/3125   Loss: 0.753088 mae: 0.687001 (1992.7328012162675 steps/sec)\n",
      "Step #14945\tEpoch   4 Batch 2444/3125   Loss: 0.742139 mae: 0.687924 (1986.0898552920676 steps/sec)\n",
      "Step #14946\tEpoch   4 Batch 2445/3125   Loss: 0.867274 mae: 0.743118 (2045.4830969705245 steps/sec)\n",
      "Step #14947\tEpoch   4 Batch 2446/3125   Loss: 0.710095 mae: 0.660450 (1945.3919722451553 steps/sec)\n",
      "Step #14948\tEpoch   4 Batch 2447/3125   Loss: 0.730523 mae: 0.675165 (1689.1810039306656 steps/sec)\n",
      "Step #14949\tEpoch   4 Batch 2448/3125   Loss: 0.718947 mae: 0.680090 (2087.944166226939 steps/sec)\n",
      "Step #14950\tEpoch   4 Batch 2449/3125   Loss: 0.764329 mae: 0.695506 (2000.6410745630772 steps/sec)\n",
      "Step #14951\tEpoch   4 Batch 2450/3125   Loss: 0.845377 mae: 0.734049 (2028.8998103788554 steps/sec)\n",
      "Step #14952\tEpoch   4 Batch 2451/3125   Loss: 0.784387 mae: 0.700093 (2122.7094214340664 steps/sec)\n",
      "Step #14953\tEpoch   4 Batch 2452/3125   Loss: 0.833775 mae: 0.725880 (2050.783778762187 steps/sec)\n",
      "Step #14954\tEpoch   4 Batch 2453/3125   Loss: 0.845246 mae: 0.717217 (2017.7534035695387 steps/sec)\n",
      "Step #14955\tEpoch   4 Batch 2454/3125   Loss: 0.698483 mae: 0.635699 (1968.4359718037526 steps/sec)\n",
      "Step #14956\tEpoch   4 Batch 2455/3125   Loss: 0.856745 mae: 0.717143 (1930.1194617777533 steps/sec)\n",
      "Step #14957\tEpoch   4 Batch 2456/3125   Loss: 0.717774 mae: 0.702028 (1937.1439128025124 steps/sec)\n",
      "Step #14958\tEpoch   4 Batch 2457/3125   Loss: 0.704483 mae: 0.650522 (2159.808030978692 steps/sec)\n",
      "Step #14959\tEpoch   4 Batch 2458/3125   Loss: 0.781612 mae: 0.707600 (2253.451388291928 steps/sec)\n",
      "Step #14960\tEpoch   4 Batch 2459/3125   Loss: 0.751665 mae: 0.693348 (2053.816472431691 steps/sec)\n",
      "Step #14961\tEpoch   4 Batch 2460/3125   Loss: 0.800993 mae: 0.713549 (2265.721694036301 steps/sec)\n",
      "Step #14962\tEpoch   4 Batch 2461/3125   Loss: 0.684501 mae: 0.646983 (1969.6006611818625 steps/sec)\n",
      "Step #14963\tEpoch   4 Batch 2462/3125   Loss: 0.837689 mae: 0.719166 (2154.9698408294544 steps/sec)\n",
      "Step #14964\tEpoch   4 Batch 2463/3125   Loss: 0.772387 mae: 0.683340 (1997.19251464216 steps/sec)\n",
      "Step #14965\tEpoch   4 Batch 2464/3125   Loss: 0.814540 mae: 0.699481 (1867.5548114770156 steps/sec)\n",
      "Step #14966\tEpoch   4 Batch 2465/3125   Loss: 0.680812 mae: 0.671246 (2190.8548624677455 steps/sec)\n",
      "Step #14967\tEpoch   4 Batch 2466/3125   Loss: 0.725002 mae: 0.649360 (2093.091402678803 steps/sec)\n",
      "Step #14968\tEpoch   4 Batch 2467/3125   Loss: 0.725198 mae: 0.671302 (2143.340998518064 steps/sec)\n",
      "Step #14969\tEpoch   4 Batch 2468/3125   Loss: 0.984572 mae: 0.796260 (2256.5307681547715 steps/sec)\n",
      "Step #14970\tEpoch   4 Batch 2469/3125   Loss: 0.779509 mae: 0.677848 (2040.5869302922974 steps/sec)\n",
      "Step #14971\tEpoch   4 Batch 2470/3125   Loss: 0.849729 mae: 0.740602 (2137.6606696906374 steps/sec)\n",
      "Step #14972\tEpoch   4 Batch 2471/3125   Loss: 0.806871 mae: 0.704222 (2093.174967561633 steps/sec)\n",
      "Step #14973\tEpoch   4 Batch 2472/3125   Loss: 0.735826 mae: 0.658481 (2101.186277653094 steps/sec)\n",
      "Step #14974\tEpoch   4 Batch 2473/3125   Loss: 0.801953 mae: 0.706101 (2090.4833580877003 steps/sec)\n",
      "Step #14975\tEpoch   4 Batch 2474/3125   Loss: 0.860210 mae: 0.712905 (1964.802878129216 steps/sec)\n",
      "Step #14976\tEpoch   4 Batch 2475/3125   Loss: 0.815221 mae: 0.722355 (2122.7738807405385 steps/sec)\n",
      "Step #14977\tEpoch   4 Batch 2476/3125   Loss: 0.781211 mae: 0.705078 (2076.490915391851 steps/sec)\n",
      "Step #14978\tEpoch   4 Batch 2477/3125   Loss: 0.814299 mae: 0.705991 (2162.9935228351005 steps/sec)\n",
      "Step #14979\tEpoch   4 Batch 2478/3125   Loss: 0.759258 mae: 0.703641 (2304.5626373626374 steps/sec)\n",
      "Step #14980\tEpoch   4 Batch 2479/3125   Loss: 0.791424 mae: 0.705921 (2189.8254114109095 steps/sec)\n",
      "Step #14981\tEpoch   4 Batch 2480/3125   Loss: 0.857273 mae: 0.712754 (2154.0402017276265 steps/sec)\n",
      "Step #14982\tEpoch   4 Batch 2481/3125   Loss: 0.748581 mae: 0.683178 (2079.6001745269923 steps/sec)\n",
      "Step #14983\tEpoch   4 Batch 2482/3125   Loss: 0.845863 mae: 0.721789 (1836.4174503931768 steps/sec)\n",
      "Step #14984\tEpoch   4 Batch 2483/3125   Loss: 0.856706 mae: 0.749165 (2066.8112114164073 steps/sec)\n",
      "Step #14985\tEpoch   4 Batch 2484/3125   Loss: 0.729644 mae: 0.687776 (2084.62341328615 steps/sec)\n",
      "Step #14986\tEpoch   4 Batch 2485/3125   Loss: 0.732005 mae: 0.670873 (2133.9194318100876 steps/sec)\n",
      "Step #14987\tEpoch   4 Batch 2486/3125   Loss: 0.806989 mae: 0.677527 (2195.304043798218 steps/sec)\n",
      "Step #14988\tEpoch   4 Batch 2487/3125   Loss: 0.732127 mae: 0.657413 (2030.9432500484215 steps/sec)\n",
      "Step #14989\tEpoch   4 Batch 2488/3125   Loss: 0.778424 mae: 0.700683 (2178.8365835160153 steps/sec)\n",
      "Step #14990\tEpoch   4 Batch 2489/3125   Loss: 0.848019 mae: 0.733845 (1860.7279115575036 steps/sec)\n",
      "Step #14991\tEpoch   4 Batch 2490/3125   Loss: 0.873632 mae: 0.730880 (2058.9785379072005 steps/sec)\n",
      "Step #14992\tEpoch   4 Batch 2491/3125   Loss: 0.829576 mae: 0.712838 (2006.0185760883082 steps/sec)\n",
      "Step #14993\tEpoch   4 Batch 2492/3125   Loss: 0.901008 mae: 0.742110 (1919.8359515178147 steps/sec)\n",
      "Step #14994\tEpoch   4 Batch 2493/3125   Loss: 0.824602 mae: 0.710602 (2196.4306661080855 steps/sec)\n",
      "Step #14995\tEpoch   4 Batch 2494/3125   Loss: 0.750056 mae: 0.680286 (2296.738582849633 steps/sec)\n",
      "Step #14996\tEpoch   4 Batch 2495/3125   Loss: 0.818313 mae: 0.707266 (2155.656517895689 steps/sec)\n",
      "Step #14997\tEpoch   4 Batch 2496/3125   Loss: 0.791716 mae: 0.716036 (2147.951042146771 steps/sec)\n",
      "Step #14998\tEpoch   4 Batch 2497/3125   Loss: 0.680218 mae: 0.664844 (2334.266823979876 steps/sec)\n",
      "Step #14999\tEpoch   4 Batch 2498/3125   Loss: 0.828672 mae: 0.707277 (2215.223407626492 steps/sec)\n",
      "Step #15000\tEpoch   4 Batch 2499/3125   Loss: 0.886165 mae: 0.753618 (1805.7103495780955 steps/sec)\n",
      "Step #15001\tEpoch   4 Batch 2500/3125   Loss: 0.708636 mae: 0.683931 (2082.6980753570224 steps/sec)\n",
      "Step #15002\tEpoch   4 Batch 2501/3125   Loss: 0.829920 mae: 0.700742 (1867.9041265486806 steps/sec)\n",
      "Step #15003\tEpoch   4 Batch 2502/3125   Loss: 0.697973 mae: 0.661782 (2074.6010861930813 steps/sec)\n",
      "Step #15004\tEpoch   4 Batch 2503/3125   Loss: 0.816411 mae: 0.726191 (2121.5927484622857 steps/sec)\n",
      "Step #15005\tEpoch   4 Batch 2504/3125   Loss: 0.692228 mae: 0.653713 (2183.8053982006 steps/sec)\n",
      "Step #15006\tEpoch   4 Batch 2505/3125   Loss: 0.741277 mae: 0.665784 (2045.3235024479684 steps/sec)\n",
      "Step #15007\tEpoch   4 Batch 2506/3125   Loss: 0.900347 mae: 0.739301 (2131.3819948370838 steps/sec)\n",
      "Step #15008\tEpoch   4 Batch 2507/3125   Loss: 0.619688 mae: 0.618988 (2084.105499572675 steps/sec)\n",
      "Step #15009\tEpoch   4 Batch 2508/3125   Loss: 0.818534 mae: 0.705249 (1700.673894885373 steps/sec)\n",
      "Step #15010\tEpoch   4 Batch 2509/3125   Loss: 0.759336 mae: 0.703770 (2069.0950708394175 steps/sec)\n",
      "Step #15011\tEpoch   4 Batch 2510/3125   Loss: 0.858002 mae: 0.736617 (2164.5116010238626 steps/sec)\n",
      "Step #15012\tEpoch   4 Batch 2511/3125   Loss: 0.702299 mae: 0.659505 (2173.146948799519 steps/sec)\n",
      "Step #15013\tEpoch   4 Batch 2512/3125   Loss: 0.837804 mae: 0.723132 (2203.515702982989 steps/sec)\n",
      "Step #15014\tEpoch   4 Batch 2513/3125   Loss: 0.861668 mae: 0.726892 (2193.168936018908 steps/sec)\n",
      "Step #15015\tEpoch   4 Batch 2514/3125   Loss: 0.790789 mae: 0.702419 (2143.450531479967 steps/sec)\n",
      "Step #15016\tEpoch   4 Batch 2515/3125   Loss: 0.819717 mae: 0.714159 (2165.159665080168 steps/sec)\n",
      "Step #15017\tEpoch   4 Batch 2516/3125   Loss: 0.680629 mae: 0.662842 (1946.3488881464157 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15018\tEpoch   4 Batch 2517/3125   Loss: 0.683003 mae: 0.665549 (1858.5182559376108 steps/sec)\n",
      "Step #15019\tEpoch   4 Batch 2518/3125   Loss: 0.839289 mae: 0.718657 (1957.339256880991 steps/sec)\n",
      "Step #15020\tEpoch   4 Batch 2519/3125   Loss: 0.772754 mae: 0.693373 (2162.926597840324 steps/sec)\n",
      "Step #15021\tEpoch   4 Batch 2520/3125   Loss: 0.829884 mae: 0.735419 (2236.0319440446106 steps/sec)\n",
      "Step #15022\tEpoch   4 Batch 2521/3125   Loss: 0.909499 mae: 0.754061 (2036.5048845384451 steps/sec)\n",
      "Step #15023\tEpoch   4 Batch 2522/3125   Loss: 0.693170 mae: 0.649867 (2085.4318728744456 steps/sec)\n",
      "Step #15024\tEpoch   4 Batch 2523/3125   Loss: 0.825243 mae: 0.727797 (2157.0533721444513 steps/sec)\n",
      "Step #15025\tEpoch   4 Batch 2524/3125   Loss: 0.745119 mae: 0.697725 (2132.5740550544547 steps/sec)\n",
      "Step #15026\tEpoch   4 Batch 2525/3125   Loss: 0.772441 mae: 0.698418 (2249.825133563628 steps/sec)\n",
      "Step #15027\tEpoch   4 Batch 2526/3125   Loss: 0.929365 mae: 0.724523 (1718.3854738532636 steps/sec)\n",
      "Step #15028\tEpoch   4 Batch 2527/3125   Loss: 0.736064 mae: 0.672832 (2082.284488750323 steps/sec)\n",
      "Step #15029\tEpoch   4 Batch 2528/3125   Loss: 0.778021 mae: 0.690379 (2098.999119224918 steps/sec)\n",
      "Step #15030\tEpoch   4 Batch 2529/3125   Loss: 0.835014 mae: 0.723190 (2175.9203154181364 steps/sec)\n",
      "Step #15031\tEpoch   4 Batch 2530/3125   Loss: 0.689810 mae: 0.654298 (2232.0575588573374 steps/sec)\n",
      "Step #15032\tEpoch   4 Batch 2531/3125   Loss: 0.841124 mae: 0.730429 (2086.718407960199 steps/sec)\n",
      "Step #15033\tEpoch   4 Batch 2532/3125   Loss: 0.808400 mae: 0.712618 (2037.6525456665372 steps/sec)\n",
      "Step #15034\tEpoch   4 Batch 2533/3125   Loss: 0.831131 mae: 0.717017 (2126.1958351075696 steps/sec)\n",
      "Step #15035\tEpoch   4 Batch 2534/3125   Loss: 0.917534 mae: 0.743250 (2026.4100259925983 steps/sec)\n",
      "Step #15036\tEpoch   4 Batch 2535/3125   Loss: 0.939434 mae: 0.754451 (1912.6222092514226 steps/sec)\n",
      "Step #15037\tEpoch   4 Batch 2536/3125   Loss: 0.754364 mae: 0.710074 (1893.214891849926 steps/sec)\n",
      "Step #15038\tEpoch   4 Batch 2537/3125   Loss: 0.809839 mae: 0.714223 (1975.8356887130205 steps/sec)\n",
      "Step #15039\tEpoch   4 Batch 2538/3125   Loss: 0.779104 mae: 0.717883 (2194.7296816459802 steps/sec)\n",
      "Step #15040\tEpoch   4 Batch 2539/3125   Loss: 0.853902 mae: 0.740235 (2251.9511199879735 steps/sec)\n",
      "Step #15041\tEpoch   4 Batch 2540/3125   Loss: 0.721558 mae: 0.692785 (1993.2252361852986 steps/sec)\n",
      "Step #15042\tEpoch   4 Batch 2541/3125   Loss: 0.754364 mae: 0.688524 (1944.9228857336288 steps/sec)\n",
      "Step #15043\tEpoch   4 Batch 2542/3125   Loss: 0.941852 mae: 0.756147 (2116.9454398627163 steps/sec)\n",
      "Step #15044\tEpoch   4 Batch 2543/3125   Loss: 0.737184 mae: 0.685825 (2095.8316260755723 steps/sec)\n",
      "Step #15045\tEpoch   4 Batch 2544/3125   Loss: 0.891377 mae: 0.747468 (2052.7108109430824 steps/sec)\n",
      "Step #15046\tEpoch   4 Batch 2545/3125   Loss: 0.858021 mae: 0.743889 (1918.8522490209714 steps/sec)\n",
      "Step #15047\tEpoch   4 Batch 2546/3125   Loss: 0.726785 mae: 0.689854 (2295.029438157981 steps/sec)\n",
      "Step #15048\tEpoch   4 Batch 2547/3125   Loss: 0.990637 mae: 0.777904 (2083.422247389702 steps/sec)\n",
      "Step #15049\tEpoch   4 Batch 2548/3125   Loss: 0.839035 mae: 0.710369 (2122.322747788775 steps/sec)\n",
      "Step #15050\tEpoch   4 Batch 2549/3125   Loss: 0.596307 mae: 0.629140 (2086.5523142436423 steps/sec)\n",
      "Step #15051\tEpoch   4 Batch 2550/3125   Loss: 0.875584 mae: 0.737009 (2275.2099290472365 steps/sec)\n",
      "Step #15052\tEpoch   4 Batch 2551/3125   Loss: 0.733686 mae: 0.669801 (2102.134057716789 steps/sec)\n",
      "Step #15053\tEpoch   4 Batch 2552/3125   Loss: 0.754969 mae: 0.673171 (1952.2010705143123 steps/sec)\n",
      "Step #15054\tEpoch   4 Batch 2553/3125   Loss: 0.846262 mae: 0.733597 (2117.2019020120542 steps/sec)\n",
      "Step #15055\tEpoch   4 Batch 2554/3125   Loss: 0.812913 mae: 0.698255 (2144.5464771449024 steps/sec)\n",
      "Step #15056\tEpoch   4 Batch 2555/3125   Loss: 0.694906 mae: 0.649924 (2111.1064133925247 steps/sec)\n",
      "Step #15057\tEpoch   4 Batch 2556/3125   Loss: 0.957410 mae: 0.773438 (2058.0288711592625 steps/sec)\n",
      "Step #15058\tEpoch   4 Batch 2557/3125   Loss: 0.883430 mae: 0.744248 (2124.3005611717754 steps/sec)\n",
      "Step #15059\tEpoch   4 Batch 2558/3125   Loss: 0.837926 mae: 0.745275 (2218.0818208740534 steps/sec)\n",
      "Step #15060\tEpoch   4 Batch 2559/3125   Loss: 0.892591 mae: 0.752896 (2201.087344402695 steps/sec)\n",
      "Step #15061\tEpoch   4 Batch 2560/3125   Loss: 0.618775 mae: 0.623873 (2395.7868281258925 steps/sec)\n",
      "Step #15062\tEpoch   4 Batch 2561/3125   Loss: 0.617499 mae: 0.617621 (1918.202855600984 steps/sec)\n",
      "Step #15063\tEpoch   4 Batch 2562/3125   Loss: 0.837043 mae: 0.718223 (2092.7363263513985 steps/sec)\n",
      "Step #15064\tEpoch   4 Batch 2563/3125   Loss: 0.748701 mae: 0.672610 (2180.3769896967237 steps/sec)\n",
      "Step #15065\tEpoch   4 Batch 2564/3125   Loss: 0.798207 mae: 0.709218 (2305.52538422638 steps/sec)\n",
      "Step #15066\tEpoch   4 Batch 2565/3125   Loss: 0.771152 mae: 0.693080 (2214.7555180061254 steps/sec)\n",
      "Step #15067\tEpoch   4 Batch 2566/3125   Loss: 0.741684 mae: 0.669315 (2159.074249474941 steps/sec)\n",
      "Step #15068\tEpoch   4 Batch 2567/3125   Loss: 0.786394 mae: 0.686177 (2141.2401343666083 steps/sec)\n",
      "Step #15069\tEpoch   4 Batch 2568/3125   Loss: 0.854540 mae: 0.718793 (2016.550477417618 steps/sec)\n",
      "Step #15070\tEpoch   4 Batch 2569/3125   Loss: 0.760659 mae: 0.683121 (2248.7154192579883 steps/sec)\n",
      "Step #15071\tEpoch   4 Batch 2570/3125   Loss: 0.870133 mae: 0.761245 (1881.5963250071777 steps/sec)\n",
      "Step #15072\tEpoch   4 Batch 2571/3125   Loss: 0.715346 mae: 0.670802 (2109.2591475066884 steps/sec)\n",
      "Step #15073\tEpoch   4 Batch 2572/3125   Loss: 0.839240 mae: 0.720239 (2120.541573556326 steps/sec)\n",
      "Step #15074\tEpoch   4 Batch 2573/3125   Loss: 0.818041 mae: 0.730444 (2173.9128631995773 steps/sec)\n",
      "Step #15075\tEpoch   4 Batch 2574/3125   Loss: 0.866166 mae: 0.704585 (2204.02518102805 steps/sec)\n",
      "Step #15076\tEpoch   4 Batch 2575/3125   Loss: 0.718127 mae: 0.658430 (2145.753312528777 steps/sec)\n",
      "Step #15077\tEpoch   4 Batch 2576/3125   Loss: 0.856027 mae: 0.731654 (2092.7572098592955 steps/sec)\n",
      "Step #15078\tEpoch   4 Batch 2577/3125   Loss: 0.775918 mae: 0.693407 (2055.084421884034 steps/sec)\n",
      "Step #15079\tEpoch   4 Batch 2578/3125   Loss: 0.793123 mae: 0.718487 (2085.058659773315 steps/sec)\n",
      "Step #15080\tEpoch   4 Batch 2579/3125   Loss: 0.682428 mae: 0.647342 (1856.1659718718745 steps/sec)\n",
      "Step #15081\tEpoch   4 Batch 2580/3125   Loss: 0.788645 mae: 0.692535 (2083.9191136284594 steps/sec)\n",
      "Step #15082\tEpoch   4 Batch 2581/3125   Loss: 0.727766 mae: 0.682576 (2147.5771105558515 steps/sec)\n",
      "Step #15083\tEpoch   4 Batch 2582/3125   Loss: 0.874802 mae: 0.744545 (2106.5048816746353 steps/sec)\n",
      "Step #15084\tEpoch   4 Batch 2583/3125   Loss: 0.769802 mae: 0.702268 (2231.0602353241557 steps/sec)\n",
      "Step #15085\tEpoch   4 Batch 2584/3125   Loss: 0.939091 mae: 0.773960 (2251.660976185875 steps/sec)\n",
      "Step #15086\tEpoch   4 Batch 2585/3125   Loss: 0.710320 mae: 0.681956 (2249.4872785000216 steps/sec)\n",
      "Step #15087\tEpoch   4 Batch 2586/3125   Loss: 0.851798 mae: 0.713601 (2129.6504661128824 steps/sec)\n",
      "Step #15088\tEpoch   4 Batch 2587/3125   Loss: 0.766799 mae: 0.714367 (2180.3543208849705 steps/sec)\n",
      "Step #15089\tEpoch   4 Batch 2588/3125   Loss: 0.778336 mae: 0.692615 (1887.01410883962 steps/sec)\n",
      "Step #15090\tEpoch   4 Batch 2589/3125   Loss: 0.845937 mae: 0.725424 (1923.1634066044917 steps/sec)\n",
      "Step #15091\tEpoch   4 Batch 2590/3125   Loss: 0.847937 mae: 0.725314 (1963.0189173757171 steps/sec)\n",
      "Step #15092\tEpoch   4 Batch 2591/3125   Loss: 0.932828 mae: 0.753138 (2141.96184172897 steps/sec)\n",
      "Step #15093\tEpoch   4 Batch 2592/3125   Loss: 0.735635 mae: 0.679108 (2116.1337194635885 steps/sec)\n",
      "Step #15094\tEpoch   4 Batch 2593/3125   Loss: 0.784263 mae: 0.694288 (1989.707779886148 steps/sec)\n",
      "Step #15095\tEpoch   4 Batch 2594/3125   Loss: 0.714954 mae: 0.687150 (2099.2722649876373 steps/sec)\n",
      "Step #15096\tEpoch   4 Batch 2595/3125   Loss: 0.748937 mae: 0.681868 (2082.387870001688 steps/sec)\n",
      "Step #15097\tEpoch   4 Batch 2596/3125   Loss: 0.799187 mae: 0.696943 (2302.513147637817 steps/sec)\n",
      "Step #15098\tEpoch   4 Batch 2597/3125   Loss: 0.795355 mae: 0.680140 (1781.1418185524283 steps/sec)\n",
      "Step #15099\tEpoch   4 Batch 2598/3125   Loss: 0.796744 mae: 0.718644 (1950.9298106888693 steps/sec)\n",
      "Step #15100\tEpoch   4 Batch 2599/3125   Loss: 0.930964 mae: 0.725912 (2009.151178386664 steps/sec)\n",
      "Step #15101\tEpoch   4 Batch 2600/3125   Loss: 0.763045 mae: 0.672084 (2063.313656040929 steps/sec)\n",
      "Step #15102\tEpoch   4 Batch 2601/3125   Loss: 0.824754 mae: 0.713953 (2114.256333739956 steps/sec)\n",
      "Step #15103\tEpoch   4 Batch 2602/3125   Loss: 0.775982 mae: 0.705484 (2277.4831128776527 steps/sec)\n",
      "Step #15104\tEpoch   4 Batch 2603/3125   Loss: 0.943312 mae: 0.765288 (2146.5000358235843 steps/sec)\n",
      "Step #15105\tEpoch   4 Batch 2604/3125   Loss: 0.880101 mae: 0.712196 (2209.9710206017176 steps/sec)\n",
      "Step #15106\tEpoch   4 Batch 2605/3125   Loss: 0.721779 mae: 0.675954 (2098.7260445334 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15107\tEpoch   4 Batch 2606/3125   Loss: 0.789973 mae: 0.704397 (1681.9736293349588 steps/sec)\n",
      "Step #15108\tEpoch   4 Batch 2607/3125   Loss: 0.749095 mae: 0.706843 (2052.22871346231 steps/sec)\n",
      "Step #15109\tEpoch   4 Batch 2608/3125   Loss: 0.746469 mae: 0.665031 (2126.0233977413272 steps/sec)\n",
      "Step #15110\tEpoch   4 Batch 2609/3125   Loss: 0.852111 mae: 0.712457 (2195.9706806282725 steps/sec)\n",
      "Step #15111\tEpoch   4 Batch 2610/3125   Loss: 0.841202 mae: 0.723980 (2232.627858450794 steps/sec)\n",
      "Step #15112\tEpoch   4 Batch 2611/3125   Loss: 0.745259 mae: 0.676036 (2119.1701781509887 steps/sec)\n",
      "Step #15113\tEpoch   4 Batch 2612/3125   Loss: 0.646586 mae: 0.637750 (2280.157435797073 steps/sec)\n",
      "Step #15114\tEpoch   4 Batch 2613/3125   Loss: 0.955516 mae: 0.767591 (2279.1662138370248 steps/sec)\n",
      "Step #15115\tEpoch   4 Batch 2614/3125   Loss: 0.892942 mae: 0.753841 (2358.1257800816343 steps/sec)\n",
      "Step #15116\tEpoch   4 Batch 2615/3125   Loss: 0.946170 mae: 0.769426 (1959.2227204783258 steps/sec)\n",
      "Step #15117\tEpoch   4 Batch 2616/3125   Loss: 0.772371 mae: 0.695679 (1830.1672077355395 steps/sec)\n",
      "Step #15118\tEpoch   4 Batch 2617/3125   Loss: 0.740614 mae: 0.682562 (2117.0309203420115 steps/sec)\n",
      "Step #15119\tEpoch   4 Batch 2618/3125   Loss: 0.734939 mae: 0.696210 (2067.4632280452697 steps/sec)\n",
      "Step #15120\tEpoch   4 Batch 2619/3125   Loss: 0.882188 mae: 0.749916 (2121.6142118627777 steps/sec)\n",
      "Step #15121\tEpoch   4 Batch 2620/3125   Loss: 0.719567 mae: 0.670143 (2198.987092241714 steps/sec)\n",
      "Step #15122\tEpoch   4 Batch 2621/3125   Loss: 0.756745 mae: 0.681565 (2199.448342405269 steps/sec)\n",
      "Step #15123\tEpoch   4 Batch 2622/3125   Loss: 0.731473 mae: 0.665246 (1982.335148215366 steps/sec)\n",
      "Step #15124\tEpoch   4 Batch 2623/3125   Loss: 0.824817 mae: 0.721506 (2004.331412296547 steps/sec)\n",
      "Step #15125\tEpoch   4 Batch 2624/3125   Loss: 0.919023 mae: 0.748787 (1793.203933304831 steps/sec)\n",
      "Step #15126\tEpoch   4 Batch 2625/3125   Loss: 0.877400 mae: 0.750355 (1887.7275100365457 steps/sec)\n",
      "Step #15127\tEpoch   4 Batch 2626/3125   Loss: 0.753736 mae: 0.681808 (2167.0390080082666 steps/sec)\n",
      "Step #15128\tEpoch   4 Batch 2627/3125   Loss: 0.713862 mae: 0.685325 (2095.580314763927 steps/sec)\n",
      "Step #15129\tEpoch   4 Batch 2628/3125   Loss: 0.920028 mae: 0.774744 (2120.713122794244 steps/sec)\n",
      "Step #15130\tEpoch   4 Batch 2629/3125   Loss: 0.787698 mae: 0.701556 (2069.70767621341 steps/sec)\n",
      "Step #15131\tEpoch   4 Batch 2630/3125   Loss: 0.566712 mae: 0.592712 (1924.9286350243697 steps/sec)\n",
      "Step #15132\tEpoch   4 Batch 2631/3125   Loss: 0.614144 mae: 0.624674 (2010.5380220117345 steps/sec)\n",
      "Step #15133\tEpoch   4 Batch 2632/3125   Loss: 0.866539 mae: 0.724150 (1812.9220767993913 steps/sec)\n",
      "Step #15134\tEpoch   4 Batch 2633/3125   Loss: 0.656017 mae: 0.630634 (1890.8081108616664 steps/sec)\n",
      "Step #15135\tEpoch   4 Batch 2634/3125   Loss: 0.838107 mae: 0.723133 (2077.5400221905215 steps/sec)\n",
      "Step #15136\tEpoch   4 Batch 2635/3125   Loss: 0.750413 mae: 0.685664 (2080.466657407591 steps/sec)\n",
      "Step #15137\tEpoch   4 Batch 2636/3125   Loss: 0.941397 mae: 0.750943 (1988.2364094882346 steps/sec)\n",
      "Step #15138\tEpoch   4 Batch 2637/3125   Loss: 0.711084 mae: 0.656874 (2008.7470426528482 steps/sec)\n",
      "Step #15139\tEpoch   4 Batch 2638/3125   Loss: 0.859453 mae: 0.707936 (2200.2329119236215 steps/sec)\n",
      "Step #15140\tEpoch   4 Batch 2639/3125   Loss: 0.806802 mae: 0.729287 (2257.8426622740435 steps/sec)\n",
      "Step #15141\tEpoch   4 Batch 2640/3125   Loss: 0.729605 mae: 0.692483 (2032.7740459643103 steps/sec)\n",
      "Step #15142\tEpoch   4 Batch 2641/3125   Loss: 0.695673 mae: 0.666700 (1903.7500340417034 steps/sec)\n",
      "Step #15143\tEpoch   4 Batch 2642/3125   Loss: 0.807149 mae: 0.733299 (1963.9380800314657 steps/sec)\n",
      "Step #15144\tEpoch   4 Batch 2643/3125   Loss: 0.721214 mae: 0.666527 (2004.8870958490277 steps/sec)\n",
      "Step #15145\tEpoch   4 Batch 2644/3125   Loss: 0.903562 mae: 0.743555 (2066.7908425233322 steps/sec)\n",
      "Step #15146\tEpoch   4 Batch 2645/3125   Loss: 0.790467 mae: 0.723432 (2104.919151669661 steps/sec)\n",
      "Step #15147\tEpoch   4 Batch 2646/3125   Loss: 0.700358 mae: 0.649916 (2076.0592381404927 steps/sec)\n",
      "Step #15148\tEpoch   4 Batch 2647/3125   Loss: 0.708893 mae: 0.680192 (2131.2087152700146 steps/sec)\n",
      "Step #15149\tEpoch   4 Batch 2648/3125   Loss: 0.792848 mae: 0.703552 (2236.7474055823973 steps/sec)\n",
      "Step #15150\tEpoch   4 Batch 2649/3125   Loss: 0.849705 mae: 0.738634 (2287.59421870739 steps/sec)\n",
      "Step #15151\tEpoch   4 Batch 2650/3125   Loss: 0.704765 mae: 0.682143 (1884.860196111915 steps/sec)\n",
      "Step #15152\tEpoch   4 Batch 2651/3125   Loss: 0.890149 mae: 0.734141 (2036.1687460556338 steps/sec)\n",
      "Step #15153\tEpoch   4 Batch 2652/3125   Loss: 0.806991 mae: 0.718591 (2064.329166256521 steps/sec)\n",
      "Step #15154\tEpoch   4 Batch 2653/3125   Loss: 0.816803 mae: 0.729200 (2039.8128604915817 steps/sec)\n",
      "Step #15155\tEpoch   4 Batch 2654/3125   Loss: 0.830589 mae: 0.731369 (2084.105499572675 steps/sec)\n",
      "Step #15156\tEpoch   4 Batch 2655/3125   Loss: 0.874589 mae: 0.729432 (2014.168267383788 steps/sec)\n",
      "Step #15157\tEpoch   4 Batch 2656/3125   Loss: 0.915204 mae: 0.765660 (1705.2650409412836 steps/sec)\n",
      "Step #15158\tEpoch   4 Batch 2657/3125   Loss: 0.832330 mae: 0.733127 (1983.8916270137831 steps/sec)\n",
      "Step #15159\tEpoch   4 Batch 2658/3125   Loss: 0.850841 mae: 0.711386 (2103.9266437930137 steps/sec)\n",
      "Step #15160\tEpoch   4 Batch 2659/3125   Loss: 0.699694 mae: 0.656926 (1943.9493516003745 steps/sec)\n",
      "Step #15161\tEpoch   4 Batch 2660/3125   Loss: 0.865125 mae: 0.718726 (2074.9705646637444 steps/sec)\n",
      "Step #15162\tEpoch   4 Batch 2661/3125   Loss: 0.772960 mae: 0.689359 (2002.8000878608743 steps/sec)\n",
      "Step #15163\tEpoch   4 Batch 2662/3125   Loss: 0.660331 mae: 0.660187 (2095.7269057041212 steps/sec)\n",
      "Step #15164\tEpoch   4 Batch 2663/3125   Loss: 0.742446 mae: 0.683272 (2245.777558844317 steps/sec)\n",
      "Step #15165\tEpoch   4 Batch 2664/3125   Loss: 0.749580 mae: 0.705611 (1979.9769633112408 steps/sec)\n",
      "Step #15166\tEpoch   4 Batch 2665/3125   Loss: 0.809731 mae: 0.721308 (2217.284472732655 steps/sec)\n",
      "Step #15167\tEpoch   4 Batch 2666/3125   Loss: 0.858155 mae: 0.727957 (2108.283738137365 steps/sec)\n",
      "Step #15168\tEpoch   4 Batch 2667/3125   Loss: 0.676281 mae: 0.659038 (1781.2477173312948 steps/sec)\n",
      "Step #15169\tEpoch   4 Batch 2668/3125   Loss: 0.818497 mae: 0.737749 (2076.922772198784 steps/sec)\n",
      "Step #15170\tEpoch   4 Batch 2669/3125   Loss: 0.761503 mae: 0.692022 (2116.1337194635885 steps/sec)\n",
      "Step #15171\tEpoch   4 Batch 2670/3125   Loss: 0.756682 mae: 0.692508 (2172.8542417837457 steps/sec)\n",
      "Step #15172\tEpoch   4 Batch 2671/3125   Loss: 0.700028 mae: 0.627997 (2111.2126764249906 steps/sec)\n",
      "Step #15173\tEpoch   4 Batch 2672/3125   Loss: 0.840933 mae: 0.723843 (2180.830464939738 steps/sec)\n",
      "Step #15174\tEpoch   4 Batch 2673/3125   Loss: 0.760375 mae: 0.687265 (2089.775093918468 steps/sec)\n",
      "Step #15175\tEpoch   4 Batch 2674/3125   Loss: 0.781842 mae: 0.696750 (2158.51868625009 steps/sec)\n",
      "Step #15176\tEpoch   4 Batch 2675/3125   Loss: 0.768725 mae: 0.694557 (2157.5859833948907 steps/sec)\n",
      "Step #15177\tEpoch   4 Batch 2676/3125   Loss: 0.752986 mae: 0.679702 (1770.0323258581545 steps/sec)\n",
      "Step #15178\tEpoch   4 Batch 2677/3125   Loss: 0.758348 mae: 0.707851 (2071.506746478595 steps/sec)\n",
      "Step #15179\tEpoch   4 Batch 2678/3125   Loss: 0.798249 mae: 0.697539 (2147.1373576869523 steps/sec)\n",
      "Step #15180\tEpoch   4 Batch 2679/3125   Loss: 0.672526 mae: 0.621658 (2052.449646694983 steps/sec)\n",
      "Step #15181\tEpoch   4 Batch 2680/3125   Loss: 0.709623 mae: 0.671965 (2177.2305392329895 steps/sec)\n",
      "Step #15182\tEpoch   4 Batch 2681/3125   Loss: 0.719389 mae: 0.662024 (2157.5859833948907 steps/sec)\n",
      "Step #15183\tEpoch   4 Batch 2682/3125   Loss: 0.897244 mae: 0.746869 (1961.4766594648186 steps/sec)\n",
      "Step #15184\tEpoch   4 Batch 2683/3125   Loss: 0.809778 mae: 0.677613 (2250.7426805185883 steps/sec)\n",
      "Step #15185\tEpoch   4 Batch 2684/3125   Loss: 0.836339 mae: 0.714320 (2108.0082424486104 steps/sec)\n",
      "Step #15186\tEpoch   4 Batch 2685/3125   Loss: 0.777598 mae: 0.684859 (1627.5538792267157 steps/sec)\n",
      "Step #15187\tEpoch   4 Batch 2686/3125   Loss: 0.712522 mae: 0.685579 (1930.0661715305962 steps/sec)\n",
      "Step #15188\tEpoch   4 Batch 2687/3125   Loss: 0.774320 mae: 0.694711 (2000.144969003338 steps/sec)\n",
      "Step #15189\tEpoch   4 Batch 2688/3125   Loss: 0.645748 mae: 0.635700 (2020.6112460014645 steps/sec)\n",
      "Step #15190\tEpoch   4 Batch 2689/3125   Loss: 0.760416 mae: 0.694166 (2020.7475356760872 steps/sec)\n",
      "Step #15191\tEpoch   4 Batch 2690/3125   Loss: 0.866184 mae: 0.731811 (2012.9696108732794 steps/sec)\n",
      "Step #15192\tEpoch   4 Batch 2691/3125   Loss: 0.761147 mae: 0.695181 (2051.124760377137 steps/sec)\n",
      "Step #15193\tEpoch   4 Batch 2692/3125   Loss: 0.905155 mae: 0.762022 (2037.6525456665372 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15194\tEpoch   4 Batch 2693/3125   Loss: 0.683005 mae: 0.655474 (1879.774837761285 steps/sec)\n",
      "Step #15195\tEpoch   4 Batch 2694/3125   Loss: 0.809108 mae: 0.730670 (2177.1401283142663 steps/sec)\n",
      "Step #15196\tEpoch   4 Batch 2695/3125   Loss: 0.780376 mae: 0.700308 (2035.2597509729137 steps/sec)\n",
      "Step #15197\tEpoch   4 Batch 2696/3125   Loss: 0.671925 mae: 0.669595 (2081.519786404105 steps/sec)\n",
      "Step #15198\tEpoch   4 Batch 2697/3125   Loss: 0.659347 mae: 0.647066 (1973.0101983216045 steps/sec)\n",
      "Step #15199\tEpoch   4 Batch 2698/3125   Loss: 0.785954 mae: 0.693478 (2093.99007498677 steps/sec)\n",
      "Step #15200\tEpoch   4 Batch 2699/3125   Loss: 0.752304 mae: 0.687545 (2019.7160852899822 steps/sec)\n",
      "Step #15201\tEpoch   4 Batch 2700/3125   Loss: 0.774854 mae: 0.686962 (1912.709429694554 steps/sec)\n",
      "Step #15202\tEpoch   4 Batch 2701/3125   Loss: 0.843118 mae: 0.716074 (1973.4186506069445 steps/sec)\n",
      "Step #15203\tEpoch   4 Batch 2702/3125   Loss: 0.806668 mae: 0.731698 (1869.5026609732834 steps/sec)\n",
      "Step #15204\tEpoch   4 Batch 2703/3125   Loss: 0.762858 mae: 0.702312 (2202.7519273995335 steps/sec)\n",
      "Step #15205\tEpoch   4 Batch 2704/3125   Loss: 0.729309 mae: 0.665157 (2020.9422671073808 steps/sec)\n",
      "Step #15206\tEpoch   4 Batch 2705/3125   Loss: 0.758032 mae: 0.677057 (1847.7924137627208 steps/sec)\n",
      "Step #15207\tEpoch   4 Batch 2706/3125   Loss: 0.680137 mae: 0.669648 (2172.111570290733 steps/sec)\n",
      "Step #15208\tEpoch   4 Batch 2707/3125   Loss: 0.787949 mae: 0.694600 (2102.450174439588 steps/sec)\n",
      "Step #15209\tEpoch   4 Batch 2708/3125   Loss: 0.685174 mae: 0.668848 (2148.4131374597905 steps/sec)\n",
      "Step #15210\tEpoch   4 Batch 2709/3125   Loss: 0.702842 mae: 0.673448 (2157.208689927584 steps/sec)\n",
      "Step #15211\tEpoch   4 Batch 2710/3125   Loss: 0.691620 mae: 0.667155 (2015.4070884908126 steps/sec)\n",
      "Step #15212\tEpoch   4 Batch 2711/3125   Loss: 0.781708 mae: 0.711979 (2102.49233051952 steps/sec)\n",
      "Step #15213\tEpoch   4 Batch 2712/3125   Loss: 0.838294 mae: 0.708174 (2064.552712666988 steps/sec)\n",
      "Step #15214\tEpoch   4 Batch 2713/3125   Loss: 0.822807 mae: 0.741467 (1878.6971010857492 steps/sec)\n",
      "Step #15215\tEpoch   4 Batch 2714/3125   Loss: 0.826719 mae: 0.728970 (2127.468425057063 steps/sec)\n",
      "Step #15216\tEpoch   4 Batch 2715/3125   Loss: 0.751305 mae: 0.679358 (2142.9029785929597 steps/sec)\n",
      "Step #15217\tEpoch   4 Batch 2716/3125   Loss: 0.744974 mae: 0.687881 (1963.2578168882233 steps/sec)\n",
      "Step #15218\tEpoch   4 Batch 2717/3125   Loss: 0.743535 mae: 0.690284 (1924.698972099853 steps/sec)\n",
      "Step #15219\tEpoch   4 Batch 2718/3125   Loss: 0.728225 mae: 0.679109 (1807.0795850136146 steps/sec)\n",
      "Step #15220\tEpoch   4 Batch 2719/3125   Loss: 0.738128 mae: 0.680365 (1804.9177647149952 steps/sec)\n",
      "Step #15221\tEpoch   4 Batch 2720/3125   Loss: 0.701733 mae: 0.656877 (1924.02796381585 steps/sec)\n",
      "Step #15222\tEpoch   4 Batch 2721/3125   Loss: 0.770487 mae: 0.707168 (1997.8203711465915 steps/sec)\n",
      "Step #15223\tEpoch   4 Batch 2722/3125   Loss: 0.766903 mae: 0.700181 (2027.2523393395716 steps/sec)\n",
      "Step #15224\tEpoch   4 Batch 2723/3125   Loss: 0.701391 mae: 0.669511 (2139.492557717224 steps/sec)\n",
      "Step #15225\tEpoch   4 Batch 2724/3125   Loss: 0.745988 mae: 0.684180 (1894.1382611680124 steps/sec)\n",
      "Step #15226\tEpoch   4 Batch 2725/3125   Loss: 0.834065 mae: 0.705148 (2102.9561589988366 steps/sec)\n",
      "Step #15227\tEpoch   4 Batch 2726/3125   Loss: 0.738026 mae: 0.683116 (2167.218163216799 steps/sec)\n",
      "Step #15228\tEpoch   4 Batch 2727/3125   Loss: 0.780971 mae: 0.686779 (2019.4049109292248 steps/sec)\n",
      "Step #15229\tEpoch   4 Batch 2728/3125   Loss: 0.873051 mae: 0.736751 (2024.8643429564545 steps/sec)\n",
      "Step #15230\tEpoch   4 Batch 2729/3125   Loss: 0.885477 mae: 0.736163 (1829.7041451093642 steps/sec)\n",
      "Step #15231\tEpoch   4 Batch 2730/3125   Loss: 0.885163 mae: 0.727459 (1989.5001470434775 steps/sec)\n",
      "Step #15232\tEpoch   4 Batch 2731/3125   Loss: 0.691072 mae: 0.649364 (2052.128304988551 steps/sec)\n",
      "Step #15233\tEpoch   4 Batch 2732/3125   Loss: 0.752280 mae: 0.692570 (2188.682710972886 steps/sec)\n",
      "Step #15234\tEpoch   4 Batch 2733/3125   Loss: 0.786686 mae: 0.713970 (2014.2069574905395 steps/sec)\n",
      "Step #15235\tEpoch   4 Batch 2734/3125   Loss: 0.699534 mae: 0.679779 (2034.0753241966615 steps/sec)\n",
      "Step #15236\tEpoch   4 Batch 2735/3125   Loss: 0.695277 mae: 0.649316 (1702.1785006980292 steps/sec)\n",
      "Step #15237\tEpoch   4 Batch 2736/3125   Loss: 0.804563 mae: 0.709367 (1839.106909524603 steps/sec)\n",
      "Step #15238\tEpoch   4 Batch 2737/3125   Loss: 0.766629 mae: 0.711266 (2017.986393772312 steps/sec)\n",
      "Step #15239\tEpoch   4 Batch 2738/3125   Loss: 0.781644 mae: 0.729152 (2169.5723241811675 steps/sec)\n",
      "Step #15240\tEpoch   4 Batch 2739/3125   Loss: 0.847312 mae: 0.728043 (1965.2447709723367 steps/sec)\n",
      "Step #15241\tEpoch   4 Batch 2740/3125   Loss: 0.792059 mae: 0.686645 (2097.403688442613 steps/sec)\n",
      "Step #15242\tEpoch   4 Batch 2741/3125   Loss: 0.745258 mae: 0.683556 (2131.5553025837007 steps/sec)\n",
      "Step #15243\tEpoch   4 Batch 2742/3125   Loss: 0.798498 mae: 0.691564 (2002.9531150014805 steps/sec)\n",
      "Step #15244\tEpoch   4 Batch 2743/3125   Loss: 0.706877 mae: 0.649918 (2163.2835790103463 steps/sec)\n",
      "Step #15245\tEpoch   4 Batch 2744/3125   Loss: 0.802455 mae: 0.708023 (1925.9008926275576 steps/sec)\n",
      "Step #15246\tEpoch   4 Batch 2745/3125   Loss: 0.829599 mae: 0.709458 (1917.5363683743726 steps/sec)\n",
      "Step #15247\tEpoch   4 Batch 2746/3125   Loss: 0.626815 mae: 0.637282 (2202.058045276996 steps/sec)\n",
      "Step #15248\tEpoch   4 Batch 2747/3125   Loss: 0.868032 mae: 0.722556 (2059.9897842913833 steps/sec)\n",
      "Step #15249\tEpoch   4 Batch 2748/3125   Loss: 0.768341 mae: 0.692593 (2189.8025457089457 steps/sec)\n",
      "Step #15250\tEpoch   4 Batch 2749/3125   Loss: 0.903897 mae: 0.739884 (2166.0318116091717 steps/sec)\n",
      "Step #15251\tEpoch   4 Batch 2750/3125   Loss: 0.886200 mae: 0.743370 (2082.449903680019 steps/sec)\n",
      "Step #15252\tEpoch   4 Batch 2751/3125   Loss: 0.945974 mae: 0.755470 (2118.720575458164 steps/sec)\n",
      "Step #15253\tEpoch   4 Batch 2752/3125   Loss: 0.778784 mae: 0.708575 (2182.260145681582 steps/sec)\n",
      "Step #15254\tEpoch   4 Batch 2753/3125   Loss: 0.767830 mae: 0.693390 (1891.3537937067667 steps/sec)\n",
      "Step #15255\tEpoch   4 Batch 2754/3125   Loss: 0.730984 mae: 0.694486 (1838.8488956307488 steps/sec)\n",
      "Step #15256\tEpoch   4 Batch 2755/3125   Loss: 0.835527 mae: 0.708615 (2225.590847828163 steps/sec)\n",
      "Step #15257\tEpoch   4 Batch 2756/3125   Loss: 0.909953 mae: 0.736924 (2330.3760334251933 steps/sec)\n",
      "Step #15258\tEpoch   4 Batch 2757/3125   Loss: 0.755482 mae: 0.714507 (2089.0464995816233 steps/sec)\n",
      "Step #15259\tEpoch   4 Batch 2758/3125   Loss: 0.784579 mae: 0.715280 (2340.0490961838877 steps/sec)\n",
      "Step #15260\tEpoch   4 Batch 2759/3125   Loss: 0.764951 mae: 0.687293 (2107.1186713153215 steps/sec)\n",
      "Step #15261\tEpoch   4 Batch 2760/3125   Loss: 0.713412 mae: 0.666678 (2208.667628566313 steps/sec)\n",
      "Step #15262\tEpoch   4 Batch 2761/3125   Loss: 0.828610 mae: 0.721254 (2089.87922031331 steps/sec)\n",
      "Step #15263\tEpoch   4 Batch 2762/3125   Loss: 0.676985 mae: 0.636335 (1933.946273942032 steps/sec)\n",
      "Step #15264\tEpoch   4 Batch 2763/3125   Loss: 0.664483 mae: 0.638504 (1755.9969186455437 steps/sec)\n",
      "Step #15265\tEpoch   4 Batch 2764/3125   Loss: 0.737442 mae: 0.684844 (1991.88108467493 steps/sec)\n",
      "Step #15266\tEpoch   4 Batch 2765/3125   Loss: 0.742800 mae: 0.675797 (2212.092316779883 steps/sec)\n",
      "Step #15267\tEpoch   4 Batch 2766/3125   Loss: 0.688138 mae: 0.650506 (2000.08774188626 steps/sec)\n",
      "Step #15268\tEpoch   4 Batch 2767/3125   Loss: 0.698806 mae: 0.683954 (2094.0528018532573 steps/sec)\n",
      "Step #15269\tEpoch   4 Batch 2768/3125   Loss: 0.802004 mae: 0.703653 (2185.785606336964 steps/sec)\n",
      "Step #15270\tEpoch   4 Batch 2769/3125   Loss: 0.756677 mae: 0.686171 (2113.361482571322 steps/sec)\n",
      "Step #15271\tEpoch   4 Batch 2770/3125   Loss: 0.765756 mae: 0.682278 (2018.4136822552236 steps/sec)\n",
      "Step #15272\tEpoch   4 Batch 2771/3125   Loss: 0.834908 mae: 0.715375 (1907.299416119468 steps/sec)\n",
      "Step #15273\tEpoch   4 Batch 2772/3125   Loss: 0.903850 mae: 0.762219 (2066.8926910037057 steps/sec)\n",
      "Step #15274\tEpoch   4 Batch 2773/3125   Loss: 0.932811 mae: 0.745199 (2130.2129042743377 steps/sec)\n",
      "Step #15275\tEpoch   4 Batch 2774/3125   Loss: 0.702479 mae: 0.656990 (2069.544279313952 steps/sec)\n",
      "Step #15276\tEpoch   4 Batch 2775/3125   Loss: 0.828365 mae: 0.710114 (2077.725268737306 steps/sec)\n",
      "Step #15277\tEpoch   4 Batch 2776/3125   Loss: 0.732115 mae: 0.682043 (2143.9983642590605 steps/sec)\n",
      "Step #15278\tEpoch   4 Batch 2777/3125   Loss: 0.899046 mae: 0.732822 (1896.9480977621795 steps/sec)\n",
      "Step #15279\tEpoch   4 Batch 2778/3125   Loss: 0.718688 mae: 0.654059 (2134.375508874776 steps/sec)\n",
      "Step #15280\tEpoch   4 Batch 2779/3125   Loss: 0.754535 mae: 0.659269 (1928.2028649712215 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15281\tEpoch   4 Batch 2780/3125   Loss: 0.823431 mae: 0.702204 (1652.6801897646856 steps/sec)\n",
      "Step #15282\tEpoch   4 Batch 2781/3125   Loss: 0.905810 mae: 0.748350 (2212.092316779883 steps/sec)\n",
      "Step #15283\tEpoch   4 Batch 2782/3125   Loss: 0.894851 mae: 0.748283 (2044.645503470868 steps/sec)\n",
      "Step #15284\tEpoch   4 Batch 2783/3125   Loss: 0.797468 mae: 0.704545 (2116.9454398627163 steps/sec)\n",
      "Step #15285\tEpoch   4 Batch 2784/3125   Loss: 0.723314 mae: 0.680508 (2308.4946887555725 steps/sec)\n",
      "Step #15286\tEpoch   4 Batch 2785/3125   Loss: 0.814061 mae: 0.739546 (2028.664293453026 steps/sec)\n",
      "Step #15287\tEpoch   4 Batch 2786/3125   Loss: 0.890768 mae: 0.738599 (2234.1262823722423 steps/sec)\n",
      "Step #15288\tEpoch   4 Batch 2787/3125   Loss: 0.727426 mae: 0.677240 (2150.3737503204306 steps/sec)\n",
      "Step #15289\tEpoch   4 Batch 2788/3125   Loss: 0.904512 mae: 0.735842 (2231.2501329928714 steps/sec)\n",
      "Step #15290\tEpoch   4 Batch 2789/3125   Loss: 0.781726 mae: 0.693663 (2179.2214809734605 steps/sec)\n",
      "Step #15291\tEpoch   4 Batch 2790/3125   Loss: 0.783015 mae: 0.717253 (1789.363571983174 steps/sec)\n",
      "Step #15292\tEpoch   4 Batch 2791/3125   Loss: 0.749220 mae: 0.685483 (2005.6924254016833 steps/sec)\n",
      "Step #15293\tEpoch   4 Batch 2792/3125   Loss: 0.667313 mae: 0.663418 (2029.4692021096434 steps/sec)\n",
      "Step #15294\tEpoch   4 Batch 2793/3125   Loss: 0.914679 mae: 0.774701 (2252.5558264680294 steps/sec)\n",
      "Step #15295\tEpoch   4 Batch 2794/3125   Loss: 0.763891 mae: 0.692990 (2304.2840975266727 steps/sec)\n",
      "Step #15296\tEpoch   4 Batch 2795/3125   Loss: 0.804261 mae: 0.705884 (1991.067902172262 steps/sec)\n",
      "Step #15297\tEpoch   4 Batch 2796/3125   Loss: 0.796856 mae: 0.703996 (2202.3818025246264 steps/sec)\n",
      "Step #15298\tEpoch   4 Batch 2797/3125   Loss: 0.934163 mae: 0.751441 (2156.764984162244 steps/sec)\n",
      "Step #15299\tEpoch   4 Batch 2798/3125   Loss: 0.710167 mae: 0.664829 (1610.357140114721 steps/sec)\n",
      "Step #15300\tEpoch   4 Batch 2799/3125   Loss: 0.804352 mae: 0.705365 (2061.265369909869 steps/sec)\n",
      "Step #15301\tEpoch   4 Batch 2800/3125   Loss: 0.929360 mae: 0.748636 (2112.7227667912516 steps/sec)\n",
      "Step #15302\tEpoch   4 Batch 2801/3125   Loss: 0.717035 mae: 0.684341 (2076.3264457491364 steps/sec)\n",
      "Step #15303\tEpoch   4 Batch 2802/3125   Loss: 0.790620 mae: 0.714305 (1956.8826515377725 steps/sec)\n",
      "Step #15304\tEpoch   4 Batch 2803/3125   Loss: 0.831532 mae: 0.726885 (2164.2212154673325 steps/sec)\n",
      "Step #15305\tEpoch   4 Batch 2804/3125   Loss: 0.676022 mae: 0.641142 (1940.531687501735 steps/sec)\n",
      "Step #15306\tEpoch   4 Batch 2805/3125   Loss: 0.769103 mae: 0.686868 (2114.959962887513 steps/sec)\n",
      "Step #15307\tEpoch   4 Batch 2806/3125   Loss: 0.763163 mae: 0.682375 (2049.0205082609505 steps/sec)\n",
      "Step #15308\tEpoch   4 Batch 2807/3125   Loss: 0.886164 mae: 0.751934 (1999.5156507727659 steps/sec)\n",
      "Step #15309\tEpoch   4 Batch 2808/3125   Loss: 0.875327 mae: 0.735769 (2074.8884469640757 steps/sec)\n",
      "Step #15310\tEpoch   4 Batch 2809/3125   Loss: 0.814819 mae: 0.732402 (2197.236104562837 steps/sec)\n",
      "Step #15311\tEpoch   4 Batch 2810/3125   Loss: 0.733282 mae: 0.674087 (2291.143085007593 steps/sec)\n",
      "Step #15312\tEpoch   4 Batch 2811/3125   Loss: 0.718539 mae: 0.665553 (2209.2725836186464 steps/sec)\n",
      "Step #15313\tEpoch   4 Batch 2812/3125   Loss: 0.818115 mae: 0.735168 (2244.2874876931637 steps/sec)\n",
      "Step #15314\tEpoch   4 Batch 2813/3125   Loss: 0.867450 mae: 0.748349 (2144.349124224174 steps/sec)\n",
      "Step #15315\tEpoch   4 Batch 2814/3125   Loss: 0.738460 mae: 0.675653 (1925.9008926275576 steps/sec)\n",
      "Step #15316\tEpoch   4 Batch 2815/3125   Loss: 0.751964 mae: 0.676610 (1991.6919131962582 steps/sec)\n",
      "Step #15317\tEpoch   4 Batch 2816/3125   Loss: 0.841298 mae: 0.701347 (1845.5483882322917 steps/sec)\n",
      "Step #15318\tEpoch   4 Batch 2817/3125   Loss: 0.721923 mae: 0.660705 (1939.7240001479893 steps/sec)\n",
      "Step #15319\tEpoch   4 Batch 2818/3125   Loss: 0.861976 mae: 0.706621 (2068.2380323083294 steps/sec)\n",
      "Step #15320\tEpoch   4 Batch 2819/3125   Loss: 0.859224 mae: 0.743992 (2008.0930722458945 steps/sec)\n",
      "Step #15321\tEpoch   4 Batch 2820/3125   Loss: 0.960406 mae: 0.763121 (2031.120279706734 steps/sec)\n",
      "Step #15322\tEpoch   4 Batch 2821/3125   Loss: 0.766941 mae: 0.700302 (2215.691494981511 steps/sec)\n",
      "Step #15323\tEpoch   4 Batch 2822/3125   Loss: 0.772358 mae: 0.700161 (2234.6262040746738 steps/sec)\n",
      "Step #15324\tEpoch   4 Batch 2823/3125   Loss: 0.713662 mae: 0.663197 (2139.9510204081635 steps/sec)\n",
      "Step #15325\tEpoch   4 Batch 2824/3125   Loss: 0.779457 mae: 0.714993 (1971.1371988758658 steps/sec)\n",
      "Step #15326\tEpoch   4 Batch 2825/3125   Loss: 0.771870 mae: 0.667285 (1788.5242546223647 steps/sec)\n",
      "Step #15327\tEpoch   4 Batch 2826/3125   Loss: 0.782241 mae: 0.691918 (1996.2419684926942 steps/sec)\n",
      "Step #15328\tEpoch   4 Batch 2827/3125   Loss: 0.733131 mae: 0.699546 (1989.1038773806813 steps/sec)\n",
      "Step #15329\tEpoch   4 Batch 2828/3125   Loss: 0.920313 mae: 0.743217 (1965.9264119990626 steps/sec)\n",
      "Step #15330\tEpoch   4 Batch 2829/3125   Loss: 0.671807 mae: 0.646336 (2213.5866582225035 steps/sec)\n",
      "Step #15331\tEpoch   4 Batch 2830/3125   Loss: 0.698176 mae: 0.641296 (2197.9960591959084 steps/sec)\n",
      "Step #15332\tEpoch   4 Batch 2831/3125   Loss: 0.839640 mae: 0.724702 (1800.5168491092509 steps/sec)\n",
      "Step #15333\tEpoch   4 Batch 2832/3125   Loss: 0.722700 mae: 0.678541 (1679.3068656812031 steps/sec)\n",
      "Step #15334\tEpoch   4 Batch 2833/3125   Loss: 0.649251 mae: 0.634366 (1556.5128326925646 steps/sec)\n",
      "Step #15335\tEpoch   4 Batch 2834/3125   Loss: 0.700714 mae: 0.654243 (1354.7581056725173 steps/sec)\n",
      "Step #15336\tEpoch   4 Batch 2835/3125   Loss: 0.858506 mae: 0.710760 (1751.6262131867766 steps/sec)\n",
      "Step #15337\tEpoch   4 Batch 2836/3125   Loss: 0.770779 mae: 0.684097 (1911.5936084297264 steps/sec)\n",
      "Step #15338\tEpoch   4 Batch 2837/3125   Loss: 0.715545 mae: 0.676257 (2038.385350349426 steps/sec)\n",
      "Step #15339\tEpoch   4 Batch 2838/3125   Loss: 0.564037 mae: 0.573729 (2171.9316051658607 steps/sec)\n",
      "Step #15340\tEpoch   4 Batch 2839/3125   Loss: 0.760999 mae: 0.706319 (2035.8524817737912 steps/sec)\n",
      "Step #15341\tEpoch   4 Batch 2840/3125   Loss: 0.754434 mae: 0.692587 (1929.9063184436714 steps/sec)\n",
      "Step #15342\tEpoch   4 Batch 2841/3125   Loss: 0.855261 mae: 0.711279 (1747.4082406365872 steps/sec)\n",
      "Step #15343\tEpoch   4 Batch 2842/3125   Loss: 0.795668 mae: 0.709147 (2061.0020244904376 steps/sec)\n",
      "Step #15344\tEpoch   4 Batch 2843/3125   Loss: 0.826900 mae: 0.716805 (2383.8857818396764 steps/sec)\n",
      "Step #15345\tEpoch   4 Batch 2844/3125   Loss: 0.813065 mae: 0.715767 (2147.291250703937 steps/sec)\n",
      "Step #15346\tEpoch   4 Batch 2845/3125   Loss: 0.781577 mae: 0.691830 (2069.360488637597 steps/sec)\n",
      "Step #15347\tEpoch   4 Batch 2846/3125   Loss: 0.726836 mae: 0.669773 (2255.63275754512 steps/sec)\n",
      "Step #15348\tEpoch   4 Batch 2847/3125   Loss: 0.751657 mae: 0.683681 (2160.8317105086912 steps/sec)\n",
      "Step #15349\tEpoch   4 Batch 2848/3125   Loss: 0.727365 mae: 0.707099 (2101.628468638199 steps/sec)\n",
      "Step #15350\tEpoch   4 Batch 2849/3125   Loss: 0.773495 mae: 0.714063 (1996.8882413993392 steps/sec)\n",
      "Step #15351\tEpoch   4 Batch 2850/3125   Loss: 0.814873 mae: 0.714186 (2071.5681335506497 steps/sec)\n",
      "Step #15352\tEpoch   4 Batch 2851/3125   Loss: 0.776054 mae: 0.672462 (1771.9017202338707 steps/sec)\n",
      "Step #15353\tEpoch   4 Batch 2852/3125   Loss: 0.821123 mae: 0.709271 (2051.445787846774 steps/sec)\n",
      "Step #15354\tEpoch   4 Batch 2853/3125   Loss: 0.799565 mae: 0.724239 (2111.127665143248 steps/sec)\n",
      "Step #15355\tEpoch   4 Batch 2854/3125   Loss: 0.852847 mae: 0.724627 (2187.2217934544547 steps/sec)\n",
      "Step #15356\tEpoch   4 Batch 2855/3125   Loss: 0.869586 mae: 0.744609 (2002.0926413869477 steps/sec)\n",
      "Step #15357\tEpoch   4 Batch 2856/3125   Loss: 0.778060 mae: 0.693314 (1978.7813024853276 steps/sec)\n",
      "Step #15358\tEpoch   4 Batch 2857/3125   Loss: 0.669810 mae: 0.643659 (1823.9593661396093 steps/sec)\n",
      "Step #15359\tEpoch   4 Batch 2858/3125   Loss: 0.988891 mae: 0.777772 (2014.1876122513663 steps/sec)\n",
      "Step #15360\tEpoch   4 Batch 2859/3125   Loss: 0.748626 mae: 0.678133 (1951.6742047759972 steps/sec)\n",
      "Step #15361\tEpoch   4 Batch 2860/3125   Loss: 0.765140 mae: 0.694437 (2044.326600639476 steps/sec)\n",
      "Step #15362\tEpoch   4 Batch 2861/3125   Loss: 0.706076 mae: 0.668833 (2013.7814480507009 steps/sec)\n",
      "Step #15363\tEpoch   4 Batch 2862/3125   Loss: 0.752147 mae: 0.689432 (2085.4318728744456 steps/sec)\n",
      "Step #15364\tEpoch   4 Batch 2863/3125   Loss: 0.714348 mae: 0.662588 (2012.641196172708 steps/sec)\n",
      "Step #15365\tEpoch   4 Batch 2864/3125   Loss: 0.821966 mae: 0.714944 (2071.588596716518 steps/sec)\n",
      "Step #15366\tEpoch   4 Batch 2865/3125   Loss: 0.711631 mae: 0.681110 (2134.3972316930435 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15367\tEpoch   4 Batch 2866/3125   Loss: 0.850721 mae: 0.708143 (1901.0061821280299 steps/sec)\n",
      "Step #15368\tEpoch   4 Batch 2867/3125   Loss: 0.907517 mae: 0.780185 (1908.7749956766695 steps/sec)\n",
      "Step #15369\tEpoch   4 Batch 2868/3125   Loss: 0.734397 mae: 0.664293 (2113.9792750292327 steps/sec)\n",
      "Step #15370\tEpoch   4 Batch 2869/3125   Loss: 0.674808 mae: 0.670595 (2158.5853387405564 steps/sec)\n",
      "Step #15371\tEpoch   4 Batch 2870/3125   Loss: 0.636606 mae: 0.625278 (2103.5045838432065 steps/sec)\n",
      "Step #15372\tEpoch   4 Batch 2871/3125   Loss: 0.737796 mae: 0.683466 (2148.5892260721675 steps/sec)\n",
      "Step #15373\tEpoch   4 Batch 2872/3125   Loss: 0.813216 mae: 0.709029 (2199.632896655164 steps/sec)\n",
      "Step #15374\tEpoch   4 Batch 2873/3125   Loss: 0.738684 mae: 0.691836 (2046.3413444180985 steps/sec)\n",
      "Step #15375\tEpoch   4 Batch 2874/3125   Loss: 0.811564 mae: 0.686914 (2212.092316779883 steps/sec)\n",
      "Step #15376\tEpoch   4 Batch 2875/3125   Loss: 0.817199 mae: 0.716650 (1882.947852320069 steps/sec)\n",
      "Step #15377\tEpoch   4 Batch 2876/3125   Loss: 0.712949 mae: 0.676621 (2003.6037413178688 steps/sec)\n",
      "Step #15378\tEpoch   4 Batch 2877/3125   Loss: 0.756092 mae: 0.687213 (1934.981223646211 steps/sec)\n",
      "Step #15379\tEpoch   4 Batch 2878/3125   Loss: 0.789568 mae: 0.695624 (2184.82919562024 steps/sec)\n",
      "Step #15380\tEpoch   4 Batch 2879/3125   Loss: 0.657021 mae: 0.652551 (2117.2019020120542 steps/sec)\n",
      "Step #15381\tEpoch   4 Batch 2880/3125   Loss: 0.764648 mae: 0.711327 (2033.8977790708952 steps/sec)\n",
      "Step #15382\tEpoch   4 Batch 2881/3125   Loss: 0.692578 mae: 0.664447 (2055.829820605823 steps/sec)\n",
      "Step #15383\tEpoch   4 Batch 2882/3125   Loss: 0.821699 mae: 0.721690 (2156.831528390567 steps/sec)\n",
      "Step #15384\tEpoch   4 Batch 2883/3125   Loss: 0.924473 mae: 0.777115 (2106.8646460181435 steps/sec)\n",
      "Step #15385\tEpoch   4 Batch 2884/3125   Loss: 0.710201 mae: 0.664026 (1986.7860357159775 steps/sec)\n",
      "Step #15386\tEpoch   4 Batch 2885/3125   Loss: 0.801240 mae: 0.721496 (1746.5350822402665 steps/sec)\n",
      "Step #15387\tEpoch   4 Batch 2886/3125   Loss: 0.705525 mae: 0.659010 (2043.5696049580013 steps/sec)\n",
      "Step #15388\tEpoch   4 Batch 2887/3125   Loss: 0.860837 mae: 0.731634 (2063.7197402086204 steps/sec)\n",
      "Step #15389\tEpoch   4 Batch 2888/3125   Loss: 0.817426 mae: 0.721021 (1882.7619021968453 steps/sec)\n",
      "Step #15390\tEpoch   4 Batch 2889/3125   Loss: 0.745003 mae: 0.695378 (2237.821456772734 steps/sec)\n",
      "Step #15391\tEpoch   4 Batch 2890/3125   Loss: 0.711794 mae: 0.656342 (1972.6204697449982 steps/sec)\n",
      "Step #15392\tEpoch   4 Batch 2891/3125   Loss: 0.852645 mae: 0.733310 (2151.0795646866954 steps/sec)\n",
      "Step #15393\tEpoch   4 Batch 2892/3125   Loss: 0.735545 mae: 0.683489 (1813.4237241236187 steps/sec)\n",
      "Step #15394\tEpoch   4 Batch 2893/3125   Loss: 0.702722 mae: 0.665635 (2089.775093918468 steps/sec)\n",
      "Step #15395\tEpoch   4 Batch 2894/3125   Loss: 0.790245 mae: 0.701945 (1936.4105594592847 steps/sec)\n",
      "Step #15396\tEpoch   4 Batch 2895/3125   Loss: 0.746692 mae: 0.680059 (2321.7846664821477 steps/sec)\n",
      "Step #15397\tEpoch   4 Batch 2896/3125   Loss: 0.698341 mae: 0.666977 (2229.565919988093 steps/sec)\n",
      "Step #15398\tEpoch   4 Batch 2897/3125   Loss: 0.735919 mae: 0.681199 (2088.2144421874377 steps/sec)\n",
      "Step #15399\tEpoch   4 Batch 2898/3125   Loss: 0.805842 mae: 0.709100 (2076.4292362224996 steps/sec)\n",
      "Step #15400\tEpoch   4 Batch 2899/3125   Loss: 0.830201 mae: 0.730730 (2287.718992036653 steps/sec)\n",
      "Step #15401\tEpoch   4 Batch 2900/3125   Loss: 0.730480 mae: 0.682642 (2092.8407480589985 steps/sec)\n",
      "Step #15402\tEpoch   4 Batch 2901/3125   Loss: 0.684854 mae: 0.662995 (2021.5072005552236 steps/sec)\n",
      "Step #15403\tEpoch   4 Batch 2902/3125   Loss: 0.746845 mae: 0.658718 (1925.8124649898527 steps/sec)\n",
      "Step #15404\tEpoch   4 Batch 2903/3125   Loss: 0.710441 mae: 0.665104 (2122.7953680459955 steps/sec)\n",
      "Step #15405\tEpoch   4 Batch 2904/3125   Loss: 0.738516 mae: 0.679001 (2207.1334603281516 steps/sec)\n",
      "Step #15406\tEpoch   4 Batch 2905/3125   Loss: 0.873316 mae: 0.730268 (2058.372266499156 steps/sec)\n",
      "Step #15407\tEpoch   4 Batch 2906/3125   Loss: 0.917836 mae: 0.743436 (2106.2510043387433 steps/sec)\n",
      "Step #15408\tEpoch   4 Batch 2907/3125   Loss: 0.643251 mae: 0.641737 (1979.8835002785042 steps/sec)\n",
      "Step #15409\tEpoch   4 Batch 2908/3125   Loss: 0.831079 mae: 0.719813 (2218.691944732443 steps/sec)\n",
      "Step #15410\tEpoch   4 Batch 2909/3125   Loss: 0.782138 mae: 0.711204 (2182.668970254574 steps/sec)\n",
      "Step #15411\tEpoch   4 Batch 2910/3125   Loss: 0.846456 mae: 0.727476 (1849.6666078673488 steps/sec)\n",
      "Step #15412\tEpoch   4 Batch 2911/3125   Loss: 0.826494 mae: 0.734287 (1982.9349470499244 steps/sec)\n",
      "Step #15413\tEpoch   4 Batch 2912/3125   Loss: 0.699081 mae: 0.679196 (2126.476105494773 steps/sec)\n",
      "Step #15414\tEpoch   4 Batch 2913/3125   Loss: 0.692250 mae: 0.650802 (2110.957663116784 steps/sec)\n",
      "Step #15415\tEpoch   4 Batch 2914/3125   Loss: 0.744745 mae: 0.687178 (2028.0561277282968 steps/sec)\n",
      "Step #15416\tEpoch   4 Batch 2915/3125   Loss: 0.823479 mae: 0.720101 (2211.9989874272214 steps/sec)\n",
      "Step #15417\tEpoch   4 Batch 2916/3125   Loss: 1.039120 mae: 0.802139 (2163.305894246044 steps/sec)\n",
      "Step #15418\tEpoch   4 Batch 2917/3125   Loss: 0.875991 mae: 0.738503 (2231.2501329928714 steps/sec)\n",
      "Step #15419\tEpoch   4 Batch 2918/3125   Loss: 0.715417 mae: 0.677743 (2119.5557037890503 steps/sec)\n",
      "Step #15420\tEpoch   4 Batch 2919/3125   Loss: 0.729621 mae: 0.670542 (1947.7950737452168 steps/sec)\n",
      "Step #15421\tEpoch   4 Batch 2920/3125   Loss: 0.759377 mae: 0.699306 (1785.965390380161 steps/sec)\n",
      "Step #15422\tEpoch   4 Batch 2921/3125   Loss: 0.755761 mae: 0.698504 (2192.3893953332777 steps/sec)\n",
      "Step #15423\tEpoch   4 Batch 2922/3125   Loss: 0.957449 mae: 0.777423 (2009.478455008001 steps/sec)\n",
      "Step #15424\tEpoch   4 Batch 2923/3125   Loss: 0.701530 mae: 0.673916 (2232.865569302187 steps/sec)\n",
      "Step #15425\tEpoch   4 Batch 2924/3125   Loss: 0.819553 mae: 0.723969 (2125.8509883426254 steps/sec)\n",
      "Step #15426\tEpoch   4 Batch 2925/3125   Loss: 0.818402 mae: 0.722588 (2334.0589872008904 steps/sec)\n",
      "Step #15427\tEpoch   4 Batch 2926/3125   Loss: 0.685159 mae: 0.654195 (2300.7701590784422 steps/sec)\n",
      "Step #15428\tEpoch   4 Batch 2927/3125   Loss: 0.707041 mae: 0.663144 (2251.008425911018 steps/sec)\n",
      "Step #15429\tEpoch   4 Batch 2928/3125   Loss: 0.829820 mae: 0.727294 (1908.8444909661857 steps/sec)\n",
      "Step #15430\tEpoch   4 Batch 2929/3125   Loss: 0.793231 mae: 0.710642 (2103.2303356700863 steps/sec)\n",
      "Step #15431\tEpoch   4 Batch 2930/3125   Loss: 0.635092 mae: 0.623323 (1918.8698062969504 steps/sec)\n",
      "Step #15432\tEpoch   4 Batch 2931/3125   Loss: 0.724805 mae: 0.668408 (2033.7597098441577 steps/sec)\n",
      "Step #15433\tEpoch   4 Batch 2932/3125   Loss: 0.647087 mae: 0.655081 (2080.735000843346 steps/sec)\n",
      "Step #15434\tEpoch   4 Batch 2933/3125   Loss: 0.778061 mae: 0.659460 (1952.2192433720584 steps/sec)\n",
      "Step #15435\tEpoch   4 Batch 2934/3125   Loss: 0.630681 mae: 0.639015 (2224.1274352801433 steps/sec)\n",
      "Step #15436\tEpoch   4 Batch 2935/3125   Loss: 0.813152 mae: 0.735234 (2201.387707972498 steps/sec)\n",
      "Step #15437\tEpoch   4 Batch 2936/3125   Loss: 0.712565 mae: 0.660827 (1922.0529740628724 steps/sec)\n",
      "Step #15438\tEpoch   4 Batch 2937/3125   Loss: 0.814322 mae: 0.717141 (1690.6653337955386 steps/sec)\n",
      "Step #15439\tEpoch   4 Batch 2938/3125   Loss: 0.777125 mae: 0.718915 (1820.0968565030985 steps/sec)\n",
      "Step #15440\tEpoch   4 Batch 2939/3125   Loss: 0.774777 mae: 0.682450 (2118.185582837577 steps/sec)\n",
      "Step #15441\tEpoch   4 Batch 2940/3125   Loss: 0.790961 mae: 0.709654 (2094.4501592945103 steps/sec)\n",
      "Step #15442\tEpoch   4 Batch 2941/3125   Loss: 0.752315 mae: 0.697770 (2022.9697010620544 steps/sec)\n",
      "Step #15443\tEpoch   4 Batch 2942/3125   Loss: 0.895044 mae: 0.743212 (2131.1004298474704 steps/sec)\n",
      "Step #15444\tEpoch   4 Batch 2943/3125   Loss: 1.008489 mae: 0.810892 (2123.8057623170794 steps/sec)\n",
      "Step #15445\tEpoch   4 Batch 2944/3125   Loss: 0.812168 mae: 0.709562 (1935.5884334591635 steps/sec)\n",
      "Step #15446\tEpoch   4 Batch 2945/3125   Loss: 0.768742 mae: 0.708861 (1897.1025374281967 steps/sec)\n",
      "Step #15447\tEpoch   4 Batch 2946/3125   Loss: 0.708072 mae: 0.659137 (1976.2080663399925 steps/sec)\n",
      "Step #15448\tEpoch   4 Batch 2947/3125   Loss: 0.884776 mae: 0.742699 (2020.202487260257 steps/sec)\n",
      "Step #15449\tEpoch   4 Batch 2948/3125   Loss: 0.752475 mae: 0.690108 (2162.547434416763 steps/sec)\n",
      "Step #15450\tEpoch   4 Batch 2949/3125   Loss: 0.835311 mae: 0.723349 (2206.8315268862466 steps/sec)\n",
      "Step #15451\tEpoch   4 Batch 2950/3125   Loss: 0.897424 mae: 0.750357 (2138.4017701461185 steps/sec)\n",
      "Step #15452\tEpoch   4 Batch 2951/3125   Loss: 0.798516 mae: 0.710904 (2050.2825411102203 steps/sec)\n",
      "Step #15453\tEpoch   4 Batch 2952/3125   Loss: 0.822342 mae: 0.719566 (2005.4622652335233 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15454\tEpoch   4 Batch 2953/3125   Loss: 0.833755 mae: 0.692687 (1770.2863317125878 steps/sec)\n",
      "Step #15455\tEpoch   4 Batch 2954/3125   Loss: 0.734405 mae: 0.651345 (2041.3218474716505 steps/sec)\n",
      "Step #15456\tEpoch   4 Batch 2955/3125   Loss: 0.762406 mae: 0.678783 (2241.1456051295754 steps/sec)\n",
      "Step #15457\tEpoch   4 Batch 2956/3125   Loss: 0.833687 mae: 0.715044 (2081.1479720945927 steps/sec)\n",
      "Step #15458\tEpoch   4 Batch 2957/3125   Loss: 0.845794 mae: 0.713117 (1937.0186668144495 steps/sec)\n",
      "Step #15459\tEpoch   4 Batch 2958/3125   Loss: 0.926526 mae: 0.775913 (1979.8087362051224 steps/sec)\n",
      "Step #15460\tEpoch   4 Batch 2959/3125   Loss: 0.710739 mae: 0.678086 (2089.2546175456773 steps/sec)\n",
      "Step #15461\tEpoch   4 Batch 2960/3125   Loss: 0.765082 mae: 0.696428 (2137.0942923235266 steps/sec)\n",
      "Step #15462\tEpoch   4 Batch 2961/3125   Loss: 0.761065 mae: 0.704387 (2284.8775385688136 steps/sec)\n",
      "Step #15463\tEpoch   4 Batch 2962/3125   Loss: 0.807120 mae: 0.703927 (1843.731153017715 steps/sec)\n",
      "Step #15464\tEpoch   4 Batch 2963/3125   Loss: 0.766627 mae: 0.701890 (1905.2547423504616 steps/sec)\n",
      "Step #15465\tEpoch   4 Batch 2964/3125   Loss: 0.929407 mae: 0.752372 (2105.447463004237 steps/sec)\n",
      "Step #15466\tEpoch   4 Batch 2965/3125   Loss: 0.841755 mae: 0.744721 (2219.8427063817176 steps/sec)\n",
      "Step #15467\tEpoch   4 Batch 2966/3125   Loss: 0.666583 mae: 0.658869 (2250.211377926565 steps/sec)\n",
      "Step #15468\tEpoch   4 Batch 2967/3125   Loss: 0.890587 mae: 0.763840 (2196.246648793566 steps/sec)\n",
      "Step #15469\tEpoch   4 Batch 2968/3125   Loss: 0.698808 mae: 0.673280 (2281.223961449348 steps/sec)\n",
      "Step #15470\tEpoch   4 Batch 2969/3125   Loss: 0.668540 mae: 0.638542 (2119.5557037890503 steps/sec)\n",
      "Step #15471\tEpoch   4 Batch 2970/3125   Loss: 0.835901 mae: 0.719604 (2198.9640348117855 steps/sec)\n",
      "Step #15472\tEpoch   4 Batch 2971/3125   Loss: 0.692877 mae: 0.664698 (2078.5901896068112 steps/sec)\n",
      "Step #15473\tEpoch   4 Batch 2972/3125   Loss: 0.701122 mae: 0.670526 (1825.4835396319702 steps/sec)\n",
      "Step #15474\tEpoch   4 Batch 2973/3125   Loss: 0.830057 mae: 0.707302 (1950.312938835105 steps/sec)\n",
      "Step #15475\tEpoch   4 Batch 2974/3125   Loss: 0.693982 mae: 0.664558 (2029.1353819955104 steps/sec)\n",
      "Step #15476\tEpoch   4 Batch 2975/3125   Loss: 0.695551 mae: 0.665449 (1872.7413982479484 steps/sec)\n",
      "Step #15477\tEpoch   4 Batch 2976/3125   Loss: 0.878285 mae: 0.744743 (1883.7427804076206 steps/sec)\n",
      "Step #15478\tEpoch   4 Batch 2977/3125   Loss: 0.851286 mae: 0.731119 (1939.6522382537921 steps/sec)\n",
      "Step #15479\tEpoch   4 Batch 2978/3125   Loss: 0.700270 mae: 0.643789 (2097.4875980156826 steps/sec)\n",
      "Step #15480\tEpoch   4 Batch 2979/3125   Loss: 0.854594 mae: 0.726282 (1787.685724271381 steps/sec)\n",
      "Step #15481\tEpoch   4 Batch 2980/3125   Loss: 0.748878 mae: 0.681538 (1859.968781041578 steps/sec)\n",
      "Step #15482\tEpoch   4 Batch 2981/3125   Loss: 0.652481 mae: 0.656124 (1981.2302198373186 steps/sec)\n",
      "Step #15483\tEpoch   4 Batch 2982/3125   Loss: 0.785105 mae: 0.706581 (2184.5560891259283 steps/sec)\n",
      "Step #15484\tEpoch   4 Batch 2983/3125   Loss: 0.738037 mae: 0.678907 (2104.89802473101 steps/sec)\n",
      "Step #15485\tEpoch   4 Batch 2984/3125   Loss: 0.727276 mae: 0.675694 (2076.244220697575 steps/sec)\n",
      "Step #15486\tEpoch   4 Batch 2985/3125   Loss: 0.765221 mae: 0.688739 (2065.528754764555 steps/sec)\n",
      "Step #15487\tEpoch   4 Batch 2986/3125   Loss: 0.784615 mae: 0.702669 (2070.300206323978 steps/sec)\n",
      "Step #15488\tEpoch   4 Batch 2987/3125   Loss: 0.727873 mae: 0.665620 (2033.523063347846 steps/sec)\n",
      "Step #15489\tEpoch   4 Batch 2988/3125   Loss: 0.748287 mae: 0.686039 (1975.4262353761233 steps/sec)\n",
      "Step #15490\tEpoch   4 Batch 2989/3125   Loss: 0.798855 mae: 0.694167 (2102.682053801498 steps/sec)\n",
      "Step #15491\tEpoch   4 Batch 2990/3125   Loss: 0.720277 mae: 0.668718 (2014.5939403254627 steps/sec)\n",
      "Step #15492\tEpoch   4 Batch 2991/3125   Loss: 0.859460 mae: 0.738633 (2088.713597067846 steps/sec)\n",
      "Step #15493\tEpoch   4 Batch 2992/3125   Loss: 0.795674 mae: 0.706703 (2000.5647346128897 steps/sec)\n",
      "Step #15494\tEpoch   4 Batch 2993/3125   Loss: 0.673209 mae: 0.642782 (2052.0279063395924 steps/sec)\n",
      "Step #15495\tEpoch   4 Batch 2994/3125   Loss: 0.766196 mae: 0.663239 (2046.1616516411036 steps/sec)\n",
      "Step #15496\tEpoch   4 Batch 2995/3125   Loss: 0.725247 mae: 0.688001 (1715.3354763248512 steps/sec)\n",
      "Step #15497\tEpoch   4 Batch 2996/3125   Loss: 0.728316 mae: 0.661812 (1832.1658527209665 steps/sec)\n",
      "Step #15498\tEpoch   4 Batch 2997/3125   Loss: 0.927831 mae: 0.746423 (2000.011444157281 steps/sec)\n",
      "Step #15499\tEpoch   4 Batch 2998/3125   Loss: 0.801987 mae: 0.711290 (2239.853036986404 steps/sec)\n",
      "Step #15500\tEpoch   4 Batch 2999/3125   Loss: 0.740929 mae: 0.693224 (1974.7937775434102 steps/sec)\n",
      "Step #15501\tEpoch   4 Batch 3000/3125   Loss: 0.863380 mae: 0.706023 (2188.842617236017 steps/sec)\n",
      "Step #15502\tEpoch   4 Batch 3001/3125   Loss: 0.774346 mae: 0.706967 (2080.487296753008 steps/sec)\n",
      "Step #15503\tEpoch   4 Batch 3002/3125   Loss: 0.682982 mae: 0.650083 (2098.201100550275 steps/sec)\n",
      "Step #15504\tEpoch   4 Batch 3003/3125   Loss: 0.737896 mae: 0.658277 (2217.4954796823617 steps/sec)\n",
      "Step #15505\tEpoch   4 Batch 3004/3125   Loss: 0.783453 mae: 0.693851 (1835.260348297891 steps/sec)\n",
      "Step #15506\tEpoch   4 Batch 3005/3125   Loss: 0.702496 mae: 0.662685 (1997.6871564789149 steps/sec)\n",
      "Step #15507\tEpoch   4 Batch 3006/3125   Loss: 0.648005 mae: 0.640079 (2001.2711015258944 steps/sec)\n",
      "Step #15508\tEpoch   4 Batch 3007/3125   Loss: 0.775715 mae: 0.678773 (2064.9186203365466 steps/sec)\n",
      "Step #15509\tEpoch   4 Batch 3008/3125   Loss: 0.805072 mae: 0.686240 (2057.5240861016814 steps/sec)\n",
      "Step #15510\tEpoch   4 Batch 3009/3125   Loss: 0.798658 mae: 0.690547 (2068.686868686869 steps/sec)\n",
      "Step #15511\tEpoch   4 Batch 3010/3125   Loss: 0.791014 mae: 0.683824 (2039.2376507195643 steps/sec)\n",
      "Step #15512\tEpoch   4 Batch 3011/3125   Loss: 0.794546 mae: 0.678545 (2158.318753473437 steps/sec)\n",
      "Step #15513\tEpoch   4 Batch 3012/3125   Loss: 0.780705 mae: 0.710643 (2068.421623647536 steps/sec)\n",
      "Step #15514\tEpoch   4 Batch 3013/3125   Loss: 0.866099 mae: 0.722031 (1851.7244424038004 steps/sec)\n",
      "Step #15515\tEpoch   4 Batch 3014/3125   Loss: 0.841830 mae: 0.703950 (2103.5889822857944 steps/sec)\n",
      "Step #15516\tEpoch   4 Batch 3015/3125   Loss: 0.841001 mae: 0.715132 (1900.9200257425923 steps/sec)\n",
      "Step #15517\tEpoch   4 Batch 3016/3125   Loss: 0.747828 mae: 0.693706 (2077.293078172669 steps/sec)\n",
      "Step #15518\tEpoch   4 Batch 3017/3125   Loss: 0.816604 mae: 0.703299 (2182.441826582858 steps/sec)\n",
      "Step #15519\tEpoch   4 Batch 3018/3125   Loss: 0.793577 mae: 0.701926 (2181.783377201652 steps/sec)\n",
      "Step #15520\tEpoch   4 Batch 3019/3125   Loss: 0.840283 mae: 0.705783 (2228.2629946024053 steps/sec)\n",
      "Step #15521\tEpoch   4 Batch 3020/3125   Loss: 0.880875 mae: 0.723901 (1930.1372258474225 steps/sec)\n",
      "Step #15522\tEpoch   4 Batch 3021/3125   Loss: 0.723924 mae: 0.678034 (1996.0899650685778 steps/sec)\n",
      "Step #15523\tEpoch   4 Batch 3022/3125   Loss: 0.616718 mae: 0.619300 (1750.1936173053812 steps/sec)\n",
      "Step #15524\tEpoch   4 Batch 3023/3125   Loss: 0.825991 mae: 0.685603 (1994.969654306425 steps/sec)\n",
      "Step #15525\tEpoch   4 Batch 3024/3125   Loss: 0.664159 mae: 0.665741 (1957.1748544124234 steps/sec)\n",
      "Step #15526\tEpoch   4 Batch 3025/3125   Loss: 0.759834 mae: 0.699238 (2237.8453362927235 steps/sec)\n",
      "Step #15527\tEpoch   4 Batch 3026/3125   Loss: 0.676043 mae: 0.641195 (2288.9674743505784 steps/sec)\n",
      "Step #15528\tEpoch   4 Batch 3027/3125   Loss: 0.806400 mae: 0.723029 (2077.107908681226 steps/sec)\n",
      "Step #15529\tEpoch   4 Batch 3028/3125   Loss: 0.735104 mae: 0.673217 (2039.2376507195643 steps/sec)\n",
      "Step #15530\tEpoch   4 Batch 3029/3125   Loss: 0.751053 mae: 0.689070 (2160.4977953599537 steps/sec)\n",
      "Step #15531\tEpoch   4 Batch 3030/3125   Loss: 0.793255 mae: 0.685369 (1905.5663583331818 steps/sec)\n",
      "Step #15532\tEpoch   4 Batch 3031/3125   Loss: 0.745895 mae: 0.684604 (1670.6247859094567 steps/sec)\n",
      "Step #15533\tEpoch   4 Batch 3032/3125   Loss: 0.761105 mae: 0.673267 (1847.743572573966 steps/sec)\n",
      "Step #15534\tEpoch   4 Batch 3033/3125   Loss: 0.767952 mae: 0.698746 (1941.4658532295243 steps/sec)\n",
      "Step #15535\tEpoch   4 Batch 3034/3125   Loss: 0.719731 mae: 0.681568 (2060.819747845484 steps/sec)\n",
      "Step #15536\tEpoch   4 Batch 3035/3125   Loss: 0.840777 mae: 0.733586 (1732.324467206344 steps/sec)\n",
      "Step #15537\tEpoch   4 Batch 3036/3125   Loss: 0.543803 mae: 0.580123 (2128.116089096352 steps/sec)\n",
      "Step #15538\tEpoch   4 Batch 3037/3125   Loss: 0.814756 mae: 0.718144 (2185.603368315738 steps/sec)\n",
      "Step #15539\tEpoch   4 Batch 3038/3125   Loss: 0.767602 mae: 0.717636 (1823.0238966593356 steps/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #15540\tEpoch   4 Batch 3039/3125   Loss: 0.713961 mae: 0.674304 (1945.6807533515796 steps/sec)\n",
      "Step #15541\tEpoch   4 Batch 3040/3125   Loss: 0.854882 mae: 0.723105 (2075.2991004720297 steps/sec)\n",
      "Step #15542\tEpoch   4 Batch 3041/3125   Loss: 0.785193 mae: 0.705436 (2297.2669215349056 steps/sec)\n",
      "Step #15543\tEpoch   4 Batch 3042/3125   Loss: 0.781099 mae: 0.700452 (2090.54587503489 steps/sec)\n",
      "Step #15544\tEpoch   4 Batch 3043/3125   Loss: 0.878821 mae: 0.742302 (2278.522381573229 steps/sec)\n",
      "Step #15545\tEpoch   4 Batch 3044/3125   Loss: 0.825921 mae: 0.711722 (2429.536950150025 steps/sec)\n",
      "Step #15546\tEpoch   4 Batch 3045/3125   Loss: 0.824713 mae: 0.697397 (2090.400007974243 steps/sec)\n",
      "Step #15547\tEpoch   4 Batch 3046/3125   Loss: 0.608544 mae: 0.618073 (2095.1197338581574 steps/sec)\n",
      "Step #15548\tEpoch   4 Batch 3047/3125   Loss: 0.647100 mae: 0.637141 (2186.172963055625 steps/sec)\n",
      "Step #15549\tEpoch   4 Batch 3048/3125   Loss: 0.708769 mae: 0.668298 (1808.2481871405537 steps/sec)\n",
      "Step #15550\tEpoch   4 Batch 3049/3125   Loss: 0.752986 mae: 0.708860 (1943.8592588473018 steps/sec)\n",
      "Step #15551\tEpoch   4 Batch 3050/3125   Loss: 0.770370 mae: 0.699208 (2333.0722677109293 steps/sec)\n",
      "Step #15552\tEpoch   4 Batch 3051/3125   Loss: 0.786936 mae: 0.684232 (2080.9001696748396 steps/sec)\n",
      "Step #15553\tEpoch   4 Batch 3052/3125   Loss: 0.706969 mae: 0.666160 (2125.6570611905654 steps/sec)\n",
      "Step #15554\tEpoch   4 Batch 3053/3125   Loss: 0.802848 mae: 0.695213 (2203.3304966327314 steps/sec)\n",
      "Step #15555\tEpoch   4 Batch 3054/3125   Loss: 0.737874 mae: 0.669479 (1940.9088385006942 steps/sec)\n",
      "Step #15556\tEpoch   4 Batch 3055/3125   Loss: 0.838953 mae: 0.721671 (2134.4624028009607 steps/sec)\n",
      "Step #15557\tEpoch   4 Batch 3056/3125   Loss: 0.829951 mae: 0.719940 (2058.6349402675933 steps/sec)\n",
      "Step #15558\tEpoch   4 Batch 3057/3125   Loss: 0.949585 mae: 0.753028 (2029.881719805641 steps/sec)\n",
      "Step #15559\tEpoch   4 Batch 3058/3125   Loss: 0.872137 mae: 0.747451 (2008.4969448541383 steps/sec)\n",
      "Step #15560\tEpoch   4 Batch 3059/3125   Loss: 0.687868 mae: 0.650142 (2187.0393158827824 steps/sec)\n",
      "Step #15561\tEpoch   4 Batch 3060/3125   Loss: 0.665530 mae: 0.659815 (2049.0205082609505 steps/sec)\n",
      "Step #15562\tEpoch   4 Batch 3061/3125   Loss: 0.854557 mae: 0.718069 (1960.1566516183907 steps/sec)\n",
      "Step #15563\tEpoch   4 Batch 3062/3125   Loss: 0.711789 mae: 0.653440 (2023.535768733476 steps/sec)\n",
      "Step #15564\tEpoch   4 Batch 3063/3125   Loss: 0.831290 mae: 0.747089 (2019.9689850800899 steps/sec)\n",
      "Step #15565\tEpoch   4 Batch 3064/3125   Loss: 0.804254 mae: 0.695313 (2014.9229926691712 steps/sec)\n",
      "Step #15566\tEpoch   4 Batch 3065/3125   Loss: 0.726301 mae: 0.668472 (1984.9807384691105 steps/sec)\n",
      "Step #15567\tEpoch   4 Batch 3066/3125   Loss: 0.781888 mae: 0.681739 (2105.806866220165 steps/sec)\n",
      "Step #15568\tEpoch   4 Batch 3067/3125   Loss: 0.692087 mae: 0.652673 (2037.256654361764 steps/sec)\n",
      "Step #15569\tEpoch   4 Batch 3068/3125   Loss: 0.904918 mae: 0.745457 (2220.759472223987 steps/sec)\n",
      "Step #15570\tEpoch   4 Batch 3069/3125   Loss: 0.650492 mae: 0.646486 (2120.327176034052 steps/sec)\n",
      "Step #15571\tEpoch   4 Batch 3070/3125   Loss: 0.758649 mae: 0.682602 (2070.238894373149 steps/sec)\n",
      "Step #15572\tEpoch   4 Batch 3071/3125   Loss: 0.818309 mae: 0.728973 (2051.3655215588074 steps/sec)\n",
      "Step #15573\tEpoch   4 Batch 3072/3125   Loss: 0.817659 mae: 0.703519 (2240.0205080002565 steps/sec)\n",
      "Step #15574\tEpoch   4 Batch 3073/3125   Loss: 0.980381 mae: 0.782160 (2174.093156819855 steps/sec)\n",
      "Step #15575\tEpoch   4 Batch 3074/3125   Loss: 0.749848 mae: 0.690503 (1847.873821482069 steps/sec)\n",
      "Step #15576\tEpoch   4 Batch 3075/3125   Loss: 0.750397 mae: 0.692818 (2000.5647346128897 steps/sec)\n",
      "Step #15577\tEpoch   4 Batch 3076/3125   Loss: 0.877312 mae: 0.745055 (2076.244220697575 steps/sec)\n",
      "Step #15578\tEpoch   4 Batch 3077/3125   Loss: 0.662899 mae: 0.643312 (2106.8646460181435 steps/sec)\n",
      "Step #15579\tEpoch   4 Batch 3078/3125   Loss: 0.619288 mae: 0.622387 (2079.105365428084 steps/sec)\n",
      "Step #15580\tEpoch   4 Batch 3079/3125   Loss: 0.752702 mae: 0.678440 (1990.444282039844 steps/sec)\n",
      "Step #15581\tEpoch   4 Batch 3080/3125   Loss: 0.894724 mae: 0.753503 (1798.3861147556447 steps/sec)\n",
      "Step #15582\tEpoch   4 Batch 3081/3125   Loss: 0.696200 mae: 0.634099 (2236.7474055823973 steps/sec)\n",
      "Step #15583\tEpoch   4 Batch 3082/3125   Loss: 0.796120 mae: 0.700995 (2164.6009661037942 steps/sec)\n",
      "Step #15584\tEpoch   4 Batch 3083/3125   Loss: 0.728252 mae: 0.694418 (2104.5811715355203 steps/sec)\n",
      "Step #15585\tEpoch   4 Batch 3084/3125   Loss: 0.773635 mae: 0.705496 (2350.14512242954 steps/sec)\n",
      "Step #15586\tEpoch   4 Batch 3085/3125   Loss: 0.752666 mae: 0.689038 (2181.1253250130007 steps/sec)\n",
      "Step #15587\tEpoch   4 Batch 3086/3125   Loss: 0.762375 mae: 0.681763 (2220.6183820415076 steps/sec)\n",
      "Step #15588\tEpoch   4 Batch 3087/3125   Loss: 0.890017 mae: 0.722923 (2214.8490801174407 steps/sec)\n",
      "Step #15589\tEpoch   4 Batch 3088/3125   Loss: 0.633438 mae: 0.637852 (2318.7041848637296 steps/sec)\n",
      "Step #15590\tEpoch   4 Batch 3089/3125   Loss: 0.800418 mae: 0.711467 (2036.0896707735026 steps/sec)\n",
      "Step #15591\tEpoch   4 Batch 3090/3125   Loss: 0.607773 mae: 0.632560 (2047.0004880429478 steps/sec)\n",
      "Step #15592\tEpoch   4 Batch 3091/3125   Loss: 0.839482 mae: 0.740370 (2005.615699475919 steps/sec)\n",
      "Step #15593\tEpoch   4 Batch 3092/3125   Loss: 0.621655 mae: 0.631683 (2004.4271978284557 steps/sec)\n",
      "Step #15594\tEpoch   4 Batch 3093/3125   Loss: 0.788561 mae: 0.706293 (2173.5298385257965 steps/sec)\n",
      "Step #15595\tEpoch   4 Batch 3094/3125   Loss: 0.820686 mae: 0.736277 (1871.4044778383588 steps/sec)\n",
      "Step #15596\tEpoch   4 Batch 3095/3125   Loss: 0.724590 mae: 0.662046 (2072.0184166065624 steps/sec)\n",
      "Step #15597\tEpoch   4 Batch 3096/3125   Loss: 0.730294 mae: 0.678591 (2301.502397910471 steps/sec)\n",
      "Step #15598\tEpoch   4 Batch 3097/3125   Loss: 0.687823 mae: 0.636992 (2311.5226065295506 steps/sec)\n",
      "Step #15599\tEpoch   4 Batch 3098/3125   Loss: 0.890754 mae: 0.725616 (2258.645126548196 steps/sec)\n",
      "Step #15600\tEpoch   4 Batch 3099/3125   Loss: 0.837616 mae: 0.740225 (2194.4081700987776 steps/sec)\n",
      "Step #15601\tEpoch   4 Batch 3100/3125   Loss: 0.946374 mae: 0.757990 (2096.8794056772617 steps/sec)\n",
      "Step #15602\tEpoch   4 Batch 3101/3125   Loss: 0.814334 mae: 0.730875 (2121.957685341644 steps/sec)\n",
      "Step #15603\tEpoch   4 Batch 3102/3125   Loss: 0.778634 mae: 0.676015 (2273.754513026791 steps/sec)\n",
      "Step #15604\tEpoch   4 Batch 3103/3125   Loss: 0.849628 mae: 0.719311 (2033.523063347846 steps/sec)\n",
      "Step #15605\tEpoch   4 Batch 3104/3125   Loss: 0.745401 mae: 0.655558 (2047.4201642113073 steps/sec)\n",
      "Step #15606\tEpoch   4 Batch 3105/3125   Loss: 0.792765 mae: 0.735181 (2278.324352511733 steps/sec)\n",
      "Step #15607\tEpoch   4 Batch 3106/3125   Loss: 0.792532 mae: 0.714965 (2119.1915925626517 steps/sec)\n",
      "Step #15608\tEpoch   4 Batch 3107/3125   Loss: 0.646717 mae: 0.645109 (2314.379676429691 steps/sec)\n",
      "Step #15609\tEpoch   4 Batch 3108/3125   Loss: 0.761256 mae: 0.659491 (2236.127312470011 steps/sec)\n",
      "Step #15610\tEpoch   4 Batch 3109/3125   Loss: 0.659645 mae: 0.632722 (1946.4211464211464 steps/sec)\n",
      "Step #15611\tEpoch   4 Batch 3110/3125   Loss: 0.674953 mae: 0.649892 (1844.1364755539923 steps/sec)\n",
      "Step #15612\tEpoch   4 Batch 3111/3125   Loss: 0.869008 mae: 0.737537 (1964.9501536616447 steps/sec)\n",
      "Step #15613\tEpoch   4 Batch 3112/3125   Loss: 0.807939 mae: 0.708287 (2100.050069095352 steps/sec)\n",
      "Step #15614\tEpoch   4 Batch 3113/3125   Loss: 0.786817 mae: 0.692424 (2237.0337184123227 steps/sec)\n",
      "Step #15615\tEpoch   4 Batch 3114/3125   Loss: 0.821719 mae: 0.707072 (2132.183779497138 steps/sec)\n",
      "Step #15616\tEpoch   4 Batch 3115/3125   Loss: 0.723050 mae: 0.655669 (2321.142224681793 steps/sec)\n",
      "Step #15617\tEpoch   4 Batch 3116/3125   Loss: 0.706840 mae: 0.671664 (2106.2510043387433 steps/sec)\n",
      "Step #15618\tEpoch   4 Batch 3117/3125   Loss: 0.717270 mae: 0.676346 (2107.5633630132857 steps/sec)\n",
      "Step #15619\tEpoch   4 Batch 3118/3125   Loss: 0.798627 mae: 0.696848 (1978.1653539593453 steps/sec)\n",
      "Step #15620\tEpoch   4 Batch 3119/3125   Loss: 0.842988 mae: 0.746283 (2088.9008416753823 steps/sec)\n",
      "Step #15621\tEpoch   4 Batch 3120/3125   Loss: 0.719302 mae: 0.667040 (2146.104646998025 steps/sec)\n",
      "Step #15622\tEpoch   4 Batch 3121/3125   Loss: 0.792973 mae: 0.707415 (2291.4685314685316 steps/sec)\n",
      "Step #15623\tEpoch   4 Batch 3122/3125   Loss: 0.918399 mae: 0.761384 (2353.2569543409227 steps/sec)\n",
      "Step #15624\tEpoch   4 Batch 3123/3125   Loss: 0.772193 mae: 0.704901 (2143.8887753015742 steps/sec)\n",
      "Step #15625\tEpoch   4 Batch 3124/3125   Loss: 0.815674 mae: 0.707708 (2097.403688442613 steps/sec)\n",
      "\n",
      "Train time for epoch #5 (15625 total steps): 79.85171484947205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test set loss: 0.801197 mae: 0.706091\n",
      "best loss = 0.8011972308158875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0416 20:19:29.823322 140735557628800 deprecation.py:506] From /Users/luoyonggui/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "mv_net=mv_network()\n",
    "mv_net.training(features, targets_values, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mv_net.losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 378
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mv_net.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "199px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
