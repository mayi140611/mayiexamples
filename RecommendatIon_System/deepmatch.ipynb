{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/126282487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 背景\n",
    "随着深度学习技术的普及，越来越多的深度学习算法被应用到了工业界中。笔者自去年毕业进入企业后，有幸参与了某新业务的推荐系统搭建以及用户体验和业务指标的优化当中，其中在召回部分也进行过一些基于向量召回的探索并取得了一些收益。\n",
    "\n",
    "之前在读研期间出于个人兴趣开发过一个基于深度学习的点击率预测算法库[DeepCTR](https://github.com/shenweichen/DeepCTR)，随着时间的迭代得到了一些同学的支持和认可，自己也亲身使用到了里面的算法应用到了自己的业务当中并取得了显著的收益。\n",
    "\n",
    "相比于排序中各种点击率预估模型，自己对于召回模块的了解还有很多欠缺，借着这个机会，抱着学习的心态，和几位热心的优秀小伙伴一起做了DeepMatch这个项目，希望它能够帮助到大家！\n",
    "\n",
    "https://github.com/shenweichen/DeepMatch\n",
    "\n",
    "\n",
    "下面简单介绍一下如何\n",
    "## 安装和使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepmatch==0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U deepmatch\n",
    "!pip freeze | grep deepmatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例1: YoutubeDNN-ml_1m\n",
    "https://github.com/shenweichen/DeepMatch/blob/master/examples/colab_MovieLen1M_YoutubeDNN.ipynb\n",
    "    \n",
    "下面已大家比较熟悉的YoutubeDNN为例子，给大家介绍如何使用deepmatch进行召回模型的训练，用户和物品向量的导出，以及使用faiss进行近似最近邻搜索。\n",
    "\n",
    "整段代码不到100行，可以是非常的方便进行学习和使用了～\n",
    "\n",
    "__运行环境 tf = 1.14.0, tf2会报错!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9UxNHuPMuW2"
   },
   "source": [
    "## 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "C_ZR6gzp1E2N",
    "outputId": "8401132a-5090-464a-879d-a27492416f4c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepctr.inputs import SparseFeat, VarLenSparseFeat\n",
    "from preprocess import gen_data_set, gen_model_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQq6O9XAMzPF"
   },
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "lcO29zFb21Od",
    "outputId": "97d9023e-cbd5-43dd-d2ef-769e3a34cf2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoyonggui/anaconda3/envs/tf14/lib/python3.7/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/luoyonggui/anaconda3/envs/tf14/lib/python3.7/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n",
      "/Users/luoyonggui/anaconda3/envs/tf14/lib/python3.7/site-packages/ipykernel_launcher.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/luoyonggui/PycharmProjects/mayiexamples/RecommendatIon_System/\"\n",
    "\n",
    "unames = ['user_id','gender','age','occupation','zip']\n",
    "user = pd.read_csv(data_path+'ml_1m/users.dat',sep='::',header=None,names=unames)\n",
    "rnames = ['user_id','movie_id','rating','timestamp']\n",
    "ratings = pd.read_csv(data_path+'ml_1m/ratings.dat',sep='::',header=None,names=rnames)\n",
    "mnames = ['movie_id','title','genres']\n",
    "movies = pd.read_csv(data_path+'ml_1m/movies.dat',sep='::',header=None,names=mnames)\n",
    "\n",
    "data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456790</th>\n",
       "      <td>6040</td>\n",
       "      <td>803</td>\n",
       "      <td>4</td>\n",
       "      <td>956703932</td>\n",
       "      <td>Godfather, The (1972)</td>\n",
       "      <td>Action|Crime|Drama</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456672</th>\n",
       "      <td>6040</td>\n",
       "      <td>580</td>\n",
       "      <td>5</td>\n",
       "      <td>956703954</td>\n",
       "      <td>Silence of the Lambs, The (1991)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456732</th>\n",
       "      <td>6040</td>\n",
       "      <td>2192</td>\n",
       "      <td>4</td>\n",
       "      <td>956703954</td>\n",
       "      <td>Babe: Pig in the City (1998)</td>\n",
       "      <td>Children's|Comedy</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456641</th>\n",
       "      <td>6040</td>\n",
       "      <td>1782</td>\n",
       "      <td>4</td>\n",
       "      <td>956703977</td>\n",
       "      <td>Rain Man (1988)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456842</th>\n",
       "      <td>6040</td>\n",
       "      <td>1840</td>\n",
       "      <td>5</td>\n",
       "      <td>956703977</td>\n",
       "      <td>Seven Samurai (The Magnificent Seven) (Shichin...</td>\n",
       "      <td>Action|Drama</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  movie_id  rating  timestamp  \\\n",
       "456790     6040       803       4  956703932   \n",
       "456672     6040       580       5  956703954   \n",
       "456732     6040      2192       4  956703954   \n",
       "456641     6040      1782       4  956703977   \n",
       "456842     6040      1840       5  956703977   \n",
       "\n",
       "                                                    title              genres  \\\n",
       "456790                              Godfather, The (1972)  Action|Crime|Drama   \n",
       "456672                   Silence of the Lambs, The (1991)      Drama|Thriller   \n",
       "456732                       Babe: Pig in the City (1998)   Children's|Comedy   \n",
       "456641                                    Rain Man (1988)               Drama   \n",
       "456842  Seven Samurai (The Magnificent Seven) (Shichin...        Action|Drama   \n",
       "\n",
       "        gender  age  occupation  zip  \n",
       "456790       2    3           7  467  \n",
       "456672       2    3           7  467  \n",
       "456732       2    3           7  467  \n",
       "456641       2    3           7  467  \n",
       "456842       2    3           7  467  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0yCWxQxM3se"
   },
   "source": [
    "## 构建特征列，训练模型，导出embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                    \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
    "SEQ_LEN = 50\n",
    "negsample = 0\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "\n",
    "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip']\n",
    "feature_max_idx = {}\n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1    [1105, 640, 854, 3178, 2163, 1108, 1196, 2600,...\n",
       "2    [1105, 2890, 2129, 1783, 1118, 1849, 1155, 126...\n",
       "Name: movie_id, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_list.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:14<00:00, 410.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = gen_data_set(data, negsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3101, [2780, 1164, 3399], 336, 1, 3, 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = [line[1] for line in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2780, 1164, 3399],\n",
       " [880,\n",
       "  1136,\n",
       "  2720,\n",
       "  2874,\n",
       "  1831,\n",
       "  909,\n",
       "  853,\n",
       "  2880,\n",
       "  1169,\n",
       "  1115,\n",
       "  1130,\n",
       "  3547,\n",
       "  3216,\n",
       "  844,\n",
       "  1123,\n",
       "  803,\n",
       "  862,\n",
       "  1161,\n",
       "  3204,\n",
       "  1784,\n",
       "  2447,\n",
       "  3131,\n",
       "  890,\n",
       "  1014,\n",
       "  203,\n",
       "  1779,\n",
       "  1211,\n",
       "  2427,\n",
       "  860]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2780, 1164, 3399,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0],\n",
       "       [ 880, 1136, 2720, 2874, 1831,  909,  853, 2880, 1169, 1115, 1130,\n",
       "        3547, 3216,  844, 1123,  803,  862, 1161, 3204, 1784, 2447, 3131,\n",
       "         890, 1014,  203, 1779, 1211, 2427,  860,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(train_seq[:2], maxlen=50, padding='post', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6040/6040 [00:13<00:00, 436.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': array([2215, 1447,   92, ..., 4680, 5915, 4344]),\n",
       " 'movie_id': array([1018,   15, 3476, ..., 2914, 2427, 2277]),\n",
       " 'hist_movie_id': array([[3044,  414,  703, ..., 3445,  610, 1512],\n",
       "        [2618, 1367,  703, ...,  529,  402,  181],\n",
       "        [2405, 3015, 2404, ..., 1282,  854, 1207],\n",
       "        ...,\n",
       "        [2243,  477,  519, ..., 1803, 2780, 3266],\n",
       "        [ 368, 1903, 1306, ..., 2167, 2502, 2111],\n",
       "        [1277, 2224, 3246, ..., 1098,  579, 3167]], dtype=int32),\n",
       " 'hist_len': array([128, 691, 279, ..., 271, 210, 333]),\n",
       " 'gender': array([1, 2, 1, ..., 2, 2, 2]),\n",
       " 'age': array([3, 2, 2, ..., 4, 2, 3]),\n",
       " 'occupation': array([1, 5, 5, ..., 1, 5, 2]),\n",
       " 'zip': array([3029, 2563, 1440, ...,  242, 1976, 1438])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseFeat(name='user_id', vocabulary_size=6041, embedding_dim=16, use_hash=False, dtype='int32', embedding_name='user_id', group_name='default_group')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseFeat('user_id', feature_max_idx['user_id'], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n",
    "embedding_dim = 32\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], 16),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], 16),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], 16),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], 16),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], 16),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Define Model and train\n",
    "\n",
    "K.set_learning_phase(True)\n",
    "\n",
    "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=200, user_dnn_hidden_units=(128,64, embedding_dim))\n",
    "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=True,p=1,k_max=2,num_sampled=5,user_dnn_hidden_units=(64,16),init_std=0.001)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 3.0541\n",
      "Epoch 2/20\n",
      "988129/988129 [==============================] - 22s 22us/sample - loss: 2.9761\n",
      "Epoch 3/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.9290\n",
      "Epoch 4/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.8716\n",
      "Epoch 5/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.8367\n",
      "Epoch 6/20\n",
      "988129/988129 [==============================] - 18s 18us/sample - loss: 2.8057\n",
      "Epoch 7/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.7758\n",
      "Epoch 8/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.7564\n",
      "Epoch 9/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.7387\n",
      "Epoch 10/20\n",
      "988129/988129 [==============================] - 19s 19us/sample - loss: 2.7178\n",
      "Epoch 11/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6993\n",
      "Epoch 12/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6867\n",
      "Epoch 13/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6711\n",
      "Epoch 14/20\n",
      "988129/988129 [==============================] - 19s 19us/sample - loss: 2.6559\n",
      "Epoch 15/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6480\n",
      "Epoch 16/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6314\n",
      "Epoch 17/20\n",
      "988129/988129 [==============================] - 21s 21us/sample - loss: 2.6283\n",
      "Epoch 18/20\n",
      "988129/988129 [==============================] - 19s 19us/sample - loss: 2.6173\n",
      "Epoch 19/20\n",
      "988129/988129 [==============================] - 18s 19us/sample - loss: 2.6117\n",
      "Epoch 20/20\n",
      "988129/988129 [==============================] - 19s 19us/sample - loss: 2.6023\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=512, epochs=24, verbose=1, validation_split=0.0, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完整后，由于在实际使用时，我们需要根据当前的用户特征实时产生用户侧向量，并对物品侧向量构建索引进行近似最近邻查找。这里由于是离线模拟，所以我们导出所有待测试用户的表示向量，和所有物品的表示向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BMOvk_de2ML3",
    "outputId": "1e43a5e7-f71c-45a4-bab4-e1d6b1a73669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 32)\n",
      "(3706, 32)\n"
     ]
    }
   ],
   "source": [
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values,}\n",
    "\n",
    "# 以下两行是deepmatch中的通用使用方法，分别获得用户向量模型和物品向量模型\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "\n",
    "# 输入对应的数据拿到对应的向量\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_G3KWslKmJo"
   },
   "source": [
    "## 使用faiss进行ANN查找并评估结果\n",
    "[可选的]如果有安装faiss库的同学，可以体验以下将上一步导出的物品向量构建索引，然后用用户向量来进行ANN查找并评估效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j2ZNYNBOOqrN",
    "outputId": "2938673c-ff81-49a2-86d8-4266d2060ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.6.3-cp37-cp37m-macosx_10_9_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy in /Users/luoyonggui/anaconda3/envs/tf14/lib/python3.7/site-packages (from faiss-cpu) (1.18.3)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.6.3\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_label = {line[0]:[line[2]] for line in test_set}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "6TY1l27iJU8U",
    "outputId": "5316c37f-fef1-44b3-8c31-6600d1e44da5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [00:01, 4215.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "recall 0.29238410596026493\n",
      "hit rate 0.29238410596026493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from deepmatch.utils import recall_N\n",
    "\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "# faiss.normalize_L2(user_embs)\n",
    "D, I = index.search(user_embs, 50)\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "    try:\n",
    "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
    "        filter_item = None\n",
    "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "        s.append(recall_score)\n",
    "        if test_true_label[uid] in pred:\n",
    "            hit += 1\n",
    "    except:\n",
    "        print(i)\n",
    "print(\"\")\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hit rate\", hit / len(test_user_model_input['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
